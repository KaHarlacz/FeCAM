=========================================
2024-03-04 20:18:19,792 [trainer.py] => config: ./exps/FeCAM_cifar100.json
2024-03-04 20:18:19,792 [trainer.py] => prefix: train
2024-03-04 20:18:19,792 [trainer.py] => dataset: cifar100
2024-03-04 20:18:19,792 [trainer.py] => memory_size: 0
2024-03-04 20:18:19,793 [trainer.py] => shuffle: True
2024-03-04 20:18:19,793 [trainer.py] => init_cls: 50
2024-03-04 20:18:19,793 [trainer.py] => increment: 10
2024-03-04 20:18:19,793 [trainer.py] => model_name: fecam
2024-03-04 20:18:19,793 [trainer.py] => convnet_type: resnet18
2024-03-04 20:18:19,793 [trainer.py] => device: [device(type='cuda', index=0)]
2024-03-04 20:18:19,793 [trainer.py] => seed: 1993
2024-03-04 20:18:19,793 [trainer.py] => init_epochs: 200
2024-03-04 20:18:19,793 [trainer.py] => init_lr: 0.1
2024-03-04 20:18:19,793 [trainer.py] => init_weight_decay: 0.0005
2024-03-04 20:18:19,793 [trainer.py] => batch_size: 128
2024-03-04 20:18:19,793 [trainer.py] => num_workers: 8
2024-03-04 20:18:19,793 [trainer.py] => T: 5
2024-03-04 20:18:19,793 [trainer.py] => beta: 0.5
2024-03-04 20:18:19,793 [trainer.py] => alpha1: 1
2024-03-04 20:18:19,793 [trainer.py] => alpha2: 1
2024-03-04 20:18:19,793 [trainer.py] => ncm: False
2024-03-04 20:18:19,793 [trainer.py] => tukey: False
2024-03-04 20:18:19,793 [trainer.py] => diagonal: False
2024-03-04 20:18:19,793 [trainer.py] => per_class: True
2024-03-04 20:18:19,793 [trainer.py] => full_cov: True
2024-03-04 20:18:19,793 [trainer.py] => shrink: True
2024-03-04 20:18:19,793 [trainer.py] => norm_cov: False
2024-03-04 20:18:19,793 [trainer.py] => vecnorm: False
2024-03-04 20:18:19,793 [trainer.py] => ae_type: wae
2024-03-04 20:18:19,793 [trainer.py] => epochs: 1000
2024-03-04 20:18:19,793 [trainer.py] => ae_latent_dim: 32
2024-03-04 20:18:19,793 [trainer.py] => wae_sigma: 1
2024-03-04 20:18:19,793 [trainer.py] => wae_C: 0.1
Files already downloaded and verified
Files already downloaded and verified
2024-03-04 20:18:21,427 [data_manager.py] => [68, 56, 78, 8, 23, 84, 90, 65, 74, 76, 40, 89, 3, 92, 55, 9, 26, 80, 43, 38, 58, 70, 77, 1, 85, 19, 17, 50, 28, 53, 13, 81, 45, 82, 6, 59, 83, 16, 15, 44, 91, 41, 72, 60, 79, 52, 20, 10, 31, 54, 37, 95, 14, 71, 96, 98, 97, 2, 64, 66, 42, 22, 35, 86, 24, 34, 87, 21, 99, 0, 88, 27, 18, 94, 11, 12, 47, 25, 30, 46, 62, 69, 36, 61, 7, 63, 75, 5, 32, 4, 51, 48, 73, 93, 39, 67, 29, 49, 57, 33]
2024-03-04 20:18:21,698 [fecam.py] => Learning on 0-50
/home/z1165703/miniconda3/lib/python3.11/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
/home/z1165703/miniconda3/lib/python3.11/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
Processing class: 0
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.0446283773
Epoch:   200  |  train loss: 0.0372764952
Epoch:   300  |  train loss: 0.0360585228
Epoch:   400  |  train loss: 0.0329515547
Epoch:   500  |  train loss: 0.0315792460
Epoch:   600  |  train loss: 0.0309822556
Epoch:   700  |  train loss: 0.0300168630
Epoch:   800  |  train loss: 0.0296679378
Epoch:   900  |  train loss: 0.0289001942
Epoch:  1000  |  train loss: 0.0282123525
Processing class: 1
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.0527679592
Epoch:   200  |  train loss: 0.0515871093
Epoch:   300  |  train loss: 0.0448640928
Epoch:   400  |  train loss: 0.0416744985
Epoch:   500  |  train loss: 0.0387779303
Epoch:   600  |  train loss: 0.0376002386
Epoch:   700  |  train loss: 0.0360980660
Epoch:   800  |  train loss: 0.0345737822
Epoch:   900  |  train loss: 0.0335768744
Epoch:  1000  |  train loss: 0.0325236499
Processing class: 2
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.0576437965
Epoch:   200  |  train loss: 0.0510338314
Epoch:   300  |  train loss: 0.0441581212
Epoch:   400  |  train loss: 0.0402280636
Epoch:   500  |  train loss: 0.0371402055
Epoch:   600  |  train loss: 0.0342428528
Epoch:   700  |  train loss: 0.0323133383
Epoch:   800  |  train loss: 0.0308812555
Epoch:   900  |  train loss: 0.0294420343
Epoch:  1000  |  train loss: 0.0281683713
Processing class: 3
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.0435508393
Epoch:   200  |  train loss: 0.0435727224
Epoch:   300  |  train loss: 0.0394335240
Epoch:   400  |  train loss: 0.0344039716
Epoch:   500  |  train loss: 0.0320699733
Epoch:   600  |  train loss: 0.0309160858
Epoch:   700  |  train loss: 0.0284917157
Epoch:   800  |  train loss: 0.0274109516
Epoch:   900  |  train loss: 0.0258207545
Epoch:  1000  |  train loss: 0.0244935729
Processing class: 4
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.0527669251
Epoch:   200  |  train loss: 0.0512267217
Epoch:   300  |  train loss: 0.0434622735
Epoch:   400  |  train loss: 0.0398837075
Epoch:   500  |  train loss: 0.0369995296
Epoch:   600  |  train loss: 0.0341860592
Epoch:   700  |  train loss: 0.0330247294
Epoch:   800  |  train loss: 0.0320506737
Epoch:   900  |  train loss: 0.0309134085
Epoch:  1000  |  train loss: 0.0297134317
Processing class: 5
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.0588493027
Epoch:   200  |  train loss: 0.0492293105
Epoch:   300  |  train loss: 0.0453458443
Epoch:   400  |  train loss: 0.0396013893
Epoch:   500  |  train loss: 0.0378822781
Epoch:   600  |  train loss: 0.0357221745
Epoch:   700  |  train loss: 0.0329474214
Epoch:   800  |  train loss: 0.0313364390
Epoch:   900  |  train loss: 0.0299646787
Epoch:  1000  |  train loss: 0.0289078850
Processing class: 6
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.0517995432
Epoch:   200  |  train loss: 0.0514811963
Epoch:   300  |  train loss: 0.0446115814
Epoch:   400  |  train loss: 0.0396686964
Epoch:   500  |  train loss: 0.0374038048
Epoch:   600  |  train loss: 0.0363272935
Epoch:   700  |  train loss: 0.0352136977
Epoch:   800  |  train loss: 0.0334875584
Epoch:   900  |  train loss: 0.0316158839
Epoch:  1000  |  train loss: 0.0306046225
Processing class: 7
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.0584421344
Epoch:   200  |  train loss: 0.0511725135
Epoch:   300  |  train loss: 0.0477079213
Epoch:   400  |  train loss: 0.0421633542
Epoch:   500  |  train loss: 0.0385523476
Epoch:   600  |  train loss: 0.0366312146
Epoch:   700  |  train loss: 0.0347123571
Epoch:   800  |  train loss: 0.0333418012
Epoch:   900  |  train loss: 0.0318137560
Epoch:  1000  |  train loss: 0.0306434330
Processing class: 8
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.0552305013
Epoch:   200  |  train loss: 0.0500443384
Epoch:   300  |  train loss: 0.0418657444
Epoch:   400  |  train loss: 0.0385384351
Epoch:   500  |  train loss: 0.0363618881
Epoch:   600  |  train loss: 0.0337084509
Epoch:   700  |  train loss: 0.0321055047
Epoch:   800  |  train loss: 0.0308495544
Epoch:   900  |  train loss: 0.0297047630
Epoch:  1000  |  train loss: 0.0284184642
Processing class: 9
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.0511314392
Epoch:   200  |  train loss: 0.0505409449
Epoch:   300  |  train loss: 0.0466633268
Epoch:   400  |  train loss: 0.0429776840
Epoch:   500  |  train loss: 0.0410010330
Epoch:   600  |  train loss: 0.0398661867
Epoch:   700  |  train loss: 0.0387942486
Epoch:   800  |  train loss: 0.0377336912
Epoch:   900  |  train loss: 0.0367490731
Epoch:  1000  |  train loss: 0.0356349252
Processing class: 10
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.0585900411
Epoch:   200  |  train loss: 0.0517633192
Epoch:   300  |  train loss: 0.0481641419
Epoch:   400  |  train loss: 0.0444804564
Epoch:   500  |  train loss: 0.0414141461
Epoch:   600  |  train loss: 0.0392271541
Epoch:   700  |  train loss: 0.0372549392
Epoch:   800  |  train loss: 0.0359138452
Epoch:   900  |  train loss: 0.0347358428
Epoch:  1000  |  train loss: 0.0338467613
Processing class: 11
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.0592555232
Epoch:   200  |  train loss: 0.0493611477
Epoch:   300  |  train loss: 0.0462265372
Epoch:   400  |  train loss: 0.0419211403
Epoch:   500  |  train loss: 0.0385237530
Epoch:   600  |  train loss: 0.0367463030
Epoch:   700  |  train loss: 0.0349643655
Epoch:   800  |  train loss: 0.0334317911
Epoch:   900  |  train loss: 0.0320932455
Epoch:  1000  |  train loss: 0.0308022674
Processing class: 12
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.0553328499
Epoch:   200  |  train loss: 0.0507250562
Epoch:   300  |  train loss: 0.0455471918
Epoch:   400  |  train loss: 0.0405572258
Epoch:   500  |  train loss: 0.0376513213
Epoch:   600  |  train loss: 0.0349315234
Epoch:   700  |  train loss: 0.0331262473
Epoch:   800  |  train loss: 0.0320741188
Epoch:   900  |  train loss: 0.0308644656
Epoch:  1000  |  train loss: 0.0298779894
Processing class: 13
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.0530368246
Epoch:   200  |  train loss: 0.0459016345
Epoch:   300  |  train loss: 0.0405951768
Epoch:   400  |  train loss: 0.0365942530
Epoch:   500  |  train loss: 0.0339736521
Epoch:   600  |  train loss: 0.0327578407
Epoch:   700  |  train loss: 0.0313013304
Epoch:   800  |  train loss: 0.0303239673
Epoch:   900  |  train loss: 0.0294433795
Epoch:  1000  |  train loss: 0.0284452025
Processing class: 14
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.0627904244
Epoch:   200  |  train loss: 0.0619066335
Epoch:   300  |  train loss: 0.0549279787
Epoch:   400  |  train loss: 0.0487987086
Epoch:   500  |  train loss: 0.0460235290
Epoch:   600  |  train loss: 0.0433413573
Epoch:   700  |  train loss: 0.0415604755
Epoch:   800  |  train loss: 0.0400635980
Epoch:   900  |  train loss: 0.0390033782
Epoch:  1000  |  train loss: 0.0380460322
Processing class: 15
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.0501128398
Epoch:   200  |  train loss: 0.0463048227
Epoch:   300  |  train loss: 0.0404776186
Epoch:   400  |  train loss: 0.0376315594
Epoch:   500  |  train loss: 0.0353056595
Epoch:   600  |  train loss: 0.0340521276
Epoch:   700  |  train loss: 0.0330884892
Epoch:   800  |  train loss: 0.0318329047
Epoch:   900  |  train loss: 0.0313286833
Epoch:  1000  |  train loss: 0.0303753395
Processing class: 16
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.0534291051
Epoch:   200  |  train loss: 0.0443177879
Epoch:   300  |  train loss: 0.0407106183
Epoch:   400  |  train loss: 0.0381659679
Epoch:   500  |  train loss: 0.0371888690
Epoch:   600  |  train loss: 0.0354980141
Epoch:   700  |  train loss: 0.0337767802
Epoch:   800  |  train loss: 0.0325507499
Epoch:   900  |  train loss: 0.0314795543
Epoch:  1000  |  train loss: 0.0310136613
Processing class: 17
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.0531946182
Epoch:   200  |  train loss: 0.0523163758
Epoch:   300  |  train loss: 0.0499628358
Epoch:   400  |  train loss: 0.0448257513
Epoch:   500  |  train loss: 0.0400172211
Epoch:   600  |  train loss: 0.0377986863
Epoch:   700  |  train loss: 0.0363331474
Epoch:   800  |  train loss: 0.0350147292
Epoch:   900  |  train loss: 0.0338153735
Epoch:  1000  |  train loss: 0.0328737013
Processing class: 18
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.0522696845
Epoch:   200  |  train loss: 0.0513311967
Epoch:   300  |  train loss: 0.0395470224
Epoch:   400  |  train loss: 0.0359782979
Epoch:   500  |  train loss: 0.0321617134
Epoch:   600  |  train loss: 0.0297846153
Epoch:   700  |  train loss: 0.0285129406
Epoch:   800  |  train loss: 0.0271200556
Epoch:   900  |  train loss: 0.0261029471
Epoch:  1000  |  train loss: 0.0252349637
Processing class: 19
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.0479396783
Epoch:   200  |  train loss: 0.0431845300
Epoch:   300  |  train loss: 0.0384799533
Epoch:   400  |  train loss: 0.0373894252
Epoch:   500  |  train loss: 0.0339801617
Epoch:   600  |  train loss: 0.0324421864
Epoch:   700  |  train loss: 0.0311879229
Epoch:   800  |  train loss: 0.0297228660
Epoch:   900  |  train loss: 0.0284836598
Epoch:  1000  |  train loss: 0.0275051255
Processing class: 20
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.0520756587
Epoch:   200  |  train loss: 0.0452089414
Epoch:   300  |  train loss: 0.0435532480
Epoch:   400  |  train loss: 0.0386238165
Epoch:   500  |  train loss: 0.0374866635
Epoch:   600  |  train loss: 0.0354018450
Epoch:   700  |  train loss: 0.0335064307
Epoch:   800  |  train loss: 0.0328015070
Epoch:   900  |  train loss: 0.0317661885
Epoch:  1000  |  train loss: 0.0311366953
Processing class: 21
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.0534064680
Epoch:   200  |  train loss: 0.0458988495
Epoch:   300  |  train loss: 0.0366062231
Epoch:   400  |  train loss: 0.0329272944
Epoch:   500  |  train loss: 0.0306607779
Epoch:   600  |  train loss: 0.0292829797
Epoch:   700  |  train loss: 0.0281789921
Epoch:   800  |  train loss: 0.0271784425
Epoch:   900  |  train loss: 0.0260888774
Epoch:  1000  |  train loss: 0.0249998588
Processing class: 22
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.0575995103
Epoch:   200  |  train loss: 0.0565704018
Epoch:   300  |  train loss: 0.0519583650
Epoch:   400  |  train loss: 0.0444348283
Epoch:   500  |  train loss: 0.0417646326
Epoch:   600  |  train loss: 0.0388983026
Epoch:   700  |  train loss: 0.0364592589
Epoch:   800  |  train loss: 0.0346852653
Epoch:   900  |  train loss: 0.0333830118
Epoch:  1000  |  train loss: 0.0324116059
Processing class: 23
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.0497097835
Epoch:   200  |  train loss: 0.0487501770
Epoch:   300  |  train loss: 0.0431665204
Epoch:   400  |  train loss: 0.0394259021
Epoch:   500  |  train loss: 0.0375512071
Epoch:   600  |  train loss: 0.0345902510
Epoch:   700  |  train loss: 0.0327132307
Epoch:   800  |  train loss: 0.0309019815
Epoch:   900  |  train loss: 0.0296089511
Epoch:  1000  |  train loss: 0.0282800838
Processing class: 24
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.0522649080
Epoch:   200  |  train loss: 0.0519915186
Epoch:   300  |  train loss: 0.0513418756
Epoch:   400  |  train loss: 0.0441846125
Epoch:   500  |  train loss: 0.0416861691
Epoch:   600  |  train loss: 0.0395332463
Epoch:   700  |  train loss: 0.0376852408
Epoch:   800  |  train loss: 0.0363303907
Epoch:   900  |  train loss: 0.0348435767
Epoch:  1000  |  train loss: 0.0335772693
Processing class: 25
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.0551515296
Epoch:   200  |  train loss: 0.0496780999
Epoch:   300  |  train loss: 0.0459884666
Epoch:   400  |  train loss: 0.0404864915
Epoch:   500  |  train loss: 0.0371912777
Epoch:   600  |  train loss: 0.0353609115
Epoch:   700  |  train loss: 0.0339093570
Epoch:   800  |  train loss: 0.0324984226
Epoch:   900  |  train loss: 0.0311933786
Epoch:  1000  |  train loss: 0.0305175632
Processing class: 26
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.0490786135
Epoch:   200  |  train loss: 0.0486822590
Epoch:   300  |  train loss: 0.0493210964
Epoch:   400  |  train loss: 0.0446091875
Epoch:   500  |  train loss: 0.0407763623
Epoch:   600  |  train loss: 0.0366753519
Epoch:   700  |  train loss: 0.0353521116
Epoch:   800  |  train loss: 0.0340083502
Epoch:   900  |  train loss: 0.0325017609
Epoch:  1000  |  train loss: 0.0315029945
Processing class: 27
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.0576043949
Epoch:   200  |  train loss: 0.0502730243
Epoch:   300  |  train loss: 0.0476971462
Epoch:   400  |  train loss: 0.0422493100
Epoch:   500  |  train loss: 0.0381916620
Epoch:   600  |  train loss: 0.0367118075
Epoch:   700  |  train loss: 0.0357219309
Epoch:   800  |  train loss: 0.0343772009
Epoch:   900  |  train loss: 0.0326598987
Epoch:  1000  |  train loss: 0.0310070347
Processing class: 28
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.0530856028
Epoch:   200  |  train loss: 0.0519385070
Epoch:   300  |  train loss: 0.0455869570
Epoch:   400  |  train loss: 0.0412536658
Epoch:   500  |  train loss: 0.0378791347
Epoch:   600  |  train loss: 0.0367186353
Epoch:   700  |  train loss: 0.0355899096
Epoch:   800  |  train loss: 0.0342810728
Epoch:   900  |  train loss: 0.0334917635
Epoch:  1000  |  train loss: 0.0324555580
Processing class: 29
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.0537935026
Epoch:   200  |  train loss: 0.0463995710
Epoch:   300  |  train loss: 0.0414877966
Epoch:   400  |  train loss: 0.0370650776
Epoch:   500  |  train loss: 0.0348854333
Epoch:   600  |  train loss: 0.0333925933
Epoch:   700  |  train loss: 0.0313482672
Epoch:   800  |  train loss: 0.0295536358
Epoch:   900  |  train loss: 0.0284958169
Epoch:  1000  |  train loss: 0.0276820682
Processing class: 30
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.0483316690
Epoch:   200  |  train loss: 0.0472001188
Epoch:   300  |  train loss: 0.0386335723
Epoch:   400  |  train loss: 0.0366508439
Epoch:   500  |  train loss: 0.0360893890
Epoch:   600  |  train loss: 0.0344020173
Epoch:   700  |  train loss: 0.0324144792
Epoch:   800  |  train loss: 0.0311232027
Epoch:   900  |  train loss: 0.0300784301
Epoch:  1000  |  train loss: 0.0294157870
Processing class: 31
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.0543429531
Epoch:   200  |  train loss: 0.0474665925
Epoch:   300  |  train loss: 0.0425530948
Epoch:   400  |  train loss: 0.0414747670
Epoch:   500  |  train loss: 0.0384163514
Epoch:   600  |  train loss: 0.0369109325
Epoch:   700  |  train loss: 0.0354506820
Epoch:   800  |  train loss: 0.0344612353
Epoch:   900  |  train loss: 0.0329648308
Epoch:  1000  |  train loss: 0.0318681844
Processing class: 32
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.0603063405
Epoch:   200  |  train loss: 0.0571171112
Epoch:   300  |  train loss: 0.0519949622
Epoch:   400  |  train loss: 0.0472023852
Epoch:   500  |  train loss: 0.0426186368
Epoch:   600  |  train loss: 0.0393076457
Epoch:   700  |  train loss: 0.0368885815
Epoch:   800  |  train loss: 0.0355163608
Epoch:   900  |  train loss: 0.0342270624
Epoch:  1000  |  train loss: 0.0331746548
Processing class: 33
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.0555244863
Epoch:   200  |  train loss: 0.0505701952
Epoch:   300  |  train loss: 0.0434776999
Epoch:   400  |  train loss: 0.0391964607
Epoch:   500  |  train loss: 0.0369429976
Epoch:   600  |  train loss: 0.0343954556
Epoch:   700  |  train loss: 0.0330378622
Epoch:   800  |  train loss: 0.0318426821
Epoch:   900  |  train loss: 0.0306493964
Epoch:  1000  |  train loss: 0.0293848705
Processing class: 34
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.0568393394
Epoch:   200  |  train loss: 0.0546957418
Epoch:   300  |  train loss: 0.0456280008
Epoch:   400  |  train loss: 0.0428150594
Epoch:   500  |  train loss: 0.0392222166
Epoch:   600  |  train loss: 0.0364223152
Epoch:   700  |  train loss: 0.0345895164
Epoch:   800  |  train loss: 0.0330906682
Epoch:   900  |  train loss: 0.0317670975
Epoch:  1000  |  train loss: 0.0306286655
Processing class: 35
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.0668872118
Epoch:   200  |  train loss: 0.0546345562
Epoch:   300  |  train loss: 0.0497785822
Epoch:   400  |  train loss: 0.0474028826
Epoch:   500  |  train loss: 0.0439973876
Epoch:   600  |  train loss: 0.0427596562
Epoch:   700  |  train loss: 0.0407541148
Epoch:   800  |  train loss: 0.0391267240
Epoch:   900  |  train loss: 0.0377215423
Epoch:  1000  |  train loss: 0.0364544369
Processing class: 36
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.0474618167
Epoch:   200  |  train loss: 0.0434755675
Epoch:   300  |  train loss: 0.0394388713
Epoch:   400  |  train loss: 0.0379309304
Epoch:   500  |  train loss: 0.0353885189
Epoch:   600  |  train loss: 0.0328353405
Epoch:   700  |  train loss: 0.0309688941
Epoch:   800  |  train loss: 0.0295953624
Epoch:   900  |  train loss: 0.0283449288
Epoch:  1000  |  train loss: 0.0273117561
Processing class: 37
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.0593485616
Epoch:   200  |  train loss: 0.0556321755
Epoch:   300  |  train loss: 0.0493221939
Epoch:   400  |  train loss: 0.0444417171
Epoch:   500  |  train loss: 0.0415667862
Epoch:   600  |  train loss: 0.0395020887
Epoch:   700  |  train loss: 0.0378330044
Epoch:   800  |  train loss: 0.0365946434
Epoch:   900  |  train loss: 0.0349506579
Epoch:  1000  |  train loss: 0.0338194672
Processing class: 38
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.0496204294
Epoch:   200  |  train loss: 0.0482540421
Epoch:   300  |  train loss: 0.0450831726
Epoch:   400  |  train loss: 0.0427010126
Epoch:   500  |  train loss: 0.0391333342
Epoch:   600  |  train loss: 0.0365201458
Epoch:   700  |  train loss: 0.0339976475
Epoch:   800  |  train loss: 0.0330522306
Epoch:   900  |  train loss: 0.0321483474
Epoch:  1000  |  train loss: 0.0314171653
Processing class: 39
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.0544917449
Epoch:   200  |  train loss: 0.0512512811
Epoch:   300  |  train loss: 0.0470908247
Epoch:   400  |  train loss: 0.0433491781
Epoch:   500  |  train loss: 0.0395872749
Epoch:   600  |  train loss: 0.0376794785
Epoch:   700  |  train loss: 0.0357981212
Epoch:   800  |  train loss: 0.0341575071
Epoch:   900  |  train loss: 0.0326272286
Epoch:  1000  |  train loss: 0.0313307565
Processing class: 40
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.0519598700
Epoch:   200  |  train loss: 0.0456650183
Epoch:   300  |  train loss: 0.0433725141
Epoch:   400  |  train loss: 0.0404584810
Epoch:   500  |  train loss: 0.0385882586
Epoch:   600  |  train loss: 0.0353163384
Epoch:   700  |  train loss: 0.0342069458
Epoch:   800  |  train loss: 0.0329162005
Epoch:   900  |  train loss: 0.0319406513
Epoch:  1000  |  train loss: 0.0308121081
Processing class: 41
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.0468773164
Epoch:   200  |  train loss: 0.0471324608
Epoch:   300  |  train loss: 0.0395385571
Epoch:   400  |  train loss: 0.0358749323
Epoch:   500  |  train loss: 0.0340037934
Epoch:   600  |  train loss: 0.0327701747
Epoch:   700  |  train loss: 0.0314854320
Epoch:   800  |  train loss: 0.0304829121
Epoch:   900  |  train loss: 0.0295665681
Epoch:  1000  |  train loss: 0.0288518425
Processing class: 42
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.0570677675
Epoch:   200  |  train loss: 0.0502260983
Epoch:   300  |  train loss: 0.0448248528
Epoch:   400  |  train loss: 0.0407421425
Epoch:   500  |  train loss: 0.0376161300
Epoch:   600  |  train loss: 0.0347089484
Epoch:   700  |  train loss: 0.0329955041
Epoch:   800  |  train loss: 0.0313565787
Epoch:   900  |  train loss: 0.0299304467
Epoch:  1000  |  train loss: 0.0287299931
Processing class: 43
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.0593962580
Epoch:   200  |  train loss: 0.0533966087
Epoch:   300  |  train loss: 0.0461988926
Epoch:   400  |  train loss: 0.0441468060
Epoch:   500  |  train loss: 0.0424290419
Epoch:   600  |  train loss: 0.0417473882
Epoch:   700  |  train loss: 0.0409766719
Epoch:   800  |  train loss: 0.0401103497
Epoch:   900  |  train loss: 0.0390007772
Epoch:  1000  |  train loss: 0.0379155301
Processing class: 44
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.0606911078
Epoch:   200  |  train loss: 0.0517151810
Epoch:   300  |  train loss: 0.0489640169
Epoch:   400  |  train loss: 0.0441405185
Epoch:   500  |  train loss: 0.0419970013
Epoch:   600  |  train loss: 0.0402683169
Epoch:   700  |  train loss: 0.0387030803
Epoch:   800  |  train loss: 0.0372049689
Epoch:   900  |  train loss: 0.0355540931
Epoch:  1000  |  train loss: 0.0341967389
Processing class: 45
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.0523533456
Epoch:   200  |  train loss: 0.0462740332
Epoch:   300  |  train loss: 0.0418557569
Epoch:   400  |  train loss: 0.0421385810
Epoch:   500  |  train loss: 0.0413526252
Epoch:   600  |  train loss: 0.0391986527
Epoch:   700  |  train loss: 0.0374718174
Epoch:   800  |  train loss: 0.0366131909
Epoch:   900  |  train loss: 0.0360947631
Epoch:  1000  |  train loss: 0.0353940681
Processing class: 46
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.0482007921
Epoch:   200  |  train loss: 0.0464358583
Epoch:   300  |  train loss: 0.0410784438
Epoch:   400  |  train loss: 0.0404078968
Epoch:   500  |  train loss: 0.0390365653
Epoch:   600  |  train loss: 0.0380208537
Epoch:   700  |  train loss: 0.0364604399
Epoch:   800  |  train loss: 0.0353505574
Epoch:   900  |  train loss: 0.0341741353
Epoch:  1000  |  train loss: 0.0332139358
Processing class: 47
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.0581889771
Epoch:   200  |  train loss: 0.0520076543
Epoch:   300  |  train loss: 0.0441419087
Epoch:   400  |  train loss: 0.0407193862
Epoch:   500  |  train loss: 0.0383695297
Epoch:   600  |  train loss: 0.0358494483
Epoch:   700  |  train loss: 0.0338262413
Epoch:   800  |  train loss: 0.0324238293
Epoch:   900  |  train loss: 0.0313317709
Epoch:  1000  |  train loss: 0.0304949403
Processing class: 48
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.0565611817
Epoch:   200  |  train loss: 0.0542028494
Epoch:   300  |  train loss: 0.0435333207
Epoch:   400  |  train loss: 0.0403415374
Epoch:   500  |  train loss: 0.0395405278
Epoch:   600  |  train loss: 0.0369541220
Epoch:   700  |  train loss: 0.0358174264
Epoch:   800  |  train loss: 0.0349809594
Epoch:   900  |  train loss: 0.0340705305
Epoch:  1000  |  train loss: 0.0330639761
Processing class: 49
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.0585853212
Epoch:   200  |  train loss: 0.0529045299
Epoch:   300  |  train loss: 0.0465164222
Epoch:   400  |  train loss: 0.0433531977
Epoch:   500  |  train loss: 0.0395820588
Epoch:   600  |  train loss: 0.0377451606
Epoch:   700  |  train loss: 0.0356334120
Epoch:   800  |  train loss: 0.0345729452
Epoch:   900  |  train loss: 0.0334275924
Epoch:  1000  |  train loss: 0.0322174203
/home/z1165703/FeCAM/models/base.py:87: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  vec = torch.tensor(vec).cuda()
2024-03-04 20:35:53,331 [trainer.py] => CNN: {'total': 83.44, '00-09': 87.7, '10-19': 79.6, '20-29': 84.9, '30-39': 81.0, '40-49': 84.0, 'old': 0, 'new': 83.44}
2024-03-04 20:35:53,332 [trainer.py] => No NME accuracy
2024-03-04 20:35:53,332 [trainer.py] => FeCAM: {'total': 82.9, '00-09': 86.2, '10-19': 79.7, '20-29': 85.0, '30-39': 80.6, '40-49': 83.0, 'old': 0, 'new': 82.9}
2024-03-04 20:35:53,333 [trainer.py] => CNN top1 curve: [83.44]
2024-03-04 20:35:53,333 [trainer.py] => CNN top5 curve: [96.5]
2024-03-04 20:35:53,333 [trainer.py] => FeCAM top1 curve: [82.9]
2024-03-04 20:35:53,333 [trainer.py] => FeCAM top5 curve: [95.84]

2024-03-04 20:35:53,345 [fecam.py] => Learning on 50-60
Processing class: 50
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.0767421752
Epoch:   200  |  train loss: 0.0652433664
Epoch:   300  |  train loss: 0.0594107643
Epoch:   400  |  train loss: 0.0538244769
Epoch:   500  |  train loss: 0.0486976661
Epoch:   600  |  train loss: 0.0452573985
Epoch:   700  |  train loss: 0.0425966837
Epoch:   800  |  train loss: 0.0404696673
Epoch:   900  |  train loss: 0.0389184110
Epoch:  1000  |  train loss: 0.0376097240
Processing class: 51
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.0775965109
Epoch:   200  |  train loss: 0.0662939802
Epoch:   300  |  train loss: 0.0590277895
Epoch:   400  |  train loss: 0.0548943989
Epoch:   500  |  train loss: 0.0491708100
Epoch:   600  |  train loss: 0.0449054435
Epoch:   700  |  train loss: 0.0421952792
Epoch:   800  |  train loss: 0.0405413061
Epoch:   900  |  train loss: 0.0389742732
Epoch:  1000  |  train loss: 0.0373026110
Processing class: 52
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.0892738655
Epoch:   200  |  train loss: 0.0834398359
Epoch:   300  |  train loss: 0.0734997511
Epoch:   400  |  train loss: 0.0670044690
Epoch:   500  |  train loss: 0.0618761212
Epoch:   600  |  train loss: 0.0573598281
Epoch:   700  |  train loss: 0.0538321152
Epoch:   800  |  train loss: 0.0509622738
Epoch:   900  |  train loss: 0.0481970765
Epoch:  1000  |  train loss: 0.0458450586
Processing class: 53
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.0645856827
Epoch:   200  |  train loss: 0.0554190248
Epoch:   300  |  train loss: 0.0516108133
Epoch:   400  |  train loss: 0.0473017864
Epoch:   500  |  train loss: 0.0454085939
Epoch:   600  |  train loss: 0.0437681369
Epoch:   700  |  train loss: 0.0419164509
Epoch:   800  |  train loss: 0.0403555430
Epoch:   900  |  train loss: 0.0391405657
Epoch:  1000  |  train loss: 0.0380093902
Processing class: 54
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.0594429575
Epoch:   200  |  train loss: 0.0525329627
Epoch:   300  |  train loss: 0.0460193224
Epoch:   400  |  train loss: 0.0429618470
Epoch:   500  |  train loss: 0.0402873844
Epoch:   600  |  train loss: 0.0381650031
Epoch:   700  |  train loss: 0.0363004111
Epoch:   800  |  train loss: 0.0348332755
Epoch:   900  |  train loss: 0.0336091626
Epoch:  1000  |  train loss: 0.0323082246
Processing class: 55
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.0963980719
Epoch:   200  |  train loss: 0.0901015162
Epoch:   300  |  train loss: 0.0834200114
Epoch:   400  |  train loss: 0.0769143254
Epoch:   500  |  train loss: 0.0710818350
Epoch:   600  |  train loss: 0.0654207207
Epoch:   700  |  train loss: 0.0612667605
Epoch:   800  |  train loss: 0.0576726235
Epoch:   900  |  train loss: 0.0545040503
Epoch:  1000  |  train loss: 0.0519305021
Processing class: 56
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.0740393981
Epoch:   200  |  train loss: 0.0601623908
Epoch:   300  |  train loss: 0.0500184603
Epoch:   400  |  train loss: 0.0443702862
Epoch:   500  |  train loss: 0.0403527588
Epoch:   600  |  train loss: 0.0372560993
Epoch:   700  |  train loss: 0.0350538328
Epoch:   800  |  train loss: 0.0331095442
Epoch:   900  |  train loss: 0.0317408875
Epoch:  1000  |  train loss: 0.0305165622
Processing class: 57
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.0962022677
Epoch:   200  |  train loss: 0.0882760212
Epoch:   300  |  train loss: 0.0814192340
Epoch:   400  |  train loss: 0.0747595683
Epoch:   500  |  train loss: 0.0688021317
Epoch:   600  |  train loss: 0.0649409823
Epoch:   700  |  train loss: 0.0616866961
Epoch:   800  |  train loss: 0.0584447399
Epoch:   900  |  train loss: 0.0553781644
Epoch:  1000  |  train loss: 0.0523499638
Processing class: 58
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.0795744836
Epoch:   200  |  train loss: 0.0591792844
Epoch:   300  |  train loss: 0.0519195519
Epoch:   400  |  train loss: 0.0452685289
Epoch:   500  |  train loss: 0.0422406994
Epoch:   600  |  train loss: 0.0399135150
Epoch:   700  |  train loss: 0.0377277605
Epoch:   800  |  train loss: 0.0360440850
Epoch:   900  |  train loss: 0.0344371490
Epoch:  1000  |  train loss: 0.0330193900
Processing class: 59
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.0882780179
Epoch:   200  |  train loss: 0.0771880239
Epoch:   300  |  train loss: 0.0673495710
Epoch:   400  |  train loss: 0.0598707192
Epoch:   500  |  train loss: 0.0545961663
Epoch:   600  |  train loss: 0.0514929831
Epoch:   700  |  train loss: 0.0487614267
Epoch:   800  |  train loss: 0.0467385575
Epoch:   900  |  train loss: 0.0447342142
Epoch:  1000  |  train loss: 0.0433372825
2024-03-04 20:41:38,166 [trainer.py] => CNN: {'total': 71.75, '00-09': 82.2, '10-19': 72.8, '20-29': 78.5, '30-39': 74.9, '40-49': 68.4, '50-59': 53.7, 'old': 75.36, 'new': 53.7}
2024-03-04 20:41:38,167 [trainer.py] => No NME accuracy
2024-03-04 20:41:38,167 [trainer.py] => FeCAM: {'total': 72.78, '00-09': 82.6, '10-19': 74.0, '20-29': 81.0, '30-39': 76.4, '40-49': 76.8, '50-59': 45.9, 'old': 78.16, 'new': 45.9}
2024-03-04 20:41:38,167 [trainer.py] => CNN top1 curve: [83.44, 71.75]
2024-03-04 20:41:38,167 [trainer.py] => CNN top5 curve: [96.5, 89.6]
2024-03-04 20:41:38,167 [trainer.py] => FeCAM top1 curve: [82.9, 72.78]
2024-03-04 20:41:38,167 [trainer.py] => FeCAM top5 curve: [95.84, 91.18]

2024-03-04 20:41:38,174 [fecam.py] => Learning on 60-70
Processing class: 60
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.0859531328
Epoch:   200  |  train loss: 0.0764795139
Epoch:   300  |  train loss: 0.0680929691
Epoch:   400  |  train loss: 0.0609625421
Epoch:   500  |  train loss: 0.0552575506
Epoch:   600  |  train loss: 0.0508380786
Epoch:   700  |  train loss: 0.0472678430
Epoch:   800  |  train loss: 0.0440818787
Epoch:   900  |  train loss: 0.0420299344
Epoch:  1000  |  train loss: 0.0400095537
Processing class: 61
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.0734936997
Epoch:   200  |  train loss: 0.0599195525
Epoch:   300  |  train loss: 0.0510341987
Epoch:   400  |  train loss: 0.0449643441
Epoch:   500  |  train loss: 0.0414287306
Epoch:   600  |  train loss: 0.0387172051
Epoch:   700  |  train loss: 0.0364059187
Epoch:   800  |  train loss: 0.0343745146
Epoch:   900  |  train loss: 0.0329981767
Epoch:  1000  |  train loss: 0.0317429006
Processing class: 62
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.0963740066
Epoch:   200  |  train loss: 0.0888044760
Epoch:   300  |  train loss: 0.0828967109
Epoch:   400  |  train loss: 0.0782275572
Epoch:   500  |  train loss: 0.0740878910
Epoch:   600  |  train loss: 0.0703558192
Epoch:   700  |  train loss: 0.0666207343
Epoch:   800  |  train loss: 0.0632272623
Epoch:   900  |  train loss: 0.0602631427
Epoch:  1000  |  train loss: 0.0575726248
Processing class: 63
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.0817362309
Epoch:   200  |  train loss: 0.0739005893
Epoch:   300  |  train loss: 0.0592223898
Epoch:   400  |  train loss: 0.0517787457
Epoch:   500  |  train loss: 0.0469213836
Epoch:   600  |  train loss: 0.0436830074
Epoch:   700  |  train loss: 0.0412926525
Epoch:   800  |  train loss: 0.0395285487
Epoch:   900  |  train loss: 0.0377128795
Epoch:  1000  |  train loss: 0.0362190753
Processing class: 64
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.0738896683
Epoch:   200  |  train loss: 0.0545100138
Epoch:   300  |  train loss: 0.0470727973
Epoch:   400  |  train loss: 0.0426561289
Epoch:   500  |  train loss: 0.0384128362
Epoch:   600  |  train loss: 0.0354789078
Epoch:   700  |  train loss: 0.0334621467
Epoch:   800  |  train loss: 0.0318795945
Epoch:   900  |  train loss: 0.0305524062
Epoch:  1000  |  train loss: 0.0291066229
Processing class: 65
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.0740082204
Epoch:   200  |  train loss: 0.0596871860
Epoch:   300  |  train loss: 0.0477421731
Epoch:   400  |  train loss: 0.0427606486
Epoch:   500  |  train loss: 0.0394903794
Epoch:   600  |  train loss: 0.0366494492
Epoch:   700  |  train loss: 0.0343565725
Epoch:   800  |  train loss: 0.0326491933
Epoch:   900  |  train loss: 0.0309393711
Epoch:  1000  |  train loss: 0.0296977706
Processing class: 66
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.0816397682
Epoch:   200  |  train loss: 0.0712466568
Epoch:   300  |  train loss: 0.0624954686
Epoch:   400  |  train loss: 0.0571379423
Epoch:   500  |  train loss: 0.0540411621
Epoch:   600  |  train loss: 0.0512898393
Epoch:   700  |  train loss: 0.0493138000
Epoch:   800  |  train loss: 0.0477663234
Epoch:   900  |  train loss: 0.0461821891
Epoch:  1000  |  train loss: 0.0448999360
Processing class: 67
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.0854908302
Epoch:   200  |  train loss: 0.0747343540
Epoch:   300  |  train loss: 0.0672314197
Epoch:   400  |  train loss: 0.0619858697
Epoch:   500  |  train loss: 0.0582055263
Epoch:   600  |  train loss: 0.0549397103
Epoch:   700  |  train loss: 0.0519263610
Epoch:   800  |  train loss: 0.0496282674
Epoch:   900  |  train loss: 0.0474982850
Epoch:  1000  |  train loss: 0.0457776137
Processing class: 68
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.0723352075
Epoch:   200  |  train loss: 0.0651250616
Epoch:   300  |  train loss: 0.0543368839
Epoch:   400  |  train loss: 0.0500903159
Epoch:   500  |  train loss: 0.0470053449
Epoch:   600  |  train loss: 0.0442785032
Epoch:   700  |  train loss: 0.0422043465
Epoch:   800  |  train loss: 0.0403363138
Epoch:   900  |  train loss: 0.0387449086
Epoch:  1000  |  train loss: 0.0374543600
Processing class: 69
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.0426437095
Epoch:   200  |  train loss: 0.0370089382
Epoch:   300  |  train loss: 0.0321230210
Epoch:   400  |  train loss: 0.0292237997
Epoch:   500  |  train loss: 0.0274724361
Epoch:   600  |  train loss: 0.0265039884
Epoch:   700  |  train loss: 0.0258347761
Epoch:   800  |  train loss: 0.0250639107
Epoch:   900  |  train loss: 0.0246639840
Epoch:  1000  |  train loss: 0.0240711473
2024-03-04 20:48:16,305 [trainer.py] => CNN: {'total': 64.66, '00-09': 75.0, '10-19': 70.1, '20-29': 77.1, '30-39': 70.6, '40-49': 64.0, '50-59': 45.8, '60-69': 50.0, 'old': 67.1, 'new': 50.0}
2024-03-04 20:48:16,306 [trainer.py] => No NME accuracy
2024-03-04 20:48:16,306 [trainer.py] => FeCAM: {'total': 67.27, '00-09': 79.9, '10-19': 70.6, '20-29': 79.1, '30-39': 71.9, '40-49': 74.1, '50-59': 41.2, '60-69': 54.1, 'old': 69.47, 'new': 54.1}
2024-03-04 20:48:16,306 [trainer.py] => CNN top1 curve: [83.44, 71.75, 64.66]
2024-03-04 20:48:16,306 [trainer.py] => CNN top5 curve: [96.5, 89.6, 86.54]
2024-03-04 20:48:16,306 [trainer.py] => FeCAM top1 curve: [82.9, 72.78, 67.27]
2024-03-04 20:48:16,306 [trainer.py] => FeCAM top5 curve: [95.84, 91.18, 87.21]

2024-03-04 20:48:16,312 [fecam.py] => Learning on 70-80
Processing class: 70
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.0778868228
Epoch:   200  |  train loss: 0.0690307766
Epoch:   300  |  train loss: 0.0581557527
Epoch:   400  |  train loss: 0.0528533682
Epoch:   500  |  train loss: 0.0484146677
Epoch:   600  |  train loss: 0.0446860790
Epoch:   700  |  train loss: 0.0414263315
Epoch:   800  |  train loss: 0.0388201281
Epoch:   900  |  train loss: 0.0373901635
Epoch:  1000  |  train loss: 0.0361005306
Processing class: 71
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.0874344677
Epoch:   200  |  train loss: 0.0778173625
Epoch:   300  |  train loss: 0.0652731761
Epoch:   400  |  train loss: 0.0585033372
Epoch:   500  |  train loss: 0.0533053972
Epoch:   600  |  train loss: 0.0492935501
Epoch:   700  |  train loss: 0.0457576446
Epoch:   800  |  train loss: 0.0430561617
Epoch:   900  |  train loss: 0.0406698145
Epoch:  1000  |  train loss: 0.0386166357
Processing class: 72
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.0842502862
Epoch:   200  |  train loss: 0.0748349652
Epoch:   300  |  train loss: 0.0655281484
Epoch:   400  |  train loss: 0.0593483113
Epoch:   500  |  train loss: 0.0554188818
Epoch:   600  |  train loss: 0.0517264292
Epoch:   700  |  train loss: 0.0487981617
Epoch:   800  |  train loss: 0.0461153902
Epoch:   900  |  train loss: 0.0437972747
Epoch:  1000  |  train loss: 0.0419789001
Processing class: 73
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.0817451745
Epoch:   200  |  train loss: 0.0702066466
Epoch:   300  |  train loss: 0.0579429425
Epoch:   400  |  train loss: 0.0533597440
Epoch:   500  |  train loss: 0.0506947339
Epoch:   600  |  train loss: 0.0483258963
Epoch:   700  |  train loss: 0.0461370878
Epoch:   800  |  train loss: 0.0441323571
Epoch:   900  |  train loss: 0.0423332803
Epoch:  1000  |  train loss: 0.0408769667
Processing class: 74
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.0936758086
Epoch:   200  |  train loss: 0.0893908828
Epoch:   300  |  train loss: 0.0796348825
Epoch:   400  |  train loss: 0.0718046099
Epoch:   500  |  train loss: 0.0651421383
Epoch:   600  |  train loss: 0.0604272872
Epoch:   700  |  train loss: 0.0569686428
Epoch:   800  |  train loss: 0.0536634475
Epoch:   900  |  train loss: 0.0510642014
Epoch:  1000  |  train loss: 0.0489094049
Processing class: 75
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.0838582098
Epoch:   200  |  train loss: 0.0745296210
Epoch:   300  |  train loss: 0.0643739738
Epoch:   400  |  train loss: 0.0594049059
Epoch:   500  |  train loss: 0.0549905203
Epoch:   600  |  train loss: 0.0514639720
Epoch:   700  |  train loss: 0.0486463889
Epoch:   800  |  train loss: 0.0464434721
Epoch:   900  |  train loss: 0.0446752720
Epoch:  1000  |  train loss: 0.0429505661
Processing class: 76
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.0481752232
Epoch:   200  |  train loss: 0.0401133232
Epoch:   300  |  train loss: 0.0342764460
Epoch:   400  |  train loss: 0.0316434495
Epoch:   500  |  train loss: 0.0301871978
Epoch:   600  |  train loss: 0.0286686998
Epoch:   700  |  train loss: 0.0278787658
Epoch:   800  |  train loss: 0.0265606929
Epoch:   900  |  train loss: 0.0258471552
Epoch:  1000  |  train loss: 0.0251659013
Processing class: 77
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.0735986665
Epoch:   200  |  train loss: 0.0646027632
Epoch:   300  |  train loss: 0.0558586143
Epoch:   400  |  train loss: 0.0505642913
Epoch:   500  |  train loss: 0.0451175369
Epoch:   600  |  train loss: 0.0409864172
Epoch:   700  |  train loss: 0.0381540902
Epoch:   800  |  train loss: 0.0359641396
Epoch:   900  |  train loss: 0.0343282774
Epoch:  1000  |  train loss: 0.0326015830
Processing class: 78
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.0772864848
Epoch:   200  |  train loss: 0.0618795946
Epoch:   300  |  train loss: 0.0566065870
Epoch:   400  |  train loss: 0.0511944748
Epoch:   500  |  train loss: 0.0463749588
Epoch:   600  |  train loss: 0.0432611994
Epoch:   700  |  train loss: 0.0410604306
Epoch:   800  |  train loss: 0.0389899306
Epoch:   900  |  train loss: 0.0376680478
Epoch:  1000  |  train loss: 0.0363659881
Processing class: 79
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.0963785827
Epoch:   200  |  train loss: 0.0858472258
Epoch:   300  |  train loss: 0.0785057500
Epoch:   400  |  train loss: 0.0713057607
Epoch:   500  |  train loss: 0.0663220972
Epoch:   600  |  train loss: 0.0619224809
Epoch:   700  |  train loss: 0.0581703156
Epoch:   800  |  train loss: 0.0545915283
Epoch:   900  |  train loss: 0.0516350538
Epoch:  1000  |  train loss: 0.0490382440
2024-03-04 20:55:57,385 [trainer.py] => CNN: {'total': 59.18, '00-09': 73.6, '10-19': 68.4, '20-29': 76.9, '30-39': 69.1, '40-49': 60.6, '50-59': 37.2, '60-69': 45.7, '70-79': 41.9, 'old': 61.64, 'new': 41.9}
2024-03-04 20:55:57,386 [trainer.py] => No NME accuracy
2024-03-04 20:55:57,386 [trainer.py] => FeCAM: {'total': 61.72, '00-09': 78.1, '10-19': 69.3, '20-29': 78.0, '30-39': 69.5, '40-49': 72.1, '50-59': 35.7, '60-69': 49.1, '70-79': 42.0, 'old': 64.54, 'new': 42.0}
2024-03-04 20:55:57,386 [trainer.py] => CNN top1 curve: [83.44, 71.75, 64.66, 59.18]
2024-03-04 20:55:57,386 [trainer.py] => CNN top5 curve: [96.5, 89.6, 86.54, 84.09]
2024-03-04 20:55:57,386 [trainer.py] => FeCAM top1 curve: [82.9, 72.78, 67.27, 61.72]
2024-03-04 20:55:57,386 [trainer.py] => FeCAM top5 curve: [95.84, 91.18, 87.21, 84.09]

2024-03-04 20:55:57,391 [fecam.py] => Learning on 80-90
Processing class: 80
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.0568194866
Epoch:   200  |  train loss: 0.0412278064
Epoch:   300  |  train loss: 0.0366933011
Epoch:   400  |  train loss: 0.0338700041
Epoch:   500  |  train loss: 0.0315860692
Epoch:   600  |  train loss: 0.0303565923
Epoch:   700  |  train loss: 0.0290656094
Epoch:   800  |  train loss: 0.0284041949
Epoch:   900  |  train loss: 0.0275673553
Epoch:  1000  |  train loss: 0.0268084746
Processing class: 81
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.0735153556
Epoch:   200  |  train loss: 0.0605505161
Epoch:   300  |  train loss: 0.0541123413
Epoch:   400  |  train loss: 0.0498075746
Epoch:   500  |  train loss: 0.0462170467
Epoch:   600  |  train loss: 0.0436352268
Epoch:   700  |  train loss: 0.0414630383
Epoch:   800  |  train loss: 0.0399279952
Epoch:   900  |  train loss: 0.0380640447
Epoch:  1000  |  train loss: 0.0364134133
Processing class: 82
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.0667587474
Epoch:   200  |  train loss: 0.0546658166
Epoch:   300  |  train loss: 0.0487864591
Epoch:   400  |  train loss: 0.0440481521
Epoch:   500  |  train loss: 0.0403518498
Epoch:   600  |  train loss: 0.0380339161
Epoch:   700  |  train loss: 0.0363023214
Epoch:   800  |  train loss: 0.0347460710
Epoch:   900  |  train loss: 0.0334319208
Epoch:  1000  |  train loss: 0.0321659535
Processing class: 83
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.0613500558
Epoch:   200  |  train loss: 0.0446362823
Epoch:   300  |  train loss: 0.0382530183
Epoch:   400  |  train loss: 0.0340130486
Epoch:   500  |  train loss: 0.0315144446
Epoch:   600  |  train loss: 0.0298735652
Epoch:   700  |  train loss: 0.0288168106
Epoch:   800  |  train loss: 0.0277270537
Epoch:   900  |  train loss: 0.0267429713
Epoch:  1000  |  train loss: 0.0260004506
Processing class: 84
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.0804128811
Epoch:   200  |  train loss: 0.0624763079
Epoch:   300  |  train loss: 0.0547387771
Epoch:   400  |  train loss: 0.0484147400
Epoch:   500  |  train loss: 0.0445961431
Epoch:   600  |  train loss: 0.0417936169
Epoch:   700  |  train loss: 0.0396086797
Epoch:   800  |  train loss: 0.0379483022
Epoch:   900  |  train loss: 0.0361827247
Epoch:  1000  |  train loss: 0.0346442327
Processing class: 85
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.0827843472
Epoch:   200  |  train loss: 0.0654008523
Epoch:   300  |  train loss: 0.0558113292
Epoch:   400  |  train loss: 0.0501154199
Epoch:   500  |  train loss: 0.0466822244
Epoch:   600  |  train loss: 0.0436732940
Epoch:   700  |  train loss: 0.0416260384
Epoch:   800  |  train loss: 0.0397527725
Epoch:   900  |  train loss: 0.0380345114
Epoch:  1000  |  train loss: 0.0366506517
Processing class: 86
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.0953384131
Epoch:   200  |  train loss: 0.0830310002
Epoch:   300  |  train loss: 0.0692508176
Epoch:   400  |  train loss: 0.0616661832
Epoch:   500  |  train loss: 0.0566124134
Epoch:   600  |  train loss: 0.0525628194
Epoch:   700  |  train loss: 0.0493168160
Epoch:   800  |  train loss: 0.0467155352
Epoch:   900  |  train loss: 0.0447227493
Epoch:  1000  |  train loss: 0.0431094199
Processing class: 87
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.0666617021
Epoch:   200  |  train loss: 0.0529026456
Epoch:   300  |  train loss: 0.0485849239
Epoch:   400  |  train loss: 0.0433541968
Epoch:   500  |  train loss: 0.0403976522
Epoch:   600  |  train loss: 0.0388108216
Epoch:   700  |  train loss: 0.0372516103
Epoch:   800  |  train loss: 0.0357390366
Epoch:   900  |  train loss: 0.0344254218
Epoch:  1000  |  train loss: 0.0334230632
Processing class: 88
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.0799340829
Epoch:   200  |  train loss: 0.0695622325
Epoch:   300  |  train loss: 0.0612982519
Epoch:   400  |  train loss: 0.0550349556
Epoch:   500  |  train loss: 0.0495740704
Epoch:   600  |  train loss: 0.0455150589
Epoch:   700  |  train loss: 0.0429592744
Epoch:   800  |  train loss: 0.0408181116
Epoch:   900  |  train loss: 0.0391779743
Epoch:  1000  |  train loss: 0.0374788254
Processing class: 89
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.0751856789
Epoch:   200  |  train loss: 0.0631444648
Epoch:   300  |  train loss: 0.0532428145
Epoch:   400  |  train loss: 0.0487683333
Epoch:   500  |  train loss: 0.0445256032
Epoch:   600  |  train loss: 0.0410679653
Epoch:   700  |  train loss: 0.0385481924
Epoch:   800  |  train loss: 0.0366391808
Epoch:   900  |  train loss: 0.0347961430
Epoch:  1000  |  train loss: 0.0332652263
2024-03-04 21:04:57,255 [trainer.py] => CNN: {'total': 54.08, '00-09': 67.9, '10-19': 63.6, '20-29': 73.2, '30-39': 68.8, '40-49': 57.2, '50-59': 32.6, '60-69': 40.9, '70-79': 38.1, '80-89': 44.4, 'old': 55.29, 'new': 44.4}
2024-03-04 21:04:57,510 [trainer.py] => No NME accuracy
2024-03-04 21:04:57,510 [trainer.py] => FeCAM: {'total': 57.66, '00-09': 76.1, '10-19': 65.8, '20-29': 75.3, '30-39': 67.9, '40-49': 68.5, '50-59': 33.0, '60-69': 45.0, '70-79': 39.0, '80-89': 48.3, 'old': 58.82, 'new': 48.3}
2024-03-04 21:04:57,512 [trainer.py] => CNN top1 curve: [83.44, 71.75, 64.66, 59.18, 54.08]
2024-03-04 21:04:57,512 [trainer.py] => CNN top5 curve: [96.5, 89.6, 86.54, 84.09, 81.74]
2024-03-04 21:04:57,512 [trainer.py] => FeCAM top1 curve: [82.9, 72.78, 67.27, 61.72, 57.66]
2024-03-04 21:04:57,512 [trainer.py] => FeCAM top5 curve: [95.84, 91.18, 87.21, 84.09, 81.63]

2024-03-04 21:04:57,525 [fecam.py] => Learning on 90-100
Processing class: 90
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.0824023113
Epoch:   200  |  train loss: 0.0737678394
Epoch:   300  |  train loss: 0.0627907656
Epoch:   400  |  train loss: 0.0549066052
Epoch:   500  |  train loss: 0.0494290411
Epoch:   600  |  train loss: 0.0449936479
Epoch:   700  |  train loss: 0.0415321544
Epoch:   800  |  train loss: 0.0393836595
Epoch:   900  |  train loss: 0.0377603427
Epoch:  1000  |  train loss: 0.0362067640
Processing class: 91
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.0465593204
Epoch:   200  |  train loss: 0.0419305377
Epoch:   300  |  train loss: 0.0370293818
Epoch:   400  |  train loss: 0.0350066528
Epoch:   500  |  train loss: 0.0323560566
Epoch:   600  |  train loss: 0.0305333689
Epoch:   700  |  train loss: 0.0283625614
Epoch:   800  |  train loss: 0.0266530234
Epoch:   900  |  train loss: 0.0254443031
Epoch:  1000  |  train loss: 0.0246951234
Processing class: 92
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.0725775048
Epoch:   200  |  train loss: 0.0558962435
Epoch:   300  |  train loss: 0.0515104152
Epoch:   400  |  train loss: 0.0475175217
Epoch:   500  |  train loss: 0.0428381845
Epoch:   600  |  train loss: 0.0385422029
Epoch:   700  |  train loss: 0.0358512446
Epoch:   800  |  train loss: 0.0340460360
Epoch:   900  |  train loss: 0.0326041587
Epoch:  1000  |  train loss: 0.0313204378
Processing class: 93
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.0779362097
Epoch:   200  |  train loss: 0.0603902891
Epoch:   300  |  train loss: 0.0517999053
Epoch:   400  |  train loss: 0.0472816676
Epoch:   500  |  train loss: 0.0432562999
Epoch:   600  |  train loss: 0.0399907701
Epoch:   700  |  train loss: 0.0376850262
Epoch:   800  |  train loss: 0.0357760035
Epoch:   900  |  train loss: 0.0342007466
Epoch:  1000  |  train loss: 0.0325208187
Processing class: 94
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.0699572697
Epoch:   200  |  train loss: 0.0599524915
Epoch:   300  |  train loss: 0.0497392245
Epoch:   400  |  train loss: 0.0445851251
Epoch:   500  |  train loss: 0.0410981961
Epoch:   600  |  train loss: 0.0386806168
Epoch:   700  |  train loss: 0.0366960123
Epoch:   800  |  train loss: 0.0346461020
Epoch:   900  |  train loss: 0.0332372814
Epoch:  1000  |  train loss: 0.0318749271
Processing class: 95
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.0867310077
Epoch:   200  |  train loss: 0.0784374058
Epoch:   300  |  train loss: 0.0697797194
Epoch:   400  |  train loss: 0.0613589250
Epoch:   500  |  train loss: 0.0550427400
Epoch:   600  |  train loss: 0.0511000983
Epoch:   700  |  train loss: 0.0476570807
Epoch:   800  |  train loss: 0.0449599691
Epoch:   900  |  train loss: 0.0426829442
Epoch:  1000  |  train loss: 0.0408089608
Processing class: 96
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.0883937731
Epoch:   200  |  train loss: 0.0780262277
Epoch:   300  |  train loss: 0.0665074393
Epoch:   400  |  train loss: 0.0604770787
Epoch:   500  |  train loss: 0.0553381614
Epoch:   600  |  train loss: 0.0517347716
Epoch:   700  |  train loss: 0.0486936785
Epoch:   800  |  train loss: 0.0459539890
Epoch:   900  |  train loss: 0.0433338121
Epoch:  1000  |  train loss: 0.0414376348
Processing class: 97
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.0769409120
Epoch:   200  |  train loss: 0.0608567342
Epoch:   300  |  train loss: 0.0514912017
Epoch:   400  |  train loss: 0.0472034127
Epoch:   500  |  train loss: 0.0434814073
Epoch:   600  |  train loss: 0.0407531925
Epoch:   700  |  train loss: 0.0383974038
Epoch:   800  |  train loss: 0.0366490602
Epoch:   900  |  train loss: 0.0349345408
Epoch:  1000  |  train loss: 0.0333541609
Processing class: 98
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.0654929332
Epoch:   200  |  train loss: 0.0557959653
Epoch:   300  |  train loss: 0.0467174314
Epoch:   400  |  train loss: 0.0419524714
Epoch:   500  |  train loss: 0.0385962106
Epoch:   600  |  train loss: 0.0359552637
Epoch:   700  |  train loss: 0.0340871416
Epoch:   800  |  train loss: 0.0327553488
Epoch:   900  |  train loss: 0.0315392755
Epoch:  1000  |  train loss: 0.0307950329
Processing class: 99
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.0811732471
Epoch:   200  |  train loss: 0.0697047070
Epoch:   300  |  train loss: 0.0570191957
Epoch:   400  |  train loss: 0.0501850456
Epoch:   500  |  train loss: 0.0452277392
Epoch:   600  |  train loss: 0.0421926759
Epoch:   700  |  train loss: 0.0399453938
Epoch:   800  |  train loss: 0.0381678455
Epoch:   900  |  train loss: 0.0365441784
Epoch:  1000  |  train loss: 0.0350690201
2024-03-04 21:15:20,335 [trainer.py] => CNN: {'total': 50.36, '00-09': 57.7, '10-19': 63.6, '20-29': 71.4, '30-39': 68.5, '40-49': 56.8, '50-59': 29.6, '60-69': 38.9, '70-79': 36.6, '80-89': 42.7, '90-99': 37.8, 'old': 51.76, 'new': 37.8}
2024-03-04 21:15:20,337 [trainer.py] => No NME accuracy
2024-03-04 21:15:20,337 [trainer.py] => FeCAM: {'total': 54.74, '00-09': 73.3, '10-19': 65.1, '20-29': 74.1, '30-39': 67.2, '40-49': 67.1, '50-59': 30.8, '60-69': 43.3, '70-79': 37.2, '80-89': 46.9, '90-99': 42.4, 'old': 56.11, 'new': 42.4}
2024-03-04 21:15:20,337 [trainer.py] => CNN top1 curve: [83.44, 71.75, 64.66, 59.18, 54.08, 50.36]
2024-03-04 21:15:20,337 [trainer.py] => CNN top5 curve: [96.5, 89.6, 86.54, 84.09, 81.74, 79.63]
2024-03-04 21:15:20,337 [trainer.py] => FeCAM top1 curve: [82.9, 72.78, 67.27, 61.72, 57.66, 54.74]
2024-03-04 21:15:20,337 [trainer.py] => FeCAM top5 curve: [95.84, 91.18, 87.21, 84.09, 81.63, 79.21]

=========================================
2024-03-04 21:15:29,227 [trainer.py] => config: ./exps/FeCAM_cifar100.json
2024-03-04 21:15:29,227 [trainer.py] => prefix: train
2024-03-04 21:15:29,227 [trainer.py] => dataset: cifar100
2024-03-04 21:15:29,227 [trainer.py] => memory_size: 0
2024-03-04 21:15:29,227 [trainer.py] => shuffle: True
2024-03-04 21:15:29,227 [trainer.py] => init_cls: 50
2024-03-04 21:15:29,227 [trainer.py] => increment: 10
2024-03-04 21:15:29,227 [trainer.py] => model_name: fecam
2024-03-04 21:15:29,227 [trainer.py] => convnet_type: resnet18
2024-03-04 21:15:29,227 [trainer.py] => device: [device(type='cuda', index=0)]
2024-03-04 21:15:29,227 [trainer.py] => seed: 1993
2024-03-04 21:15:29,227 [trainer.py] => init_epochs: 200
2024-03-04 21:15:29,227 [trainer.py] => init_lr: 0.1
2024-03-04 21:15:29,227 [trainer.py] => init_weight_decay: 0.0005
2024-03-04 21:15:29,227 [trainer.py] => batch_size: 128
2024-03-04 21:15:29,227 [trainer.py] => num_workers: 8
2024-03-04 21:15:29,227 [trainer.py] => T: 5
2024-03-04 21:15:29,227 [trainer.py] => beta: 0.5
2024-03-04 21:15:29,227 [trainer.py] => alpha1: 1
2024-03-04 21:15:29,227 [trainer.py] => alpha2: 1
2024-03-04 21:15:29,227 [trainer.py] => ncm: False
2024-03-04 21:15:29,227 [trainer.py] => tukey: False
2024-03-04 21:15:29,227 [trainer.py] => diagonal: False
2024-03-04 21:15:29,227 [trainer.py] => per_class: True
2024-03-04 21:15:29,227 [trainer.py] => full_cov: True
2024-03-04 21:15:29,227 [trainer.py] => shrink: True
2024-03-04 21:15:29,227 [trainer.py] => norm_cov: False
2024-03-04 21:15:29,227 [trainer.py] => vecnorm: False
2024-03-04 21:15:29,227 [trainer.py] => ae_type: wae
2024-03-04 21:15:29,228 [trainer.py] => epochs: 1000
2024-03-04 21:15:29,228 [trainer.py] => ae_latent_dim: 32
2024-03-04 21:15:29,228 [trainer.py] => wae_sigma: 5
2024-03-04 21:15:29,228 [trainer.py] => wae_C: 0.1
Files already downloaded and verified
Files already downloaded and verified
2024-03-04 21:15:30,868 [data_manager.py] => [68, 56, 78, 8, 23, 84, 90, 65, 74, 76, 40, 89, 3, 92, 55, 9, 26, 80, 43, 38, 58, 70, 77, 1, 85, 19, 17, 50, 28, 53, 13, 81, 45, 82, 6, 59, 83, 16, 15, 44, 91, 41, 72, 60, 79, 52, 20, 10, 31, 54, 37, 95, 14, 71, 96, 98, 97, 2, 64, 66, 42, 22, 35, 86, 24, 34, 87, 21, 99, 0, 88, 27, 18, 94, 11, 12, 47, 25, 30, 46, 62, 69, 36, 61, 7, 63, 75, 5, 32, 4, 51, 48, 73, 93, 39, 67, 29, 49, 57, 33]
2024-03-04 21:15:31,149 [fecam.py] => Learning on 0-50
/home/z1165703/miniconda3/lib/python3.11/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
/home/z1165703/miniconda3/lib/python3.11/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
Processing class: 0
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.0567283958
Epoch:   200  |  train loss: 0.0524038680
Epoch:   300  |  train loss: 0.0521823578
Epoch:   400  |  train loss: 0.0504948705
Epoch:   500  |  train loss: 0.0494937405
Epoch:   600  |  train loss: 0.0491783015
Epoch:   700  |  train loss: 0.0486238718
Epoch:   800  |  train loss: 0.0483932205
Epoch:   900  |  train loss: 0.0477130130
Epoch:  1000  |  train loss: 0.0470486358
Processing class: 1
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.0624536417
Epoch:   200  |  train loss: 0.0616205677
Epoch:   300  |  train loss: 0.0591376670
Epoch:   400  |  train loss: 0.0574780621
Epoch:   500  |  train loss: 0.0554158717
Epoch:   600  |  train loss: 0.0550846241
Epoch:   700  |  train loss: 0.0539451718
Epoch:   800  |  train loss: 0.0531223208
Epoch:   900  |  train loss: 0.0526105426
Epoch:  1000  |  train loss: 0.0515115030
Processing class: 2
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.0657236889
Epoch:   200  |  train loss: 0.0624265805
Epoch:   300  |  train loss: 0.0593591847
Epoch:   400  |  train loss: 0.0565788738
Epoch:   500  |  train loss: 0.0539603420
Epoch:   600  |  train loss: 0.0521458767
Epoch:   700  |  train loss: 0.0504628934
Epoch:   800  |  train loss: 0.0495300971
Epoch:   900  |  train loss: 0.0480545714
Epoch:  1000  |  train loss: 0.0468192056
Processing class: 3
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.0561610788
Epoch:   200  |  train loss: 0.0563865773
Epoch:   300  |  train loss: 0.0546878844
Epoch:   400  |  train loss: 0.0517935097
Epoch:   500  |  train loss: 0.0500611804
Epoch:   600  |  train loss: 0.0495619729
Epoch:   700  |  train loss: 0.0479071490
Epoch:   800  |  train loss: 0.0468537524
Epoch:   900  |  train loss: 0.0455778986
Epoch:  1000  |  train loss: 0.0444534145
Processing class: 4
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.0623123415
Epoch:   200  |  train loss: 0.0611539170
Epoch:   300  |  train loss: 0.0576373041
Epoch:   400  |  train loss: 0.0553619929
Epoch:   500  |  train loss: 0.0536655180
Epoch:   600  |  train loss: 0.0515393361
Epoch:   700  |  train loss: 0.0509722888
Epoch:   800  |  train loss: 0.0502621070
Epoch:   900  |  train loss: 0.0494406886
Epoch:  1000  |  train loss: 0.0486940645
Processing class: 5
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.0664759174
Epoch:   200  |  train loss: 0.0616247840
Epoch:   300  |  train loss: 0.0593017802
Epoch:   400  |  train loss: 0.0562055163
Epoch:   500  |  train loss: 0.0550361775
Epoch:   600  |  train loss: 0.0539781734
Epoch:   700  |  train loss: 0.0520883918
Epoch:   800  |  train loss: 0.0505222999
Epoch:   900  |  train loss: 0.0492248297
Epoch:  1000  |  train loss: 0.0484774157
Processing class: 6
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.0617652342
Epoch:   200  |  train loss: 0.0617407784
Epoch:   300  |  train loss: 0.0586372927
Epoch:   400  |  train loss: 0.0558242910
Epoch:   500  |  train loss: 0.0546064325
Epoch:   600  |  train loss: 0.0537574403
Epoch:   700  |  train loss: 0.0528635740
Epoch:   800  |  train loss: 0.0517567784
Epoch:   900  |  train loss: 0.0501172900
Epoch:  1000  |  train loss: 0.0494644769
Processing class: 7
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.0659507737
Epoch:   200  |  train loss: 0.0626709729
Epoch:   300  |  train loss: 0.0608324103
Epoch:   400  |  train loss: 0.0577396937
Epoch:   500  |  train loss: 0.0554545633
Epoch:   600  |  train loss: 0.0540514305
Epoch:   700  |  train loss: 0.0528751113
Epoch:   800  |  train loss: 0.0520901047
Epoch:   900  |  train loss: 0.0505901620
Epoch:  1000  |  train loss: 0.0497492023
Processing class: 8
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.0639288239
Epoch:   200  |  train loss: 0.0618120223
Epoch:   300  |  train loss: 0.0574199021
Epoch:   400  |  train loss: 0.0557326525
Epoch:   500  |  train loss: 0.0541107550
Epoch:   600  |  train loss: 0.0523215130
Epoch:   700  |  train loss: 0.0513994843
Epoch:   800  |  train loss: 0.0500919245
Epoch:   900  |  train loss: 0.0492850095
Epoch:  1000  |  train loss: 0.0480180435
Processing class: 9
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.0610972233
Epoch:   200  |  train loss: 0.0609569393
Epoch:   300  |  train loss: 0.0598071791
Epoch:   400  |  train loss: 0.0574190184
Epoch:   500  |  train loss: 0.0566404499
Epoch:   600  |  train loss: 0.0557323128
Epoch:   700  |  train loss: 0.0553213283
Epoch:   800  |  train loss: 0.0547732636
Epoch:   900  |  train loss: 0.0541572735
Epoch:  1000  |  train loss: 0.0531676233
Processing class: 10
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.0656884715
Epoch:   200  |  train loss: 0.0622169606
Epoch:   300  |  train loss: 0.0606385142
Epoch:   400  |  train loss: 0.0590394340
Epoch:   500  |  train loss: 0.0565640010
Epoch:   600  |  train loss: 0.0552851088
Epoch:   700  |  train loss: 0.0541375138
Epoch:   800  |  train loss: 0.0529727571
Epoch:   900  |  train loss: 0.0522497065
Epoch:  1000  |  train loss: 0.0514910787
Processing class: 11
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.0666808993
Epoch:   200  |  train loss: 0.0620006181
Epoch:   300  |  train loss: 0.0599534959
Epoch:   400  |  train loss: 0.0578446917
Epoch:   500  |  train loss: 0.0555261686
Epoch:   600  |  train loss: 0.0545114852
Epoch:   700  |  train loss: 0.0536128871
Epoch:   800  |  train loss: 0.0525056504
Epoch:   900  |  train loss: 0.0514314413
Epoch:  1000  |  train loss: 0.0504934601
Processing class: 12
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.0638554886
Epoch:   200  |  train loss: 0.0623516135
Epoch:   300  |  train loss: 0.0596287437
Epoch:   400  |  train loss: 0.0567157201
Epoch:   500  |  train loss: 0.0548716843
Epoch:   600  |  train loss: 0.0528669156
Epoch:   700  |  train loss: 0.0516640350
Epoch:   800  |  train loss: 0.0509602353
Epoch:   900  |  train loss: 0.0501803115
Epoch:  1000  |  train loss: 0.0491870567
Processing class: 13
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.0622850403
Epoch:   200  |  train loss: 0.0589000128
Epoch:   300  |  train loss: 0.0560660109
Epoch:   400  |  train loss: 0.0534317724
Epoch:   500  |  train loss: 0.0517697684
Epoch:   600  |  train loss: 0.0506912664
Epoch:   700  |  train loss: 0.0496941395
Epoch:   800  |  train loss: 0.0491327502
Epoch:   900  |  train loss: 0.0485756502
Epoch:  1000  |  train loss: 0.0478573926
Processing class: 14
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.0687280044
Epoch:   200  |  train loss: 0.0684909120
Epoch:   300  |  train loss: 0.0656730562
Epoch:   400  |  train loss: 0.0629210733
Epoch:   500  |  train loss: 0.0611596517
Epoch:   600  |  train loss: 0.0595829405
Epoch:   700  |  train loss: 0.0587510899
Epoch:   800  |  train loss: 0.0581436746
Epoch:   900  |  train loss: 0.0573696822
Epoch:  1000  |  train loss: 0.0564970903
Processing class: 15
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.0607532375
Epoch:   200  |  train loss: 0.0592123277
Epoch:   300  |  train loss: 0.0554810017
Epoch:   400  |  train loss: 0.0538443066
Epoch:   500  |  train loss: 0.0524952248
Epoch:   600  |  train loss: 0.0513436183
Epoch:   700  |  train loss: 0.0509395637
Epoch:   800  |  train loss: 0.0499323741
Epoch:   900  |  train loss: 0.0498163939
Epoch:  1000  |  train loss: 0.0490748510
Processing class: 16
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.0626947641
Epoch:   200  |  train loss: 0.0582449079
Epoch:   300  |  train loss: 0.0564355239
Epoch:   400  |  train loss: 0.0548515216
Epoch:   500  |  train loss: 0.0544512033
Epoch:   600  |  train loss: 0.0533139579
Epoch:   700  |  train loss: 0.0521205269
Epoch:   800  |  train loss: 0.0515377879
Epoch:   900  |  train loss: 0.0503606230
Epoch:  1000  |  train loss: 0.0502949208
Processing class: 17
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.0620297499
Epoch:   200  |  train loss: 0.0617935352
Epoch:   300  |  train loss: 0.0612296343
Epoch:   400  |  train loss: 0.0589211553
Epoch:   500  |  train loss: 0.0555392422
Epoch:   600  |  train loss: 0.0543265752
Epoch:   700  |  train loss: 0.0532465108
Epoch:   800  |  train loss: 0.0522824340
Epoch:   900  |  train loss: 0.0514487967
Epoch:  1000  |  train loss: 0.0507696204
Processing class: 18
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.0618860207
Epoch:   200  |  train loss: 0.0616097674
Epoch:   300  |  train loss: 0.0555643775
Epoch:   400  |  train loss: 0.0533388212
Epoch:   500  |  train loss: 0.0508009769
Epoch:   600  |  train loss: 0.0486795023
Epoch:   700  |  train loss: 0.0480274789
Epoch:   800  |  train loss: 0.0467641450
Epoch:   900  |  train loss: 0.0459732212
Epoch:  1000  |  train loss: 0.0452693962
Processing class: 19
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.0586682528
Epoch:   200  |  train loss: 0.0567126751
Epoch:   300  |  train loss: 0.0543012626
Epoch:   400  |  train loss: 0.0535090320
Epoch:   500  |  train loss: 0.0511593260
Epoch:   600  |  train loss: 0.0502863668
Epoch:   700  |  train loss: 0.0494551852
Epoch:   800  |  train loss: 0.0482439741
Epoch:   900  |  train loss: 0.0476241827
Epoch:  1000  |  train loss: 0.0468856923
Processing class: 20
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.0620496340
Epoch:   200  |  train loss: 0.0588642702
Epoch:   300  |  train loss: 0.0580346890
Epoch:   400  |  train loss: 0.0554186016
Epoch:   500  |  train loss: 0.0550377823
Epoch:   600  |  train loss: 0.0535527579
Epoch:   700  |  train loss: 0.0522853442
Epoch:   800  |  train loss: 0.0518128693
Epoch:   900  |  train loss: 0.0510301650
Epoch:  1000  |  train loss: 0.0508006155
Processing class: 21
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.0628738761
Epoch:   200  |  train loss: 0.0596278168
Epoch:   300  |  train loss: 0.0542349420
Epoch:   400  |  train loss: 0.0515577249
Epoch:   500  |  train loss: 0.0498372659
Epoch:   600  |  train loss: 0.0488259278
Epoch:   700  |  train loss: 0.0479244031
Epoch:   800  |  train loss: 0.0468538843
Epoch:   900  |  train loss: 0.0460776277
Epoch:  1000  |  train loss: 0.0449043214
Processing class: 22
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.0656062514
Epoch:   200  |  train loss: 0.0650659487
Epoch:   300  |  train loss: 0.0636622503
Epoch:   400  |  train loss: 0.0599893145
Epoch:   500  |  train loss: 0.0582854941
Epoch:   600  |  train loss: 0.0563400127
Epoch:   700  |  train loss: 0.0548672050
Epoch:   800  |  train loss: 0.0533974774
Epoch:   900  |  train loss: 0.0523439586
Epoch:  1000  |  train loss: 0.0520126596
Processing class: 23
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.0606433719
Epoch:   200  |  train loss: 0.0603573658
Epoch:   300  |  train loss: 0.0581111692
Epoch:   400  |  train loss: 0.0557599038
Epoch:   500  |  train loss: 0.0544242695
Epoch:   600  |  train loss: 0.0521769643
Epoch:   700  |  train loss: 0.0508582585
Epoch:   800  |  train loss: 0.0494165912
Epoch:   900  |  train loss: 0.0485723220
Epoch:  1000  |  train loss: 0.0472620390
Processing class: 24
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.0621967316
Epoch:   200  |  train loss: 0.0618985116
Epoch:   300  |  train loss: 0.0617959522
Epoch:   400  |  train loss: 0.0588068299
Epoch:   500  |  train loss: 0.0571960978
Epoch:   600  |  train loss: 0.0561039515
Epoch:   700  |  train loss: 0.0545391068
Epoch:   800  |  train loss: 0.0538405374
Epoch:   900  |  train loss: 0.0530581921
Epoch:  1000  |  train loss: 0.0519510686
Processing class: 25
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.0638896540
Epoch:   200  |  train loss: 0.0613718383
Epoch:   300  |  train loss: 0.0595619231
Epoch:   400  |  train loss: 0.0561156057
Epoch:   500  |  train loss: 0.0541412644
Epoch:   600  |  train loss: 0.0526609451
Epoch:   700  |  train loss: 0.0519860320
Epoch:   800  |  train loss: 0.0507787630
Epoch:   900  |  train loss: 0.0496722274
Epoch:  1000  |  train loss: 0.0492169410
Processing class: 26
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.0601973467
Epoch:   200  |  train loss: 0.0601786338
Epoch:   300  |  train loss: 0.0605713092
Epoch:   400  |  train loss: 0.0588153489
Epoch:   500  |  train loss: 0.0570984900
Epoch:   600  |  train loss: 0.0541746035
Epoch:   700  |  train loss: 0.0533424683
Epoch:   800  |  train loss: 0.0526430413
Epoch:   900  |  train loss: 0.0514653735
Epoch:  1000  |  train loss: 0.0509680308
Processing class: 27
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.0657934025
Epoch:   200  |  train loss: 0.0619797252
Epoch:   300  |  train loss: 0.0609046400
Epoch:   400  |  train loss: 0.0579189233
Epoch:   500  |  train loss: 0.0555600345
Epoch:   600  |  train loss: 0.0546432853
Epoch:   700  |  train loss: 0.0539768361
Epoch:   800  |  train loss: 0.0530569293
Epoch:   900  |  train loss: 0.0520073012
Epoch:  1000  |  train loss: 0.0503719144
Processing class: 28
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.0631565854
Epoch:   200  |  train loss: 0.0626732171
Epoch:   300  |  train loss: 0.0595523670
Epoch:   400  |  train loss: 0.0572136685
Epoch:   500  |  train loss: 0.0553966187
Epoch:   600  |  train loss: 0.0546269149
Epoch:   700  |  train loss: 0.0540963568
Epoch:   800  |  train loss: 0.0534004636
Epoch:   900  |  train loss: 0.0528832689
Epoch:  1000  |  train loss: 0.0520272762
Processing class: 29
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.0633674167
Epoch:   200  |  train loss: 0.0599028930
Epoch:   300  |  train loss: 0.0569663793
Epoch:   400  |  train loss: 0.0539656870
Epoch:   500  |  train loss: 0.0526080213
Epoch:   600  |  train loss: 0.0514800288
Epoch:   700  |  train loss: 0.0499344431
Epoch:   800  |  train loss: 0.0483536340
Epoch:   900  |  train loss: 0.0474880964
Epoch:  1000  |  train loss: 0.0466651037
Processing class: 30
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.0599226914
Epoch:   200  |  train loss: 0.0595707260
Epoch:   300  |  train loss: 0.0555833124
Epoch:   400  |  train loss: 0.0546638131
Epoch:   500  |  train loss: 0.0542110771
Epoch:   600  |  train loss: 0.0530827850
Epoch:   700  |  train loss: 0.0515488289
Epoch:   800  |  train loss: 0.0508555479
Epoch:   900  |  train loss: 0.0502134167
Epoch:  1000  |  train loss: 0.0493853979
Processing class: 31
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.0636998162
Epoch:   200  |  train loss: 0.0606288351
Epoch:   300  |  train loss: 0.0579427794
Epoch:   400  |  train loss: 0.0574865170
Epoch:   500  |  train loss: 0.0558824100
Epoch:   600  |  train loss: 0.0551172659
Epoch:   700  |  train loss: 0.0539915048
Epoch:   800  |  train loss: 0.0533746861
Epoch:   900  |  train loss: 0.0525386155
Epoch:  1000  |  train loss: 0.0514969103
Processing class: 32
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.0671774432
Epoch:   200  |  train loss: 0.0662511244
Epoch:   300  |  train loss: 0.0638648681
Epoch:   400  |  train loss: 0.0610805027
Epoch:   500  |  train loss: 0.0588984653
Epoch:   600  |  train loss: 0.0563911952
Epoch:   700  |  train loss: 0.0550479196
Epoch:   800  |  train loss: 0.0540949784
Epoch:   900  |  train loss: 0.0530425400
Epoch:  1000  |  train loss: 0.0521042861
Processing class: 33
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.0639593929
Epoch:   200  |  train loss: 0.0630083546
Epoch:   300  |  train loss: 0.0586561300
Epoch:   400  |  train loss: 0.0559607461
Epoch:   500  |  train loss: 0.0542633429
Epoch:   600  |  train loss: 0.0528416112
Epoch:   700  |  train loss: 0.0515825398
Epoch:   800  |  train loss: 0.0507052258
Epoch:   900  |  train loss: 0.0496819474
Epoch:  1000  |  train loss: 0.0485855229
Processing class: 34
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.0650909632
Epoch:   200  |  train loss: 0.0639358968
Epoch:   300  |  train loss: 0.0600145265
Epoch:   400  |  train loss: 0.0582359925
Epoch:   500  |  train loss: 0.0560398757
Epoch:   600  |  train loss: 0.0544815950
Epoch:   700  |  train loss: 0.0528866567
Epoch:   800  |  train loss: 0.0520469531
Epoch:   900  |  train loss: 0.0511314809
Epoch:  1000  |  train loss: 0.0502335533
Processing class: 35
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.0706519902
Epoch:   200  |  train loss: 0.0653667003
Epoch:   300  |  train loss: 0.0632626615
Epoch:   400  |  train loss: 0.0619119532
Epoch:   500  |  train loss: 0.0598458871
Epoch:   600  |  train loss: 0.0593071006
Epoch:   700  |  train loss: 0.0581061095
Epoch:   800  |  train loss: 0.0569957174
Epoch:   900  |  train loss: 0.0561340623
Epoch:  1000  |  train loss: 0.0551086649
Processing class: 36
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.0582040653
Epoch:   200  |  train loss: 0.0563446507
Epoch:   300  |  train loss: 0.0542018376
Epoch:   400  |  train loss: 0.0532381400
Epoch:   500  |  train loss: 0.0518739901
Epoch:   600  |  train loss: 0.0500519238
Epoch:   700  |  train loss: 0.0485053770
Epoch:   800  |  train loss: 0.0477429412
Epoch:   900  |  train loss: 0.0464106314
Epoch:  1000  |  train loss: 0.0456805751
Processing class: 37
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.0670980707
Epoch:   200  |  train loss: 0.0654546812
Epoch:   300  |  train loss: 0.0622124940
Epoch:   400  |  train loss: 0.0597534120
Epoch:   500  |  train loss: 0.0581204765
Epoch:   600  |  train loss: 0.0565362342
Epoch:   700  |  train loss: 0.0553425968
Epoch:   800  |  train loss: 0.0545965128
Epoch:   900  |  train loss: 0.0531739801
Epoch:  1000  |  train loss: 0.0524789982
Processing class: 38
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.0602739111
Epoch:   200  |  train loss: 0.0597654603
Epoch:   300  |  train loss: 0.0584314272
Epoch:   400  |  train loss: 0.0575332671
Epoch:   500  |  train loss: 0.0554147907
Epoch:   600  |  train loss: 0.0535203002
Epoch:   700  |  train loss: 0.0517742723
Epoch:   800  |  train loss: 0.0513412200
Epoch:   900  |  train loss: 0.0503431991
Epoch:  1000  |  train loss: 0.0499701060
Processing class: 39
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.0632711992
Epoch:   200  |  train loss: 0.0616791092
Epoch:   300  |  train loss: 0.0598427236
Epoch:   400  |  train loss: 0.0579738230
Epoch:   500  |  train loss: 0.0557559103
Epoch:   600  |  train loss: 0.0546332359
Epoch:   700  |  train loss: 0.0534103341
Epoch:   800  |  train loss: 0.0523693971
Epoch:   900  |  train loss: 0.0511958785
Epoch:  1000  |  train loss: 0.0502270646
Processing class: 40
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.0618278250
Epoch:   200  |  train loss: 0.0581176832
Epoch:   300  |  train loss: 0.0573635958
Epoch:   400  |  train loss: 0.0559285812
Epoch:   500  |  train loss: 0.0548106760
Epoch:   600  |  train loss: 0.0524898149
Epoch:   700  |  train loss: 0.0519143045
Epoch:   800  |  train loss: 0.0510359809
Epoch:   900  |  train loss: 0.0502397612
Epoch:  1000  |  train loss: 0.0498250984
Processing class: 41
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.0583316110
Epoch:   200  |  train loss: 0.0585904524
Epoch:   300  |  train loss: 0.0546958566
Epoch:   400  |  train loss: 0.0520663507
Epoch:   500  |  train loss: 0.0507169344
Epoch:   600  |  train loss: 0.0497639850
Epoch:   700  |  train loss: 0.0490311570
Epoch:   800  |  train loss: 0.0481785916
Epoch:   900  |  train loss: 0.0474356011
Epoch:  1000  |  train loss: 0.0468311422
Processing class: 42
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.0651671901
Epoch:   200  |  train loss: 0.0624288678
Epoch:   300  |  train loss: 0.0596952923
Epoch:   400  |  train loss: 0.0571491860
Epoch:   500  |  train loss: 0.0551841341
Epoch:   600  |  train loss: 0.0531852685
Epoch:   700  |  train loss: 0.0519452073
Epoch:   800  |  train loss: 0.0506164595
Epoch:   900  |  train loss: 0.0492826097
Epoch:  1000  |  train loss: 0.0482657254
Processing class: 43
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.0667339340
Epoch:   200  |  train loss: 0.0641061850
Epoch:   300  |  train loss: 0.0602382936
Epoch:   400  |  train loss: 0.0593364388
Epoch:   500  |  train loss: 0.0584311150
Epoch:   600  |  train loss: 0.0579757445
Epoch:   700  |  train loss: 0.0577154040
Epoch:   800  |  train loss: 0.0571095452
Epoch:   900  |  train loss: 0.0560783654
Epoch:  1000  |  train loss: 0.0556750484
Processing class: 44
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.0671495333
Epoch:   200  |  train loss: 0.0632553138
Epoch:   300  |  train loss: 0.0617219701
Epoch:   400  |  train loss: 0.0591789111
Epoch:   500  |  train loss: 0.0580191933
Epoch:   600  |  train loss: 0.0569412731
Epoch:   700  |  train loss: 0.0561519302
Epoch:   800  |  train loss: 0.0550594695
Epoch:   900  |  train loss: 0.0538284309
Epoch:  1000  |  train loss: 0.0530565701
Processing class: 45
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.0621967465
Epoch:   200  |  train loss: 0.0599003620
Epoch:   300  |  train loss: 0.0568427972
Epoch:   400  |  train loss: 0.0573746562
Epoch:   500  |  train loss: 0.0568433695
Epoch:   600  |  train loss: 0.0559910499
Epoch:   700  |  train loss: 0.0549387835
Epoch:   800  |  train loss: 0.0543186098
Epoch:   900  |  train loss: 0.0541132674
Epoch:  1000  |  train loss: 0.0537329525
Processing class: 46
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.0594496526
Epoch:   200  |  train loss: 0.0581240915
Epoch:   300  |  train loss: 0.0554349519
Epoch:   400  |  train loss: 0.0552192561
Epoch:   500  |  train loss: 0.0545598768
Epoch:   600  |  train loss: 0.0541239038
Epoch:   700  |  train loss: 0.0534328334
Epoch:   800  |  train loss: 0.0528985016
Epoch:   900  |  train loss: 0.0519179799
Epoch:  1000  |  train loss: 0.0514974765
Processing class: 47
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.0659355015
Epoch:   200  |  train loss: 0.0633993596
Epoch:   300  |  train loss: 0.0590281762
Epoch:   400  |  train loss: 0.0570302092
Epoch:   500  |  train loss: 0.0553696357
Epoch:   600  |  train loss: 0.0535194360
Epoch:   700  |  train loss: 0.0520950787
Epoch:   800  |  train loss: 0.0508019060
Epoch:   900  |  train loss: 0.0499200337
Epoch:  1000  |  train loss: 0.0490802139
Processing class: 48
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.0649342135
Epoch:   200  |  train loss: 0.0639092974
Epoch:   300  |  train loss: 0.0585109629
Epoch:   400  |  train loss: 0.0563414820
Epoch:   500  |  train loss: 0.0563253351
Epoch:   600  |  train loss: 0.0547807075
Epoch:   700  |  train loss: 0.0537559055
Epoch:   800  |  train loss: 0.0532083884
Epoch:   900  |  train loss: 0.0529983759
Epoch:  1000  |  train loss: 0.0522986114
Processing class: 49
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.0661555350
Epoch:   200  |  train loss: 0.0638152450
Epoch:   300  |  train loss: 0.0609053366
Epoch:   400  |  train loss: 0.0589971669
Epoch:   500  |  train loss: 0.0568909943
Epoch:   600  |  train loss: 0.0557876900
Epoch:   700  |  train loss: 0.0543915786
Epoch:   800  |  train loss: 0.0534817755
Epoch:   900  |  train loss: 0.0526192397
Epoch:  1000  |  train loss: 0.0517867088
/home/z1165703/FeCAM/models/base.py:87: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  vec = torch.tensor(vec).cuda()
2024-03-04 21:33:39,363 [trainer.py] => CNN: {'total': 83.44, '00-09': 87.7, '10-19': 79.6, '20-29': 84.9, '30-39': 81.0, '40-49': 84.0, 'old': 0, 'new': 83.44}
2024-03-04 21:33:39,364 [trainer.py] => No NME accuracy
2024-03-04 21:33:39,364 [trainer.py] => FeCAM: {'total': 82.9, '00-09': 86.2, '10-19': 79.7, '20-29': 85.0, '30-39': 80.6, '40-49': 83.0, 'old': 0, 'new': 82.9}
2024-03-04 21:33:39,364 [trainer.py] => CNN top1 curve: [83.44]
2024-03-04 21:33:39,365 [trainer.py] => CNN top5 curve: [96.5]
2024-03-04 21:33:39,365 [trainer.py] => FeCAM top1 curve: [82.9]
2024-03-04 21:33:39,365 [trainer.py] => FeCAM top5 curve: [95.84]

2024-03-04 21:33:39,377 [fecam.py] => Learning on 50-60
Processing class: 50
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.0743250415
Epoch:   200  |  train loss: 0.0700427458
Epoch:   300  |  train loss: 0.0674746335
Epoch:   400  |  train loss: 0.0650171876
Epoch:   500  |  train loss: 0.0622875154
Epoch:   600  |  train loss: 0.0600479856
Epoch:   700  |  train loss: 0.0585817263
Epoch:   800  |  train loss: 0.0572977617
Epoch:   900  |  train loss: 0.0564080447
Epoch:  1000  |  train loss: 0.0554950789
Processing class: 51
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.0750262514
Epoch:   200  |  train loss: 0.0703398898
Epoch:   300  |  train loss: 0.0676397890
Epoch:   400  |  train loss: 0.0656098470
Epoch:   500  |  train loss: 0.0625656702
Epoch:   600  |  train loss: 0.0601046398
Epoch:   700  |  train loss: 0.0581304282
Epoch:   800  |  train loss: 0.0570079774
Epoch:   900  |  train loss: 0.0562271856
Epoch:  1000  |  train loss: 0.0547504388
Processing class: 52
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.0790992901
Epoch:   200  |  train loss: 0.0770871952
Epoch:   300  |  train loss: 0.0741628960
Epoch:   400  |  train loss: 0.0717496023
Epoch:   500  |  train loss: 0.0694434568
Epoch:   600  |  train loss: 0.0671785265
Epoch:   700  |  train loss: 0.0657210350
Epoch:   800  |  train loss: 0.0641467325
Epoch:   900  |  train loss: 0.0626293465
Epoch:  1000  |  train loss: 0.0609957241
Processing class: 53
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.0695214272
Epoch:   200  |  train loss: 0.0650498495
Epoch:   300  |  train loss: 0.0629260652
Epoch:   400  |  train loss: 0.0606705129
Epoch:   500  |  train loss: 0.0596951120
Epoch:   600  |  train loss: 0.0588279605
Epoch:   700  |  train loss: 0.0579112463
Epoch:   800  |  train loss: 0.0563220046
Epoch:   900  |  train loss: 0.0558087550
Epoch:  1000  |  train loss: 0.0547737375
Processing class: 54
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.0650622807
Epoch:   200  |  train loss: 0.0614760198
Epoch:   300  |  train loss: 0.0580289252
Epoch:   400  |  train loss: 0.0562861241
Epoch:   500  |  train loss: 0.0547152907
Epoch:   600  |  train loss: 0.0535836875
Epoch:   700  |  train loss: 0.0523717985
Epoch:   800  |  train loss: 0.0515554354
Epoch:   900  |  train loss: 0.0508233994
Epoch:  1000  |  train loss: 0.0496748812
Processing class: 55
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.0805200517
Epoch:   200  |  train loss: 0.0790477619
Epoch:   300  |  train loss: 0.0770567670
Epoch:   400  |  train loss: 0.0754838616
Epoch:   500  |  train loss: 0.0739360154
Epoch:   600  |  train loss: 0.0718399405
Epoch:   700  |  train loss: 0.0702050045
Epoch:   800  |  train loss: 0.0688856035
Epoch:   900  |  train loss: 0.0667840779
Epoch:  1000  |  train loss: 0.0656632140
Processing class: 56
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.0730518520
Epoch:   200  |  train loss: 0.0674150452
Epoch:   300  |  train loss: 0.0615598008
Epoch:   400  |  train loss: 0.0584961079
Epoch:   500  |  train loss: 0.0558145456
Epoch:   600  |  train loss: 0.0535265438
Epoch:   700  |  train loss: 0.0518529780
Epoch:   800  |  train loss: 0.0501519486
Epoch:   900  |  train loss: 0.0491976611
Epoch:  1000  |  train loss: 0.0482755899
Processing class: 57
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.0802231997
Epoch:   200  |  train loss: 0.0788199142
Epoch:   300  |  train loss: 0.0767315671
Epoch:   400  |  train loss: 0.0745721892
Epoch:   500  |  train loss: 0.0726248398
Epoch:   600  |  train loss: 0.0707177296
Epoch:   700  |  train loss: 0.0697191298
Epoch:   800  |  train loss: 0.0681496248
Epoch:   900  |  train loss: 0.0668302953
Epoch:  1000  |  train loss: 0.0648377851
Processing class: 58
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.0753307745
Epoch:   200  |  train loss: 0.0669406146
Epoch:   300  |  train loss: 0.0631688461
Epoch:   400  |  train loss: 0.0596536241
Epoch:   500  |  train loss: 0.0576092333
Epoch:   600  |  train loss: 0.0562406875
Epoch:   700  |  train loss: 0.0545303367
Epoch:   800  |  train loss: 0.0535202727
Epoch:   900  |  train loss: 0.0522992320
Epoch:  1000  |  train loss: 0.0511394553
Processing class: 59
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.0789364174
Epoch:   200  |  train loss: 0.0761086911
Epoch:   300  |  train loss: 0.0719959199
Epoch:   400  |  train loss: 0.0687353954
Epoch:   500  |  train loss: 0.0663213030
Epoch:   600  |  train loss: 0.0646759495
Epoch:   700  |  train loss: 0.0632837981
Epoch:   800  |  train loss: 0.0620741189
Epoch:   900  |  train loss: 0.0607938170
Epoch:  1000  |  train loss: 0.0598783642
2024-03-04 21:39:21,954 [trainer.py] => CNN: {'total': 71.75, '00-09': 82.2, '10-19': 72.8, '20-29': 78.5, '30-39': 74.9, '40-49': 68.4, '50-59': 53.7, 'old': 75.36, 'new': 53.7}
2024-03-04 21:39:22,663 [trainer.py] => No NME accuracy
2024-03-04 21:39:22,663 [trainer.py] => FeCAM: {'total': 72.78, '00-09': 82.6, '10-19': 74.0, '20-29': 81.0, '30-39': 76.4, '40-49': 76.8, '50-59': 45.9, 'old': 78.16, 'new': 45.9}
2024-03-04 21:39:22,663 [trainer.py] => CNN top1 curve: [83.44, 71.75]
2024-03-04 21:39:22,663 [trainer.py] => CNN top5 curve: [96.5, 89.6]
2024-03-04 21:39:22,663 [trainer.py] => FeCAM top1 curve: [82.9, 72.78]
2024-03-04 21:39:22,664 [trainer.py] => FeCAM top5 curve: [95.84, 91.18]

2024-03-04 21:39:22,671 [fecam.py] => Learning on 60-70
Processing class: 60
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.0782232106
Epoch:   200  |  train loss: 0.0751460329
Epoch:   300  |  train loss: 0.0714550182
Epoch:   400  |  train loss: 0.0682266474
Epoch:   500  |  train loss: 0.0656732120
Epoch:   600  |  train loss: 0.0631697081
Epoch:   700  |  train loss: 0.0616386518
Epoch:   800  |  train loss: 0.0591508240
Epoch:   900  |  train loss: 0.0578433990
Epoch:  1000  |  train loss: 0.0566027060
Processing class: 61
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.0724953473
Epoch:   200  |  train loss: 0.0665389344
Epoch:   300  |  train loss: 0.0621824011
Epoch:   400  |  train loss: 0.0584574677
Epoch:   500  |  train loss: 0.0564507984
Epoch:   600  |  train loss: 0.0543228760
Epoch:   700  |  train loss: 0.0528837435
Epoch:   800  |  train loss: 0.0516010322
Epoch:   900  |  train loss: 0.0500743873
Epoch:  1000  |  train loss: 0.0493059747
Processing class: 62
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.0808335319
Epoch:   200  |  train loss: 0.0788170725
Epoch:   300  |  train loss: 0.0774440363
Epoch:   400  |  train loss: 0.0759641424
Epoch:   500  |  train loss: 0.0742985621
Epoch:   600  |  train loss: 0.0732457146
Epoch:   700  |  train loss: 0.0719451055
Epoch:   800  |  train loss: 0.0704893067
Epoch:   900  |  train loss: 0.0689268515
Epoch:  1000  |  train loss: 0.0677747115
Processing class: 63
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.0759584740
Epoch:   200  |  train loss: 0.0737726048
Epoch:   300  |  train loss: 0.0683950305
Epoch:   400  |  train loss: 0.0645869717
Epoch:   500  |  train loss: 0.0617829986
Epoch:   600  |  train loss: 0.0601069190
Epoch:   700  |  train loss: 0.0580216832
Epoch:   800  |  train loss: 0.0573617615
Epoch:   900  |  train loss: 0.0556651935
Epoch:  1000  |  train loss: 0.0547300883
Processing class: 64
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.0738331154
Epoch:   200  |  train loss: 0.0648155361
Epoch:   300  |  train loss: 0.0609799072
Epoch:   400  |  train loss: 0.0581812710
Epoch:   500  |  train loss: 0.0551758625
Epoch:   600  |  train loss: 0.0529403828
Epoch:   700  |  train loss: 0.0510743521
Epoch:   800  |  train loss: 0.0500061929
Epoch:   900  |  train loss: 0.0492225826
Epoch:  1000  |  train loss: 0.0478543863
Processing class: 65
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.0728360415
Epoch:   200  |  train loss: 0.0666246772
Epoch:   300  |  train loss: 0.0611509182
Epoch:   400  |  train loss: 0.0581402883
Epoch:   500  |  train loss: 0.0557788022
Epoch:   600  |  train loss: 0.0541842863
Epoch:   700  |  train loss: 0.0525350712
Epoch:   800  |  train loss: 0.0515161932
Epoch:   900  |  train loss: 0.0499542885
Epoch:  1000  |  train loss: 0.0492576472
Processing class: 66
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.0767296463
Epoch:   200  |  train loss: 0.0730693236
Epoch:   300  |  train loss: 0.0694847122
Epoch:   400  |  train loss: 0.0668686435
Epoch:   500  |  train loss: 0.0655428305
Epoch:   600  |  train loss: 0.0640329182
Epoch:   700  |  train loss: 0.0631648421
Epoch:   800  |  train loss: 0.0622850642
Epoch:   900  |  train loss: 0.0612733424
Epoch:  1000  |  train loss: 0.0604702696
Processing class: 67
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.0772248223
Epoch:   200  |  train loss: 0.0741299883
Epoch:   300  |  train loss: 0.0711615890
Epoch:   400  |  train loss: 0.0690222323
Epoch:   500  |  train loss: 0.0672161579
Epoch:   600  |  train loss: 0.0656391323
Epoch:   700  |  train loss: 0.0642133556
Epoch:   800  |  train loss: 0.0631395556
Epoch:   900  |  train loss: 0.0618547782
Epoch:  1000  |  train loss: 0.0607982680
Processing class: 68
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.0713406608
Epoch:   200  |  train loss: 0.0688660294
Epoch:   300  |  train loss: 0.0633654296
Epoch:   400  |  train loss: 0.0609716713
Epoch:   500  |  train loss: 0.0592892908
Epoch:   600  |  train loss: 0.0576093033
Epoch:   700  |  train loss: 0.0564253889
Epoch:   800  |  train loss: 0.0551121689
Epoch:   900  |  train loss: 0.0540655024
Epoch:  1000  |  train loss: 0.0535540298
Processing class: 69
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.0558815211
Epoch:   200  |  train loss: 0.0524495900
Epoch:   300  |  train loss: 0.0490848839
Epoch:   400  |  train loss: 0.0468525976
Epoch:   500  |  train loss: 0.0453607269
Epoch:   600  |  train loss: 0.0442386106
Epoch:   700  |  train loss: 0.0438309722
Epoch:   800  |  train loss: 0.0431058913
Epoch:   900  |  train loss: 0.0431456953
Epoch:  1000  |  train loss: 0.0421907991
2024-03-04 21:45:57,837 [trainer.py] => CNN: {'total': 64.66, '00-09': 75.0, '10-19': 70.1, '20-29': 77.1, '30-39': 70.6, '40-49': 64.0, '50-59': 45.8, '60-69': 50.0, 'old': 67.1, 'new': 50.0}
2024-03-04 21:45:57,837 [trainer.py] => No NME accuracy
2024-03-04 21:45:57,837 [trainer.py] => FeCAM: {'total': 67.27, '00-09': 79.9, '10-19': 70.6, '20-29': 79.1, '30-39': 71.9, '40-49': 74.1, '50-59': 41.2, '60-69': 54.1, 'old': 69.47, 'new': 54.1}
2024-03-04 21:45:57,837 [trainer.py] => CNN top1 curve: [83.44, 71.75, 64.66]
2024-03-04 21:45:57,837 [trainer.py] => CNN top5 curve: [96.5, 89.6, 86.54]
2024-03-04 21:45:57,837 [trainer.py] => FeCAM top1 curve: [82.9, 72.78, 67.27]
2024-03-04 21:45:57,837 [trainer.py] => FeCAM top5 curve: [95.84, 91.18, 87.21]

2024-03-04 21:45:57,841 [fecam.py] => Learning on 70-80
Processing class: 70
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.0744900078
Epoch:   200  |  train loss: 0.0711408138
Epoch:   300  |  train loss: 0.0664212674
Epoch:   400  |  train loss: 0.0639236137
Epoch:   500  |  train loss: 0.0610860586
Epoch:   600  |  train loss: 0.0591505967
Epoch:   700  |  train loss: 0.0572951376
Epoch:   800  |  train loss: 0.0550405823
Epoch:   900  |  train loss: 0.0542814940
Epoch:  1000  |  train loss: 0.0531871617
Processing class: 71
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.0781334177
Epoch:   200  |  train loss: 0.0756578803
Epoch:   300  |  train loss: 0.0708146498
Epoch:   400  |  train loss: 0.0680842549
Epoch:   500  |  train loss: 0.0656506881
Epoch:   600  |  train loss: 0.0635213099
Epoch:   700  |  train loss: 0.0615016349
Epoch:   800  |  train loss: 0.0598737977
Epoch:   900  |  train loss: 0.0584920138
Epoch:  1000  |  train loss: 0.0569387153
Processing class: 72
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.0765533462
Epoch:   200  |  train loss: 0.0736565232
Epoch:   300  |  train loss: 0.0702132404
Epoch:   400  |  train loss: 0.0672821134
Epoch:   500  |  train loss: 0.0655326948
Epoch:   600  |  train loss: 0.0634294041
Epoch:   700  |  train loss: 0.0619668528
Epoch:   800  |  train loss: 0.0603462644
Epoch:   900  |  train loss: 0.0590331018
Epoch:  1000  |  train loss: 0.0581041768
Processing class: 73
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.0765345737
Epoch:   200  |  train loss: 0.0724791974
Epoch:   300  |  train loss: 0.0678866833
Epoch:   400  |  train loss: 0.0653622359
Epoch:   500  |  train loss: 0.0645408660
Epoch:   600  |  train loss: 0.0632062964
Epoch:   700  |  train loss: 0.0616453804
Epoch:   800  |  train loss: 0.0604864292
Epoch:   900  |  train loss: 0.0592837714
Epoch:  1000  |  train loss: 0.0584519684
Processing class: 74
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.0802826911
Epoch:   200  |  train loss: 0.0793919116
Epoch:   300  |  train loss: 0.0763162166
Epoch:   400  |  train loss: 0.0733287781
Epoch:   500  |  train loss: 0.0708182290
Epoch:   600  |  train loss: 0.0685835168
Epoch:   700  |  train loss: 0.0674449131
Epoch:   800  |  train loss: 0.0653126515
Epoch:   900  |  train loss: 0.0639619529
Epoch:  1000  |  train loss: 0.0630513504
Processing class: 75
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.0771542549
Epoch:   200  |  train loss: 0.0740064964
Epoch:   300  |  train loss: 0.0703966409
Epoch:   400  |  train loss: 0.0680286452
Epoch:   500  |  train loss: 0.0661580682
Epoch:   600  |  train loss: 0.0645552784
Epoch:   700  |  train loss: 0.0622544281
Epoch:   800  |  train loss: 0.0614311486
Epoch:   900  |  train loss: 0.0602872260
Epoch:  1000  |  train loss: 0.0594156034
Processing class: 76
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.0569271997
Epoch:   200  |  train loss: 0.0521681510
Epoch:   300  |  train loss: 0.0489724204
Epoch:   400  |  train loss: 0.0474277541
Epoch:   500  |  train loss: 0.0464813732
Epoch:   600  |  train loss: 0.0455703720
Epoch:   700  |  train loss: 0.0451272815
Epoch:   800  |  train loss: 0.0437639274
Epoch:   900  |  train loss: 0.0434421010
Epoch:  1000  |  train loss: 0.0430668443
Processing class: 77
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.0723996639
Epoch:   200  |  train loss: 0.0686097056
Epoch:   300  |  train loss: 0.0642822683
Epoch:   400  |  train loss: 0.0615552060
Epoch:   500  |  train loss: 0.0583545059
Epoch:   600  |  train loss: 0.0557961106
Epoch:   700  |  train loss: 0.0539947316
Epoch:   800  |  train loss: 0.0527165741
Epoch:   900  |  train loss: 0.0513524257
Epoch:  1000  |  train loss: 0.0500387058
Processing class: 78
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.0748227626
Epoch:   200  |  train loss: 0.0690267637
Epoch:   300  |  train loss: 0.0665318534
Epoch:   400  |  train loss: 0.0636044592
Epoch:   500  |  train loss: 0.0606816582
Epoch:   600  |  train loss: 0.0589994833
Epoch:   700  |  train loss: 0.0575803302
Epoch:   800  |  train loss: 0.0562293731
Epoch:   900  |  train loss: 0.0553519994
Epoch:  1000  |  train loss: 0.0543846160
Processing class: 79
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.0806712955
Epoch:   200  |  train loss: 0.0779125139
Epoch:   300  |  train loss: 0.0762174487
Epoch:   400  |  train loss: 0.0736631289
Epoch:   500  |  train loss: 0.0717684016
Epoch:   600  |  train loss: 0.0701812834
Epoch:   700  |  train loss: 0.0684540719
Epoch:   800  |  train loss: 0.0667923570
Epoch:   900  |  train loss: 0.0654646978
Epoch:  1000  |  train loss: 0.0637251139
2024-03-04 21:53:42,353 [trainer.py] => CNN: {'total': 59.18, '00-09': 73.6, '10-19': 68.4, '20-29': 76.9, '30-39': 69.1, '40-49': 60.6, '50-59': 37.2, '60-69': 45.7, '70-79': 41.9, 'old': 61.64, 'new': 41.9}
2024-03-04 21:53:42,394 [trainer.py] => No NME accuracy
2024-03-04 21:53:42,394 [trainer.py] => FeCAM: {'total': 61.72, '00-09': 78.1, '10-19': 69.3, '20-29': 78.0, '30-39': 69.5, '40-49': 72.1, '50-59': 35.7, '60-69': 49.1, '70-79': 42.0, 'old': 64.54, 'new': 42.0}
2024-03-04 21:53:42,394 [trainer.py] => CNN top1 curve: [83.44, 71.75, 64.66, 59.18]
2024-03-04 21:53:42,394 [trainer.py] => CNN top5 curve: [96.5, 89.6, 86.54, 84.09]
2024-03-04 21:53:42,394 [trainer.py] => FeCAM top1 curve: [82.9, 72.78, 67.27, 61.72]
2024-03-04 21:53:42,394 [trainer.py] => FeCAM top5 curve: [95.84, 91.18, 87.21, 84.09]

2024-03-04 21:53:42,398 [fecam.py] => Learning on 80-90
Processing class: 80
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.0643843904
Epoch:   200  |  train loss: 0.0557363659
Epoch:   300  |  train loss: 0.0527521104
Epoch:   400  |  train loss: 0.0509184889
Epoch:   500  |  train loss: 0.0491669536
Epoch:   600  |  train loss: 0.0482105330
Epoch:   700  |  train loss: 0.0468662545
Epoch:   800  |  train loss: 0.0466796294
Epoch:   900  |  train loss: 0.0459522322
Epoch:  1000  |  train loss: 0.0454581946
Processing class: 81
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.0728134423
Epoch:   200  |  train loss: 0.0682786062
Epoch:   300  |  train loss: 0.0647660956
Epoch:   400  |  train loss: 0.0628651768
Epoch:   500  |  train loss: 0.0607152820
Epoch:   600  |  train loss: 0.0593967758
Epoch:   700  |  train loss: 0.0578274213
Epoch:   800  |  train loss: 0.0570206441
Epoch:   900  |  train loss: 0.0556474954
Epoch:  1000  |  train loss: 0.0544248670
Processing class: 82
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.0692834020
Epoch:   200  |  train loss: 0.0642852828
Epoch:   300  |  train loss: 0.0611431636
Epoch:   400  |  train loss: 0.0582940899
Epoch:   500  |  train loss: 0.0560078532
Epoch:   600  |  train loss: 0.0543488920
Epoch:   700  |  train loss: 0.0531066231
Epoch:   800  |  train loss: 0.0521239638
Epoch:   900  |  train loss: 0.0510518178
Epoch:  1000  |  train loss: 0.0500565052
Processing class: 83
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.0673102185
Epoch:   200  |  train loss: 0.0590584740
Epoch:   300  |  train loss: 0.0546697021
Epoch:   400  |  train loss: 0.0516319677
Epoch:   500  |  train loss: 0.0499027140
Epoch:   600  |  train loss: 0.0483341835
Epoch:   700  |  train loss: 0.0477270566
Epoch:   800  |  train loss: 0.0471360803
Epoch:   900  |  train loss: 0.0461233646
Epoch:  1000  |  train loss: 0.0452610880
Processing class: 84
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.0761288241
Epoch:   200  |  train loss: 0.0690781429
Epoch:   300  |  train loss: 0.0655882090
Epoch:   400  |  train loss: 0.0621222243
Epoch:   500  |  train loss: 0.0599470854
Epoch:   600  |  train loss: 0.0579526022
Epoch:   700  |  train loss: 0.0567295089
Epoch:   800  |  train loss: 0.0558133624
Epoch:   900  |  train loss: 0.0542163067
Epoch:  1000  |  train loss: 0.0530641682
Processing class: 85
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.0766749516
Epoch:   200  |  train loss: 0.0705240712
Epoch:   300  |  train loss: 0.0663137719
Epoch:   400  |  train loss: 0.0631053843
Epoch:   500  |  train loss: 0.0612173848
Epoch:   600  |  train loss: 0.0595007792
Epoch:   700  |  train loss: 0.0585075371
Epoch:   800  |  train loss: 0.0569607101
Epoch:   900  |  train loss: 0.0558543332
Epoch:  1000  |  train loss: 0.0550547466
Processing class: 86
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.0807332963
Epoch:   200  |  train loss: 0.0773986965
Epoch:   300  |  train loss: 0.0733041435
Epoch:   400  |  train loss: 0.0698797226
Epoch:   500  |  train loss: 0.0675907314
Epoch:   600  |  train loss: 0.0656121492
Epoch:   700  |  train loss: 0.0638452537
Epoch:   800  |  train loss: 0.0622782603
Epoch:   900  |  train loss: 0.0610479154
Epoch:  1000  |  train loss: 0.0603443250
Processing class: 87
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.0700203940
Epoch:   200  |  train loss: 0.0631393671
Epoch:   300  |  train loss: 0.0604211345
Epoch:   400  |  train loss: 0.0571783237
Epoch:   500  |  train loss: 0.0550925143
Epoch:   600  |  train loss: 0.0542374954
Epoch:   700  |  train loss: 0.0531968519
Epoch:   800  |  train loss: 0.0520773105
Epoch:   900  |  train loss: 0.0508829542
Epoch:  1000  |  train loss: 0.0504764795
Processing class: 88
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.0749116689
Epoch:   200  |  train loss: 0.0710911632
Epoch:   300  |  train loss: 0.0671708658
Epoch:   400  |  train loss: 0.0640693739
Epoch:   500  |  train loss: 0.0609855562
Epoch:   600  |  train loss: 0.0583500989
Epoch:   700  |  train loss: 0.0569526456
Epoch:   800  |  train loss: 0.0554961197
Epoch:   900  |  train loss: 0.0544146061
Epoch:  1000  |  train loss: 0.0531383298
Processing class: 89
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.0738577947
Epoch:   200  |  train loss: 0.0690593839
Epoch:   300  |  train loss: 0.0644113436
Epoch:   400  |  train loss: 0.0620973550
Epoch:   500  |  train loss: 0.0598666556
Epoch:   600  |  train loss: 0.0575953208
Epoch:   700  |  train loss: 0.0557064444
Epoch:   800  |  train loss: 0.0548027314
Epoch:   900  |  train loss: 0.0533119895
Epoch:  1000  |  train loss: 0.0522668891
2024-03-04 22:02:38,031 [trainer.py] => CNN: {'total': 54.08, '00-09': 67.9, '10-19': 63.6, '20-29': 73.2, '30-39': 68.8, '40-49': 57.2, '50-59': 32.6, '60-69': 40.9, '70-79': 38.1, '80-89': 44.4, 'old': 55.29, 'new': 44.4}
2024-03-04 22:02:38,033 [trainer.py] => No NME accuracy
2024-03-04 22:02:38,033 [trainer.py] => FeCAM: {'total': 57.66, '00-09': 76.1, '10-19': 65.8, '20-29': 75.3, '30-39': 67.9, '40-49': 68.5, '50-59': 33.0, '60-69': 45.0, '70-79': 39.0, '80-89': 48.3, 'old': 58.82, 'new': 48.3}
2024-03-04 22:02:38,033 [trainer.py] => CNN top1 curve: [83.44, 71.75, 64.66, 59.18, 54.08]
2024-03-04 22:02:38,033 [trainer.py] => CNN top5 curve: [96.5, 89.6, 86.54, 84.09, 81.74]
2024-03-04 22:02:38,033 [trainer.py] => FeCAM top1 curve: [82.9, 72.78, 67.27, 61.72, 57.66]
2024-03-04 22:02:38,033 [trainer.py] => FeCAM top5 curve: [95.84, 91.18, 87.21, 84.09, 81.63]

2024-03-04 22:02:38,037 [fecam.py] => Learning on 90-100
Processing class: 90
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.0764663592
Epoch:   200  |  train loss: 0.0733703509
Epoch:   300  |  train loss: 0.0681742564
Epoch:   400  |  train loss: 0.0642991289
Epoch:   500  |  train loss: 0.0612044819
Epoch:   600  |  train loss: 0.0586814485
Epoch:   700  |  train loss: 0.0562951364
Epoch:   800  |  train loss: 0.0548676021
Epoch:   900  |  train loss: 0.0538843118
Epoch:  1000  |  train loss: 0.0529997729
Processing class: 91
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.0574809678
Epoch:   200  |  train loss: 0.0543745555
Epoch:   300  |  train loss: 0.0516855657
Epoch:   400  |  train loss: 0.0505510367
Epoch:   500  |  train loss: 0.0487348840
Epoch:   600  |  train loss: 0.0475281850
Epoch:   700  |  train loss: 0.0457785159
Epoch:   800  |  train loss: 0.0444476277
Epoch:   900  |  train loss: 0.0434485719
Epoch:  1000  |  train loss: 0.0428050317
Processing class: 92
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.0728635490
Epoch:   200  |  train loss: 0.0650180504
Epoch:   300  |  train loss: 0.0621868365
Epoch:   400  |  train loss: 0.0599936612
Epoch:   500  |  train loss: 0.0572932363
Epoch:   600  |  train loss: 0.0544996940
Epoch:   700  |  train loss: 0.0525550894
Epoch:   800  |  train loss: 0.0507158197
Epoch:   900  |  train loss: 0.0496668726
Epoch:  1000  |  train loss: 0.0486596704
Processing class: 93
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.0749792963
Epoch:   200  |  train loss: 0.0681616485
Epoch:   300  |  train loss: 0.0641670287
Epoch:   400  |  train loss: 0.0616941959
Epoch:   500  |  train loss: 0.0590207957
Epoch:   600  |  train loss: 0.0565836154
Epoch:   700  |  train loss: 0.0553548746
Epoch:   800  |  train loss: 0.0537988737
Epoch:   900  |  train loss: 0.0526600264
Epoch:  1000  |  train loss: 0.0512391128
Processing class: 94
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.0683748811
Epoch:   200  |  train loss: 0.0640395910
Epoch:   300  |  train loss: 0.0584848501
Epoch:   400  |  train loss: 0.0555202506
Epoch:   500  |  train loss: 0.0528951339
Epoch:   600  |  train loss: 0.0515438870
Epoch:   700  |  train loss: 0.0503966905
Epoch:   800  |  train loss: 0.0484021641
Epoch:   900  |  train loss: 0.0477191135
Epoch:  1000  |  train loss: 0.0465854071
Processing class: 95
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.0778915539
Epoch:   200  |  train loss: 0.0758821934
Epoch:   300  |  train loss: 0.0728042319
Epoch:   400  |  train loss: 0.0693262368
Epoch:   500  |  train loss: 0.0667140767
Epoch:   600  |  train loss: 0.0646019906
Epoch:   700  |  train loss: 0.0628102757
Epoch:   800  |  train loss: 0.0610592395
Epoch:   900  |  train loss: 0.0597965293
Epoch:  1000  |  train loss: 0.0587323800
Processing class: 96
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.0789190561
Epoch:   200  |  train loss: 0.0751410365
Epoch:   300  |  train loss: 0.0709795088
Epoch:   400  |  train loss: 0.0686932102
Epoch:   500  |  train loss: 0.0660331741
Epoch:   600  |  train loss: 0.0640820011
Epoch:   700  |  train loss: 0.0623414159
Epoch:   800  |  train loss: 0.0604936562
Epoch:   900  |  train loss: 0.0591480315
Epoch:  1000  |  train loss: 0.0577425241
Processing class: 97
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.0748853818
Epoch:   200  |  train loss: 0.0682120532
Epoch:   300  |  train loss: 0.0635986820
Epoch:   400  |  train loss: 0.0611163616
Epoch:   500  |  train loss: 0.0590730384
Epoch:   600  |  train loss: 0.0573620640
Epoch:   700  |  train loss: 0.0559182681
Epoch:   800  |  train loss: 0.0545522533
Epoch:   900  |  train loss: 0.0533221811
Epoch:  1000  |  train loss: 0.0521416992
Processing class: 98
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.0685679436
Epoch:   200  |  train loss: 0.0638695955
Epoch:   300  |  train loss: 0.0590188220
Epoch:   400  |  train loss: 0.0560678363
Epoch:   500  |  train loss: 0.0536151998
Epoch:   600  |  train loss: 0.0517997630
Epoch:   700  |  train loss: 0.0501474813
Epoch:   800  |  train loss: 0.0492886148
Epoch:   900  |  train loss: 0.0483975865
Epoch:  1000  |  train loss: 0.0475997642
Processing class: 99
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.0766048461
Epoch:   200  |  train loss: 0.0721700653
Epoch:   300  |  train loss: 0.0669869468
Epoch:   400  |  train loss: 0.0636195354
Epoch:   500  |  train loss: 0.0605385393
Epoch:   600  |  train loss: 0.0584173538
Epoch:   700  |  train loss: 0.0568587676
Epoch:   800  |  train loss: 0.0557978928
Epoch:   900  |  train loss: 0.0543833077
Epoch:  1000  |  train loss: 0.0533238344
2024-03-04 22:13:03,247 [trainer.py] => CNN: {'total': 50.36, '00-09': 57.7, '10-19': 63.6, '20-29': 71.4, '30-39': 68.5, '40-49': 56.8, '50-59': 29.6, '60-69': 38.9, '70-79': 36.6, '80-89': 42.7, '90-99': 37.8, 'old': 51.76, 'new': 37.8}
2024-03-04 22:13:03,248 [trainer.py] => No NME accuracy
2024-03-04 22:13:03,248 [trainer.py] => FeCAM: {'total': 54.74, '00-09': 73.3, '10-19': 65.1, '20-29': 74.1, '30-39': 67.2, '40-49': 67.1, '50-59': 30.8, '60-69': 43.3, '70-79': 37.2, '80-89': 46.9, '90-99': 42.4, 'old': 56.11, 'new': 42.4}
2024-03-04 22:13:03,248 [trainer.py] => CNN top1 curve: [83.44, 71.75, 64.66, 59.18, 54.08, 50.36]
2024-03-04 22:13:03,248 [trainer.py] => CNN top5 curve: [96.5, 89.6, 86.54, 84.09, 81.74, 79.63]
2024-03-04 22:13:03,248 [trainer.py] => FeCAM top1 curve: [82.9, 72.78, 67.27, 61.72, 57.66, 54.74]
2024-03-04 22:13:03,248 [trainer.py] => FeCAM top5 curve: [95.84, 91.18, 87.21, 84.09, 81.63, 79.21]

=========================================
2024-03-04 22:13:13,463 [trainer.py] => config: ./exps/FeCAM_cifar100.json
2024-03-04 22:13:13,463 [trainer.py] => prefix: train
2024-03-04 22:13:13,463 [trainer.py] => dataset: cifar100
2024-03-04 22:13:13,463 [trainer.py] => memory_size: 0
2024-03-04 22:13:13,464 [trainer.py] => shuffle: True
2024-03-04 22:13:13,464 [trainer.py] => init_cls: 50
2024-03-04 22:13:13,464 [trainer.py] => increment: 10
2024-03-04 22:13:13,464 [trainer.py] => model_name: fecam
2024-03-04 22:13:13,464 [trainer.py] => convnet_type: resnet18
2024-03-04 22:13:13,464 [trainer.py] => device: [device(type='cuda', index=0)]
2024-03-04 22:13:13,464 [trainer.py] => seed: 1993
2024-03-04 22:13:13,464 [trainer.py] => init_epochs: 200
2024-03-04 22:13:13,464 [trainer.py] => init_lr: 0.1
2024-03-04 22:13:13,464 [trainer.py] => init_weight_decay: 0.0005
2024-03-04 22:13:13,464 [trainer.py] => batch_size: 128
2024-03-04 22:13:13,464 [trainer.py] => num_workers: 8
2024-03-04 22:13:13,464 [trainer.py] => T: 5
2024-03-04 22:13:13,464 [trainer.py] => beta: 0.5
2024-03-04 22:13:13,464 [trainer.py] => alpha1: 1
2024-03-04 22:13:13,464 [trainer.py] => alpha2: 1
2024-03-04 22:13:13,464 [trainer.py] => ncm: False
2024-03-04 22:13:13,464 [trainer.py] => tukey: False
2024-03-04 22:13:13,464 [trainer.py] => diagonal: False
2024-03-04 22:13:13,464 [trainer.py] => per_class: True
2024-03-04 22:13:13,464 [trainer.py] => full_cov: True
2024-03-04 22:13:13,465 [trainer.py] => shrink: True
2024-03-04 22:13:13,465 [trainer.py] => norm_cov: False
2024-03-04 22:13:13,465 [trainer.py] => vecnorm: False
2024-03-04 22:13:13,465 [trainer.py] => ae_type: wae
2024-03-04 22:13:13,465 [trainer.py] => epochs: 1000
2024-03-04 22:13:13,465 [trainer.py] => ae_latent_dim: 32
2024-03-04 22:13:13,465 [trainer.py] => wae_sigma: 10
2024-03-04 22:13:13,465 [trainer.py] => wae_C: 0.1
Files already downloaded and verified
Files already downloaded and verified
2024-03-04 22:13:15,117 [data_manager.py] => [68, 56, 78, 8, 23, 84, 90, 65, 74, 76, 40, 89, 3, 92, 55, 9, 26, 80, 43, 38, 58, 70, 77, 1, 85, 19, 17, 50, 28, 53, 13, 81, 45, 82, 6, 59, 83, 16, 15, 44, 91, 41, 72, 60, 79, 52, 20, 10, 31, 54, 37, 95, 14, 71, 96, 98, 97, 2, 64, 66, 42, 22, 35, 86, 24, 34, 87, 21, 99, 0, 88, 27, 18, 94, 11, 12, 47, 25, 30, 46, 62, 69, 36, 61, 7, 63, 75, 5, 32, 4, 51, 48, 73, 93, 39, 67, 29, 49, 57, 33]
2024-03-04 22:13:15,407 [fecam.py] => Learning on 0-50
/home/z1165703/miniconda3/lib/python3.11/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
/home/z1165703/miniconda3/lib/python3.11/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
Processing class: 0
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.0535358913
Epoch:   200  |  train loss: 0.0510571331
Epoch:   300  |  train loss: 0.0512374640
Epoch:   400  |  train loss: 0.0503644168
Epoch:   500  |  train loss: 0.0496262453
Epoch:   600  |  train loss: 0.0494880907
Epoch:   700  |  train loss: 0.0492332593
Epoch:   800  |  train loss: 0.0491254717
Epoch:   900  |  train loss: 0.0485796534
Epoch:  1000  |  train loss: 0.0481373914
Processing class: 1
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.0568843104
Epoch:   200  |  train loss: 0.0563081563
Epoch:   300  |  train loss: 0.0555403680
Epoch:   400  |  train loss: 0.0547789864
Epoch:   500  |  train loss: 0.0531950243
Epoch:   600  |  train loss: 0.0534702010
Epoch:   700  |  train loss: 0.0525811359
Epoch:   800  |  train loss: 0.0523052163
Epoch:   900  |  train loss: 0.0523143724
Epoch:  1000  |  train loss: 0.0513009332
Processing class: 2
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.0588156886
Epoch:   200  |  train loss: 0.0569802798
Epoch:   300  |  train loss: 0.0557149641
Epoch:   400  |  train loss: 0.0539443597
Epoch:   500  |  train loss: 0.0517556734
Epoch:   600  |  train loss: 0.0511114389
Epoch:   700  |  train loss: 0.0499565072
Epoch:   800  |  train loss: 0.0496662118
Epoch:   900  |  train loss: 0.0484726556
Epoch:  1000  |  train loss: 0.0475618169
Processing class: 3
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.0537270255
Epoch:   200  |  train loss: 0.0541747719
Epoch:   300  |  train loss: 0.0533933848
Epoch:   400  |  train loss: 0.0517749541
Epoch:   500  |  train loss: 0.0505512774
Epoch:   600  |  train loss: 0.0505546615
Epoch:   700  |  train loss: 0.0497295983
Epoch:   800  |  train loss: 0.0488132395
Epoch:   900  |  train loss: 0.0481370606
Epoch:  1000  |  train loss: 0.0474337175
Processing class: 4
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.0567184769
Epoch:   200  |  train loss: 0.0558554195
Epoch:   300  |  train loss: 0.0543795630
Epoch:   400  |  train loss: 0.0529920213
Epoch:   500  |  train loss: 0.0521562897
Epoch:   600  |  train loss: 0.0507174008
Epoch:   700  |  train loss: 0.0505954340
Epoch:   800  |  train loss: 0.0502031393
Epoch:   900  |  train loss: 0.0497694887
Epoch:  1000  |  train loss: 0.0495537542
Processing class: 5
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.0593341216
Epoch:   200  |  train loss: 0.0565450437
Epoch:   300  |  train loss: 0.0552938960
Epoch:   400  |  train loss: 0.0535696708
Epoch:   500  |  train loss: 0.0528410248
Epoch:   600  |  train loss: 0.0527175926
Epoch:   700  |  train loss: 0.0516650796
Epoch:   800  |  train loss: 0.0504177950
Epoch:   900  |  train loss: 0.0494926438
Epoch:  1000  |  train loss: 0.0492217362
Processing class: 6
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.0565121971
Epoch:   200  |  train loss: 0.0566695385
Epoch:   300  |  train loss: 0.0553937271
Epoch:   400  |  train loss: 0.0537207991
Epoch:   500  |  train loss: 0.0533200711
Epoch:   600  |  train loss: 0.0526477225
Epoch:   700  |  train loss: 0.0519776314
Epoch:   800  |  train loss: 0.0514356680
Epoch:   900  |  train loss: 0.0501764759
Epoch:  1000  |  train loss: 0.0499303780
Processing class: 7
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.0587508149
Epoch:   200  |  train loss: 0.0570561297
Epoch:   300  |  train loss: 0.0561624654
Epoch:   400  |  train loss: 0.0545088872
Epoch:   500  |  train loss: 0.0532872975
Epoch:   600  |  train loss: 0.0523337036
Epoch:   700  |  train loss: 0.0518105559
Epoch:   800  |  train loss: 0.0516367175
Epoch:   900  |  train loss: 0.0503786087
Epoch:  1000  |  train loss: 0.0500581495
Processing class: 8
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.0576593265
Epoch:   200  |  train loss: 0.0567787744
Epoch:   300  |  train loss: 0.0542044707
Epoch:   400  |  train loss: 0.0536665574
Epoch:   500  |  train loss: 0.0525085814
Epoch:   600  |  train loss: 0.0515096135
Epoch:   700  |  train loss: 0.0512683429
Epoch:   800  |  train loss: 0.0501492701
Epoch:   900  |  train loss: 0.0498351753
Epoch:  1000  |  train loss: 0.0488455035
Processing class: 9
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.0556936212
Epoch:   200  |  train loss: 0.0558675751
Epoch:   300  |  train loss: 0.0557055131
Epoch:   400  |  train loss: 0.0539741769
Epoch:   500  |  train loss: 0.0539089322
Epoch:   600  |  train loss: 0.0531159468
Epoch:   700  |  train loss: 0.0530407928
Epoch:   800  |  train loss: 0.0528293163
Epoch:   900  |  train loss: 0.0524710052
Epoch:  1000  |  train loss: 0.0516130559
Processing class: 10
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.0582291380
Epoch:   200  |  train loss: 0.0562162772
Epoch:   300  |  train loss: 0.0554739021
Epoch:   400  |  train loss: 0.0550718248
Epoch:   500  |  train loss: 0.0530679293
Epoch:   600  |  train loss: 0.0524791926
Epoch:   700  |  train loss: 0.0519583255
Epoch:   800  |  train loss: 0.0510985792
Epoch:   900  |  train loss: 0.0508013643
Epoch:  1000  |  train loss: 0.0503063016
Processing class: 11
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.0594647072
Epoch:   200  |  train loss: 0.0570372298
Epoch:   300  |  train loss: 0.0556539416
Epoch:   400  |  train loss: 0.0546856105
Epoch:   500  |  train loss: 0.0532690309
Epoch:   600  |  train loss: 0.0528934695
Epoch:   700  |  train loss: 0.0527366705
Epoch:   800  |  train loss: 0.0520266280
Epoch:   900  |  train loss: 0.0513494357
Epoch:  1000  |  train loss: 0.0509418830
Processing class: 12
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.0572571859
Epoch:   200  |  train loss: 0.0571265601
Epoch:   300  |  train loss: 0.0555843323
Epoch:   400  |  train loss: 0.0541239798
Epoch:   500  |  train loss: 0.0531022914
Epoch:   600  |  train loss: 0.0517130017
Epoch:   700  |  train loss: 0.0510664612
Epoch:   800  |  train loss: 0.0507361598
Epoch:   900  |  train loss: 0.0504683122
Epoch:  1000  |  train loss: 0.0496750146
Processing class: 13
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.0565191053
Epoch:   200  |  train loss: 0.0550614804
Epoch:   300  |  train loss: 0.0537322655
Epoch:   400  |  train loss: 0.0521862254
Epoch:   500  |  train loss: 0.0513823435
Epoch:   600  |  train loss: 0.0504970387
Epoch:   700  |  train loss: 0.0500005864
Epoch:   800  |  train loss: 0.0498386458
Epoch:   900  |  train loss: 0.0496052727
Epoch:  1000  |  train loss: 0.0492325284
Processing class: 14
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.0604685932
Epoch:   200  |  train loss: 0.0605627254
Epoch:   300  |  train loss: 0.0592283987
Epoch:   400  |  train loss: 0.0580539666
Epoch:   500  |  train loss: 0.0568999611
Epoch:   600  |  train loss: 0.0559758775
Epoch:   700  |  train loss: 0.0557606384
Epoch:   800  |  train loss: 0.0557995938
Epoch:   900  |  train loss: 0.0552181564
Epoch:  1000  |  train loss: 0.0544448882
Processing class: 15
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.0558449373
Epoch:   200  |  train loss: 0.0551933512
Epoch:   300  |  train loss: 0.0532652892
Epoch:   400  |  train loss: 0.0523868904
Epoch:   500  |  train loss: 0.0518242173
Epoch:   600  |  train loss: 0.0508386783
Epoch:   700  |  train loss: 0.0509085752
Epoch:   800  |  train loss: 0.0500214785
Epoch:   900  |  train loss: 0.0502792493
Epoch:  1000  |  train loss: 0.0497276589
Processing class: 16
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.0569881745
Epoch:   200  |  train loss: 0.0552069597
Epoch:   300  |  train loss: 0.0544561200
Epoch:   400  |  train loss: 0.0533477895
Epoch:   500  |  train loss: 0.0533678815
Epoch:   600  |  train loss: 0.0525323749
Epoch:   700  |  train loss: 0.0517147101
Epoch:   800  |  train loss: 0.0517313093
Epoch:   900  |  train loss: 0.0506310359
Epoch:  1000  |  train loss: 0.0509493075
Processing class: 17
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.0559915975
Epoch:   200  |  train loss: 0.0561404876
Epoch:   300  |  train loss: 0.0562325940
Epoch:   400  |  train loss: 0.0550577462
Epoch:   500  |  train loss: 0.0525862046
Epoch:   600  |  train loss: 0.0521175668
Epoch:   700  |  train loss: 0.0513805367
Epoch:   800  |  train loss: 0.0508065529
Epoch:   900  |  train loss: 0.0503722303
Epoch:  1000  |  train loss: 0.0499679685
Processing class: 18
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.0563236795
Epoch:   200  |  train loss: 0.0562600881
Epoch:   300  |  train loss: 0.0531633422
Epoch:   400  |  train loss: 0.0518369175
Epoch:   500  |  train loss: 0.0506138682
Epoch:   600  |  train loss: 0.0490939498
Epoch:   700  |  train loss: 0.0491853990
Epoch:   800  |  train loss: 0.0483079076
Epoch:   900  |  train loss: 0.0478714719
Epoch:  1000  |  train loss: 0.0476629876
Processing class: 19
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.0544240795
Epoch:   200  |  train loss: 0.0539381959
Epoch:   300  |  train loss: 0.0525937296
Epoch:   400  |  train loss: 0.0521018162
Epoch:   500  |  train loss: 0.0504770286
Epoch:   600  |  train loss: 0.0500432685
Epoch:   700  |  train loss: 0.0496488325
Epoch:   800  |  train loss: 0.0487953268
Epoch:   900  |  train loss: 0.0488952622
Epoch:  1000  |  train loss: 0.0485333078
Processing class: 20
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.0566490956
Epoch:   200  |  train loss: 0.0550995871
Epoch:   300  |  train loss: 0.0548285522
Epoch:   400  |  train loss: 0.0534480385
Epoch:   500  |  train loss: 0.0535679221
Epoch:   600  |  train loss: 0.0524892308
Epoch:   700  |  train loss: 0.0517903231
Epoch:   800  |  train loss: 0.0515207082
Epoch:   900  |  train loss: 0.0511439249
Epoch:  1000  |  train loss: 0.0512274995
Processing class: 21
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.0571889155
Epoch:   200  |  train loss: 0.0557403974
Epoch:   300  |  train loss: 0.0527029209
Epoch:   400  |  train loss: 0.0511310421
Epoch:   500  |  train loss: 0.0501879774
Epoch:   600  |  train loss: 0.0497026280
Epoch:   700  |  train loss: 0.0491024233
Epoch:   800  |  train loss: 0.0482766509
Epoch:   900  |  train loss: 0.0480278686
Epoch:  1000  |  train loss: 0.0471362948
Processing class: 22
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.0586044446
Epoch:   200  |  train loss: 0.0583461635
Epoch:   300  |  train loss: 0.0580136962
Epoch:   400  |  train loss: 0.0563504383
Epoch:   500  |  train loss: 0.0553434163
Epoch:   600  |  train loss: 0.0540381901
Epoch:   700  |  train loss: 0.0533033796
Epoch:   800  |  train loss: 0.0522644982
Epoch:   900  |  train loss: 0.0515208170
Epoch:  1000  |  train loss: 0.0517989025
Processing class: 23
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.0560434870
Epoch:   200  |  train loss: 0.0561669394
Epoch:   300  |  train loss: 0.0553352915
Epoch:   400  |  train loss: 0.0536533847
Epoch:   500  |  train loss: 0.0528046735
Epoch:   600  |  train loss: 0.0512480505
Epoch:   700  |  train loss: 0.0505673818
Epoch:   800  |  train loss: 0.0495563351
Epoch:   900  |  train loss: 0.0492151797
Epoch:  1000  |  train loss: 0.0481689721
Processing class: 24
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.0568316355
Epoch:   200  |  train loss: 0.0565954275
Epoch:   300  |  train loss: 0.0562878311
Epoch:   400  |  train loss: 0.0550645784
Epoch:   500  |  train loss: 0.0539593123
Epoch:   600  |  train loss: 0.0535399631
Epoch:   700  |  train loss: 0.0521782897
Epoch:   800  |  train loss: 0.0519184485
Epoch:   900  |  train loss: 0.0517133817
Epoch:  1000  |  train loss: 0.0508923307
Processing class: 25
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.0575584859
Epoch:   200  |  train loss: 0.0564324990
Epoch:   300  |  train loss: 0.0557122208
Epoch:   400  |  train loss: 0.0534393057
Epoch:   500  |  train loss: 0.0525587246
Epoch:   600  |  train loss: 0.0514941342
Epoch:   700  |  train loss: 0.0514220320
Epoch:   800  |  train loss: 0.0505376406
Epoch:   900  |  train loss: 0.0497100659
Epoch:  1000  |  train loss: 0.0495475508
Processing class: 26
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.0558677912
Epoch:   200  |  train loss: 0.0562042318
Epoch:   300  |  train loss: 0.0562653072
Epoch:   400  |  train loss: 0.0556472033
Epoch:   500  |  train loss: 0.0549778007
Epoch:   600  |  train loss: 0.0528025560
Epoch:   700  |  train loss: 0.0524323948
Epoch:   800  |  train loss: 0.0522010460
Epoch:   900  |  train loss: 0.0513803452
Epoch:  1000  |  train loss: 0.0513481490
Processing class: 27
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.0588639364
Epoch:   200  |  train loss: 0.0565470785
Epoch:   300  |  train loss: 0.0561369896
Epoch:   400  |  train loss: 0.0545366175
Epoch:   500  |  train loss: 0.0533513807
Epoch:   600  |  train loss: 0.0528534502
Epoch:   700  |  train loss: 0.0525008284
Epoch:   800  |  train loss: 0.0519370727
Epoch:   900  |  train loss: 0.0514243089
Epoch:  1000  |  train loss: 0.0500653729
Processing class: 28
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.0576060392
Epoch:   200  |  train loss: 0.0576326355
Epoch:   300  |  train loss: 0.0561531149
Epoch:   400  |  train loss: 0.0549548507
Epoch:   500  |  train loss: 0.0540280543
Epoch:   600  |  train loss: 0.0535336122
Epoch:   700  |  train loss: 0.0533287749
Epoch:   800  |  train loss: 0.0530432455
Epoch:   900  |  train loss: 0.0527951963
Epoch:  1000  |  train loss: 0.0520676695
Processing class: 29
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.0574982889
Epoch:   200  |  train loss: 0.0557011932
Epoch:   300  |  train loss: 0.0539742328
Epoch:   400  |  train loss: 0.0520482033
Epoch:   500  |  train loss: 0.0515185684
Epoch:   600  |  train loss: 0.0509075135
Epoch:   700  |  train loss: 0.0499387115
Epoch:   800  |  train loss: 0.0487490207
Epoch:   900  |  train loss: 0.0482380845
Epoch:  1000  |  train loss: 0.0476253875
Processing class: 30
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.0558021531
Epoch:   200  |  train loss: 0.0559007674
Epoch:   300  |  train loss: 0.0541739494
Epoch:   400  |  train loss: 0.0540120199
Epoch:   500  |  train loss: 0.0535744742
Epoch:   600  |  train loss: 0.0528062798
Epoch:   700  |  train loss: 0.0517822988
Epoch:   800  |  train loss: 0.0516516358
Epoch:   900  |  train loss: 0.0514510170
Epoch:  1000  |  train loss: 0.0506743200
Processing class: 31
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.0578629643
Epoch:   200  |  train loss: 0.0565860860
Epoch:   300  |  train loss: 0.0550379045
Epoch:   400  |  train loss: 0.0551319622
Epoch:   500  |  train loss: 0.0543672085
Epoch:   600  |  train loss: 0.0541494928
Epoch:   700  |  train loss: 0.0533917680
Epoch:   800  |  train loss: 0.0530586720
Epoch:   900  |  train loss: 0.0528036945
Epoch:  1000  |  train loss: 0.0518646866
Processing class: 32
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.0596137717
Epoch:   200  |  train loss: 0.0594771937
Epoch:   300  |  train loss: 0.0581092708
Epoch:   400  |  train loss: 0.0562512517
Epoch:   500  |  train loss: 0.0556110248
Epoch:   600  |  train loss: 0.0539102972
Epoch:   700  |  train loss: 0.0535379365
Epoch:   800  |  train loss: 0.0530680075
Epoch:   900  |  train loss: 0.0523284361
Epoch:  1000  |  train loss: 0.0516127504
Processing class: 33
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.0572413430
Epoch:   200  |  train loss: 0.0577644214
Epoch:   300  |  train loss: 0.0546935707
Epoch:   400  |  train loss: 0.0531747445
Epoch:   500  |  train loss: 0.0521604449
Epoch:   600  |  train loss: 0.0517765038
Epoch:   700  |  train loss: 0.0507543892
Epoch:   800  |  train loss: 0.0503075860
Epoch:   900  |  train loss: 0.0496166527
Epoch:  1000  |  train loss: 0.0488249138
Processing class: 34
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.0585228965
Epoch:   200  |  train loss: 0.0577606246
Epoch:   300  |  train loss: 0.0559591167
Epoch:   400  |  train loss: 0.0547379561
Epoch:   500  |  train loss: 0.0534300387
Epoch:   600  |  train loss: 0.0528196372
Epoch:   700  |  train loss: 0.0515684739
Epoch:   800  |  train loss: 0.0513375506
Epoch:   900  |  train loss: 0.0508697823
Epoch:  1000  |  train loss: 0.0503960572
Processing class: 35
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.0612564914
Epoch:   200  |  train loss: 0.0588549949
Epoch:   300  |  train loss: 0.0580076560
Epoch:   400  |  train loss: 0.0573289953
Epoch:   500  |  train loss: 0.0559364580
Epoch:   600  |  train loss: 0.0558596179
Epoch:   700  |  train loss: 0.0552162871
Epoch:   800  |  train loss: 0.0545440361
Epoch:   900  |  train loss: 0.0541139096
Epoch:  1000  |  train loss: 0.0533703454
Processing class: 36
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.0542447314
Epoch:   200  |  train loss: 0.0532482594
Epoch:   300  |  train loss: 0.0521997713
Epoch:   400  |  train loss: 0.0516197354
Epoch:   500  |  train loss: 0.0511346593
Epoch:   600  |  train loss: 0.0498453259
Epoch:   700  |  train loss: 0.0486830533
Epoch:   800  |  train loss: 0.0485775456
Epoch:   900  |  train loss: 0.0473781891
Epoch:  1000  |  train loss: 0.0471193202
Processing class: 37
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.0598585613
Epoch:   200  |  train loss: 0.0588869303
Epoch:   300  |  train loss: 0.0570684679
Epoch:   400  |  train loss: 0.0559123509
Epoch:   500  |  train loss: 0.0551303230
Epoch:   600  |  train loss: 0.0539629668
Epoch:   700  |  train loss: 0.0532676287
Epoch:   800  |  train loss: 0.0529429048
Epoch:   900  |  train loss: 0.0518712342
Epoch:  1000  |  train loss: 0.0515993737
Processing class: 38
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.0555410303
Epoch:   200  |  train loss: 0.0553794749
Epoch:   300  |  train loss: 0.0548988156
Epoch:   400  |  train loss: 0.0546176992
Epoch:   500  |  train loss: 0.0534222953
Epoch:   600  |  train loss: 0.0521630265
Epoch:   700  |  train loss: 0.0511416510
Epoch:   800  |  train loss: 0.0511529297
Epoch:   900  |  train loss: 0.0501692347
Epoch:  1000  |  train loss: 0.0501589648
Processing class: 39
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.0573220469
Epoch:   200  |  train loss: 0.0563189231
Epoch:   300  |  train loss: 0.0555178136
Epoch:   400  |  train loss: 0.0546188712
Epoch:   500  |  train loss: 0.0532486193
Epoch:   600  |  train loss: 0.0526936606
Epoch:   700  |  train loss: 0.0520581216
Epoch:   800  |  train loss: 0.0515958875
Epoch:   900  |  train loss: 0.0509278774
Epoch:  1000  |  train loss: 0.0503902048
Processing class: 40
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.0565888733
Epoch:   200  |  train loss: 0.0541788317
Epoch:   300  |  train loss: 0.0543064170
Epoch:   400  |  train loss: 0.0536516860
Epoch:   500  |  train loss: 0.0531320117
Epoch:   600  |  train loss: 0.0515210599
Epoch:   700  |  train loss: 0.0513351515
Epoch:   800  |  train loss: 0.0507859267
Epoch:   900  |  train loss: 0.0501577482
Epoch:  1000  |  train loss: 0.0503862120
Processing class: 41
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.0544081457
Epoch:   200  |  train loss: 0.0546291545
Epoch:   300  |  train loss: 0.0526548766
Epoch:   400  |  train loss: 0.0508783080
Epoch:   500  |  train loss: 0.0500120535
Epoch:   600  |  train loss: 0.0494323529
Epoch:   700  |  train loss: 0.0492138632
Epoch:   800  |  train loss: 0.0485522747
Epoch:   900  |  train loss: 0.0480029359
Epoch:  1000  |  train loss: 0.0476025768
Processing class: 42
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.0582455836
Epoch:   200  |  train loss: 0.0571909167
Epoch:   300  |  train loss: 0.0558881931
Epoch:   400  |  train loss: 0.0544594005
Epoch:   500  |  train loss: 0.0534346223
Epoch:   600  |  train loss: 0.0523418359
Epoch:   700  |  train loss: 0.0517050043
Epoch:   800  |  train loss: 0.0509164587
Epoch:   900  |  train loss: 0.0499057524
Epoch:  1000  |  train loss: 0.0492382318
Processing class: 43
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.0592813194
Epoch:   200  |  train loss: 0.0578226335
Epoch:   300  |  train loss: 0.0561761573
Epoch:   400  |  train loss: 0.0561881252
Epoch:   500  |  train loss: 0.0557986990
Epoch:   600  |  train loss: 0.0554121561
Epoch:   700  |  train loss: 0.0554660738
Epoch:   800  |  train loss: 0.0550121285
Epoch:   900  |  train loss: 0.0540772572
Epoch:  1000  |  train loss: 0.0541962393
Processing class: 44
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.0593296781
Epoch:   200  |  train loss: 0.0576900907
Epoch:   300  |  train loss: 0.0566892311
Epoch:   400  |  train loss: 0.0553314261
Epoch:   500  |  train loss: 0.0548381701
Epoch:   600  |  train loss: 0.0540966213
Epoch:   700  |  train loss: 0.0539264627
Epoch:   800  |  train loss: 0.0531871170
Epoch:   900  |  train loss: 0.0523287170
Epoch:  1000  |  train loss: 0.0521547958
Processing class: 45
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.0568235129
Epoch:   200  |  train loss: 0.0561519213
Epoch:   300  |  train loss: 0.0541517943
Epoch:   400  |  train loss: 0.0548381507
Epoch:   500  |  train loss: 0.0544583522
Epoch:   600  |  train loss: 0.0544001833
Epoch:   700  |  train loss: 0.0538596407
Epoch:   800  |  train loss: 0.0533767886
Epoch:   900  |  train loss: 0.0533157416
Epoch:  1000  |  train loss: 0.0532874674
Processing class: 46
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.0553352654
Epoch:   200  |  train loss: 0.0545168534
Epoch:   300  |  train loss: 0.0531233959
Epoch:   400  |  train loss: 0.0531399377
Epoch:   500  |  train loss: 0.0527983807
Epoch:   600  |  train loss: 0.0525945850
Epoch:   700  |  train loss: 0.0522569247
Epoch:   800  |  train loss: 0.0522376128
Epoch:   900  |  train loss: 0.0514253743
Epoch:  1000  |  train loss: 0.0513609178
Processing class: 47
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.0587253973
Epoch:   200  |  train loss: 0.0577161044
Epoch:   300  |  train loss: 0.0550565757
Epoch:   400  |  train loss: 0.0542243659
Epoch:   500  |  train loss: 0.0532282814
Epoch:   600  |  train loss: 0.0520791538
Epoch:   700  |  train loss: 0.0512810655
Epoch:   800  |  train loss: 0.0502765350
Epoch:   900  |  train loss: 0.0498016700
Epoch:  1000  |  train loss: 0.0491278462
Processing class: 48
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.0583267048
Epoch:   200  |  train loss: 0.0581150763
Epoch:   300  |  train loss: 0.0550565913
Epoch:   400  |  train loss: 0.0534645297
Epoch:   500  |  train loss: 0.0539408222
Epoch:   600  |  train loss: 0.0530238286
Epoch:   700  |  train loss: 0.0521412723
Epoch:   800  |  train loss: 0.0518224835
Epoch:   900  |  train loss: 0.0521675833
Epoch:  1000  |  train loss: 0.0518207565
Processing class: 49
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.0589580223
Epoch:   200  |  train loss: 0.0578358635
Epoch:   300  |  train loss: 0.0567804717
Epoch:   400  |  train loss: 0.0557605021
Epoch:   500  |  train loss: 0.0546644963
Epoch:   600  |  train loss: 0.0540561408
Epoch:   700  |  train loss: 0.0532124713
Epoch:   800  |  train loss: 0.0525341012
Epoch:   900  |  train loss: 0.0519627102
Epoch:  1000  |  train loss: 0.0515428133
/home/z1165703/FeCAM/models/base.py:87: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  vec = torch.tensor(vec).cuda()
2024-03-04 22:30:49,706 [trainer.py] => CNN: {'total': 83.44, '00-09': 87.7, '10-19': 79.6, '20-29': 84.9, '30-39': 81.0, '40-49': 84.0, 'old': 0, 'new': 83.44}
2024-03-04 22:30:49,708 [trainer.py] => No NME accuracy
2024-03-04 22:30:49,708 [trainer.py] => FeCAM: {'total': 82.9, '00-09': 86.2, '10-19': 79.7, '20-29': 85.0, '30-39': 80.6, '40-49': 83.0, 'old': 0, 'new': 82.9}
2024-03-04 22:30:49,708 [trainer.py] => CNN top1 curve: [83.44]
2024-03-04 22:30:49,708 [trainer.py] => CNN top5 curve: [96.5]
2024-03-04 22:30:49,708 [trainer.py] => FeCAM top1 curve: [82.9]
2024-03-04 22:30:49,708 [trainer.py] => FeCAM top5 curve: [95.84]

2024-03-04 22:30:49,720 [fecam.py] => Learning on 50-60
Processing class: 50
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.0629597194
Epoch:   200  |  train loss: 0.0609419629
Epoch:   300  |  train loss: 0.0596232496
Epoch:   400  |  train loss: 0.0586529121
Epoch:   500  |  train loss: 0.0572777070
Epoch:   600  |  train loss: 0.0558011249
Epoch:   700  |  train loss: 0.0552496031
Epoch:   800  |  train loss: 0.0546415888
Epoch:   900  |  train loss: 0.0543163963
Epoch:  1000  |  train loss: 0.0538132995
Processing class: 51
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.0635122791
Epoch:   200  |  train loss: 0.0608046979
Epoch:   300  |  train loss: 0.0599915408
Epoch:   400  |  train loss: 0.0589884579
Epoch:   500  |  train loss: 0.0573743321
Epoch:   600  |  train loss: 0.0561296836
Epoch:   700  |  train loss: 0.0547210045
Epoch:   800  |  train loss: 0.0541094162
Epoch:   900  |  train loss: 0.0540171266
Epoch:  1000  |  train loss: 0.0529072843
Processing class: 52
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.0655815586
Epoch:   200  |  train loss: 0.0644712895
Epoch:   300  |  train loss: 0.0633132026
Epoch:   400  |  train loss: 0.0622961588
Epoch:   500  |  train loss: 0.0610538505
Epoch:   600  |  train loss: 0.0596308552
Epoch:   700  |  train loss: 0.0592096739
Epoch:   800  |  train loss: 0.0583770633
Epoch:   900  |  train loss: 0.0576120563
Epoch:  1000  |  train loss: 0.0564757481
Processing class: 53
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.0605225869
Epoch:   200  |  train loss: 0.0582565151
Epoch:   300  |  train loss: 0.0567417979
Epoch:   400  |  train loss: 0.0556269258
Epoch:   500  |  train loss: 0.0552424923
Epoch:   600  |  train loss: 0.0550135106
Epoch:   700  |  train loss: 0.0547549471
Epoch:   800  |  train loss: 0.0532221004
Epoch:   900  |  train loss: 0.0532348372
Epoch:  1000  |  train loss: 0.0523625545
Processing class: 54
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.0569687024
Epoch:   200  |  train loss: 0.0550829329
Epoch:   300  |  train loss: 0.0533780754
Epoch:   400  |  train loss: 0.0524123788
Epoch:   500  |  train loss: 0.0516293176
Epoch:   600  |  train loss: 0.0512815103
Epoch:   700  |  train loss: 0.0506379806
Epoch:   800  |  train loss: 0.0503910355
Epoch:   900  |  train loss: 0.0501232706
Epoch:  1000  |  train loss: 0.0492736205
Processing class: 55
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.0659485996
Epoch:   200  |  train loss: 0.0653253585
Epoch:   300  |  train loss: 0.0641885921
Epoch:   400  |  train loss: 0.0638744101
Epoch:   500  |  train loss: 0.0636605464
Epoch:   600  |  train loss: 0.0627096750
Epoch:   700  |  train loss: 0.0620535150
Epoch:   800  |  train loss: 0.0617623292
Epoch:   900  |  train loss: 0.0601400398
Epoch:  1000  |  train loss: 0.0598239146
Processing class: 56
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.0622062705
Epoch:   200  |  train loss: 0.0594752721
Epoch:   300  |  train loss: 0.0558250561
Epoch:   400  |  train loss: 0.0545588858
Epoch:   500  |  train loss: 0.0530446656
Epoch:   600  |  train loss: 0.0516740441
Epoch:   700  |  train loss: 0.0507325947
Epoch:   800  |  train loss: 0.0494997554
Epoch:   900  |  train loss: 0.0491259910
Epoch:  1000  |  train loss: 0.0486752249
Processing class: 57
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.0655600265
Epoch:   200  |  train loss: 0.0653896168
Epoch:   300  |  train loss: 0.0642710432
Epoch:   400  |  train loss: 0.0632692449
Epoch:   500  |  train loss: 0.0626142636
Epoch:   600  |  train loss: 0.0613046318
Epoch:   700  |  train loss: 0.0612871483
Epoch:   800  |  train loss: 0.0604353756
Epoch:   900  |  train loss: 0.0600154892
Epoch:  1000  |  train loss: 0.0585127451
Processing class: 58
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.0634427518
Epoch:   200  |  train loss: 0.0589696132
Epoch:   300  |  train loss: 0.0571860343
Epoch:   400  |  train loss: 0.0555545494
Epoch:   500  |  train loss: 0.0542415559
Epoch:   600  |  train loss: 0.0537058830
Epoch:   700  |  train loss: 0.0525484331
Epoch:   800  |  train loss: 0.0521743141
Epoch:   900  |  train loss: 0.0514250785
Epoch:  1000  |  train loss: 0.0506799109
Processing class: 59
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.0656336471
Epoch:   200  |  train loss: 0.0648287863
Epoch:   300  |  train loss: 0.0624876484
Epoch:   400  |  train loss: 0.0609548442
Epoch:   500  |  train loss: 0.0599483185
Epoch:   600  |  train loss: 0.0591018543
Epoch:   700  |  train loss: 0.0584980816
Epoch:   800  |  train loss: 0.0578665175
Epoch:   900  |  train loss: 0.0570705839
Epoch:  1000  |  train loss: 0.0564977802
2024-03-04 22:36:39,139 [trainer.py] => CNN: {'total': 71.75, '00-09': 82.2, '10-19': 72.8, '20-29': 78.5, '30-39': 74.9, '40-49': 68.4, '50-59': 53.7, 'old': 75.36, 'new': 53.7}
2024-03-04 22:36:39,140 [trainer.py] => No NME accuracy
2024-03-04 22:36:39,140 [trainer.py] => FeCAM: {'total': 72.78, '00-09': 82.6, '10-19': 74.0, '20-29': 81.0, '30-39': 76.4, '40-49': 76.8, '50-59': 45.9, 'old': 78.16, 'new': 45.9}
2024-03-04 22:36:39,140 [trainer.py] => CNN top1 curve: [83.44, 71.75]
2024-03-04 22:36:39,140 [trainer.py] => CNN top5 curve: [96.5, 89.6]
2024-03-04 22:36:39,140 [trainer.py] => FeCAM top1 curve: [82.9, 72.78]
2024-03-04 22:36:39,140 [trainer.py] => FeCAM top5 curve: [95.84, 91.18]

2024-03-04 22:36:39,144 [fecam.py] => Learning on 60-70
Processing class: 60
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.0654257149
Epoch:   200  |  train loss: 0.0639548600
Epoch:   300  |  train loss: 0.0618396603
Epoch:   400  |  train loss: 0.0600724794
Epoch:   500  |  train loss: 0.0590744011
Epoch:   600  |  train loss: 0.0576887928
Epoch:   700  |  train loss: 0.0576046728
Epoch:   800  |  train loss: 0.0556696273
Epoch:   900  |  train loss: 0.0549570605
Epoch:  1000  |  train loss: 0.0544201732
Processing class: 61
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.0618636861
Epoch:   200  |  train loss: 0.0584916286
Epoch:   300  |  train loss: 0.0564544760
Epoch:   400  |  train loss: 0.0543376535
Epoch:   500  |  train loss: 0.0536011547
Epoch:   600  |  train loss: 0.0519917428
Epoch:   700  |  train loss: 0.0514441922
Epoch:   800  |  train loss: 0.0509006582
Epoch:   900  |  train loss: 0.0495338440
Epoch:  1000  |  train loss: 0.0492451154
Processing class: 62
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.0664251983
Epoch:   200  |  train loss: 0.0652668819
Epoch:   300  |  train loss: 0.0649676323
Epoch:   400  |  train loss: 0.0644060388
Epoch:   500  |  train loss: 0.0633465901
Epoch:   600  |  train loss: 0.0632648289
Epoch:   700  |  train loss: 0.0627988867
Epoch:   800  |  train loss: 0.0621152095
Epoch:   900  |  train loss: 0.0611454166
Epoch:  1000  |  train loss: 0.0606956773
Processing class: 63
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.0634654462
Epoch:   200  |  train loss: 0.0625897788
Epoch:   300  |  train loss: 0.0605334491
Epoch:   400  |  train loss: 0.0585184120
Epoch:   500  |  train loss: 0.0570835374
Epoch:   600  |  train loss: 0.0565459013
Epoch:   700  |  train loss: 0.0548628457
Epoch:   800  |  train loss: 0.0551004708
Epoch:   900  |  train loss: 0.0536879711
Epoch:  1000  |  train loss: 0.0532651149
Processing class: 64
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.0633165807
Epoch:   200  |  train loss: 0.0582102053
Epoch:   300  |  train loss: 0.0566786103
Epoch:   400  |  train loss: 0.0551017188
Epoch:   500  |  train loss: 0.0532779530
Epoch:   600  |  train loss: 0.0518920362
Epoch:   700  |  train loss: 0.0504215837
Epoch:   800  |  train loss: 0.0499387287
Epoch:   900  |  train loss: 0.0497463293
Epoch:  1000  |  train loss: 0.0487313211
Processing class: 65
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.0622311354
Epoch:   200  |  train loss: 0.0588263653
Epoch:   300  |  train loss: 0.0564789996
Epoch:   400  |  train loss: 0.0549649157
Epoch:   500  |  train loss: 0.0534286045
Epoch:   600  |  train loss: 0.0529442742
Epoch:   700  |  train loss: 0.0519627795
Epoch:   800  |  train loss: 0.0517491803
Epoch:   900  |  train loss: 0.0505162515
Epoch:  1000  |  train loss: 0.0504947841
Processing class: 66
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.0646053702
Epoch:   200  |  train loss: 0.0630442232
Epoch:   300  |  train loss: 0.0614495613
Epoch:   400  |  train loss: 0.0600283720
Epoch:   500  |  train loss: 0.0596562862
Epoch:   600  |  train loss: 0.0588370271
Epoch:   700  |  train loss: 0.0586855844
Epoch:   800  |  train loss: 0.0582312450
Epoch:   900  |  train loss: 0.0575947084
Epoch:  1000  |  train loss: 0.0571202666
Processing class: 67
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.0641239956
Epoch:   200  |  train loss: 0.0629519157
Epoch:   300  |  train loss: 0.0614442423
Epoch:   400  |  train loss: 0.0604281634
Epoch:   500  |  train loss: 0.0594581947
Epoch:   600  |  train loss: 0.0586406305
Epoch:   700  |  train loss: 0.0579875901
Epoch:   800  |  train loss: 0.0576173224
Epoch:   900  |  train loss: 0.0568494059
Epoch:  1000  |  train loss: 0.0562308125
Processing class: 68
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.0610397831
Epoch:   200  |  train loss: 0.0598197207
Epoch:   300  |  train loss: 0.0566419721
Epoch:   400  |  train loss: 0.0553950027
Epoch:   500  |  train loss: 0.0544956885
Epoch:   600  |  train loss: 0.0534404472
Epoch:   700  |  train loss: 0.0529161468
Epoch:   800  |  train loss: 0.0519670323
Epoch:   900  |  train loss: 0.0514067218
Epoch:  1000  |  train loss: 0.0515240029
Processing class: 69
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.0521649487
Epoch:   200  |  train loss: 0.0503718957
Epoch:   300  |  train loss: 0.0487363942
Epoch:   400  |  train loss: 0.0475084037
Epoch:   500  |  train loss: 0.0465454176
Epoch:   600  |  train loss: 0.0454924561
Epoch:   700  |  train loss: 0.0454677835
Epoch:   800  |  train loss: 0.0449656554
Epoch:   900  |  train loss: 0.0454429753
Epoch:  1000  |  train loss: 0.0443631850
2024-03-04 22:43:15,029 [trainer.py] => CNN: {'total': 64.66, '00-09': 75.0, '10-19': 70.1, '20-29': 77.1, '30-39': 70.6, '40-49': 64.0, '50-59': 45.8, '60-69': 50.0, 'old': 67.1, 'new': 50.0}
2024-03-04 22:43:15,029 [trainer.py] => No NME accuracy
2024-03-04 22:43:15,029 [trainer.py] => FeCAM: {'total': 67.27, '00-09': 79.9, '10-19': 70.6, '20-29': 79.1, '30-39': 71.9, '40-49': 74.1, '50-59': 41.2, '60-69': 54.1, 'old': 69.47, 'new': 54.1}
2024-03-04 22:43:15,029 [trainer.py] => CNN top1 curve: [83.44, 71.75, 64.66]
2024-03-04 22:43:15,030 [trainer.py] => CNN top5 curve: [96.5, 89.6, 86.54]
2024-03-04 22:43:15,030 [trainer.py] => FeCAM top1 curve: [82.9, 72.78, 67.27]
2024-03-04 22:43:15,030 [trainer.py] => FeCAM top5 curve: [95.84, 91.18, 87.21]

2024-03-04 22:43:15,033 [fecam.py] => Learning on 70-80
Processing class: 70
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.0628921196
Epoch:   200  |  train loss: 0.0610431768
Epoch:   300  |  train loss: 0.0587341040
Epoch:   400  |  train loss: 0.0575938545
Epoch:   500  |  train loss: 0.0557544619
Epoch:   600  |  train loss: 0.0550267234
Epoch:   700  |  train loss: 0.0543020092
Epoch:   800  |  train loss: 0.0525079168
Epoch:   900  |  train loss: 0.0523575567
Epoch:  1000  |  train loss: 0.0515639566
Processing class: 71
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.0649451688
Epoch:   200  |  train loss: 0.0639796361
Epoch:   300  |  train loss: 0.0612818904
Epoch:   400  |  train loss: 0.0601736225
Epoch:   500  |  train loss: 0.0590857014
Epoch:   600  |  train loss: 0.0579805903
Epoch:   700  |  train loss: 0.0569237113
Epoch:   800  |  train loss: 0.0561083898
Epoch:   900  |  train loss: 0.0556304842
Epoch:  1000  |  train loss: 0.0546449058
Processing class: 72
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.0638096727
Epoch:   200  |  train loss: 0.0625678420
Epoch:   300  |  train loss: 0.0609461382
Epoch:   400  |  train loss: 0.0593728341
Epoch:   500  |  train loss: 0.0586252674
Epoch:   600  |  train loss: 0.0572615281
Epoch:   700  |  train loss: 0.0566152342
Epoch:   800  |  train loss: 0.0556111917
Epoch:   900  |  train loss: 0.0549631149
Epoch:  1000  |  train loss: 0.0546998918
Processing class: 73
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.0643290997
Epoch:   200  |  train loss: 0.0622494243
Epoch:   300  |  train loss: 0.0603850357
Epoch:   400  |  train loss: 0.0588143252
Epoch:   500  |  train loss: 0.0589218237
Epoch:   600  |  train loss: 0.0581811152
Epoch:   700  |  train loss: 0.0570365071
Epoch:   800  |  train loss: 0.0564340115
Epoch:   900  |  train loss: 0.0557613038
Epoch:  1000  |  train loss: 0.0554399759
Processing class: 74
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.0662783027
Epoch:   200  |  train loss: 0.0661419183
Epoch:   300  |  train loss: 0.0642992109
Epoch:   400  |  train loss: 0.0626531996
Epoch:   500  |  train loss: 0.0616404757
Epoch:   600  |  train loss: 0.0603528440
Epoch:   700  |  train loss: 0.0603024095
Epoch:   800  |  train loss: 0.0586750090
Epoch:   900  |  train loss: 0.0580191195
Epoch:  1000  |  train loss: 0.0578879140
Processing class: 75
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.0643404484
Epoch:   200  |  train loss: 0.0627500027
Epoch:   300  |  train loss: 0.0613815606
Epoch:   400  |  train loss: 0.0600632973
Epoch:   500  |  train loss: 0.0593848765
Epoch:   600  |  train loss: 0.0588164911
Epoch:   700  |  train loss: 0.0567737520
Epoch:   800  |  train loss: 0.0568294644
Epoch:   900  |  train loss: 0.0561331943
Epoch:  1000  |  train loss: 0.0559426174
Processing class: 76
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.0519725725
Epoch:   200  |  train loss: 0.0492111817
Epoch:   300  |  train loss: 0.0477748960
Epoch:   400  |  train loss: 0.0471133105
Epoch:   500  |  train loss: 0.0466506407
Epoch:   600  |  train loss: 0.0463162675
Epoch:   700  |  train loss: 0.0461414173
Epoch:   800  |  train loss: 0.0449265122
Epoch:   900  |  train loss: 0.0450922497
Epoch:  1000  |  train loss: 0.0450440980
Processing class: 77
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.0618297391
Epoch:   200  |  train loss: 0.0599399231
Epoch:   300  |  train loss: 0.0573265664
Epoch:   400  |  train loss: 0.0559889868
Epoch:   500  |  train loss: 0.0541993879
Epoch:   600  |  train loss: 0.0527051859
Epoch:   700  |  train loss: 0.0517341495
Epoch:   800  |  train loss: 0.0512853011
Epoch:   900  |  train loss: 0.0502984583
Epoch:  1000  |  train loss: 0.0494594231
Processing class: 78
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.0634668946
Epoch:   200  |  train loss: 0.0608348534
Epoch:   300  |  train loss: 0.0597716145
Epoch:   400  |  train loss: 0.0581542492
Epoch:   500  |  train loss: 0.0564258970
Epoch:   600  |  train loss: 0.0557509422
Epoch:   700  |  train loss: 0.0549577415
Epoch:   800  |  train loss: 0.0542406417
Epoch:   900  |  train loss: 0.0537357710
Epoch:  1000  |  train loss: 0.0531053066
Processing class: 79
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.0661645442
Epoch:   200  |  train loss: 0.0646892622
Epoch:   300  |  train loss: 0.0643375993
Epoch:   400  |  train loss: 0.0630762435
Epoch:   500  |  train loss: 0.0621891893
Epoch:   600  |  train loss: 0.0617569923
Epoch:   700  |  train loss: 0.0608966954
Epoch:   800  |  train loss: 0.0601837590
Epoch:   900  |  train loss: 0.0597750381
Epoch:  1000  |  train loss: 0.0585599832
2024-03-04 22:50:59,695 [trainer.py] => CNN: {'total': 59.18, '00-09': 73.6, '10-19': 68.4, '20-29': 76.9, '30-39': 69.1, '40-49': 60.6, '50-59': 37.2, '60-69': 45.7, '70-79': 41.9, 'old': 61.64, 'new': 41.9}
2024-03-04 22:50:59,695 [trainer.py] => No NME accuracy
2024-03-04 22:50:59,695 [trainer.py] => FeCAM: {'total': 61.72, '00-09': 78.1, '10-19': 69.3, '20-29': 78.0, '30-39': 69.5, '40-49': 72.1, '50-59': 35.7, '60-69': 49.1, '70-79': 42.0, 'old': 64.54, 'new': 42.0}
2024-03-04 22:50:59,695 [trainer.py] => CNN top1 curve: [83.44, 71.75, 64.66, 59.18]
2024-03-04 22:50:59,695 [trainer.py] => CNN top5 curve: [96.5, 89.6, 86.54, 84.09]
2024-03-04 22:50:59,695 [trainer.py] => FeCAM top1 curve: [82.9, 72.78, 67.27, 61.72]
2024-03-04 22:50:59,695 [trainer.py] => FeCAM top5 curve: [95.84, 91.18, 87.21, 84.09]

2024-03-04 22:50:59,700 [fecam.py] => Learning on 80-90
Processing class: 80
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.0572834149
Epoch:   200  |  train loss: 0.0528656036
Epoch:   300  |  train loss: 0.0512428798
Epoch:   400  |  train loss: 0.0502598241
Epoch:   500  |  train loss: 0.0491654150
Epoch:   600  |  train loss: 0.0485715024
Epoch:   700  |  train loss: 0.0474244975
Epoch:   800  |  train loss: 0.0477351144
Epoch:   900  |  train loss: 0.0471824542
Epoch:  1000  |  train loss: 0.0470825151
Processing class: 81
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.0620973349
Epoch:   200  |  train loss: 0.0604013942
Epoch:   300  |  train loss: 0.0581869625
Epoch:   400  |  train loss: 0.0575993210
Epoch:   500  |  train loss: 0.0562902212
Epoch:   600  |  train loss: 0.0558771849
Epoch:   700  |  train loss: 0.0548866786
Epoch:   800  |  train loss: 0.0546540752
Epoch:   900  |  train loss: 0.0538325630
Epoch:  1000  |  train loss: 0.0531641595
Processing class: 82
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.0600005567
Epoch:   200  |  train loss: 0.0577011786
Epoch:   300  |  train loss: 0.0560228229
Epoch:   400  |  train loss: 0.0543655120
Epoch:   500  |  train loss: 0.0531273320
Epoch:   600  |  train loss: 0.0520740375
Epoch:   700  |  train loss: 0.0513264634
Epoch:   800  |  train loss: 0.0508727930
Epoch:   900  |  train loss: 0.0502130292
Epoch:  1000  |  train loss: 0.0496186092
Processing class: 83
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.0592894115
Epoch:   200  |  train loss: 0.0551948398
Epoch:   300  |  train loss: 0.0524639763
Epoch:   400  |  train loss: 0.0507756710
Epoch:   500  |  train loss: 0.0500293054
Epoch:   600  |  train loss: 0.0488018692
Epoch:   700  |  train loss: 0.0487237632
Epoch:   800  |  train loss: 0.0487393960
Epoch:   900  |  train loss: 0.0479472540
Epoch:  1000  |  train loss: 0.0471912012
Processing class: 84
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.0641389400
Epoch:   200  |  train loss: 0.0605609909
Epoch:   300  |  train loss: 0.0590413608
Epoch:   400  |  train loss: 0.0571838103
Epoch:   500  |  train loss: 0.0561801672
Epoch:   600  |  train loss: 0.0547910251
Epoch:   700  |  train loss: 0.0543686047
Epoch:   800  |  train loss: 0.0540929817
Epoch:   900  |  train loss: 0.0527936690
Epoch:  1000  |  train loss: 0.0520707220
Processing class: 85
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.0642218620
Epoch:   200  |  train loss: 0.0611486182
Epoch:   300  |  train loss: 0.0593214929
Epoch:   400  |  train loss: 0.0574272543
Epoch:   500  |  train loss: 0.0565602005
Epoch:   600  |  train loss: 0.0558272295
Epoch:   700  |  train loss: 0.0556454830
Epoch:   800  |  train loss: 0.0544457458
Epoch:   900  |  train loss: 0.0538603239
Epoch:  1000  |  train loss: 0.0536550231
Processing class: 86
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.0664889231
Epoch:   200  |  train loss: 0.0646582037
Epoch:   300  |  train loss: 0.0632796884
Epoch:   400  |  train loss: 0.0613631852
Epoch:   500  |  train loss: 0.0602358766
Epoch:   600  |  train loss: 0.0593377337
Epoch:   700  |  train loss: 0.0584299788
Epoch:   800  |  train loss: 0.0575256512
Epoch:   900  |  train loss: 0.0568497092
Epoch:  1000  |  train loss: 0.0568456925
Processing class: 87
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.0605132543
Epoch:   200  |  train loss: 0.0570280217
Epoch:   300  |  train loss: 0.0554701746
Epoch:   400  |  train loss: 0.0537703276
Epoch:   500  |  train loss: 0.0524888366
Epoch:   600  |  train loss: 0.0521742709
Epoch:   700  |  train loss: 0.0516246416
Epoch:   800  |  train loss: 0.0509388141
Epoch:   900  |  train loss: 0.0499912210
Epoch:  1000  |  train loss: 0.0501623333
Processing class: 88
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.0630652860
Epoch:   200  |  train loss: 0.0610634349
Epoch:   300  |  train loss: 0.0589685686
Epoch:   400  |  train loss: 0.0574081376
Epoch:   500  |  train loss: 0.0556696936
Epoch:   600  |  train loss: 0.0539898135
Epoch:   700  |  train loss: 0.0534399860
Epoch:   800  |  train loss: 0.0525552511
Epoch:   900  |  train loss: 0.0519850716
Epoch:  1000  |  train loss: 0.0510731235
Processing class: 89
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.0628373057
Epoch:   200  |  train loss: 0.0606530540
Epoch:   300  |  train loss: 0.0583177872
Epoch:   400  |  train loss: 0.0571524329
Epoch:   500  |  train loss: 0.0562243864
Epoch:   600  |  train loss: 0.0549125530
Epoch:   700  |  train loss: 0.0535996407
Epoch:   800  |  train loss: 0.0535854794
Epoch:   900  |  train loss: 0.0525697522
Epoch:  1000  |  train loss: 0.0520819373
2024-03-04 22:59:58,763 [trainer.py] => CNN: {'total': 54.08, '00-09': 67.9, '10-19': 63.6, '20-29': 73.2, '30-39': 68.8, '40-49': 57.2, '50-59': 32.6, '60-69': 40.9, '70-79': 38.1, '80-89': 44.4, 'old': 55.29, 'new': 44.4}
2024-03-04 22:59:58,769 [trainer.py] => No NME accuracy
2024-03-04 22:59:58,769 [trainer.py] => FeCAM: {'total': 57.66, '00-09': 76.1, '10-19': 65.8, '20-29': 75.3, '30-39': 67.9, '40-49': 68.5, '50-59': 33.0, '60-69': 45.0, '70-79': 39.0, '80-89': 48.3, 'old': 58.82, 'new': 48.3}
2024-03-04 22:59:58,769 [trainer.py] => CNN top1 curve: [83.44, 71.75, 64.66, 59.18, 54.08]
2024-03-04 22:59:58,770 [trainer.py] => CNN top5 curve: [96.5, 89.6, 86.54, 84.09, 81.74]
2024-03-04 22:59:58,770 [trainer.py] => FeCAM top1 curve: [82.9, 72.78, 67.27, 61.72, 57.66]
2024-03-04 22:59:58,770 [trainer.py] => FeCAM top5 curve: [95.84, 91.18, 87.21, 84.09, 81.63]

2024-03-04 22:59:58,774 [fecam.py] => Learning on 90-100
Processing class: 90
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.0641513839
Epoch:   200  |  train loss: 0.0626935527
Epoch:   300  |  train loss: 0.0596833095
Epoch:   400  |  train loss: 0.0574782118
Epoch:   500  |  train loss: 0.0556712441
Epoch:   600  |  train loss: 0.0543834716
Epoch:   700  |  train loss: 0.0528989278
Epoch:   800  |  train loss: 0.0521296896
Epoch:   900  |  train loss: 0.0517052621
Epoch:  1000  |  train loss: 0.0514636584
Processing class: 91
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.0528841950
Epoch:   200  |  train loss: 0.0509165436
Epoch:   300  |  train loss: 0.0496376753
Epoch:   400  |  train loss: 0.0492308848
Epoch:   500  |  train loss: 0.0482822970
Epoch:   600  |  train loss: 0.0476701848
Epoch:   700  |  train loss: 0.0466003649
Epoch:   800  |  train loss: 0.0458644949
Epoch:   900  |  train loss: 0.0453519404
Epoch:  1000  |  train loss: 0.0449848168
Processing class: 92
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.0626450032
Epoch:   200  |  train loss: 0.0581693269
Epoch:   300  |  train loss: 0.0562230453
Epoch:   400  |  train loss: 0.0552143268
Epoch:   500  |  train loss: 0.0538223244
Epoch:   600  |  train loss: 0.0523721203
Epoch:   700  |  train loss: 0.0512102000
Epoch:   800  |  train loss: 0.0496582307
Epoch:   900  |  train loss: 0.0490990415
Epoch:  1000  |  train loss: 0.0485179603
Processing class: 93
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.0633068472
Epoch:   200  |  train loss: 0.0601218671
Epoch:   300  |  train loss: 0.0586187422
Epoch:   400  |  train loss: 0.0573959962
Epoch:   500  |  train loss: 0.0557935081
Epoch:   600  |  train loss: 0.0541330472
Epoch:   700  |  train loss: 0.0538940638
Epoch:   800  |  train loss: 0.0528817743
Epoch:   900  |  train loss: 0.0522361308
Epoch:  1000  |  train loss: 0.0512853876
Processing class: 94
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.0582703188
Epoch:   200  |  train loss: 0.0559275724
Epoch:   300  |  train loss: 0.0524068654
Epoch:   400  |  train loss: 0.0508162357
Epoch:   500  |  train loss: 0.0487696946
Epoch:   600  |  train loss: 0.0482206427
Epoch:   700  |  train loss: 0.0477764525
Epoch:   800  |  train loss: 0.0459748514
Epoch:   900  |  train loss: 0.0459463306
Epoch:  1000  |  train loss: 0.0451381490
Processing class: 95
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.0646485999
Epoch:   200  |  train loss: 0.0641371667
Epoch:   300  |  train loss: 0.0624773711
Epoch:   400  |  train loss: 0.0606646731
Epoch:   500  |  train loss: 0.0597750247
Epoch:   600  |  train loss: 0.0586121336
Epoch:   700  |  train loss: 0.0578109331
Epoch:   800  |  train loss: 0.0567162149
Epoch:   900  |  train loss: 0.0562824354
Epoch:  1000  |  train loss: 0.0559143379
Processing class: 96
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.0655955508
Epoch:   200  |  train loss: 0.0632704303
Epoch:   300  |  train loss: 0.0615804330
Epoch:   400  |  train loss: 0.0609361075
Epoch:   500  |  train loss: 0.0593649976
Epoch:   600  |  train loss: 0.0582945086
Epoch:   700  |  train loss: 0.0573777005
Epoch:   800  |  train loss: 0.0561408915
Epoch:   900  |  train loss: 0.0558291800
Epoch:  1000  |  train loss: 0.0549748294
Processing class: 97
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.0635707662
Epoch:   200  |  train loss: 0.0601961829
Epoch:   300  |  train loss: 0.0578087732
Epoch:   400  |  train loss: 0.0565380514
Epoch:   500  |  train loss: 0.0556744158
Epoch:   600  |  train loss: 0.0548482329
Epoch:   700  |  train loss: 0.0542091936
Epoch:   800  |  train loss: 0.0533702336
Epoch:   900  |  train loss: 0.0527431481
Epoch:  1000  |  train loss: 0.0520240493
Processing class: 98
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.0596516021
Epoch:   200  |  train loss: 0.0570193261
Epoch:   300  |  train loss: 0.0543688625
Epoch:   400  |  train loss: 0.0527751058
Epoch:   500  |  train loss: 0.0512145072
Epoch:   600  |  train loss: 0.0502826571
Epoch:   700  |  train loss: 0.0489705421
Epoch:   800  |  train loss: 0.0486580305
Epoch:   900  |  train loss: 0.0481303997
Epoch:  1000  |  train loss: 0.0474821836
Processing class: 99
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.0646884710
Epoch:   200  |  train loss: 0.0621637844
Epoch:   300  |  train loss: 0.0600123167
Epoch:   400  |  train loss: 0.0586151026
Epoch:   500  |  train loss: 0.0567738965
Epoch:   600  |  train loss: 0.0554563329
Epoch:   700  |  train loss: 0.0544913024
Epoch:   800  |  train loss: 0.0541529335
Epoch:   900  |  train loss: 0.0531076767
Epoch:  1000  |  train loss: 0.0525539085
2024-03-04 23:10:24,718 [trainer.py] => CNN: {'total': 50.36, '00-09': 57.7, '10-19': 63.6, '20-29': 71.4, '30-39': 68.5, '40-49': 56.8, '50-59': 29.6, '60-69': 38.9, '70-79': 36.6, '80-89': 42.7, '90-99': 37.8, 'old': 51.76, 'new': 37.8}
2024-03-04 23:10:24,718 [trainer.py] => No NME accuracy
2024-03-04 23:10:24,719 [trainer.py] => FeCAM: {'total': 54.74, '00-09': 73.3, '10-19': 65.1, '20-29': 74.1, '30-39': 67.2, '40-49': 67.1, '50-59': 30.8, '60-69': 43.3, '70-79': 37.2, '80-89': 46.9, '90-99': 42.4, 'old': 56.11, 'new': 42.4}
2024-03-04 23:10:24,719 [trainer.py] => CNN top1 curve: [83.44, 71.75, 64.66, 59.18, 54.08, 50.36]
2024-03-04 23:10:24,719 [trainer.py] => CNN top5 curve: [96.5, 89.6, 86.54, 84.09, 81.74, 79.63]
2024-03-04 23:10:24,719 [trainer.py] => FeCAM top1 curve: [82.9, 72.78, 67.27, 61.72, 57.66, 54.74]
2024-03-04 23:10:24,719 [trainer.py] => FeCAM top5 curve: [95.84, 91.18, 87.21, 84.09, 81.63, 79.21]

=========================================
2024-03-04 23:10:47,482 [trainer.py] => config: ./exps/FeCAM_cifar100.json
2024-03-04 23:10:47,482 [trainer.py] => prefix: train
2024-03-04 23:10:47,482 [trainer.py] => dataset: cifar100
2024-03-04 23:10:47,482 [trainer.py] => memory_size: 0
2024-03-04 23:10:47,482 [trainer.py] => shuffle: True
2024-03-04 23:10:47,482 [trainer.py] => init_cls: 50
2024-03-04 23:10:47,482 [trainer.py] => increment: 10
2024-03-04 23:10:47,482 [trainer.py] => model_name: fecam
2024-03-04 23:10:47,482 [trainer.py] => convnet_type: resnet18
2024-03-04 23:10:47,482 [trainer.py] => device: [device(type='cuda', index=0)]
2024-03-04 23:10:47,482 [trainer.py] => seed: 1993
2024-03-04 23:10:47,482 [trainer.py] => init_epochs: 200
2024-03-04 23:10:47,482 [trainer.py] => init_lr: 0.1
2024-03-04 23:10:47,482 [trainer.py] => init_weight_decay: 0.0005
2024-03-04 23:10:47,482 [trainer.py] => batch_size: 128
2024-03-04 23:10:47,482 [trainer.py] => num_workers: 8
2024-03-04 23:10:47,482 [trainer.py] => T: 5
2024-03-04 23:10:47,482 [trainer.py] => beta: 0.5
2024-03-04 23:10:47,482 [trainer.py] => alpha1: 1
2024-03-04 23:10:47,482 [trainer.py] => alpha2: 1
2024-03-04 23:10:47,482 [trainer.py] => ncm: False
2024-03-04 23:10:47,482 [trainer.py] => tukey: False
2024-03-04 23:10:47,482 [trainer.py] => diagonal: False
2024-03-04 23:10:47,482 [trainer.py] => per_class: True
2024-03-04 23:10:47,482 [trainer.py] => full_cov: True
2024-03-04 23:10:47,482 [trainer.py] => shrink: True
2024-03-04 23:10:47,482 [trainer.py] => norm_cov: False
2024-03-04 23:10:47,482 [trainer.py] => vecnorm: False
2024-03-04 23:10:47,482 [trainer.py] => ae_type: wae
2024-03-04 23:10:47,482 [trainer.py] => epochs: 1000
2024-03-04 23:10:47,482 [trainer.py] => ae_latent_dim: 32
2024-03-04 23:10:47,483 [trainer.py] => wae_sigma: 20
2024-03-04 23:10:47,483 [trainer.py] => wae_C: 0.1
Files already downloaded and verified
Files already downloaded and verified
2024-03-04 23:10:49,147 [data_manager.py] => [68, 56, 78, 8, 23, 84, 90, 65, 74, 76, 40, 89, 3, 92, 55, 9, 26, 80, 43, 38, 58, 70, 77, 1, 85, 19, 17, 50, 28, 53, 13, 81, 45, 82, 6, 59, 83, 16, 15, 44, 91, 41, 72, 60, 79, 52, 20, 10, 31, 54, 37, 95, 14, 71, 96, 98, 97, 2, 64, 66, 42, 22, 35, 86, 24, 34, 87, 21, 99, 0, 88, 27, 18, 94, 11, 12, 47, 25, 30, 46, 62, 69, 36, 61, 7, 63, 75, 5, 32, 4, 51, 48, 73, 93, 39, 67, 29, 49, 57, 33]
2024-03-04 23:10:49,432 [fecam.py] => Learning on 0-50
/home/z1165703/miniconda3/lib/python3.11/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
/home/z1165703/miniconda3/lib/python3.11/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
Processing class: 0
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.0436552525
Epoch:   200  |  train loss: 0.0429728821
Epoch:   300  |  train loss: 0.0434578143
Epoch:   400  |  train loss: 0.0432924829
Epoch:   500  |  train loss: 0.0428025343
Epoch:   600  |  train loss: 0.0427782439
Epoch:   700  |  train loss: 0.0428266712
Epoch:   800  |  train loss: 0.0428377964
Epoch:   900  |  train loss: 0.0424390577
Epoch:  1000  |  train loss: 0.0422457092
Processing class: 1
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.0446408942
Epoch:   200  |  train loss: 0.0442792967
Epoch:   300  |  train loss: 0.0448994495
Epoch:   400  |  train loss: 0.0448599152
Epoch:   500  |  train loss: 0.0437490188
Epoch:   600  |  train loss: 0.0445318386
Epoch:   700  |  train loss: 0.0438963726
Epoch:   800  |  train loss: 0.0441281959
Epoch:   900  |  train loss: 0.0446427099
Epoch:  1000  |  train loss: 0.0437544227
Processing class: 2
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.0452612467
Epoch:   200  |  train loss: 0.0445815593
Epoch:   300  |  train loss: 0.0447637811
Epoch:   400  |  train loss: 0.0439810790
Epoch:   500  |  train loss: 0.0422515310
Epoch:   600  |  train loss: 0.0427426443
Epoch:   700  |  train loss: 0.0421840690
Epoch:   800  |  train loss: 0.0425865501
Epoch:   900  |  train loss: 0.0417828053
Epoch:  1000  |  train loss: 0.0412597351
Processing class: 3
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.0447272874
Epoch:   200  |  train loss: 0.0454321988
Epoch:   300  |  train loss: 0.0454033263
Epoch:   400  |  train loss: 0.0449206978
Epoch:   500  |  train loss: 0.0442135572
Epoch:   600  |  train loss: 0.0447415814
Epoch:   700  |  train loss: 0.0447693147
Epoch:   800  |  train loss: 0.0440166004
Epoch:   900  |  train loss: 0.0440232940
Epoch:  1000  |  train loss: 0.0438158691
Processing class: 4
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.0444233410
Epoch:   200  |  train loss: 0.0438461758
Epoch:   300  |  train loss: 0.0440741345
Epoch:   400  |  train loss: 0.0435655117
Epoch:   500  |  train loss: 0.0435428433
Epoch:   600  |  train loss: 0.0428424090
Epoch:   700  |  train loss: 0.0431194603
Epoch:   800  |  train loss: 0.0430555843
Epoch:   900  |  train loss: 0.0430307105
Epoch:  1000  |  train loss: 0.0433719553
Processing class: 5
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.0455465764
Epoch:   200  |  train loss: 0.0444128938
Epoch:   300  |  train loss: 0.0441717319
Epoch:   400  |  train loss: 0.0435755216
Epoch:   500  |  train loss: 0.0432492852
Epoch:   600  |  train loss: 0.0439997688
Epoch:   700  |  train loss: 0.0437565088
Epoch:   800  |  train loss: 0.0428978369
Epoch:   900  |  train loss: 0.0424344458
Epoch:  1000  |  train loss: 0.0426782534
Processing class: 6
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.0445779003
Epoch:   200  |  train loss: 0.0449068807
Epoch:   300  |  train loss: 0.0451992102
Epoch:   400  |  train loss: 0.0445028760
Epoch:   500  |  train loss: 0.0449156828
Epoch:   600  |  train loss: 0.0444178335
Epoch:   700  |  train loss: 0.0439721301
Epoch:   800  |  train loss: 0.0439599901
Epoch:   900  |  train loss: 0.0431811735
Epoch:  1000  |  train loss: 0.0433321074
Processing class: 7
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.0449545309
Epoch:   200  |  train loss: 0.0444983020
Epoch:   300  |  train loss: 0.0444095559
Epoch:   400  |  train loss: 0.0439934649
Epoch:   500  |  train loss: 0.0438143022
Epoch:   600  |  train loss: 0.0432681940
Epoch:   700  |  train loss: 0.0433775663
Epoch:   800  |  train loss: 0.0438325174
Epoch:   900  |  train loss: 0.0428739838
Epoch:  1000  |  train loss: 0.0431405790
Processing class: 8
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.0447441928
Epoch:   200  |  train loss: 0.0448365234
Epoch:   300  |  train loss: 0.0437397160
Epoch:   400  |  train loss: 0.0442377418
Epoch:   500  |  train loss: 0.0435098559
Epoch:   600  |  train loss: 0.0432922110
Epoch:   700  |  train loss: 0.0436979800
Epoch:   800  |  train loss: 0.0428324692
Epoch:   900  |  train loss: 0.0430138856
Epoch:  1000  |  train loss: 0.0423620380
Processing class: 9
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.0435515657
Epoch:   200  |  train loss: 0.0440072380
Epoch:   300  |  train loss: 0.0445584133
Epoch:   400  |  train loss: 0.0433938190
Epoch:   500  |  train loss: 0.0439361580
Epoch:   600  |  train loss: 0.0432724215
Epoch:   700  |  train loss: 0.0434609361
Epoch:   800  |  train loss: 0.0435428977
Epoch:   900  |  train loss: 0.0434227020
Epoch:  1000  |  train loss: 0.0427298345
Processing class: 10
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.0441933729
Epoch:   200  |  train loss: 0.0434101045
Epoch:   300  |  train loss: 0.0433427572
Epoch:   400  |  train loss: 0.0438900031
Epoch:   500  |  train loss: 0.0423741944
Epoch:   600  |  train loss: 0.0424403913
Epoch:   700  |  train loss: 0.0424893312
Epoch:   800  |  train loss: 0.0419804357
Epoch:   900  |  train loss: 0.0420696497
Epoch:  1000  |  train loss: 0.0418517277
Processing class: 11
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.0456545994
Epoch:   200  |  train loss: 0.0450214185
Epoch:   300  |  train loss: 0.0442357846
Epoch:   400  |  train loss: 0.0442033358
Epoch:   500  |  train loss: 0.0436765283
Epoch:   600  |  train loss: 0.0439236321
Epoch:   700  |  train loss: 0.0444619142
Epoch:   800  |  train loss: 0.0441051751
Epoch:   900  |  train loss: 0.0438618399
Epoch:  1000  |  train loss: 0.0440050945
Processing class: 12
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.0439813346
Epoch:   200  |  train loss: 0.0449128248
Epoch:   300  |  train loss: 0.0443803981
Epoch:   400  |  train loss: 0.0442899354
Epoch:   500  |  train loss: 0.0440057926
Epoch:   600  |  train loss: 0.0432152107
Epoch:   700  |  train loss: 0.0431262136
Epoch:   800  |  train loss: 0.0431582920
Epoch:   900  |  train loss: 0.0433789022
Epoch:  1000  |  train loss: 0.0428408474
Processing class: 13
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.0441340029
Epoch:   200  |  train loss: 0.0443253189
Epoch:   300  |  train loss: 0.0443534166
Epoch:   400  |  train loss: 0.0438900240
Epoch:   500  |  train loss: 0.0439933494
Epoch:   600  |  train loss: 0.0432944097
Epoch:   700  |  train loss: 0.0433451660
Epoch:   800  |  train loss: 0.0435628094
Epoch:   900  |  train loss: 0.0436375260
Epoch:  1000  |  train loss: 0.0436466917
Processing class: 14
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.0457536876
Epoch:   200  |  train loss: 0.0460916996
Epoch:   300  |  train loss: 0.0458378270
Epoch:   400  |  train loss: 0.0459256940
Epoch:   500  |  train loss: 0.0453512780
Epoch:   600  |  train loss: 0.0450014956
Epoch:   700  |  train loss: 0.0452533990
Epoch:   800  |  train loss: 0.0458837874
Epoch:   900  |  train loss: 0.0454593621
Epoch:  1000  |  train loss: 0.0447724596
Processing class: 15
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.0441347525
Epoch:   200  |  train loss: 0.0441939436
Epoch:   300  |  train loss: 0.0440618522
Epoch:   400  |  train loss: 0.0439468019
Epoch:   500  |  train loss: 0.0440884233
Epoch:   600  |  train loss: 0.0433488198
Epoch:   700  |  train loss: 0.0438859947
Epoch:   800  |  train loss: 0.0430841818
Epoch:   900  |  train loss: 0.0436937056
Epoch:  1000  |  train loss: 0.0432931475
Processing class: 16
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.0446678795
Epoch:   200  |  train loss: 0.0452365287
Epoch:   300  |  train loss: 0.0454558544
Epoch:   400  |  train loss: 0.0447558552
Epoch:   500  |  train loss: 0.0451926626
Epoch:   600  |  train loss: 0.0445524618
Epoch:   700  |  train loss: 0.0440996908
Epoch:   800  |  train loss: 0.0447185025
Epoch:   900  |  train loss: 0.0437239245
Epoch:  1000  |  train loss: 0.0444117010
Processing class: 17
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.0433550455
Epoch:   200  |  train loss: 0.0437894993
Epoch:   300  |  train loss: 0.0443701155
Epoch:   400  |  train loss: 0.0440639570
Epoch:   500  |  train loss: 0.0424912050
Epoch:   600  |  train loss: 0.0427101418
Epoch:   700  |  train loss: 0.0423141532
Epoch:   800  |  train loss: 0.0421352148
Epoch:   900  |  train loss: 0.0421138920
Epoch:  1000  |  train loss: 0.0419345982
Processing class: 18
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.0440782055
Epoch:   200  |  train loss: 0.0441952489
Epoch:   300  |  train loss: 0.0435494557
Epoch:   400  |  train loss: 0.0430142969
Epoch:   500  |  train loss: 0.0431038558
Epoch:   600  |  train loss: 0.0422843508
Epoch:   700  |  train loss: 0.0431256428
Epoch:   800  |  train loss: 0.0427200660
Epoch:   900  |  train loss: 0.0426868573
Epoch:  1000  |  train loss: 0.0430533022
Processing class: 19
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.0435521968
Epoch:   200  |  train loss: 0.0443822101
Epoch:   300  |  train loss: 0.0439039633
Epoch:   400  |  train loss: 0.0437828749
Epoch:   500  |  train loss: 0.0428378657
Epoch:   600  |  train loss: 0.0427715726
Epoch:   700  |  train loss: 0.0428382412
Epoch:   800  |  train loss: 0.0423538715
Epoch:   900  |  train loss: 0.0432384312
Epoch:  1000  |  train loss: 0.0432758436
Processing class: 20
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.0445094377
Epoch:   200  |  train loss: 0.0442896396
Epoch:   300  |  train loss: 0.0445338957
Epoch:   400  |  train loss: 0.0442471072
Epoch:   500  |  train loss: 0.0447945751
Epoch:   600  |  train loss: 0.0440891914
Epoch:   700  |  train loss: 0.0439297289
Epoch:   800  |  train loss: 0.0438411817
Epoch:   900  |  train loss: 0.0438961476
Epoch:  1000  |  train loss: 0.0442736559
Processing class: 21
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.0447978266
Epoch:   200  |  train loss: 0.0447421677
Epoch:   300  |  train loss: 0.0438284069
Epoch:   400  |  train loss: 0.0433760248
Epoch:   500  |  train loss: 0.0432169221
Epoch:   600  |  train loss: 0.0433021836
Epoch:   700  |  train loss: 0.0429945298
Epoch:   800  |  train loss: 0.0425191045
Epoch:   900  |  train loss: 0.0428397238
Epoch:  1000  |  train loss: 0.0423405312
Processing class: 22
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.0449412882
Epoch:   200  |  train loss: 0.0449753165
Epoch:   300  |  train loss: 0.0453617938
Epoch:   400  |  train loss: 0.0453332789
Epoch:   500  |  train loss: 0.0450330280
Epoch:   600  |  train loss: 0.0443020158
Epoch:   700  |  train loss: 0.0442430034
Epoch:   800  |  train loss: 0.0436806574
Epoch:   900  |  train loss: 0.0432156160
Epoch:  1000  |  train loss: 0.0440432258
Processing class: 23
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.0446816839
Epoch:   200  |  train loss: 0.0451780766
Epoch:   300  |  train loss: 0.0453936018
Epoch:   400  |  train loss: 0.0442831554
Epoch:   500  |  train loss: 0.0439295016
Epoch:   600  |  train loss: 0.0430735298
Epoch:   700  |  train loss: 0.0430826224
Epoch:   800  |  train loss: 0.0424863689
Epoch:   900  |  train loss: 0.0426580265
Epoch:  1000  |  train loss: 0.0419573978
Processing class: 24
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.0446799226
Epoch:   200  |  train loss: 0.0445664637
Epoch:   300  |  train loss: 0.0440123290
Epoch:   400  |  train loss: 0.0441170640
Epoch:   500  |  train loss: 0.0434739672
Epoch:   600  |  train loss: 0.0436260104
Epoch:   700  |  train loss: 0.0424818747
Epoch:   800  |  train loss: 0.0426007882
Epoch:   900  |  train loss: 0.0429100715
Epoch:  1000  |  train loss: 0.0424273960
Processing class: 25
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.0445811488
Epoch:   200  |  train loss: 0.0445701875
Epoch:   300  |  train loss: 0.0448209234
Epoch:   400  |  train loss: 0.0436172284
Epoch:   500  |  train loss: 0.0438226022
Epoch:   600  |  train loss: 0.0431639135
Epoch:   700  |  train loss: 0.0436577812
Epoch:   800  |  train loss: 0.0431161389
Epoch:   900  |  train loss: 0.0426236153
Epoch:  1000  |  train loss: 0.0427508146
Processing class: 26
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.0447896712
Epoch:   200  |  train loss: 0.0454981171
Epoch:   300  |  train loss: 0.0451903522
Epoch:   400  |  train loss: 0.0454619974
Epoch:   500  |  train loss: 0.0456791647
Epoch:   600  |  train loss: 0.0441960156
Epoch:   700  |  train loss: 0.0442960002
Epoch:   800  |  train loss: 0.0444525972
Epoch:   900  |  train loss: 0.0440209568
Epoch:  1000  |  train loss: 0.0444120780
Processing class: 27
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.0452217273
Epoch:   200  |  train loss: 0.0441661678
Epoch:   300  |  train loss: 0.0442521013
Epoch:   400  |  train loss: 0.0438362017
Epoch:   500  |  train loss: 0.0437004216
Epoch:   600  |  train loss: 0.0435471438
Epoch:   700  |  train loss: 0.0435521595
Epoch:   800  |  train loss: 0.0433236495
Epoch:   900  |  train loss: 0.0432964660
Epoch:  1000  |  train loss: 0.0422876216
Processing class: 28
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.0452464588
Epoch:   200  |  train loss: 0.0457563050
Epoch:   300  |  train loss: 0.0457320489
Epoch:   400  |  train loss: 0.0455489941
Epoch:   500  |  train loss: 0.0453770213
Epoch:   600  |  train loss: 0.0451830417
Epoch:   700  |  train loss: 0.0452322014
Epoch:   800  |  train loss: 0.0452947378
Epoch:   900  |  train loss: 0.0453013398
Epoch:  1000  |  train loss: 0.0446620621
Processing class: 29
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.0448481955
Epoch:   200  |  train loss: 0.0444805093
Epoch:   300  |  train loss: 0.0438087992
Epoch:   400  |  train loss: 0.0428870045
Epoch:   500  |  train loss: 0.0431283720
Epoch:   600  |  train loss: 0.0430799186
Epoch:   700  |  train loss: 0.0426947914
Epoch:   800  |  train loss: 0.0419430025
Epoch:   900  |  train loss: 0.0418215677
Epoch:  1000  |  train loss: 0.0414741799
Processing class: 30
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.0448259175
Epoch:   200  |  train loss: 0.0453763142
Epoch:   300  |  train loss: 0.0455347277
Epoch:   400  |  train loss: 0.0461043961
Epoch:   500  |  train loss: 0.0456499949
Epoch:   600  |  train loss: 0.0452724777
Epoch:   700  |  train loss: 0.0447737545
Epoch:   800  |  train loss: 0.0451725282
Epoch:   900  |  train loss: 0.0453894965
Epoch:  1000  |  train loss: 0.0447454259
Processing class: 31
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.0453379467
Epoch:   200  |  train loss: 0.0455328196
Epoch:   300  |  train loss: 0.0450053416
Epoch:   400  |  train loss: 0.0456256174
Epoch:   500  |  train loss: 0.0455659591
Epoch:   600  |  train loss: 0.0459109426
Epoch:   700  |  train loss: 0.0455155484
Epoch:   800  |  train loss: 0.0454351179
Epoch:   900  |  train loss: 0.0457128055
Epoch:  1000  |  train loss: 0.0449177518
Processing class: 32
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.0455054857
Epoch:   200  |  train loss: 0.0459701933
Epoch:   300  |  train loss: 0.0453834489
Epoch:   400  |  train loss: 0.0442855552
Epoch:   500  |  train loss: 0.0449341260
Epoch:   600  |  train loss: 0.0440556251
Epoch:   700  |  train loss: 0.0445710041
Epoch:   800  |  train loss: 0.0446078733
Epoch:   900  |  train loss: 0.0441952258
Epoch:  1000  |  train loss: 0.0437381789
Processing class: 33
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.0439101025
Epoch:   200  |  train loss: 0.0453768522
Epoch:   300  |  train loss: 0.0433962427
Epoch:   400  |  train loss: 0.0429493852
Epoch:   500  |  train loss: 0.0426468663
Epoch:   600  |  train loss: 0.0431908846
Epoch:   700  |  train loss: 0.0424480289
Epoch:   800  |  train loss: 0.0424337097
Epoch:   900  |  train loss: 0.0421070091
Epoch:  1000  |  train loss: 0.0416295245
Processing class: 34
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.0453120887
Epoch:   200  |  train loss: 0.0449021935
Epoch:   300  |  train loss: 0.0447085634
Epoch:   400  |  train loss: 0.0440129295
Epoch:   500  |  train loss: 0.0434996620
Epoch:   600  |  train loss: 0.0437008925
Epoch:   700  |  train loss: 0.0428527296
Epoch:   800  |  train loss: 0.0432147257
Epoch:   900  |  train loss: 0.0431835286
Epoch:  1000  |  train loss: 0.0431189045
Processing class: 35
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.0456535414
Epoch:   200  |  train loss: 0.0453986034
Epoch:   300  |  train loss: 0.0455208600
Epoch:   400  |  train loss: 0.0454449289
Epoch:   500  |  train loss: 0.0446171336
Epoch:   600  |  train loss: 0.0448959559
Epoch:   700  |  train loss: 0.0447921924
Epoch:   800  |  train loss: 0.0445050783
Epoch:   900  |  train loss: 0.0444423757
Epoch:  1000  |  train loss: 0.0439750522
Processing class: 36
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.0437808812
Epoch:   200  |  train loss: 0.0435393438
Epoch:   300  |  train loss: 0.0434909083
Epoch:   400  |  train loss: 0.0432808548
Epoch:   500  |  train loss: 0.0436603665
Epoch:   600  |  train loss: 0.0428939603
Epoch:   700  |  train loss: 0.0421574630
Epoch:   800  |  train loss: 0.0427423328
Epoch:   900  |  train loss: 0.0417413630
Epoch:  1000  |  train loss: 0.0420099027
Processing class: 37
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.0459473766
Epoch:   200  |  train loss: 0.0455025822
Epoch:   300  |  train loss: 0.0448156349
Epoch:   400  |  train loss: 0.0447024681
Epoch:   500  |  train loss: 0.0446692236
Epoch:   600  |  train loss: 0.0439191222
Epoch:   700  |  train loss: 0.0437217198
Epoch:   800  |  train loss: 0.0437749937
Epoch:   900  |  train loss: 0.0430916794
Epoch:  1000  |  train loss: 0.0431923218
Processing class: 38
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.0440526523
Epoch:   200  |  train loss: 0.0441976964
Epoch:   300  |  train loss: 0.0444756322
Epoch:   400  |  train loss: 0.0446041495
Epoch:   500  |  train loss: 0.0442869134
Epoch:   600  |  train loss: 0.0436733499
Epoch:   700  |  train loss: 0.0433610015
Epoch:   800  |  train loss: 0.0437874556
Epoch:   900  |  train loss: 0.0428886786
Epoch:  1000  |  train loss: 0.0432364345
Processing class: 39
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.0448024333
Epoch:   200  |  train loss: 0.0442842700
Epoch:   300  |  train loss: 0.0443339191
Epoch:   400  |  train loss: 0.0442683011
Epoch:   500  |  train loss: 0.0436199658
Epoch:   600  |  train loss: 0.0435752481
Epoch:   700  |  train loss: 0.0435240261
Epoch:   800  |  train loss: 0.0436374046
Epoch:   900  |  train loss: 0.0435184047
Epoch:  1000  |  train loss: 0.0434239492
Processing class: 40
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.0446578898
Epoch:   200  |  train loss: 0.0434514984
Epoch:   300  |  train loss: 0.0443530582
Epoch:   400  |  train loss: 0.0443239607
Epoch:   500  |  train loss: 0.0443721846
Epoch:   600  |  train loss: 0.0435029969
Epoch:   700  |  train loss: 0.0436620556
Epoch:   800  |  train loss: 0.0434286200
Epoch:   900  |  train loss: 0.0429749906
Epoch:  1000  |  train loss: 0.0438086674
Processing class: 41
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.0437140673
Epoch:   200  |  train loss: 0.0438828327
Epoch:   300  |  train loss: 0.0435652725
Epoch:   400  |  train loss: 0.0427046277
Epoch:   500  |  train loss: 0.0423690006
Epoch:   600  |  train loss: 0.0421856813
Epoch:   700  |  train loss: 0.0424599215
Epoch:   800  |  train loss: 0.0420046367
Epoch:   900  |  train loss: 0.0416843034
Epoch:  1000  |  train loss: 0.0415346697
Processing class: 42
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.0446942277
Epoch:   200  |  train loss: 0.0449290372
Epoch:   300  |  train loss: 0.0448460929
Epoch:   400  |  train loss: 0.0444545649
Epoch:   500  |  train loss: 0.0443058155
Epoch:   600  |  train loss: 0.0440667406
Epoch:   700  |  train loss: 0.0440568425
Epoch:   800  |  train loss: 0.0438886359
Epoch:   900  |  train loss: 0.0432728499
Epoch:  1000  |  train loss: 0.0429794140
Processing class: 43
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.0452332780
Epoch:   200  |  train loss: 0.0446724258
Epoch:   300  |  train loss: 0.0450175866
Epoch:   400  |  train loss: 0.0458580330
Epoch:   500  |  train loss: 0.0459235594
Epoch:   600  |  train loss: 0.0455841042
Epoch:   700  |  train loss: 0.0459239341
Epoch:   800  |  train loss: 0.0456097230
Epoch:   900  |  train loss: 0.0448034503
Epoch:  1000  |  train loss: 0.0454052083
Processing class: 44
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.0450474150
Epoch:   200  |  train loss: 0.0451643325
Epoch:   300  |  train loss: 0.0445947379
Epoch:   400  |  train loss: 0.0442417353
Epoch:   500  |  train loss: 0.0443375036
Epoch:   600  |  train loss: 0.0438738078
Epoch:   700  |  train loss: 0.0442500293
Epoch:   800  |  train loss: 0.0438481249
Epoch:   900  |  train loss: 0.0433502100
Epoch:  1000  |  train loss: 0.0437498465
Processing class: 45
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.0447350867
Epoch:   200  |  train loss: 0.0453473508
Epoch:   300  |  train loss: 0.0444883898
Epoch:   400  |  train loss: 0.0452700444
Epoch:   500  |  train loss: 0.0450325646
Epoch:   600  |  train loss: 0.0456790164
Epoch:   700  |  train loss: 0.0456303939
Epoch:   800  |  train loss: 0.0452740960
Epoch:   900  |  train loss: 0.0453289270
Epoch:  1000  |  train loss: 0.0456597820
Processing class: 46
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.0444472477
Epoch:   200  |  train loss: 0.0442009389
Epoch:   300  |  train loss: 0.0439878330
Epoch:   400  |  train loss: 0.0442007013
Epoch:   500  |  train loss: 0.0441250116
Epoch:   600  |  train loss: 0.0440808594
Epoch:   700  |  train loss: 0.0440243512
Epoch:   800  |  train loss: 0.0445310980
Epoch:   900  |  train loss: 0.0439055219
Epoch:  1000  |  train loss: 0.0441357374
Processing class: 47
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.0448886290
Epoch:   200  |  train loss: 0.0450545438
Epoch:   300  |  train loss: 0.0438505255
Epoch:   400  |  train loss: 0.0440760188
Epoch:   500  |  train loss: 0.0437216327
Epoch:   600  |  train loss: 0.0432821691
Epoch:   700  |  train loss: 0.0431269735
Epoch:   800  |  train loss: 0.0424713731
Epoch:   900  |  train loss: 0.0424422257
Epoch:  1000  |  train loss: 0.0420125522
Processing class: 48
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.0450421363
Epoch:   200  |  train loss: 0.0455718920
Epoch:   300  |  train loss: 0.0444034241
Epoch:   400  |  train loss: 0.0433576502
Epoch:   500  |  train loss: 0.0442089781
Epoch:   600  |  train loss: 0.0438006587
Epoch:   700  |  train loss: 0.0431324795
Epoch:   800  |  train loss: 0.0430161640
Epoch:   900  |  train loss: 0.0438206419
Epoch:  1000  |  train loss: 0.0438474417
Processing class: 49
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.0451721072
Epoch:   200  |  train loss: 0.0449680746
Epoch:   300  |  train loss: 0.0454441279
Epoch:   400  |  train loss: 0.0452231571
Epoch:   500  |  train loss: 0.0450206980
Epoch:   600  |  train loss: 0.0448304228
Epoch:   700  |  train loss: 0.0445103206
Epoch:   800  |  train loss: 0.0441023037
Epoch:   900  |  train loss: 0.0438058168
Epoch:  1000  |  train loss: 0.0437642135
/home/z1165703/FeCAM/models/base.py:87: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  vec = torch.tensor(vec).cuda()
2024-03-04 23:28:40,621 [trainer.py] => CNN: {'total': 83.44, '00-09': 87.7, '10-19': 79.6, '20-29': 84.9, '30-39': 81.0, '40-49': 84.0, 'old': 0, 'new': 83.44}
2024-03-04 23:28:40,625 [trainer.py] => No NME accuracy
2024-03-04 23:28:40,625 [trainer.py] => FeCAM: {'total': 82.9, '00-09': 86.2, '10-19': 79.7, '20-29': 85.0, '30-39': 80.6, '40-49': 83.0, 'old': 0, 'new': 82.9}
2024-03-04 23:28:40,625 [trainer.py] => CNN top1 curve: [83.44]
2024-03-04 23:28:40,625 [trainer.py] => CNN top5 curve: [96.5]
2024-03-04 23:28:40,625 [trainer.py] => FeCAM top1 curve: [82.9]
2024-03-04 23:28:40,625 [trainer.py] => FeCAM top5 curve: [95.84]

2024-03-04 23:28:40,644 [fecam.py] => Learning on 50-60
Processing class: 50
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.0460526817
Epoch:   200  |  train loss: 0.0455847710
Epoch:   300  |  train loss: 0.0452363618
Epoch:   400  |  train loss: 0.0453978628
Epoch:   500  |  train loss: 0.0451857261
Epoch:   600  |  train loss: 0.0443870559
Epoch:   700  |  train loss: 0.0446799293
Epoch:   800  |  train loss: 0.0446966365
Epoch:   900  |  train loss: 0.0448831677
Epoch:  1000  |  train loss: 0.0448031500
Processing class: 51
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.0464686260
Epoch:   200  |  train loss: 0.0450991735
Epoch:   300  |  train loss: 0.0456464551
Epoch:   400  |  train loss: 0.0454703122
Epoch:   500  |  train loss: 0.0450383902
Epoch:   600  |  train loss: 0.0448813327
Epoch:   700  |  train loss: 0.0439961769
Epoch:   800  |  train loss: 0.0438838385
Epoch:   900  |  train loss: 0.0444316112
Epoch:  1000  |  train loss: 0.0437676594
Processing class: 52
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.0472410508
Epoch:   200  |  train loss: 0.0467022277
Epoch:   300  |  train loss: 0.0466738336
Epoch:   400  |  train loss: 0.0466139555
Epoch:   500  |  train loss: 0.0461478561
Epoch:   600  |  train loss: 0.0453733720
Epoch:   700  |  train loss: 0.0457265720
Epoch:   800  |  train loss: 0.0454969652
Epoch:   900  |  train loss: 0.0453654386
Epoch:  1000  |  train loss: 0.0446563832
Processing class: 53
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.0451838821
Epoch:   200  |  train loss: 0.0446407698
Epoch:   300  |  train loss: 0.0436439149
Epoch:   400  |  train loss: 0.0434730485
Epoch:   500  |  train loss: 0.0435918465
Epoch:   600  |  train loss: 0.0439244151
Epoch:   700  |  train loss: 0.0442703024
Epoch:   800  |  train loss: 0.0428691149
Epoch:   900  |  train loss: 0.0433166042
Epoch:  1000  |  train loss: 0.0426786624
Processing class: 54
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.0426243320
Epoch:   200  |  train loss: 0.0421427220
Epoch:   300  |  train loss: 0.0419710621
Epoch:   400  |  train loss: 0.0417581551
Epoch:   500  |  train loss: 0.0416983753
Epoch:   600  |  train loss: 0.0420839734
Epoch:   700  |  train loss: 0.0419946343
Epoch:   800  |  train loss: 0.0423080556
Epoch:   900  |  train loss: 0.0424918324
Epoch:  1000  |  train loss: 0.0420242190
Processing class: 55
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.0470125154
Epoch:   200  |  train loss: 0.0469034776
Epoch:   300  |  train loss: 0.0462882347
Epoch:   400  |  train loss: 0.0467593968
Epoch:   500  |  train loss: 0.0474290729
Epoch:   600  |  train loss: 0.0472371891
Epoch:   700  |  train loss: 0.0472715691
Epoch:   800  |  train loss: 0.0477342121
Epoch:   900  |  train loss: 0.0465371341
Epoch:  1000  |  train loss: 0.0468420342
Processing class: 56
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.0456485435
Epoch:   200  |  train loss: 0.0449985884
Epoch:   300  |  train loss: 0.0432698332
Epoch:   400  |  train loss: 0.0435200535
Epoch:   500  |  train loss: 0.0431852534
Epoch:   600  |  train loss: 0.0428005248
Epoch:   700  |  train loss: 0.0426801719
Epoch:   800  |  train loss: 0.0419705845
Epoch:   900  |  train loss: 0.0422335908
Epoch:  1000  |  train loss: 0.0423036970
Processing class: 57
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.0465975232
Epoch:   200  |  train loss: 0.0470931523
Epoch:   300  |  train loss: 0.0465606324
Epoch:   400  |  train loss: 0.0462817855
Epoch:   500  |  train loss: 0.0464785829
Epoch:   600  |  train loss: 0.0455962658
Epoch:   700  |  train loss: 0.0462896064
Epoch:   800  |  train loss: 0.0459974378
Epoch:   900  |  train loss: 0.0462574609
Epoch:  1000  |  train loss: 0.0451867610
Processing class: 58
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.0461584143
Epoch:   200  |  train loss: 0.0445077017
Epoch:   300  |  train loss: 0.0443945445
Epoch:   400  |  train loss: 0.0444037974
Epoch:   500  |  train loss: 0.0437879771
Epoch:   600  |  train loss: 0.0440128408
Epoch:   700  |  train loss: 0.0434620380
Epoch:   800  |  train loss: 0.0436668828
Epoch:   900  |  train loss: 0.0433941193
Epoch:  1000  |  train loss: 0.0431236312
Processing class: 59
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.0474049196
Epoch:   200  |  train loss: 0.0478503324
Epoch:   300  |  train loss: 0.0468038715
Epoch:   400  |  train loss: 0.0465972066
Epoch:   500  |  train loss: 0.0466465697
Epoch:   600  |  train loss: 0.0464445904
Epoch:   700  |  train loss: 0.0464949995
Epoch:   800  |  train loss: 0.0463806994
Epoch:   900  |  train loss: 0.0460213840
Epoch:  1000  |  train loss: 0.0457132764
2024-03-04 23:34:34,689 [trainer.py] => CNN: {'total': 71.75, '00-09': 82.2, '10-19': 72.8, '20-29': 78.5, '30-39': 74.9, '40-49': 68.4, '50-59': 53.7, 'old': 75.36, 'new': 53.7}
2024-03-04 23:34:34,690 [trainer.py] => No NME accuracy
2024-03-04 23:34:34,690 [trainer.py] => FeCAM: {'total': 72.78, '00-09': 82.6, '10-19': 74.0, '20-29': 81.0, '30-39': 76.4, '40-49': 76.8, '50-59': 45.9, 'old': 78.16, 'new': 45.9}
2024-03-04 23:34:34,690 [trainer.py] => CNN top1 curve: [83.44, 71.75]
2024-03-04 23:34:34,690 [trainer.py] => CNN top5 curve: [96.5, 89.6]
2024-03-04 23:34:34,690 [trainer.py] => FeCAM top1 curve: [82.9, 72.78]
2024-03-04 23:34:34,690 [trainer.py] => FeCAM top5 curve: [95.84, 91.18]

2024-03-04 23:34:34,696 [fecam.py] => Learning on 60-70
Processing class: 60
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.0475584187
Epoch:   200  |  train loss: 0.0470640101
Epoch:   300  |  train loss: 0.0460850291
Epoch:   400  |  train loss: 0.0454092294
Epoch:   500  |  train loss: 0.0456825756
Epoch:   600  |  train loss: 0.0452487700
Epoch:   700  |  train loss: 0.0464831226
Epoch:   800  |  train loss: 0.0451168232
Epoch:   900  |  train loss: 0.0449731022
Epoch:  1000  |  train loss: 0.0451189600
Processing class: 61
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.0454888605
Epoch:   200  |  train loss: 0.0439976580
Epoch:   300  |  train loss: 0.0439012617
Epoch:   400  |  train loss: 0.0432553560
Epoch:   500  |  train loss: 0.0437038004
Epoch:   600  |  train loss: 0.0426481992
Epoch:   700  |  train loss: 0.0430316806
Epoch:   800  |  train loss: 0.0432109572
Epoch:   900  |  train loss: 0.0421338007
Epoch:  1000  |  train loss: 0.0422918871
Processing class: 62
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.0475769967
Epoch:   200  |  train loss: 0.0469106682
Epoch:   300  |  train loss: 0.0472698353
Epoch:   400  |  train loss: 0.0473079912
Epoch:   500  |  train loss: 0.0466599442
Epoch:   600  |  train loss: 0.0472811818
Epoch:   700  |  train loss: 0.0473593280
Epoch:   800  |  train loss: 0.0472717747
Epoch:   900  |  train loss: 0.0467823759
Epoch:  1000  |  train loss: 0.0468405649
Processing class: 63
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.0457637638
Epoch:   200  |  train loss: 0.0456814222
Epoch:   300  |  train loss: 0.0459402032
Epoch:   400  |  train loss: 0.0453454070
Epoch:   500  |  train loss: 0.0451289229
Epoch:   600  |  train loss: 0.0455525354
Epoch:   700  |  train loss: 0.0443021171
Epoch:   800  |  train loss: 0.0453494065
Epoch:   900  |  train loss: 0.0442492701
Epoch:  1000  |  train loss: 0.0442804098
Processing class: 64
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.0469398454
Epoch:   200  |  train loss: 0.0448038779
Epoch:   300  |  train loss: 0.0452630915
Epoch:   400  |  train loss: 0.0447973795
Epoch:   500  |  train loss: 0.0441901103
Epoch:   600  |  train loss: 0.0436718233
Epoch:   700  |  train loss: 0.0426878057
Epoch:   800  |  train loss: 0.0427851334
Epoch:   900  |  train loss: 0.0431836009
Epoch:  1000  |  train loss: 0.0426251896
Processing class: 65
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.0459500693
Epoch:   200  |  train loss: 0.0447099082
Epoch:   300  |  train loss: 0.0448951125
Epoch:   400  |  train loss: 0.0447544143
Epoch:   500  |  train loss: 0.0440508552
Epoch:   600  |  train loss: 0.0446532212
Epoch:   700  |  train loss: 0.0443252437
Epoch:   800  |  train loss: 0.0449503161
Epoch:   900  |  train loss: 0.0440726317
Epoch:  1000  |  train loss: 0.0447729945
Processing class: 66
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.0471458904
Epoch:   200  |  train loss: 0.0470374480
Epoch:   300  |  train loss: 0.0469515868
Epoch:   400  |  train loss: 0.0464935303
Epoch:   500  |  train loss: 0.0468819879
Epoch:   600  |  train loss: 0.0466676056
Epoch:   700  |  train loss: 0.0471516564
Epoch:   800  |  train loss: 0.0470671959
Epoch:   900  |  train loss: 0.0468078598
Epoch:  1000  |  train loss: 0.0466003478
Processing class: 67
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.0460889690
Epoch:   200  |  train loss: 0.0460682206
Epoch:   300  |  train loss: 0.0455886379
Epoch:   400  |  train loss: 0.0453903437
Epoch:   500  |  train loss: 0.0450878464
Epoch:   600  |  train loss: 0.0448267750
Epoch:   700  |  train loss: 0.0447564512
Epoch:   800  |  train loss: 0.0449685298
Epoch:   900  |  train loss: 0.0446385399
Epoch:  1000  |  train loss: 0.0444020346
Processing class: 68
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.0449888691
Epoch:   200  |  train loss: 0.0446160644
Epoch:   300  |  train loss: 0.0433761567
Epoch:   400  |  train loss: 0.0431520805
Epoch:   500  |  train loss: 0.0429150946
Epoch:   600  |  train loss: 0.0424536906
Epoch:   700  |  train loss: 0.0425230153
Epoch:   800  |  train loss: 0.0419442914
Epoch:   900  |  train loss: 0.0418922439
Epoch:  1000  |  train loss: 0.0425693281
Processing class: 69
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.0414900824
Epoch:   200  |  train loss: 0.0412608691
Epoch:   300  |  train loss: 0.0414652884
Epoch:   400  |  train loss: 0.0413575381
Epoch:   500  |  train loss: 0.0410291210
Epoch:   600  |  train loss: 0.0400974855
Epoch:   700  |  train loss: 0.0405444972
Epoch:   800  |  train loss: 0.0403136000
Epoch:   900  |  train loss: 0.0412096187
Epoch:  1000  |  train loss: 0.0400959171
2024-03-04 23:41:28,498 [trainer.py] => CNN: {'total': 64.66, '00-09': 75.0, '10-19': 70.1, '20-29': 77.1, '30-39': 70.6, '40-49': 64.0, '50-59': 45.8, '60-69': 50.0, 'old': 67.1, 'new': 50.0}
2024-03-04 23:41:28,499 [trainer.py] => No NME accuracy
2024-03-04 23:41:28,499 [trainer.py] => FeCAM: {'total': 67.27, '00-09': 79.9, '10-19': 70.6, '20-29': 79.1, '30-39': 71.9, '40-49': 74.1, '50-59': 41.2, '60-69': 54.1, 'old': 69.47, 'new': 54.1}
2024-03-04 23:41:28,499 [trainer.py] => CNN top1 curve: [83.44, 71.75, 64.66]
2024-03-04 23:41:28,499 [trainer.py] => CNN top5 curve: [96.5, 89.6, 86.54]
2024-03-04 23:41:28,499 [trainer.py] => FeCAM top1 curve: [82.9, 72.78, 67.27]
2024-03-04 23:41:28,499 [trainer.py] => FeCAM top5 curve: [95.84, 91.18, 87.21]

2024-03-04 23:41:28,503 [fecam.py] => Learning on 70-80
Processing class: 70
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.0458168179
Epoch:   200  |  train loss: 0.0449939549
Epoch:   300  |  train loss: 0.0445011161
Epoch:   400  |  train loss: 0.0444160081
Epoch:   500  |  train loss: 0.0435384229
Epoch:   600  |  train loss: 0.0437964045
Epoch:   700  |  train loss: 0.0441120736
Epoch:   800  |  train loss: 0.0428537115
Epoch:   900  |  train loss: 0.0432598464
Epoch:  1000  |  train loss: 0.0428031772
Processing class: 71
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.0468375243
Epoch:   200  |  train loss: 0.0467824176
Epoch:   300  |  train loss: 0.0455211252
Epoch:   400  |  train loss: 0.0455938265
Epoch:   500  |  train loss: 0.0455443762
Epoch:   600  |  train loss: 0.0452548817
Epoch:   700  |  train loss: 0.0449904025
Epoch:   800  |  train loss: 0.0448614858
Epoch:   900  |  train loss: 0.0451985352
Epoch:  1000  |  train loss: 0.0447552845
Processing class: 72
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.0460555606
Epoch:   200  |  train loss: 0.0458790891
Epoch:   300  |  train loss: 0.0455031075
Epoch:   400  |  train loss: 0.0449846633
Epoch:   500  |  train loss: 0.0450578190
Epoch:   600  |  train loss: 0.0442849904
Epoch:   700  |  train loss: 0.0442959264
Epoch:   800  |  train loss: 0.0438157208
Epoch:   900  |  train loss: 0.0437455751
Epoch:  1000  |  train loss: 0.0440856978
Processing class: 73
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.0468175799
Epoch:   200  |  train loss: 0.0460441627
Epoch:   300  |  train loss: 0.0461040765
Epoch:   400  |  train loss: 0.0453093685
Epoch:   500  |  train loss: 0.0460530929
Epoch:   600  |  train loss: 0.0457925163
Epoch:   700  |  train loss: 0.0450558908
Epoch:   800  |  train loss: 0.0448887020
Epoch:   900  |  train loss: 0.0447191253
Epoch:  1000  |  train loss: 0.0448705837
Processing class: 74
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.0476822235
Epoch:   200  |  train loss: 0.0479858004
Epoch:   300  |  train loss: 0.0468923487
Epoch:   400  |  train loss: 0.0461535096
Epoch:   500  |  train loss: 0.0461694106
Epoch:   600  |  train loss: 0.0455873981
Epoch:   700  |  train loss: 0.0463097334
Epoch:   800  |  train loss: 0.0451281495
Epoch:   900  |  train loss: 0.0450150557
Epoch:  1000  |  train loss: 0.0455324978
Processing class: 75
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.0464014217
Epoch:   200  |  train loss: 0.0458140165
Epoch:   300  |  train loss: 0.0459877059
Epoch:   400  |  train loss: 0.0454906963
Epoch:   500  |  train loss: 0.0457133040
Epoch:   600  |  train loss: 0.0459636331
Epoch:   700  |  train loss: 0.0442036256
Epoch:   800  |  train loss: 0.0449392043
Epoch:   900  |  train loss: 0.0446519770
Epoch:  1000  |  train loss: 0.0450596347
Processing class: 76
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.0407782957
Epoch:   200  |  train loss: 0.0400324188
Epoch:   300  |  train loss: 0.0402417503
Epoch:   400  |  train loss: 0.0404766001
Epoch:   500  |  train loss: 0.0405088015
Epoch:   600  |  train loss: 0.0407636166
Epoch:   700  |  train loss: 0.0408148706
Epoch:   800  |  train loss: 0.0398321703
Epoch:   900  |  train loss: 0.0405048296
Epoch:  1000  |  train loss: 0.0407719202
Processing class: 77
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.0455922231
Epoch:   200  |  train loss: 0.0450964041
Epoch:   300  |  train loss: 0.0438982747
Epoch:   400  |  train loss: 0.0437267579
Epoch:   500  |  train loss: 0.0432469733
Epoch:   600  |  train loss: 0.0427448489
Epoch:   700  |  train loss: 0.0425620086
Epoch:   800  |  train loss: 0.0428912871
Epoch:   900  |  train loss: 0.0423145346
Epoch:  1000  |  train loss: 0.0419662513
Processing class: 78
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.0465323567
Epoch:   200  |  train loss: 0.0461403161
Epoch:   300  |  train loss: 0.0462352872
Epoch:   400  |  train loss: 0.0457048483
Epoch:   500  |  train loss: 0.0450413294
Epoch:   600  |  train loss: 0.0452305809
Epoch:   700  |  train loss: 0.0450638659
Epoch:   800  |  train loss: 0.0449551098
Epoch:   900  |  train loss: 0.0448304608
Epoch:  1000  |  train loss: 0.0445097916
Processing class: 79
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.0472592592
Epoch:   200  |  train loss: 0.0464980356
Epoch:   300  |  train loss: 0.0469375737
Epoch:   400  |  train loss: 0.0465692006
Epoch:   500  |  train loss: 0.0463323385
Epoch:   600  |  train loss: 0.0467508145
Epoch:   700  |  train loss: 0.0464954302
Epoch:   800  |  train loss: 0.0465267718
Epoch:   900  |  train loss: 0.0468291745
Epoch:  1000  |  train loss: 0.0460804477
2024-03-04 23:49:12,566 [trainer.py] => CNN: {'total': 59.18, '00-09': 73.6, '10-19': 68.4, '20-29': 76.9, '30-39': 69.1, '40-49': 60.6, '50-59': 37.2, '60-69': 45.7, '70-79': 41.9, 'old': 61.64, 'new': 41.9}
2024-03-04 23:49:12,567 [trainer.py] => No NME accuracy
2024-03-04 23:49:12,567 [trainer.py] => FeCAM: {'total': 61.72, '00-09': 78.1, '10-19': 69.3, '20-29': 78.0, '30-39': 69.5, '40-49': 72.1, '50-59': 35.7, '60-69': 49.1, '70-79': 42.0, 'old': 64.54, 'new': 42.0}
2024-03-04 23:49:12,567 [trainer.py] => CNN top1 curve: [83.44, 71.75, 64.66, 59.18]
2024-03-04 23:49:12,567 [trainer.py] => CNN top5 curve: [96.5, 89.6, 86.54, 84.09]
2024-03-04 23:49:12,567 [trainer.py] => FeCAM top1 curve: [82.9, 72.78, 67.27, 61.72]
2024-03-04 23:49:12,567 [trainer.py] => FeCAM top5 curve: [95.84, 91.18, 87.21, 84.09]

2024-03-04 23:49:12,578 [fecam.py] => Learning on 80-90
Processing class: 80
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.0437233992
Epoch:   200  |  train loss: 0.0431048065
Epoch:   300  |  train loss: 0.0428697146
Epoch:   400  |  train loss: 0.0427343048
Epoch:   500  |  train loss: 0.0423300289
Epoch:   600  |  train loss: 0.0421326093
Epoch:   700  |  train loss: 0.0412816197
Epoch:   800  |  train loss: 0.0420907803
Epoch:   900  |  train loss: 0.0417353123
Epoch:  1000  |  train loss: 0.0420696847
Processing class: 81
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.0456034169
Epoch:   200  |  train loss: 0.0458658673
Epoch:   300  |  train loss: 0.0447324321
Epoch:   400  |  train loss: 0.0452422641
Epoch:   500  |  train loss: 0.0446667850
Epoch:   600  |  train loss: 0.0450350851
Epoch:   700  |  train loss: 0.0446333341
Epoch:   800  |  train loss: 0.0448948853
Epoch:   900  |  train loss: 0.0446536534
Epoch:  1000  |  train loss: 0.0445757605
Processing class: 82
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.0446729578
Epoch:   200  |  train loss: 0.0443884008
Epoch:   300  |  train loss: 0.0439403281
Epoch:   400  |  train loss: 0.0433518164
Epoch:   500  |  train loss: 0.0430971168
Epoch:   600  |  train loss: 0.0426329777
Epoch:   700  |  train loss: 0.0423947863
Epoch:   800  |  train loss: 0.0423984960
Epoch:   900  |  train loss: 0.0422223158
Epoch:  1000  |  train loss: 0.0420438856
Processing class: 83
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.0448288515
Epoch:   200  |  train loss: 0.0441522367
Epoch:   300  |  train loss: 0.0430945210
Epoch:   400  |  train loss: 0.0427715421
Epoch:   500  |  train loss: 0.0430676855
Epoch:   600  |  train loss: 0.0422847584
Epoch:   700  |  train loss: 0.0427865513
Epoch:   800  |  train loss: 0.0434160940
Epoch:   900  |  train loss: 0.0428990543
Epoch:  1000  |  train loss: 0.0423889577
Processing class: 84
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.0467931606
Epoch:   200  |  train loss: 0.0456329249
Epoch:   300  |  train loss: 0.0456614770
Epoch:   400  |  train loss: 0.0451274484
Epoch:   500  |  train loss: 0.0451868974
Epoch:   600  |  train loss: 0.0443508022
Epoch:   700  |  train loss: 0.0446622215
Epoch:   800  |  train loss: 0.0449644811
Epoch:   900  |  train loss: 0.0440092064
Epoch:  1000  |  train loss: 0.0436939113
Processing class: 85
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.0465607353
Epoch:   200  |  train loss: 0.0455390409
Epoch:   300  |  train loss: 0.0454982065
Epoch:   400  |  train loss: 0.0447039835
Epoch:   500  |  train loss: 0.0447457701
Epoch:   600  |  train loss: 0.0448984057
Epoch:   700  |  train loss: 0.0454214819
Epoch:   800  |  train loss: 0.0445647649
Epoch:   900  |  train loss: 0.0444544397
Epoch:  1000  |  train loss: 0.0448300578
Processing class: 86
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.0477633648
Epoch:   200  |  train loss: 0.0467827700
Epoch:   300  |  train loss: 0.0470819153
Epoch:   400  |  train loss: 0.0462772772
Epoch:   500  |  train loss: 0.0459777184
Epoch:   600  |  train loss: 0.0459469505
Epoch:   700  |  train loss: 0.0457216851
Epoch:   800  |  train loss: 0.0453830868
Epoch:   900  |  train loss: 0.0451778643
Epoch:  1000  |  train loss: 0.0457955152
Processing class: 87
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.0448620535
Epoch:   200  |  train loss: 0.0441986136
Epoch:   300  |  train loss: 0.0437253617
Epoch:   400  |  train loss: 0.0434822418
Epoch:   500  |  train loss: 0.0430117235
Epoch:   600  |  train loss: 0.0431955397
Epoch:   700  |  train loss: 0.0431552507
Epoch:   800  |  train loss: 0.0429314561
Epoch:   900  |  train loss: 0.0423126608
Epoch:  1000  |  train loss: 0.0430656023
Processing class: 88
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.0459420890
Epoch:   200  |  train loss: 0.0451618567
Epoch:   300  |  train loss: 0.0445198834
Epoch:   400  |  train loss: 0.0441978186
Epoch:   500  |  train loss: 0.0436210059
Epoch:   600  |  train loss: 0.0428737298
Epoch:   700  |  train loss: 0.0430974901
Epoch:   800  |  train loss: 0.0428157754
Epoch:   900  |  train loss: 0.0427490287
Epoch:  1000  |  train loss: 0.0421985753
Processing class: 89
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.0461941756
Epoch:   200  |  train loss: 0.0459697165
Epoch:   300  |  train loss: 0.0454777867
Epoch:   400  |  train loss: 0.0452500649
Epoch:   500  |  train loss: 0.0454196818
Epoch:   600  |  train loss: 0.0450273186
Epoch:   700  |  train loss: 0.0442231357
Epoch:   800  |  train loss: 0.0450636946
Epoch:   900  |  train loss: 0.0445226103
Epoch:  1000  |  train loss: 0.0446014754
2024-03-04 23:58:19,671 [trainer.py] => CNN: {'total': 54.08, '00-09': 67.9, '10-19': 63.6, '20-29': 73.2, '30-39': 68.8, '40-49': 57.2, '50-59': 32.6, '60-69': 40.9, '70-79': 38.1, '80-89': 44.4, 'old': 55.29, 'new': 44.4}
2024-03-04 23:58:19,672 [trainer.py] => No NME accuracy
2024-03-04 23:58:19,672 [trainer.py] => FeCAM: {'total': 57.66, '00-09': 76.1, '10-19': 65.8, '20-29': 75.3, '30-39': 67.9, '40-49': 68.5, '50-59': 33.0, '60-69': 45.0, '70-79': 39.0, '80-89': 48.3, 'old': 58.82, 'new': 48.3}
2024-03-04 23:58:19,672 [trainer.py] => CNN top1 curve: [83.44, 71.75, 64.66, 59.18, 54.08]
2024-03-04 23:58:19,672 [trainer.py] => CNN top5 curve: [96.5, 89.6, 86.54, 84.09, 81.74]
2024-03-04 23:58:19,672 [trainer.py] => FeCAM top1 curve: [82.9, 72.78, 67.27, 61.72, 57.66]
2024-03-04 23:58:19,672 [trainer.py] => FeCAM top5 curve: [95.84, 91.18, 87.21, 84.09, 81.63]

2024-03-04 23:58:19,677 [fecam.py] => Learning on 90-100
Processing class: 90
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.0466319293
Epoch:   200  |  train loss: 0.0462977633
Epoch:   300  |  train loss: 0.0449231215
Epoch:   400  |  train loss: 0.0440859266
Epoch:   500  |  train loss: 0.0433610953
Epoch:   600  |  train loss: 0.0431682438
Epoch:   700  |  train loss: 0.0425634302
Epoch:   800  |  train loss: 0.0424380079
Epoch:   900  |  train loss: 0.0425461926
Epoch:  1000  |  train loss: 0.0429285191
Processing class: 91
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.0416725107
Epoch:   200  |  train loss: 0.0408926569
Epoch:   300  |  train loss: 0.0409195252
Epoch:   400  |  train loss: 0.0411958866
Epoch:   500  |  train loss: 0.0412029199
Epoch:   600  |  train loss: 0.0411960639
Epoch:   700  |  train loss: 0.0409180939
Epoch:   800  |  train loss: 0.0408712581
Epoch:   900  |  train loss: 0.0409555636
Epoch:  1000  |  train loss: 0.0409103729
Processing class: 92
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.0465399675
Epoch:   200  |  train loss: 0.0446447186
Epoch:   300  |  train loss: 0.0434824593
Epoch:   400  |  train loss: 0.0435098968
Epoch:   500  |  train loss: 0.0433067165
Epoch:   600  |  train loss: 0.0432209633
Epoch:   700  |  train loss: 0.0428418033
Epoch:   800  |  train loss: 0.0417069301
Epoch:   900  |  train loss: 0.0416704975
Epoch:  1000  |  train loss: 0.0415628143
Processing class: 93
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.0461630858
Epoch:   200  |  train loss: 0.0455408640
Epoch:   300  |  train loss: 0.0460913576
Epoch:   400  |  train loss: 0.0459044509
Epoch:   500  |  train loss: 0.0453227736
Epoch:   600  |  train loss: 0.0444495469
Epoch:   700  |  train loss: 0.0451889306
Epoch:   800  |  train loss: 0.0447734147
Epoch:   900  |  train loss: 0.0446379572
Epoch:  1000  |  train loss: 0.0442171000
Processing class: 94
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.0428449057
Epoch:   200  |  train loss: 0.0420278564
Epoch:   300  |  train loss: 0.0402714573
Epoch:   400  |  train loss: 0.0399712175
Epoch:   500  |  train loss: 0.0385177962
Epoch:   600  |  train loss: 0.0387188435
Epoch:   700  |  train loss: 0.0389534563
Epoch:   800  |  train loss: 0.0374302723
Epoch:   900  |  train loss: 0.0380195796
Epoch:  1000  |  train loss: 0.0375654198
Processing class: 95
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.0464615211
Epoch:   200  |  train loss: 0.0468625382
Epoch:   300  |  train loss: 0.0461052485
Epoch:   400  |  train loss: 0.0454332419
Epoch:   500  |  train loss: 0.0458503217
Epoch:   600  |  train loss: 0.0454347432
Epoch:   700  |  train loss: 0.0454471231
Epoch:   800  |  train loss: 0.0449060038
Epoch:   900  |  train loss: 0.0452065423
Epoch:  1000  |  train loss: 0.0454768136
Processing class: 96
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.0473661482
Epoch:   200  |  train loss: 0.0459836937
Epoch:   300  |  train loss: 0.0460013837
Epoch:   400  |  train loss: 0.0466013566
Epoch:   500  |  train loss: 0.0458826207
Epoch:   600  |  train loss: 0.0455114670
Epoch:   700  |  train loss: 0.0453243271
Epoch:   800  |  train loss: 0.0446446456
Epoch:   900  |  train loss: 0.0452901490
Epoch:  1000  |  train loss: 0.0449948214
Processing class: 97
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.0466126934
Epoch:   200  |  train loss: 0.0456397139
Epoch:   300  |  train loss: 0.0450399987
Epoch:   400  |  train loss: 0.0448301367
Epoch:   500  |  train loss: 0.0450318933
Epoch:   600  |  train loss: 0.0450622521
Epoch:   700  |  train loss: 0.0451736785
Epoch:   800  |  train loss: 0.0449517012
Epoch:   900  |  train loss: 0.0449262261
Epoch:  1000  |  train loss: 0.0446549729
Processing class: 98
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.0446453623
Epoch:   200  |  train loss: 0.0437200271
Epoch:   300  |  train loss: 0.0429818369
Epoch:   400  |  train loss: 0.0426851831
Epoch:   500  |  train loss: 0.0420677811
Epoch:   600  |  train loss: 0.0420386866
Epoch:   700  |  train loss: 0.0411333755
Epoch:   800  |  train loss: 0.0414141431
Epoch:   900  |  train loss: 0.0412594721
Epoch:  1000  |  train loss: 0.0408245549
Processing class: 99
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.0473676644
Epoch:   200  |  train loss: 0.0461455241
Epoch:   300  |  train loss: 0.0462842658
Epoch:   400  |  train loss: 0.0464704558
Epoch:   500  |  train loss: 0.0457615621
Epoch:   600  |  train loss: 0.0452373050
Epoch:   700  |  train loss: 0.0448177360
Epoch:   800  |  train loss: 0.0451842211
Epoch:   900  |  train loss: 0.0445943691
Epoch:  1000  |  train loss: 0.0445265561
2024-03-05 00:08:51,542 [trainer.py] => CNN: {'total': 50.36, '00-09': 57.7, '10-19': 63.6, '20-29': 71.4, '30-39': 68.5, '40-49': 56.8, '50-59': 29.6, '60-69': 38.9, '70-79': 36.6, '80-89': 42.7, '90-99': 37.8, 'old': 51.76, 'new': 37.8}
2024-03-05 00:08:51,544 [trainer.py] => No NME accuracy
2024-03-05 00:08:51,544 [trainer.py] => FeCAM: {'total': 54.74, '00-09': 73.3, '10-19': 65.1, '20-29': 74.1, '30-39': 67.2, '40-49': 67.1, '50-59': 30.8, '60-69': 43.3, '70-79': 37.2, '80-89': 46.9, '90-99': 42.4, 'old': 56.11, 'new': 42.4}
2024-03-05 00:08:51,544 [trainer.py] => CNN top1 curve: [83.44, 71.75, 64.66, 59.18, 54.08, 50.36]
2024-03-05 00:08:51,544 [trainer.py] => CNN top5 curve: [96.5, 89.6, 86.54, 84.09, 81.74, 79.63]
2024-03-05 00:08:51,544 [trainer.py] => FeCAM top1 curve: [82.9, 72.78, 67.27, 61.72, 57.66, 54.74]
2024-03-05 00:08:51,544 [trainer.py] => FeCAM top5 curve: [95.84, 91.18, 87.21, 84.09, 81.63, 79.21]

=========================================
2024-03-05 00:09:08,863 [trainer.py] => config: ./exps/FeCAM_cifar100.json
2024-03-05 00:09:08,863 [trainer.py] => prefix: train
2024-03-05 00:09:08,863 [trainer.py] => dataset: cifar100
2024-03-05 00:09:08,863 [trainer.py] => memory_size: 0
2024-03-05 00:09:08,864 [trainer.py] => shuffle: True
2024-03-05 00:09:08,864 [trainer.py] => init_cls: 50
2024-03-05 00:09:08,864 [trainer.py] => increment: 10
2024-03-05 00:09:08,864 [trainer.py] => model_name: fecam
2024-03-05 00:09:08,864 [trainer.py] => convnet_type: resnet18
2024-03-05 00:09:08,864 [trainer.py] => device: [device(type='cuda', index=0)]
2024-03-05 00:09:08,864 [trainer.py] => seed: 1993
2024-03-05 00:09:08,864 [trainer.py] => init_epochs: 200
2024-03-05 00:09:08,864 [trainer.py] => init_lr: 0.1
2024-03-05 00:09:08,865 [trainer.py] => init_weight_decay: 0.0005
2024-03-05 00:09:08,865 [trainer.py] => batch_size: 128
2024-03-05 00:09:08,865 [trainer.py] => num_workers: 8
2024-03-05 00:09:08,865 [trainer.py] => T: 5
2024-03-05 00:09:08,865 [trainer.py] => beta: 0.5
2024-03-05 00:09:08,865 [trainer.py] => alpha1: 1
2024-03-05 00:09:08,865 [trainer.py] => alpha2: 1
2024-03-05 00:09:08,865 [trainer.py] => ncm: False
2024-03-05 00:09:08,865 [trainer.py] => tukey: False
2024-03-05 00:09:08,865 [trainer.py] => diagonal: False
2024-03-05 00:09:08,865 [trainer.py] => per_class: True
2024-03-05 00:09:08,866 [trainer.py] => full_cov: True
2024-03-05 00:09:08,866 [trainer.py] => shrink: True
2024-03-05 00:09:08,866 [trainer.py] => norm_cov: False
2024-03-05 00:09:08,866 [trainer.py] => vecnorm: False
2024-03-05 00:09:08,866 [trainer.py] => ae_type: wae
2024-03-05 00:09:08,866 [trainer.py] => epochs: 1000
2024-03-05 00:09:08,866 [trainer.py] => ae_latent_dim: 32
2024-03-05 00:09:08,866 [trainer.py] => wae_sigma: 30
2024-03-05 00:09:08,866 [trainer.py] => wae_C: 0.1
Files already downloaded and verified
Files already downloaded and verified
2024-03-05 00:09:10,526 [data_manager.py] => [68, 56, 78, 8, 23, 84, 90, 65, 74, 76, 40, 89, 3, 92, 55, 9, 26, 80, 43, 38, 58, 70, 77, 1, 85, 19, 17, 50, 28, 53, 13, 81, 45, 82, 6, 59, 83, 16, 15, 44, 91, 41, 72, 60, 79, 52, 20, 10, 31, 54, 37, 95, 14, 71, 96, 98, 97, 2, 64, 66, 42, 22, 35, 86, 24, 34, 87, 21, 99, 0, 88, 27, 18, 94, 11, 12, 47, 25, 30, 46, 62, 69, 36, 61, 7, 63, 75, 5, 32, 4, 51, 48, 73, 93, 39, 67, 29, 49, 57, 33]
2024-03-05 00:09:11,047 [fecam.py] => Learning on 0-50
/home/z1165703/miniconda3/lib/python3.11/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
/home/z1165703/miniconda3/lib/python3.11/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
Processing class: 0
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.0357830025
Epoch:   200  |  train loss: 0.0359390192
Epoch:   300  |  train loss: 0.0365301222
Epoch:   400  |  train loss: 0.0366664983
Epoch:   500  |  train loss: 0.0363101475
Epoch:   600  |  train loss: 0.0363055795
Epoch:   700  |  train loss: 0.0365031563
Epoch:   800  |  train loss: 0.0365698993
Epoch:   900  |  train loss: 0.0362515479
Epoch:  1000  |  train loss: 0.0361731835
Processing class: 1
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.0357017159
Epoch:   200  |  train loss: 0.0354200102
Epoch:   300  |  train loss: 0.0365999378
Epoch:   400  |  train loss: 0.0368179239
Epoch:   500  |  train loss: 0.0359867632
Epoch:   600  |  train loss: 0.0369272001
Epoch:   700  |  train loss: 0.0364418708
Epoch:   800  |  train loss: 0.0368874483
Epoch:   900  |  train loss: 0.0375952989
Epoch:  1000  |  train loss: 0.0368170217
Processing class: 2
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.0357199147
Epoch:   200  |  train loss: 0.0354983188
Epoch:   300  |  train loss: 0.0362426952
Epoch:   400  |  train loss: 0.0359257154
Epoch:   500  |  train loss: 0.0344898351
Epoch:   600  |  train loss: 0.0354359761
Epoch:   700  |  train loss: 0.0351806708
Epoch:   800  |  train loss: 0.0358986266
Epoch:   900  |  train loss: 0.0353358775
Epoch:  1000  |  train loss: 0.0350104429
Processing class: 3
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.0372581825
Epoch:   200  |  train loss: 0.0380760439
Epoch:   300  |  train loss: 0.0383710898
Epoch:   400  |  train loss: 0.0383859158
Epoch:   500  |  train loss: 0.0379366063
Epoch:   600  |  train loss: 0.0387245946
Epoch:   700  |  train loss: 0.0391469099
Epoch:   800  |  train loss: 0.0385009050
Epoch:   900  |  train loss: 0.0388417028
Epoch:  1000  |  train loss: 0.0389004894
Processing class: 4
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.0354428805
Epoch:   200  |  train loss: 0.0350127786
Epoch:   300  |  train loss: 0.0359209098
Epoch:   400  |  train loss: 0.0358392537
Epoch:   500  |  train loss: 0.0361703701
Epoch:   600  |  train loss: 0.0358568370
Epoch:   700  |  train loss: 0.0362860493
Epoch:   800  |  train loss: 0.0363713712
Epoch:   900  |  train loss: 0.0365396410
Epoch:  1000  |  train loss: 0.0371289015
Processing class: 5
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.0358700581
Epoch:   200  |  train loss: 0.0354256503
Epoch:   300  |  train loss: 0.0356462866
Epoch:   400  |  train loss: 0.0355208226
Epoch:   500  |  train loss: 0.0353712589
Epoch:   600  |  train loss: 0.0364699289
Epoch:   700  |  train loss: 0.0365871742
Epoch:   800  |  train loss: 0.0359553747
Epoch:   900  |  train loss: 0.0357464693
Epoch:  1000  |  train loss: 0.0362217285
Processing class: 6
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.0357670993
Epoch:   200  |  train loss: 0.0361641824
Epoch:   300  |  train loss: 0.0370982774
Epoch:   400  |  train loss: 0.0368202470
Epoch:   500  |  train loss: 0.0376040407
Epoch:   600  |  train loss: 0.0372075446
Epoch:   700  |  train loss: 0.0368770882
Epoch:   800  |  train loss: 0.0370813437
Epoch:   900  |  train loss: 0.0365971588
Epoch:  1000  |  train loss: 0.0369118199
Processing class: 7
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.0353154473
Epoch:   200  |  train loss: 0.0353618868
Epoch:   300  |  train loss: 0.0356016017
Epoch:   400  |  train loss: 0.0356960714
Epoch:   500  |  train loss: 0.0360000685
Epoch:   600  |  train loss: 0.0356339835
Epoch:   700  |  train loss: 0.0360181451
Epoch:   800  |  train loss: 0.0367486827
Epoch:   900  |  train loss: 0.0359779090
Epoch:  1000  |  train loss: 0.0365332469
Processing class: 8
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.0355087981
Epoch:   200  |  train loss: 0.0359739736
Epoch:   300  |  train loss: 0.0354958653
Epoch:   400  |  train loss: 0.0364158675
Epoch:   500  |  train loss: 0.0358845048
Epoch:   600  |  train loss: 0.0360380620
Epoch:   700  |  train loss: 0.0367027447
Epoch:   800  |  train loss: 0.0360069372
Epoch:   900  |  train loss: 0.0363898195
Epoch:  1000  |  train loss: 0.0359279424
Processing class: 9
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.0346640296
Epoch:   200  |  train loss: 0.0352214560
Epoch:   300  |  train loss: 0.0360037528
Epoch:   400  |  train loss: 0.0351108916
Epoch:   500  |  train loss: 0.0358772568
Epoch:   600  |  train loss: 0.0353120252
Epoch:   700  |  train loss: 0.0355901860
Epoch:   800  |  train loss: 0.0357901007
Epoch:   900  |  train loss: 0.0357763052
Epoch:  1000  |  train loss: 0.0352050163
Processing class: 10
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.0344709292
Epoch:   200  |  train loss: 0.0342192106
Epoch:   300  |  train loss: 0.0344380558
Epoch:   400  |  train loss: 0.0353044845
Epoch:   500  |  train loss: 0.0340732545
Epoch:   600  |  train loss: 0.0344278671
Epoch:   700  |  train loss: 0.0347106114
Epoch:   800  |  train loss: 0.0344001524
Epoch:   900  |  train loss: 0.0346365750
Epoch:  1000  |  train loss: 0.0345455334
Processing class: 11
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.0359842226
Epoch:   200  |  train loss: 0.0360777341
Epoch:   300  |  train loss: 0.0355862044
Epoch:   400  |  train loss: 0.0359169394
Epoch:   500  |  train loss: 0.0358162396
Epoch:   600  |  train loss: 0.0363405727
Epoch:   700  |  train loss: 0.0371549942
Epoch:   800  |  train loss: 0.0369399041
Epoch:   900  |  train loss: 0.0369170830
Epoch:  1000  |  train loss: 0.0373059213
Processing class: 12
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.0345943697
Epoch:   200  |  train loss: 0.0358827047
Epoch:   300  |  train loss: 0.0358058214
Epoch:   400  |  train loss: 0.0363252960
Epoch:   500  |  train loss: 0.0363395564
Epoch:   600  |  train loss: 0.0358397320
Epoch:   700  |  train loss: 0.0360186026
Epoch:   800  |  train loss: 0.0362058572
Epoch:   900  |  train loss: 0.0366182387
Epoch:  1000  |  train loss: 0.0362301685
Processing class: 13
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.0351704106
Epoch:   200  |  train loss: 0.0360239714
Epoch:   300  |  train loss: 0.0366217725
Epoch:   400  |  train loss: 0.0366615295
Epoch:   500  |  train loss: 0.0372090839
Epoch:   600  |  train loss: 0.0366049178
Epoch:   700  |  train loss: 0.0369310282
Epoch:   800  |  train loss: 0.0372973323
Epoch:   900  |  train loss: 0.0374990359
Epoch:  1000  |  train loss: 0.0377031669
Processing class: 14
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.0357214317
Epoch:   200  |  train loss: 0.0361245386
Epoch:   300  |  train loss: 0.0362748653
Epoch:   400  |  train loss: 0.0368560769
Epoch:   500  |  train loss: 0.0365787156
Epoch:   600  |  train loss: 0.0364926912
Epoch:   700  |  train loss: 0.0368807197
Epoch:   800  |  train loss: 0.0377490468
Epoch:   900  |  train loss: 0.0374005467
Epoch:  1000  |  train loss: 0.0367641293
Processing class: 15
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.0353918932
Epoch:   200  |  train loss: 0.0357413761
Epoch:   300  |  train loss: 0.0364207149
Epoch:   400  |  train loss: 0.0366826452
Epoch:   500  |  train loss: 0.0370889790
Epoch:   600  |  train loss: 0.0365289040
Epoch:   700  |  train loss: 0.0372615270
Epoch:   800  |  train loss: 0.0365141816
Epoch:   900  |  train loss: 0.0372483738
Epoch:  1000  |  train loss: 0.0369022593
Processing class: 16
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.0357099235
Epoch:   200  |  train loss: 0.0372258343
Epoch:   300  |  train loss: 0.0378620021
Epoch:   400  |  train loss: 0.0373617247
Epoch:   500  |  train loss: 0.0379820392
Epoch:   600  |  train loss: 0.0374037474
Epoch:   700  |  train loss: 0.0371403694
Epoch:   800  |  train loss: 0.0380218156
Epoch:   900  |  train loss: 0.0371087834
Epoch:  1000  |  train loss: 0.0379302636
Processing class: 17
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.0342896231
Epoch:   200  |  train loss: 0.0347964935
Epoch:   300  |  train loss: 0.0355369061
Epoch:   400  |  train loss: 0.0355644815
Epoch:   500  |  train loss: 0.0344839320
Epoch:   600  |  train loss: 0.0349854805
Epoch:   700  |  train loss: 0.0347634301
Epoch:   800  |  train loss: 0.0347706087
Epoch:   900  |  train loss: 0.0349406056
Epoch:  1000  |  train loss: 0.0348339781
Processing class: 18
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.0351469949
Epoch:   200  |  train loss: 0.0353400812
Epoch:   300  |  train loss: 0.0356962055
Epoch:   400  |  train loss: 0.0355018809
Epoch:   500  |  train loss: 0.0361767694
Epoch:   600  |  train loss: 0.0357298829
Epoch:   700  |  train loss: 0.0368818045
Epoch:   800  |  train loss: 0.0367326200
Epoch:   900  |  train loss: 0.0369082704
Epoch:  1000  |  train loss: 0.0375374265
Processing class: 19
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.0352534540
Epoch:   200  |  train loss: 0.0366162986
Epoch:   300  |  train loss: 0.0365010835
Epoch:   400  |  train loss: 0.0365887672
Epoch:   500  |  train loss: 0.0359788761
Epoch:   600  |  train loss: 0.0360536382
Epoch:   700  |  train loss: 0.0363405548
Epoch:   800  |  train loss: 0.0360411070
Epoch:   900  |  train loss: 0.0372850418
Epoch:  1000  |  train loss: 0.0375029139
Processing class: 20
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.0355965018
Epoch:   200  |  train loss: 0.0359105624
Epoch:   300  |  train loss: 0.0363634437
Epoch:   400  |  train loss: 0.0365696825
Epoch:   500  |  train loss: 0.0372693792
Epoch:   600  |  train loss: 0.0367479295
Epoch:   700  |  train loss: 0.0368261665
Epoch:   800  |  train loss: 0.0368126288
Epoch:   900  |  train loss: 0.0370661736
Epoch:  1000  |  train loss: 0.0375613637
Processing class: 21
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.0357724622
Epoch:   200  |  train loss: 0.0362331390
Epoch:   300  |  train loss: 0.0362732127
Epoch:   400  |  train loss: 0.0363500006
Epoch:   500  |  train loss: 0.0365419261
Epoch:   600  |  train loss: 0.0368977152
Epoch:   700  |  train loss: 0.0367157668
Epoch:   800  |  train loss: 0.0364511214
Epoch:   900  |  train loss: 0.0370271303
Epoch:  1000  |  train loss: 0.0367600240
Processing class: 22
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.0353473216
Epoch:   200  |  train loss: 0.0355382286
Epoch:   300  |  train loss: 0.0361320667
Epoch:   400  |  train loss: 0.0367302634
Epoch:   500  |  train loss: 0.0367954098
Epoch:   600  |  train loss: 0.0363276981
Epoch:   700  |  train loss: 0.0365673348
Epoch:   800  |  train loss: 0.0362598300
Epoch:   900  |  train loss: 0.0359147906
Epoch:  1000  |  train loss: 0.0369295493
Processing class: 23
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.0361077532
Epoch:   200  |  train loss: 0.0367454395
Epoch:   300  |  train loss: 0.0373132110
Epoch:   400  |  train loss: 0.0364850171
Epoch:   500  |  train loss: 0.0363807619
Epoch:   600  |  train loss: 0.0358687662
Epoch:   700  |  train loss: 0.0362099178
Epoch:   800  |  train loss: 0.0358005442
Epoch:   900  |  train loss: 0.0361965612
Epoch:  1000  |  train loss: 0.0357032709
Processing class: 24
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.0357124008
Epoch:   200  |  train loss: 0.0356896132
Epoch:   300  |  train loss: 0.0350580178
Epoch:   400  |  train loss: 0.0356350057
Epoch:   500  |  train loss: 0.0352199472
Epoch:   600  |  train loss: 0.0355927087
Epoch:   700  |  train loss: 0.0346059687
Epoch:   800  |  train loss: 0.0348743916
Epoch:   900  |  train loss: 0.0353800997
Epoch:  1000  |  train loss: 0.0350936480
Processing class: 25
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.0353270352
Epoch:   200  |  train loss: 0.0357359387
Epoch:   300  |  train loss: 0.0363815136
Epoch:   400  |  train loss: 0.0356812693
Epoch:   500  |  train loss: 0.0363862485
Epoch:   600  |  train loss: 0.0359150112
Epoch:   700  |  train loss: 0.0366389066
Epoch:   800  |  train loss: 0.0362692952
Epoch:   900  |  train loss: 0.0359788291
Epoch:  1000  |  train loss: 0.0362210147
Processing class: 26
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.0363371290
Epoch:   200  |  train loss: 0.0371982664
Epoch:   300  |  train loss: 0.0367166013
Epoch:   400  |  train loss: 0.0373306349
Epoch:   500  |  train loss: 0.0379094496
Epoch:   600  |  train loss: 0.0367868513
Epoch:   700  |  train loss: 0.0371023968
Epoch:   800  |  train loss: 0.0373921029
Epoch:   900  |  train loss: 0.0371682264
Epoch:  1000  |  train loss: 0.0377228297
Processing class: 27
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.0356083892
Epoch:   200  |  train loss: 0.0351297393
Epoch:   300  |  train loss: 0.0353851907
Epoch:   400  |  train loss: 0.0354591690
Epoch:   500  |  train loss: 0.0357677847
Epoch:   600  |  train loss: 0.0357465491
Epoch:   700  |  train loss: 0.0359452955
Epoch:   800  |  train loss: 0.0358668208
Epoch:   900  |  train loss: 0.0360470623
Epoch:  1000  |  train loss: 0.0352649584
Processing class: 28
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.0361941673
Epoch:   200  |  train loss: 0.0368909873
Epoch:   300  |  train loss: 0.0374982953
Epoch:   400  |  train loss: 0.0377517514
Epoch:   500  |  train loss: 0.0378814116
Epoch:   600  |  train loss: 0.0378600061
Epoch:   700  |  train loss: 0.0379967637
Epoch:   800  |  train loss: 0.0381973594
Epoch:   900  |  train loss: 0.0383107945
Epoch:  1000  |  train loss: 0.0377122246
Processing class: 29
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.0356772825
Epoch:   200  |  train loss: 0.0359434098
Epoch:   300  |  train loss: 0.0357268758
Epoch:   400  |  train loss: 0.0352664210
Epoch:   500  |  train loss: 0.0358086079
Epoch:   600  |  train loss: 0.0360321060
Epoch:   700  |  train loss: 0.0359262548
Epoch:   800  |  train loss: 0.0354090311
Epoch:   900  |  train loss: 0.0354748718
Epoch:  1000  |  train loss: 0.0352704838
Processing class: 30
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.0363694698
Epoch:   200  |  train loss: 0.0371215805
Epoch:   300  |  train loss: 0.0380374484
Epoch:   400  |  train loss: 0.0389225177
Epoch:   500  |  train loss: 0.0384619996
Epoch:   600  |  train loss: 0.0383197144
Epoch:   700  |  train loss: 0.0380796060
Epoch:   800  |  train loss: 0.0386865035
Epoch:   900  |  train loss: 0.0390679739
Epoch:  1000  |  train loss: 0.0385355130
Processing class: 31
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.0362434648
Epoch:   200  |  train loss: 0.0370048717
Epoch:   300  |  train loss: 0.0369555600
Epoch:   400  |  train loss: 0.0377838798
Epoch:   500  |  train loss: 0.0380019426
Epoch:   600  |  train loss: 0.0386207141
Epoch:   700  |  train loss: 0.0383926898
Epoch:   800  |  train loss: 0.0384108394
Epoch:   900  |  train loss: 0.0389059559
Epoch:  1000  |  train loss: 0.0382253513
Processing class: 32
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.0357191823
Epoch:   200  |  train loss: 0.0363986321
Epoch:   300  |  train loss: 0.0361494236
Epoch:   400  |  train loss: 0.0354074776
Epoch:   500  |  train loss: 0.0365192510
Epoch:   600  |  train loss: 0.0360612348
Epoch:   700  |  train loss: 0.0369129278
Epoch:   800  |  train loss: 0.0371924587
Epoch:   900  |  train loss: 0.0369393222
Epoch:  1000  |  train loss: 0.0366338931
Processing class: 33
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.0345538877
Epoch:   200  |  train loss: 0.0362124950
Epoch:   300  |  train loss: 0.0347789772
Epoch:   400  |  train loss: 0.0348032832
Epoch:   500  |  train loss: 0.0348366767
Epoch:   600  |  train loss: 0.0357237108
Epoch:   700  |  train loss: 0.0351496659
Epoch:   800  |  train loss: 0.0353268020
Epoch:   900  |  train loss: 0.0351816528
Epoch:  1000  |  train loss: 0.0348572269
Processing class: 34
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.0359037802
Epoch:   200  |  train loss: 0.0356698118
Epoch:   300  |  train loss: 0.0360770017
Epoch:   400  |  train loss: 0.0356507145
Epoch:   500  |  train loss: 0.0354946606
Epoch:   600  |  train loss: 0.0359985963
Epoch:   700  |  train loss: 0.0353980452
Epoch:   800  |  train loss: 0.0360169239
Epoch:   900  |  train loss: 0.0361781247
Epoch:  1000  |  train loss: 0.0362749532
Processing class: 35
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.0352929130
Epoch:   200  |  train loss: 0.0358297735
Epoch:   300  |  train loss: 0.0363089249
Epoch:   400  |  train loss: 0.0364960223
Epoch:   500  |  train loss: 0.0359383151
Epoch:   600  |  train loss: 0.0363186255
Epoch:   700  |  train loss: 0.0364746206
Epoch:   800  |  train loss: 0.0363504305
Epoch:   900  |  train loss: 0.0364240408
Epoch:  1000  |  train loss: 0.0360966578
Processing class: 36
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.0356814794
Epoch:   200  |  train loss: 0.0357961118
Epoch:   300  |  train loss: 0.0361828133
Epoch:   400  |  train loss: 0.0361503512
Epoch:   500  |  train loss: 0.0369125158
Epoch:   600  |  train loss: 0.0364238337
Epoch:   700  |  train loss: 0.0359173022
Epoch:   800  |  train loss: 0.0368142150
Epoch:   900  |  train loss: 0.0359575354
Epoch:  1000  |  train loss: 0.0364719629
Processing class: 37
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.0362102881
Epoch:   200  |  train loss: 0.0360000379
Epoch:   300  |  train loss: 0.0357754752
Epoch:   400  |  train loss: 0.0360622101
Epoch:   500  |  train loss: 0.0363376781
Epoch:   600  |  train loss: 0.0358068913
Epoch:   700  |  train loss: 0.0358419068
Epoch:   800  |  train loss: 0.0360396221
Epoch:   900  |  train loss: 0.0355731994
Epoch:  1000  |  train loss: 0.0358061634
Processing class: 38
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.0354276128
Epoch:   200  |  train loss: 0.0357075483
Epoch:   300  |  train loss: 0.0363138527
Epoch:   400  |  train loss: 0.0365426898
Epoch:   500  |  train loss: 0.0366421238
Epoch:   600  |  train loss: 0.0363481253
Epoch:   700  |  train loss: 0.0363558166
Epoch:   800  |  train loss: 0.0369406044
Epoch:   900  |  train loss: 0.0361451827
Epoch:  1000  |  train loss: 0.0366406046
Processing class: 39
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.0357505083
Epoch:   200  |  train loss: 0.0354587972
Epoch:   300  |  train loss: 0.0358537354
Epoch:   400  |  train loss: 0.0361348480
Epoch:   500  |  train loss: 0.0358037561
Epoch:   600  |  train loss: 0.0359713502
Epoch:   700  |  train loss: 0.0361921340
Epoch:   800  |  train loss: 0.0365568668
Epoch:   900  |  train loss: 0.0367106102
Epoch:  1000  |  train loss: 0.0368228540
Processing class: 40
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.0358317353
Epoch:   200  |  train loss: 0.0352034561
Epoch:   300  |  train loss: 0.0363983288
Epoch:   400  |  train loss: 0.0366010770
Epoch:   500  |  train loss: 0.0368964352
Epoch:   600  |  train loss: 0.0364223897
Epoch:   700  |  train loss: 0.0367093466
Epoch:   800  |  train loss: 0.0366312176
Epoch:   900  |  train loss: 0.0362750471
Epoch:  1000  |  train loss: 0.0373422772
Processing class: 41
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.0354443692
Epoch:   200  |  train loss: 0.0355764389
Epoch:   300  |  train loss: 0.0359535359
Epoch:   400  |  train loss: 0.0355688877
Epoch:   500  |  train loss: 0.0355089881
Epoch:   600  |  train loss: 0.0355060592
Epoch:   700  |  train loss: 0.0359799132
Epoch:   800  |  train loss: 0.0356319368
Epoch:   900  |  train loss: 0.0354482725
Epoch:  1000  |  train loss: 0.0354363635
Processing class: 42
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.0351800323
Epoch:   200  |  train loss: 0.0358971097
Epoch:   300  |  train loss: 0.0363133527
Epoch:   400  |  train loss: 0.0363909669
Epoch:   500  |  train loss: 0.0366162203
Epoch:   600  |  train loss: 0.0367376655
Epoch:   700  |  train loss: 0.0370177545
Epoch:   800  |  train loss: 0.0371726893
Epoch:   900  |  train loss: 0.0367757745
Epoch:  1000  |  train loss: 0.0366665393
Processing class: 43
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.0354832880
Epoch:   200  |  train loss: 0.0353078529
Epoch:   300  |  train loss: 0.0364780128
Epoch:   400  |  train loss: 0.0376117885
Epoch:   500  |  train loss: 0.0378645346
Epoch:   600  |  train loss: 0.0375552677
Epoch:   700  |  train loss: 0.0380134061
Epoch:   800  |  train loss: 0.0377694726
Epoch:   900  |  train loss: 0.0370684683
Epoch:  1000  |  train loss: 0.0378583543
Processing class: 44
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.0352363065
Epoch:   200  |  train loss: 0.0360139966
Epoch:   300  |  train loss: 0.0356348976
Epoch:   400  |  train loss: 0.0357125469
Epoch:   500  |  train loss: 0.0360431395
Epoch:   600  |  train loss: 0.0357164457
Epoch:   700  |  train loss: 0.0362964623
Epoch:   800  |  train loss: 0.0360566519
Epoch:   900  |  train loss: 0.0357376717
Epoch:  1000  |  train loss: 0.0363666244
Processing class: 45
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.0358313628
Epoch:   200  |  train loss: 0.0369176172
Epoch:   300  |  train loss: 0.0366656505
Epoch:   400  |  train loss: 0.0374259710
Epoch:   500  |  train loss: 0.0372662708
Epoch:   600  |  train loss: 0.0381880447
Epoch:   700  |  train loss: 0.0383568570
Epoch:   800  |  train loss: 0.0380751505
Epoch:   900  |  train loss: 0.0381716274
Epoch:  1000  |  train loss: 0.0386562794
Processing class: 46
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.0360543601
Epoch:   200  |  train loss: 0.0361054227
Epoch:   300  |  train loss: 0.0364283636
Epoch:   400  |  train loss: 0.0367125377
Epoch:   500  |  train loss: 0.0367578611
Epoch:   600  |  train loss: 0.0367634676
Epoch:   700  |  train loss: 0.0368289724
Epoch:   800  |  train loss: 0.0375610419
Epoch:   900  |  train loss: 0.0370569311
Epoch:  1000  |  train loss: 0.0373884961
Processing class: 47
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.0352347419
Epoch:   200  |  train loss: 0.0358389296
Epoch:   300  |  train loss: 0.0352913551
Epoch:   400  |  train loss: 0.0359371379
Epoch:   500  |  train loss: 0.0358705610
Epoch:   600  |  train loss: 0.0357645929
Epoch:   700  |  train loss: 0.0359187998
Epoch:   800  |  train loss: 0.0354614250
Epoch:   900  |  train loss: 0.0356400467
Epoch:  1000  |  train loss: 0.0353678912
Processing class: 48
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.0356128722
Epoch:   200  |  train loss: 0.0364377134
Epoch:   300  |  train loss: 0.0360609740
Epoch:   400  |  train loss: 0.0353096366
Epoch:   500  |  train loss: 0.0362504452
Epoch:   600  |  train loss: 0.0360484853
Epoch:   700  |  train loss: 0.0355490088
Epoch:   800  |  train loss: 0.0355112992
Epoch:   900  |  train loss: 0.0364479452
Epoch:  1000  |  train loss: 0.0366538733
Processing class: 49
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.0355440475
Epoch:   200  |  train loss: 0.0356936216
Epoch:   300  |  train loss: 0.0367574774
Epoch:   400  |  train loss: 0.0368719354
Epoch:   500  |  train loss: 0.0370573819
Epoch:   600  |  train loss: 0.0370370276
Epoch:   700  |  train loss: 0.0369583547
Epoch:   800  |  train loss: 0.0367090061
Epoch:   900  |  train loss: 0.0365443863
Epoch:  1000  |  train loss: 0.0366531961
/home/z1165703/FeCAM/models/base.py:87: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  vec = torch.tensor(vec).cuda()
2024-03-05 00:27:13,751 [trainer.py] => CNN: {'total': 83.44, '00-09': 87.7, '10-19': 79.6, '20-29': 84.9, '30-39': 81.0, '40-49': 84.0, 'old': 0, 'new': 83.44}
2024-03-05 00:27:13,752 [trainer.py] => No NME accuracy
2024-03-05 00:27:13,752 [trainer.py] => FeCAM: {'total': 82.9, '00-09': 86.2, '10-19': 79.7, '20-29': 85.0, '30-39': 80.6, '40-49': 83.0, 'old': 0, 'new': 82.9}
2024-03-05 00:27:13,752 [trainer.py] => CNN top1 curve: [83.44]
2024-03-05 00:27:13,752 [trainer.py] => CNN top5 curve: [96.5]
2024-03-05 00:27:13,752 [trainer.py] => FeCAM top1 curve: [82.9]
2024-03-05 00:27:13,752 [trainer.py] => FeCAM top5 curve: [95.84]

2024-03-05 00:27:13,761 [fecam.py] => Learning on 50-60
Processing class: 50
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.0352300286
Epoch:   200  |  train loss: 0.0353194073
Epoch:   300  |  train loss: 0.0353715628
Epoch:   400  |  train loss: 0.0359343618
Epoch:   500  |  train loss: 0.0362140909
Epoch:   600  |  train loss: 0.0357323356
Epoch:   700  |  train loss: 0.0363779999
Epoch:   800  |  train loss: 0.0366650350
Epoch:   900  |  train loss: 0.0370536037
Epoch:  1000  |  train loss: 0.0371791042
Processing class: 51
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.0355752021
Epoch:   200  |  train loss: 0.0347398587
Epoch:   300  |  train loss: 0.0357230797
Epoch:   400  |  train loss: 0.0358793765
Epoch:   500  |  train loss: 0.0359388605
Epoch:   600  |  train loss: 0.0362446330
Epoch:   700  |  train loss: 0.0356148541
Epoch:   800  |  train loss: 0.0357282996
Epoch:   900  |  train loss: 0.0365208574
Epoch:  1000  |  train loss: 0.0361233011
Processing class: 52
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.0358650990
Epoch:   200  |  train loss: 0.0355478786
Epoch:   300  |  train loss: 0.0359085672
Epoch:   400  |  train loss: 0.0361750573
Epoch:   500  |  train loss: 0.0360146753
Epoch:   600  |  train loss: 0.0355380446
Epoch:   700  |  train loss: 0.0361450605
Epoch:   800  |  train loss: 0.0361498363
Epoch:   900  |  train loss: 0.0362757578
Epoch:  1000  |  train loss: 0.0357618034
Processing class: 53
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.0349134661
Epoch:   200  |  train loss: 0.0350348078
Epoch:   300  |  train loss: 0.0343165450
Epoch:   400  |  train loss: 0.0345193401
Epoch:   500  |  train loss: 0.0348358102
Epoch:   600  |  train loss: 0.0353679918
Epoch:   700  |  train loss: 0.0359630913
Epoch:   800  |  train loss: 0.0347122692
Epoch:   900  |  train loss: 0.0352979176
Epoch:  1000  |  train loss: 0.0348306350
Processing class: 54
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.0329241678
Epoch:   200  |  train loss: 0.0329991743
Epoch:   300  |  train loss: 0.0334658049
Epoch:   400  |  train loss: 0.0336104214
Epoch:   500  |  train loss: 0.0338597558
Epoch:   600  |  train loss: 0.0345453814
Epoch:   700  |  train loss: 0.0347058386
Epoch:   800  |  train loss: 0.0352626681
Epoch:   900  |  train loss: 0.0356429584
Epoch:  1000  |  train loss: 0.0354009695
Processing class: 55
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.0354646951
Epoch:   200  |  train loss: 0.0355342880
Epoch:   300  |  train loss: 0.0351354405
Epoch:   400  |  train loss: 0.0358440183
Epoch:   500  |  train loss: 0.0367881171
Epoch:   600  |  train loss: 0.0368610851
Epoch:   700  |  train loss: 0.0371456511
Epoch:   800  |  train loss: 0.0378535561
Epoch:   900  |  train loss: 0.0369121850
Epoch:  1000  |  train loss: 0.0374296099
Processing class: 56
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.0349622861
Epoch:   200  |  train loss: 0.0350813121
Epoch:   300  |  train loss: 0.0342373155
Epoch:   400  |  train loss: 0.0350367315
Epoch:   500  |  train loss: 0.0352549173
Epoch:   600  |  train loss: 0.0353642941
Epoch:   700  |  train loss: 0.0356461756
Epoch:   800  |  train loss: 0.0352084257
Epoch:   900  |  train loss: 0.0357656367
Epoch:  1000  |  train loss: 0.0360805519
Processing class: 57
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.0350880474
Epoch:   200  |  train loss: 0.0357287861
Epoch:   300  |  train loss: 0.0354228668
Epoch:   400  |  train loss: 0.0353959039
Epoch:   500  |  train loss: 0.0358618855
Epoch:   600  |  train loss: 0.0351863824
Epoch:   700  |  train loss: 0.0360952638
Epoch:   800  |  train loss: 0.0360419989
Epoch:   900  |  train loss: 0.0365262985
Epoch:  1000  |  train loss: 0.0357008159
Processing class: 58
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.0351866946
Epoch:   200  |  train loss: 0.0346383974
Epoch:   300  |  train loss: 0.0351951085
Epoch:   400  |  train loss: 0.0358890787
Epoch:   500  |  train loss: 0.0356115498
Epoch:   600  |  train loss: 0.0361364208
Epoch:   700  |  train loss: 0.0359095857
Epoch:   800  |  train loss: 0.0363418445
Epoch:   900  |  train loss: 0.0362856947
Epoch:  1000  |  train loss: 0.0362656996
Processing class: 59
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.0360377632
Epoch:   200  |  train loss: 0.0368615277
Epoch:   300  |  train loss: 0.0363814883
Epoch:   400  |  train loss: 0.0367083535
Epoch:   500  |  train loss: 0.0371351205
Epoch:   600  |  train loss: 0.0371867701
Epoch:   700  |  train loss: 0.0374937907
Epoch:   800  |  train loss: 0.0376046650
Epoch:   900  |  train loss: 0.0374552660
Epoch:  1000  |  train loss: 0.0372412771
2024-03-05 00:32:54,680 [trainer.py] => CNN: {'total': 71.75, '00-09': 82.2, '10-19': 72.8, '20-29': 78.5, '30-39': 74.9, '40-49': 68.4, '50-59': 53.7, 'old': 75.36, 'new': 53.7}
2024-03-05 00:32:54,681 [trainer.py] => No NME accuracy
2024-03-05 00:32:54,681 [trainer.py] => FeCAM: {'total': 72.78, '00-09': 82.6, '10-19': 74.0, '20-29': 81.0, '30-39': 76.4, '40-49': 76.8, '50-59': 45.9, 'old': 78.16, 'new': 45.9}
2024-03-05 00:32:54,681 [trainer.py] => CNN top1 curve: [83.44, 71.75]
2024-03-05 00:32:54,681 [trainer.py] => CNN top5 curve: [96.5, 89.6]
2024-03-05 00:32:54,681 [trainer.py] => FeCAM top1 curve: [82.9, 72.78]
2024-03-05 00:32:54,681 [trainer.py] => FeCAM top5 curve: [95.84, 91.18]

2024-03-05 00:32:54,686 [fecam.py] => Learning on 60-70
Processing class: 60
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.0363310821
Epoch:   200  |  train loss: 0.0361590959
Epoch:   300  |  train loss: 0.0356438383
Epoch:   400  |  train loss: 0.0354074664
Epoch:   500  |  train loss: 0.0361757576
Epoch:   600  |  train loss: 0.0361325532
Epoch:   700  |  train loss: 0.0378819570
Epoch:   800  |  train loss: 0.0368335754
Epoch:   900  |  train loss: 0.0369505771
Epoch:  1000  |  train loss: 0.0373906672
Processing class: 61
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.0348686054
Epoch:   200  |  train loss: 0.0341316707
Epoch:   300  |  train loss: 0.0348154612
Epoch:   400  |  train loss: 0.0348150119
Epoch:   500  |  train loss: 0.0357361034
Epoch:   600  |  train loss: 0.0349990740
Epoch:   700  |  train loss: 0.0358068444
Epoch:   800  |  train loss: 0.0362945363
Epoch:   900  |  train loss: 0.0354286008
Epoch:  1000  |  train loss: 0.0357646443
Processing class: 62
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.0360095061
Epoch:   200  |  train loss: 0.0355468407
Epoch:   300  |  train loss: 0.0361028582
Epoch:   400  |  train loss: 0.0363346539
Epoch:   500  |  train loss: 0.0358842455
Epoch:   600  |  train loss: 0.0367313288
Epoch:   700  |  train loss: 0.0369780168
Epoch:   800  |  train loss: 0.0371328726
Epoch:   900  |  train loss: 0.0368648894
Epoch:  1000  |  train loss: 0.0370902076
Processing class: 63
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.0346762456
Epoch:   200  |  train loss: 0.0348501727
Epoch:   300  |  train loss: 0.0358994476
Epoch:   400  |  train loss: 0.0358883731
Epoch:   500  |  train loss: 0.0362035230
Epoch:   600  |  train loss: 0.0369635627
Epoch:   700  |  train loss: 0.0359688386
Epoch:   800  |  train loss: 0.0373162732
Epoch:   900  |  train loss: 0.0364026107
Epoch:  1000  |  train loss: 0.0366067037
Processing class: 64
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.0362381019
Epoch:   200  |  train loss: 0.0353361055
Epoch:   300  |  train loss: 0.0365784749
Epoch:   400  |  train loss: 0.0365857534
Epoch:   500  |  train loss: 0.0365803272
Epoch:   600  |  train loss: 0.0364633091
Epoch:   700  |  train loss: 0.0357650481
Epoch:   800  |  train loss: 0.0360992648
Epoch:   900  |  train loss: 0.0367488675
Epoch:  1000  |  train loss: 0.0364610299
Processing class: 65
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.0353590325
Epoch:   200  |  train loss: 0.0350326009
Epoch:   300  |  train loss: 0.0362147793
Epoch:   400  |  train loss: 0.0366623886
Epoch:   500  |  train loss: 0.0363753594
Epoch:   600  |  train loss: 0.0374520294
Epoch:   700  |  train loss: 0.0374262050
Epoch:   800  |  train loss: 0.0384214178
Epoch:   900  |  train loss: 0.0377424747
Epoch:  1000  |  train loss: 0.0387677386
Processing class: 66
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.0360629156
Epoch:   200  |  train loss: 0.0364715248
Epoch:   300  |  train loss: 0.0369625196
Epoch:   400  |  train loss: 0.0369222745
Epoch:   500  |  train loss: 0.0375825740
Epoch:   600  |  train loss: 0.0376327433
Epoch:   700  |  train loss: 0.0383679301
Epoch:   800  |  train loss: 0.0384379342
Epoch:   900  |  train loss: 0.0383814640
Epoch:  1000  |  train loss: 0.0382699020
Processing class: 67
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.0349043593
Epoch:   200  |  train loss: 0.0352162398
Epoch:   300  |  train loss: 0.0351262592
Epoch:   400  |  train loss: 0.0352444418
Epoch:   500  |  train loss: 0.0352292702
Epoch:   600  |  train loss: 0.0351707332
Epoch:   700  |  train loss: 0.0353109673
Epoch:   800  |  train loss: 0.0357482620
Epoch:   900  |  train loss: 0.0356095172
Epoch:  1000  |  train loss: 0.0355409771
Processing class: 68
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.0345112577
Epoch:   200  |  train loss: 0.0344504379
Epoch:   300  |  train loss: 0.0340599827
Epoch:   400  |  train loss: 0.0342619568
Epoch:   500  |  train loss: 0.0342963420
Epoch:   600  |  train loss: 0.0341241755
Epoch:   700  |  train loss: 0.0344271332
Epoch:   800  |  train loss: 0.0340619907
Epoch:   900  |  train loss: 0.0342571728
Epoch:  1000  |  train loss: 0.0351339631
Processing class: 69
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.0332903378
Epoch:   200  |  train loss: 0.0337396927
Epoch:   300  |  train loss: 0.0347957470
Epoch:   400  |  train loss: 0.0352280281
Epoch:   500  |  train loss: 0.0352312803
Epoch:   600  |  train loss: 0.0343941800
Epoch:   700  |  train loss: 0.0350838736
Epoch:   800  |  train loss: 0.0349963263
Epoch:   900  |  train loss: 0.0360368975
Epoch:  1000  |  train loss: 0.0349803478
2024-03-05 00:39:34,842 [trainer.py] => CNN: {'total': 64.66, '00-09': 75.0, '10-19': 70.1, '20-29': 77.1, '30-39': 70.6, '40-49': 64.0, '50-59': 45.8, '60-69': 50.0, 'old': 67.1, 'new': 50.0}
2024-03-05 00:39:34,843 [trainer.py] => No NME accuracy
2024-03-05 00:39:34,843 [trainer.py] => FeCAM: {'total': 67.27, '00-09': 79.9, '10-19': 70.6, '20-29': 79.1, '30-39': 71.9, '40-49': 74.1, '50-59': 41.2, '60-69': 54.1, 'old': 69.47, 'new': 54.1}
2024-03-05 00:39:34,843 [trainer.py] => CNN top1 curve: [83.44, 71.75, 64.66]
2024-03-05 00:39:34,843 [trainer.py] => CNN top5 curve: [96.5, 89.6, 86.54]
2024-03-05 00:39:34,843 [trainer.py] => FeCAM top1 curve: [82.9, 72.78, 67.27]
2024-03-05 00:39:34,843 [trainer.py] => FeCAM top5 curve: [95.84, 91.18, 87.21]

2024-03-05 00:39:34,848 [fecam.py] => Learning on 70-80
Processing class: 70
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.0349401474
Epoch:   200  |  train loss: 0.0345238246
Epoch:   300  |  train loss: 0.0347335637
Epoch:   400  |  train loss: 0.0350329734
Epoch:   500  |  train loss: 0.0346413329
Epoch:   600  |  train loss: 0.0352419399
Epoch:   700  |  train loss: 0.0359816261
Epoch:   800  |  train loss: 0.0350489862
Epoch:   900  |  train loss: 0.0356659353
Epoch:  1000  |  train loss: 0.0353926174
Processing class: 71
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.0355553195
Epoch:   200  |  train loss: 0.0357923977
Epoch:   300  |  train loss: 0.0351122908
Epoch:   400  |  train loss: 0.0356181398
Epoch:   500  |  train loss: 0.0359690048
Epoch:   600  |  train loss: 0.0360102408
Epoch:   700  |  train loss: 0.0360626839
Epoch:   800  |  train loss: 0.0362125032
Epoch:   900  |  train loss: 0.0368683159
Epoch:  1000  |  train loss: 0.0366763882
Processing class: 72
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.0349702373
Epoch:   200  |  train loss: 0.0351527818
Epoch:   300  |  train loss: 0.0352173187
Epoch:   400  |  train loss: 0.0351273440
Epoch:   500  |  train loss: 0.0355355382
Epoch:   600  |  train loss: 0.0350294746
Epoch:   700  |  train loss: 0.0352814853
Epoch:   800  |  train loss: 0.0350385115
Epoch:   900  |  train loss: 0.0352115609
Epoch:  1000  |  train loss: 0.0357954130
Processing class: 73
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.0357283689
Epoch:   200  |  train loss: 0.0354509719
Epoch:   300  |  train loss: 0.0361957617
Epoch:   400  |  train loss: 0.0357573099
Epoch:   500  |  train loss: 0.0366534837
Epoch:   600  |  train loss: 0.0365881324
Epoch:   700  |  train loss: 0.0360871710
Epoch:   800  |  train loss: 0.0360761195
Epoch:   900  |  train loss: 0.0361335635
Epoch:  1000  |  train loss: 0.0364863314
Processing class: 74
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.0362011932
Epoch:   200  |  train loss: 0.0366088867
Epoch:   300  |  train loss: 0.0358413681
Epoch:   400  |  train loss: 0.0354705535
Epoch:   500  |  train loss: 0.0358338796
Epoch:   600  |  train loss: 0.0355474517
Epoch:   700  |  train loss: 0.0364837706
Epoch:   800  |  train loss: 0.0355671741
Epoch:   900  |  train loss: 0.0356577784
Epoch:  1000  |  train loss: 0.0364048153
Processing class: 75
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.0351903424
Epoch:   200  |  train loss: 0.0349699363
Epoch:   300  |  train loss: 0.0356643975
Epoch:   400  |  train loss: 0.0355211258
Epoch:   500  |  train loss: 0.0360642523
Epoch:   600  |  train loss: 0.0366104178
Epoch:   700  |  train loss: 0.0350854874
Epoch:   800  |  train loss: 0.0360201359
Epoch:   900  |  train loss: 0.0359213792
Epoch:  1000  |  train loss: 0.0365546480
Processing class: 76
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.0325215712
Epoch:   200  |  train loss: 0.0327584423
Epoch:   300  |  train loss: 0.0336838022
Epoch:   400  |  train loss: 0.0343434595
Epoch:   500  |  train loss: 0.0346041963
Epoch:   600  |  train loss: 0.0351344265
Epoch:   700  |  train loss: 0.0352697544
Epoch:   800  |  train loss: 0.0344592497
Epoch:   900  |  train loss: 0.0353424564
Epoch:  1000  |  train loss: 0.0357377388
Processing class: 77
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.0350514345
Epoch:   200  |  train loss: 0.0350704715
Epoch:   300  |  train loss: 0.0345148675
Epoch:   400  |  train loss: 0.0348069914
Epoch:   500  |  train loss: 0.0349191740
Epoch:   600  |  train loss: 0.0348825894
Epoch:   700  |  train loss: 0.0350516237
Epoch:   800  |  train loss: 0.0356918797
Epoch:   900  |  train loss: 0.0353347190
Epoch:  1000  |  train loss: 0.0352277160
Processing class: 78
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.0356642403
Epoch:   200  |  train loss: 0.0360904507
Epoch:   300  |  train loss: 0.0366446316
Epoch:   400  |  train loss: 0.0365640230
Epoch:   500  |  train loss: 0.0363720328
Epoch:   600  |  train loss: 0.0368897155
Epoch:   700  |  train loss: 0.0370247364
Epoch:   800  |  train loss: 0.0371942699
Epoch:   900  |  train loss: 0.0372563444
Epoch:  1000  |  train loss: 0.0370725296
Processing class: 79
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.0357053451
Epoch:   200  |  train loss: 0.0352001317
Epoch:   300  |  train loss: 0.0358613648
Epoch:   400  |  train loss: 0.0358556196
Epoch:   500  |  train loss: 0.0358354241
Epoch:   600  |  train loss: 0.0365540437
Epoch:   700  |  train loss: 0.0365151756
Epoch:   800  |  train loss: 0.0368300498
Epoch:   900  |  train loss: 0.0373815671
Epoch:  1000  |  train loss: 0.0368660852
2024-03-05 00:47:18,996 [trainer.py] => CNN: {'total': 59.18, '00-09': 73.6, '10-19': 68.4, '20-29': 76.9, '30-39': 69.1, '40-49': 60.6, '50-59': 37.2, '60-69': 45.7, '70-79': 41.9, 'old': 61.64, 'new': 41.9}
2024-03-05 00:47:18,997 [trainer.py] => No NME accuracy
2024-03-05 00:47:18,997 [trainer.py] => FeCAM: {'total': 61.72, '00-09': 78.1, '10-19': 69.3, '20-29': 78.0, '30-39': 69.5, '40-49': 72.1, '50-59': 35.7, '60-69': 49.1, '70-79': 42.0, 'old': 64.54, 'new': 42.0}
2024-03-05 00:47:18,997 [trainer.py] => CNN top1 curve: [83.44, 71.75, 64.66, 59.18]
2024-03-05 00:47:18,997 [trainer.py] => CNN top5 curve: [96.5, 89.6, 86.54, 84.09]
2024-03-05 00:47:18,997 [trainer.py] => FeCAM top1 curve: [82.9, 72.78, 67.27, 61.72]
2024-03-05 00:47:18,997 [trainer.py] => FeCAM top5 curve: [95.84, 91.18, 87.21, 84.09]

2024-03-05 00:47:19,005 [fecam.py] => Learning on 80-90
Processing class: 80
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.0342662856
Epoch:   200  |  train loss: 0.0352679342
Epoch:   300  |  train loss: 0.0356940307
Epoch:   400  |  train loss: 0.0359520026
Epoch:   500  |  train loss: 0.0358846448
Epoch:   600  |  train loss: 0.0358848937
Epoch:   700  |  train loss: 0.0352383628
Epoch:   800  |  train loss: 0.0362441249
Epoch:   900  |  train loss: 0.0360024482
Epoch:  1000  |  train loss: 0.0365382709
Processing class: 81
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.0349164225
Epoch:   200  |  train loss: 0.0358304746
Epoch:   300  |  train loss: 0.0352024719
Epoch:   400  |  train loss: 0.0361387044
Epoch:   500  |  train loss: 0.0358971670
Epoch:   600  |  train loss: 0.0365578599
Epoch:   700  |  train loss: 0.0364467286
Epoch:   800  |  train loss: 0.0368874907
Epoch:   900  |  train loss: 0.0369325504
Epoch:  1000  |  train loss: 0.0371362954
Processing class: 82
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.0344948202
Epoch:   200  |  train loss: 0.0349580124
Epoch:   300  |  train loss: 0.0350212850
Epoch:   400  |  train loss: 0.0349011660
Epoch:   500  |  train loss: 0.0350940287
Epoch:   600  |  train loss: 0.0349020943
Epoch:   700  |  train loss: 0.0349189706
Epoch:   800  |  train loss: 0.0350825436
Epoch:   900  |  train loss: 0.0351618104
Epoch:  1000  |  train loss: 0.0351777323
Processing class: 83
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.0349370323
Epoch:   200  |  train loss: 0.0356263772
Epoch:   300  |  train loss: 0.0353945531
Epoch:   400  |  train loss: 0.0356934957
Epoch:   500  |  train loss: 0.0364658467
Epoch:   600  |  train loss: 0.0359524757
Epoch:   700  |  train loss: 0.0367236815
Epoch:   800  |  train loss: 0.0376051962
Epoch:   900  |  train loss: 0.0372407436
Epoch:  1000  |  train loss: 0.0369326971
Processing class: 84
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.0357888147
Epoch:   200  |  train loss: 0.0355382502
Epoch:   300  |  train loss: 0.0361660562
Epoch:   400  |  train loss: 0.0361725524
Epoch:   500  |  train loss: 0.0366829254
Epoch:   600  |  train loss: 0.0361063637
Epoch:   700  |  train loss: 0.0367132612
Epoch:   800  |  train loss: 0.0372449465
Epoch:   900  |  train loss: 0.0364992902
Epoch:  1000  |  train loss: 0.0363616325
Processing class: 85
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.0354435898
Epoch:   200  |  train loss: 0.0351878792
Epoch:   300  |  train loss: 0.0358009979
Epoch:   400  |  train loss: 0.0354847409
Epoch:   500  |  train loss: 0.0359090306
Epoch:   600  |  train loss: 0.0364269830
Epoch:   700  |  train loss: 0.0372124322
Epoch:   800  |  train loss: 0.0365443587
Epoch:   900  |  train loss: 0.0366288126
Epoch:  1000  |  train loss: 0.0372559443
Processing class: 86
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.0362462342
Epoch:   200  |  train loss: 0.0355957605
Epoch:   300  |  train loss: 0.0364063881
Epoch:   400  |  train loss: 0.0360703357
Epoch:   500  |  train loss: 0.0360628359
Epoch:   600  |  train loss: 0.0363843925
Epoch:   700  |  train loss: 0.0364201337
Epoch:   800  |  train loss: 0.0363312326
Epoch:   900  |  train loss: 0.0363201298
Epoch:  1000  |  train loss: 0.0371703908
Processing class: 87
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.0345144279
Epoch:   200  |  train loss: 0.0350115687
Epoch:   300  |  train loss: 0.0350272208
Epoch:   400  |  train loss: 0.0354190730
Epoch:   500  |  train loss: 0.0353276275
Epoch:   600  |  train loss: 0.0357132569
Epoch:   700  |  train loss: 0.0359183200
Epoch:   800  |  train loss: 0.0359154359
Epoch:   900  |  train loss: 0.0355039574
Epoch:  1000  |  train loss: 0.0364971660
Processing class: 88
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.0350683272
Epoch:   200  |  train loss: 0.0347494178
Epoch:   300  |  train loss: 0.0347004831
Epoch:   400  |  train loss: 0.0348611474
Epoch:   500  |  train loss: 0.0347775482
Epoch:   600  |  train loss: 0.0344938830
Epoch:   700  |  train loss: 0.0350350715
Epoch:   800  |  train loss: 0.0350628279
Epoch:   900  |  train loss: 0.0352224909
Epoch:  1000  |  train loss: 0.0348527536
Processing class: 89
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.0354604259
Epoch:   200  |  train loss: 0.0359953269
Epoch:   300  |  train loss: 0.0362587631
Epoch:   400  |  train loss: 0.0364127658
Epoch:   500  |  train loss: 0.0370067075
Epoch:   600  |  train loss: 0.0370517187
Epoch:   700  |  train loss: 0.0364706501
Epoch:   800  |  train loss: 0.0376625314
Epoch:   900  |  train loss: 0.0373500496
Epoch:  1000  |  train loss: 0.0376867734
2024-03-05 00:56:14,777 [trainer.py] => CNN: {'total': 54.08, '00-09': 67.9, '10-19': 63.6, '20-29': 73.2, '30-39': 68.8, '40-49': 57.2, '50-59': 32.6, '60-69': 40.9, '70-79': 38.1, '80-89': 44.4, 'old': 55.29, 'new': 44.4}
2024-03-05 00:56:14,787 [trainer.py] => No NME accuracy
2024-03-05 00:56:14,787 [trainer.py] => FeCAM: {'total': 57.66, '00-09': 76.1, '10-19': 65.8, '20-29': 75.3, '30-39': 67.9, '40-49': 68.5, '50-59': 33.0, '60-69': 45.0, '70-79': 39.0, '80-89': 48.3, 'old': 58.82, 'new': 48.3}
2024-03-05 00:56:14,787 [trainer.py] => CNN top1 curve: [83.44, 71.75, 64.66, 59.18, 54.08]
2024-03-05 00:56:14,787 [trainer.py] => CNN top5 curve: [96.5, 89.6, 86.54, 84.09, 81.74]
2024-03-05 00:56:14,787 [trainer.py] => FeCAM top1 curve: [82.9, 72.78, 67.27, 61.72, 57.66]
2024-03-05 00:56:14,787 [trainer.py] => FeCAM top5 curve: [95.84, 91.18, 87.21, 84.09, 81.63]

2024-03-05 00:56:14,796 [fecam.py] => Learning on 90-100
Processing class: 90
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.0355897225
Epoch:   200  |  train loss: 0.0356502309
Epoch:   300  |  train loss: 0.0349340506
Epoch:   400  |  train loss: 0.0346871033
Epoch:   500  |  train loss: 0.0344216518
Epoch:   600  |  train loss: 0.0346934102
Epoch:   700  |  train loss: 0.0345081054
Epoch:   800  |  train loss: 0.0346754976
Epoch:   900  |  train loss: 0.0350034818
Epoch:  1000  |  train loss: 0.0356443435
Processing class: 91
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.0332887642
Epoch:   200  |  train loss: 0.0331089035
Epoch:   300  |  train loss: 0.0336984582
Epoch:   400  |  train loss: 0.0342514426
Epoch:   500  |  train loss: 0.0347342767
Epoch:   600  |  train loss: 0.0350079902
Epoch:   700  |  train loss: 0.0351413712
Epoch:   800  |  train loss: 0.0354474671
Epoch:   900  |  train loss: 0.0358469039
Epoch:  1000  |  train loss: 0.0359562628
Processing class: 92
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.0359511621
Epoch:   200  |  train loss: 0.0351416290
Epoch:   300  |  train loss: 0.0343482181
Epoch:   400  |  train loss: 0.0347812675
Epoch:   500  |  train loss: 0.0350837260
Epoch:   600  |  train loss: 0.0356353328
Epoch:   700  |  train loss: 0.0356151931
Epoch:   800  |  train loss: 0.0347619005
Epoch:   900  |  train loss: 0.0349681623
Epoch:  1000  |  train loss: 0.0350932084
Processing class: 93
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.0352674387
Epoch:   200  |  train loss: 0.0355879836
Epoch:   300  |  train loss: 0.0369274929
Epoch:   400  |  train loss: 0.0371388398
Epoch:   500  |  train loss: 0.0370195255
Epoch:   600  |  train loss: 0.0365479358
Epoch:   700  |  train loss: 0.0376970544
Epoch:   800  |  train loss: 0.0375845909
Epoch:   900  |  train loss: 0.0376963191
Epoch:  1000  |  train loss: 0.0375389963
Processing class: 94
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.0328621134
Epoch:   200  |  train loss: 0.0326515909
Epoch:   300  |  train loss: 0.0317146428
Epoch:   400  |  train loss: 0.0319995277
Epoch:   500  |  train loss: 0.0308866117
Epoch:   600  |  train loss: 0.0314059354
Epoch:   700  |  train loss: 0.0319247752
Epoch:   800  |  train loss: 0.0306214821
Epoch:   900  |  train loss: 0.0314540621
Epoch:  1000  |  train loss: 0.0311793879
Processing class: 95
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.0351733893
Epoch:   200  |  train loss: 0.0358324870
Epoch:   300  |  train loss: 0.0354153074
Epoch:   400  |  train loss: 0.0351728387
Epoch:   500  |  train loss: 0.0360629365
Epoch:   600  |  train loss: 0.0359510303
Epoch:   700  |  train loss: 0.0362843618
Epoch:   800  |  train loss: 0.0359879218
Epoch:   900  |  train loss: 0.0365696758
Epoch:  1000  |  train loss: 0.0371091232
Processing class: 96
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.0360094950
Epoch:   200  |  train loss: 0.0350480936
Epoch:   300  |  train loss: 0.0356542468
Epoch:   400  |  train loss: 0.0366861485
Epoch:   500  |  train loss: 0.0363322288
Epoch:   600  |  train loss: 0.0362308525
Epoch:   700  |  train loss: 0.0363648556
Epoch:   800  |  train loss: 0.0359494112
Epoch:   900  |  train loss: 0.0369786195
Epoch:  1000  |  train loss: 0.0369459473
Processing class: 97
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.0357066832
Epoch:   200  |  train loss: 0.0356675066
Epoch:   300  |  train loss: 0.0357991882
Epoch:   400  |  train loss: 0.0360235058
Epoch:   500  |  train loss: 0.0366761245
Epoch:   600  |  train loss: 0.0370819837
Epoch:   700  |  train loss: 0.0375059590
Epoch:   800  |  train loss: 0.0376158923
Epoch:   900  |  train loss: 0.0378553316
Epoch:  1000  |  train loss: 0.0377768651
Processing class: 98
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.0345859468
Epoch:   200  |  train loss: 0.0343881801
Epoch:   300  |  train loss: 0.0344711207
Epoch:   400  |  train loss: 0.0347518034
Epoch:   500  |  train loss: 0.0346104123
Epoch:   600  |  train loss: 0.0349856801
Epoch:   700  |  train loss: 0.0343217991
Epoch:   800  |  train loss: 0.0348852403
Epoch:   900  |  train loss: 0.0349071220
Epoch:  1000  |  train loss: 0.0346042052
Processing class: 99
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.0363081820
Epoch:   200  |  train loss: 0.0356167614
Epoch:   300  |  train loss: 0.0366235279
Epoch:   400  |  train loss: 0.0373875335
Epoch:   500  |  train loss: 0.0372099012
Epoch:   600  |  train loss: 0.0370724164
Epoch:   700  |  train loss: 0.0368870541
Epoch:   800  |  train loss: 0.0375474364
Epoch:   900  |  train loss: 0.0372326881
Epoch:  1000  |  train loss: 0.0373668388
2024-03-05 01:06:37,528 [trainer.py] => CNN: {'total': 50.36, '00-09': 57.7, '10-19': 63.6, '20-29': 71.4, '30-39': 68.5, '40-49': 56.8, '50-59': 29.6, '60-69': 38.9, '70-79': 36.6, '80-89': 42.7, '90-99': 37.8, 'old': 51.76, 'new': 37.8}
2024-03-05 01:06:37,530 [trainer.py] => No NME accuracy
2024-03-05 01:06:37,530 [trainer.py] => FeCAM: {'total': 54.74, '00-09': 73.3, '10-19': 65.1, '20-29': 74.1, '30-39': 67.2, '40-49': 67.1, '50-59': 30.8, '60-69': 43.3, '70-79': 37.2, '80-89': 46.9, '90-99': 42.4, 'old': 56.11, 'new': 42.4}
2024-03-05 01:06:37,531 [trainer.py] => CNN top1 curve: [83.44, 71.75, 64.66, 59.18, 54.08, 50.36]
2024-03-05 01:06:37,531 [trainer.py] => CNN top5 curve: [96.5, 89.6, 86.54, 84.09, 81.74, 79.63]
2024-03-05 01:06:37,531 [trainer.py] => FeCAM top1 curve: [82.9, 72.78, 67.27, 61.72, 57.66, 54.74]
2024-03-05 01:06:37,531 [trainer.py] => FeCAM top5 curve: [95.84, 91.18, 87.21, 84.09, 81.63, 79.21]

=========================================
2024-03-05 01:06:47,140 [trainer.py] => config: ./exps/FeCAM_cifar100.json
2024-03-05 01:06:47,140 [trainer.py] => prefix: train
2024-03-05 01:06:47,140 [trainer.py] => dataset: cifar100
2024-03-05 01:06:47,140 [trainer.py] => memory_size: 0
2024-03-05 01:06:47,140 [trainer.py] => shuffle: True
2024-03-05 01:06:47,140 [trainer.py] => init_cls: 50
2024-03-05 01:06:47,140 [trainer.py] => increment: 10
2024-03-05 01:06:47,140 [trainer.py] => model_name: fecam
2024-03-05 01:06:47,140 [trainer.py] => convnet_type: resnet18
2024-03-05 01:06:47,140 [trainer.py] => device: [device(type='cuda', index=0)]
2024-03-05 01:06:47,140 [trainer.py] => seed: 1993
2024-03-05 01:06:47,140 [trainer.py] => init_epochs: 200
2024-03-05 01:06:47,140 [trainer.py] => init_lr: 0.1
2024-03-05 01:06:47,140 [trainer.py] => init_weight_decay: 0.0005
2024-03-05 01:06:47,141 [trainer.py] => batch_size: 128
2024-03-05 01:06:47,141 [trainer.py] => num_workers: 8
2024-03-05 01:06:47,141 [trainer.py] => T: 5
2024-03-05 01:06:47,141 [trainer.py] => beta: 0.5
2024-03-05 01:06:47,141 [trainer.py] => alpha1: 1
2024-03-05 01:06:47,141 [trainer.py] => alpha2: 1
2024-03-05 01:06:47,141 [trainer.py] => ncm: False
2024-03-05 01:06:47,141 [trainer.py] => tukey: False
2024-03-05 01:06:47,141 [trainer.py] => diagonal: False
2024-03-05 01:06:47,141 [trainer.py] => per_class: True
2024-03-05 01:06:47,141 [trainer.py] => full_cov: True
2024-03-05 01:06:47,141 [trainer.py] => shrink: True
2024-03-05 01:06:47,141 [trainer.py] => norm_cov: False
2024-03-05 01:06:47,141 [trainer.py] => vecnorm: False
2024-03-05 01:06:47,141 [trainer.py] => ae_type: wae
2024-03-05 01:06:47,141 [trainer.py] => epochs: 1000
2024-03-05 01:06:47,141 [trainer.py] => ae_latent_dim: 32
2024-03-05 01:06:47,141 [trainer.py] => wae_sigma: 40
2024-03-05 01:06:47,141 [trainer.py] => wae_C: 0.1
Files already downloaded and verified
Files already downloaded and verified
2024-03-05 01:06:48,790 [data_manager.py] => [68, 56, 78, 8, 23, 84, 90, 65, 74, 76, 40, 89, 3, 92, 55, 9, 26, 80, 43, 38, 58, 70, 77, 1, 85, 19, 17, 50, 28, 53, 13, 81, 45, 82, 6, 59, 83, 16, 15, 44, 91, 41, 72, 60, 79, 52, 20, 10, 31, 54, 37, 95, 14, 71, 96, 98, 97, 2, 64, 66, 42, 22, 35, 86, 24, 34, 87, 21, 99, 0, 88, 27, 18, 94, 11, 12, 47, 25, 30, 46, 62, 69, 36, 61, 7, 63, 75, 5, 32, 4, 51, 48, 73, 93, 39, 67, 29, 49, 57, 33]
2024-03-05 01:06:49,080 [fecam.py] => Learning on 0-50
/home/z1165703/miniconda3/lib/python3.11/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
/home/z1165703/miniconda3/lib/python3.11/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
Processing class: 0
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.0299409520
Epoch:   200  |  train loss: 0.0305383708
Epoch:   300  |  train loss: 0.0311651289
Epoch:   400  |  train loss: 0.0314499259
Epoch:   500  |  train loss: 0.0311813418
Epoch:   600  |  train loss: 0.0311714970
Epoch:   700  |  train loss: 0.0314495467
Epoch:   800  |  train loss: 0.0315472383
Epoch:   900  |  train loss: 0.0312804449
Epoch:  1000  |  train loss: 0.0312602457
Processing class: 1
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.0293143462
Epoch:   200  |  train loss: 0.0290707961
Epoch:   300  |  train loss: 0.0305043139
Epoch:   400  |  train loss: 0.0308214359
Epoch:   500  |  train loss: 0.0301849894
Epoch:   600  |  train loss: 0.0311582115
Epoch:   700  |  train loss: 0.0307774864
Epoch:   800  |  train loss: 0.0313202351
Epoch:   900  |  train loss: 0.0320956811
Epoch:  1000  |  train loss: 0.0314160105
Processing class: 2
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.0290200893
Epoch:   200  |  train loss: 0.0290162604
Epoch:   300  |  train loss: 0.0300090853
Epoch:   400  |  train loss: 0.0299457971
Epoch:   500  |  train loss: 0.0287301444
Epoch:   600  |  train loss: 0.0298624698
Epoch:   700  |  train loss: 0.0297796872
Epoch:   800  |  train loss: 0.0306478027
Epoch:   900  |  train loss: 0.0302458458
Epoch:  1000  |  train loss: 0.0300308377
Processing class: 3
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.0315966427
Epoch:   200  |  train loss: 0.0324581124
Epoch:   300  |  train loss: 0.0329146270
Epoch:   400  |  train loss: 0.0331850141
Epoch:   500  |  train loss: 0.0328854129
Epoch:   600  |  train loss: 0.0338107795
Epoch:   700  |  train loss: 0.0344336279
Epoch:   800  |  train loss: 0.0338675808
Epoch:   900  |  train loss: 0.0343825690
Epoch:  1000  |  train loss: 0.0345981054
Processing class: 4
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.0290287789
Epoch:   200  |  train loss: 0.0286927104
Epoch:   300  |  train loss: 0.0299058594
Epoch:   400  |  train loss: 0.0300614230
Epoch:   500  |  train loss: 0.0305632629
Epoch:   600  |  train loss: 0.0304797694
Epoch:   700  |  train loss: 0.0309664972
Epoch:   800  |  train loss: 0.0311269622
Epoch:   900  |  train loss: 0.0313948032
Epoch:  1000  |  train loss: 0.0320989743
Processing class: 5
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.0290803846
Epoch:   200  |  train loss: 0.0289839525
Epoch:   300  |  train loss: 0.0294470802
Epoch:   400  |  train loss: 0.0295577288
Epoch:   500  |  train loss: 0.0295004230
Epoch:   600  |  train loss: 0.0307409924
Epoch:   700  |  train loss: 0.0310387835
Epoch:   800  |  train loss: 0.0305564985
Epoch:   900  |  train loss: 0.0305006396
Epoch:  1000  |  train loss: 0.0310834739
Processing class: 6
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.0294391129
Epoch:   200  |  train loss: 0.0298624773
Epoch:   300  |  train loss: 0.0310893547
Epoch:   400  |  train loss: 0.0310242575
Epoch:   500  |  train loss: 0.0319911819
Epoch:   600  |  train loss: 0.0316661015
Epoch:   700  |  train loss: 0.0314062927
Epoch:   800  |  train loss: 0.0317052964
Epoch:   900  |  train loss: 0.0314169198
Epoch:  1000  |  train loss: 0.0318033896
Processing class: 7
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.0285717089
Epoch:   200  |  train loss: 0.0288645547
Epoch:   300  |  train loss: 0.0292570941
Epoch:   400  |  train loss: 0.0295963861
Epoch:   500  |  train loss: 0.0301532023
Epoch:   600  |  train loss: 0.0298839003
Epoch:   700  |  train loss: 0.0303979535
Epoch:   800  |  train loss: 0.0312530059
Epoch:   900  |  train loss: 0.0306145810
Epoch:  1000  |  train loss: 0.0313214149
Processing class: 8
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.0289785329
Epoch:   200  |  train loss: 0.0296056185
Epoch:   300  |  train loss: 0.0294435717
Epoch:   400  |  train loss: 0.0305497285
Epoch:   500  |  train loss: 0.0301268321
Epoch:   600  |  train loss: 0.0304807376
Epoch:   700  |  train loss: 0.0312501643
Epoch:   800  |  train loss: 0.0306800686
Epoch:   900  |  train loss: 0.0311456658
Epoch:  1000  |  train loss: 0.0308044005
Processing class: 9
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.0283252873
Epoch:   200  |  train loss: 0.0289147709
Epoch:   300  |  train loss: 0.0297686048
Epoch:   400  |  train loss: 0.0290413443
Epoch:   500  |  train loss: 0.0298904426
Epoch:   600  |  train loss: 0.0294063333
Epoch:   700  |  train loss: 0.0297141332
Epoch:   800  |  train loss: 0.0299656540
Epoch:   900  |  train loss: 0.0300066099
Epoch:  1000  |  train loss: 0.0295306023
Processing class: 10
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.0277081445
Epoch:   200  |  train loss: 0.0277296402
Epoch:   300  |  train loss: 0.0280921821
Epoch:   400  |  train loss: 0.0290652372
Epoch:   500  |  train loss: 0.0280302700
Epoch:   600  |  train loss: 0.0285254583
Epoch:   700  |  train loss: 0.0289125644
Epoch:   800  |  train loss: 0.0287296180
Epoch:   900  |  train loss: 0.0290244412
Epoch:  1000  |  train loss: 0.0289990731
Processing class: 11
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.0292021982
Epoch:   200  |  train loss: 0.0296466921
Epoch:   300  |  train loss: 0.0293320280
Epoch:   400  |  train loss: 0.0298213869
Epoch:   500  |  train loss: 0.0299556527
Epoch:   600  |  train loss: 0.0306145832
Epoch:   700  |  train loss: 0.0315399766
Epoch:   800  |  train loss: 0.0313912708
Epoch:   900  |  train loss: 0.0314932894
Epoch:  1000  |  train loss: 0.0320001278
Processing class: 12
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.0280054498
Epoch:   200  |  train loss: 0.0294079222
Epoch:   300  |  train loss: 0.0295785207
Epoch:   400  |  train loss: 0.0304031726
Epoch:   500  |  train loss: 0.0305529218
Epoch:   600  |  train loss: 0.0302208006
Epoch:   700  |  train loss: 0.0305444714
Epoch:   800  |  train loss: 0.0308049910
Epoch:   900  |  train loss: 0.0312942807
Epoch:  1000  |  train loss: 0.0310045257
Processing class: 13
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.0288017698
Epoch:   200  |  train loss: 0.0299506601
Epoch:   300  |  train loss: 0.0308160447
Epoch:   400  |  train loss: 0.0311181594
Epoch:   500  |  train loss: 0.0319024447
Epoch:   600  |  train loss: 0.0313610528
Epoch:   700  |  train loss: 0.0318372227
Epoch:   800  |  train loss: 0.0322619591
Epoch:   900  |  train loss: 0.0325202174
Epoch:  1000  |  train loss: 0.0328322634
Processing class: 14
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.0287713759
Epoch:   200  |  train loss: 0.0291830570
Epoch:   300  |  train loss: 0.0295166146
Epoch:   400  |  train loss: 0.0303201526
Epoch:   500  |  train loss: 0.0302211408
Epoch:   600  |  train loss: 0.0302797098
Epoch:   700  |  train loss: 0.0306989715
Epoch:   800  |  train loss: 0.0316648439
Epoch:   900  |  train loss: 0.0313664906
Epoch:  1000  |  train loss: 0.0307698037
Processing class: 15
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.0290924743
Epoch:   200  |  train loss: 0.0295815516
Epoch:   300  |  train loss: 0.0306645796
Epoch:   400  |  train loss: 0.0311343402
Epoch:   500  |  train loss: 0.0316438690
Epoch:   600  |  train loss: 0.0312178142
Epoch:   700  |  train loss: 0.0320324808
Epoch:   800  |  train loss: 0.0313344404
Epoch:   900  |  train loss: 0.0321035363
Epoch:  1000  |  train loss: 0.0317835122
Processing class: 16
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.0293211196
Epoch:   200  |  train loss: 0.0312537372
Epoch:   300  |  train loss: 0.0320877466
Epoch:   400  |  train loss: 0.0317094665
Epoch:   500  |  train loss: 0.0324123748
Epoch:   600  |  train loss: 0.0318647113
Epoch:   700  |  train loss: 0.0317167602
Epoch:   800  |  train loss: 0.0327166744
Epoch:   900  |  train loss: 0.0318746895
Epoch:  1000  |  train loss: 0.0327347614
Processing class: 17
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.0278792579
Epoch:   200  |  train loss: 0.0283911031
Epoch:   300  |  train loss: 0.0291804485
Epoch:   400  |  train loss: 0.0293587577
Epoch:   500  |  train loss: 0.0285972532
Epoch:   600  |  train loss: 0.0292228982
Epoch:   700  |  train loss: 0.0291051153
Epoch:   800  |  train loss: 0.0292120185
Epoch:   900  |  train loss: 0.0294778530
Epoch:  1000  |  train loss: 0.0293961018
Processing class: 18
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.0287785478
Epoch:   200  |  train loss: 0.0290065642
Epoch:   300  |  train loss: 0.0298421450
Epoch:   400  |  train loss: 0.0298212599
Epoch:   500  |  train loss: 0.0307827823
Epoch:   600  |  train loss: 0.0305571929
Epoch:   700  |  train loss: 0.0318378761
Epoch:   800  |  train loss: 0.0318400070
Epoch:   900  |  train loss: 0.0321334768
Epoch:  1000  |  train loss: 0.0328844257
Processing class: 19
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.0292069677
Epoch:   200  |  train loss: 0.0307985500
Epoch:   300  |  train loss: 0.0308673456
Epoch:   400  |  train loss: 0.0310789376
Epoch:   500  |  train loss: 0.0306671664
Epoch:   600  |  train loss: 0.0308035705
Epoch:   700  |  train loss: 0.0312036689
Epoch:   800  |  train loss: 0.0310142696
Epoch:   900  |  train loss: 0.0324247859
Epoch:  1000  |  train loss: 0.0327296659
Processing class: 20
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.0292185787
Epoch:   200  |  train loss: 0.0297833446
Epoch:   300  |  train loss: 0.0303245503
Epoch:   400  |  train loss: 0.0307900399
Epoch:   500  |  train loss: 0.0315397490
Epoch:   600  |  train loss: 0.0311292950
Epoch:   700  |  train loss: 0.0313272256
Epoch:   800  |  train loss: 0.0313490748
Epoch:   900  |  train loss: 0.0317009080
Epoch:  1000  |  train loss: 0.0322429009
Processing class: 21
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.0293342788
Epoch:   200  |  train loss: 0.0300127652
Epoch:   300  |  train loss: 0.0305514496
Epoch:   400  |  train loss: 0.0309115354
Epoch:   500  |  train loss: 0.0312791061
Epoch:   600  |  train loss: 0.0317727476
Epoch:   700  |  train loss: 0.0316530779
Epoch:   800  |  train loss: 0.0315218352
Epoch:   900  |  train loss: 0.0322178040
Epoch:  1000  |  train loss: 0.0320979390
Processing class: 22
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.0286212176
Epoch:   200  |  train loss: 0.0289052028
Epoch:   300  |  train loss: 0.0295505926
Epoch:   400  |  train loss: 0.0304202642
Epoch:   500  |  train loss: 0.0306997865
Epoch:   600  |  train loss: 0.0303789493
Epoch:   700  |  train loss: 0.0307718936
Epoch:   800  |  train loss: 0.0306173477
Epoch:   900  |  train loss: 0.0303372111
Epoch:  1000  |  train loss: 0.0314027686
Processing class: 23
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.0298904024
Epoch:   200  |  train loss: 0.0305791695
Epoch:   300  |  train loss: 0.0312723182
Epoch:   400  |  train loss: 0.0306190938
Epoch:   500  |  train loss: 0.0306599855
Epoch:   600  |  train loss: 0.0303456139
Epoch:   700  |  train loss: 0.0308588151
Epoch:   800  |  train loss: 0.0305513270
Epoch:   900  |  train loss: 0.0310500372
Epoch:  1000  |  train loss: 0.0306928620
Processing class: 24
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.0292779252
Epoch:   200  |  train loss: 0.0293185748
Epoch:   300  |  train loss: 0.0286774997
Epoch:   400  |  train loss: 0.0294419587
Epoch:   500  |  train loss: 0.0291632835
Epoch:   600  |  train loss: 0.0296292309
Epoch:   700  |  train loss: 0.0287705380
Epoch:   800  |  train loss: 0.0291042987
Epoch:   900  |  train loss: 0.0296864033
Epoch:  1000  |  train loss: 0.0295266263
Processing class: 25
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.0287934251
Epoch:   200  |  train loss: 0.0293825168
Epoch:   300  |  train loss: 0.0302013464
Epoch:   400  |  train loss: 0.0297851849
Epoch:   500  |  train loss: 0.0307424303
Epoch:   600  |  train loss: 0.0303757425
Epoch:   700  |  train loss: 0.0311939616
Epoch:   800  |  train loss: 0.0309250146
Epoch:   900  |  train loss: 0.0307689395
Epoch:  1000  |  train loss: 0.0310563795
Processing class: 26
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.0301742557
Epoch:   200  |  train loss: 0.0310922954
Epoch:   300  |  train loss: 0.0305255204
Epoch:   400  |  train loss: 0.0312864706
Epoch:   500  |  train loss: 0.0320315272
Epoch:   600  |  train loss: 0.0311397992
Epoch:   700  |  train loss: 0.0315627150
Epoch:   800  |  train loss: 0.0318977948
Epoch:   900  |  train loss: 0.0317975853
Epoch:  1000  |  train loss: 0.0324179649
Processing class: 27
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.0288569819
Epoch:   200  |  train loss: 0.0286996253
Epoch:   300  |  train loss: 0.0290189706
Epoch:   400  |  train loss: 0.0293272141
Epoch:   500  |  train loss: 0.0298510849
Epoch:   600  |  train loss: 0.0298903424
Epoch:   700  |  train loss: 0.0302009940
Epoch:   800  |  train loss: 0.0302000441
Epoch:   900  |  train loss: 0.0304815933
Epoch:  1000  |  train loss: 0.0298619561
Processing class: 28
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.0297201499
Epoch:   200  |  train loss: 0.0304861370
Epoch:   300  |  train loss: 0.0314056754
Epoch:   400  |  train loss: 0.0318752624
Epoch:   500  |  train loss: 0.0321416020
Epoch:   600  |  train loss: 0.0322301593
Epoch:   700  |  train loss: 0.0323981430
Epoch:   800  |  train loss: 0.0326599173
Epoch:   900  |  train loss: 0.0328229927
Epoch:  1000  |  train loss: 0.0322549187
Processing class: 29
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.0291549195
Epoch:   200  |  train loss: 0.0297481690
Epoch:   300  |  train loss: 0.0297662705
Epoch:   400  |  train loss: 0.0295553695
Epoch:   500  |  train loss: 0.0302198093
Epoch:   600  |  train loss: 0.0305861857
Epoch:   700  |  train loss: 0.0306346186
Epoch:   800  |  train loss: 0.0302609332
Epoch:   900  |  train loss: 0.0304257520
Epoch:  1000  |  train loss: 0.0303061616
Processing class: 30
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.0301812902
Epoch:   200  |  train loss: 0.0310257304
Epoch:   300  |  train loss: 0.0322876077
Epoch:   400  |  train loss: 0.0333139345
Epoch:   500  |  train loss: 0.0328593507
Epoch:   600  |  train loss: 0.0328746088
Epoch:   700  |  train loss: 0.0327803738
Epoch:   800  |  train loss: 0.0334699258
Epoch:   900  |  train loss: 0.0339186810
Epoch:  1000  |  train loss: 0.0334736668
Processing class: 31
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.0297560729
Epoch:   200  |  train loss: 0.0307600256
Epoch:   300  |  train loss: 0.0309752148
Epoch:   400  |  train loss: 0.0318834577
Epoch:   500  |  train loss: 0.0322253715
Epoch:   600  |  train loss: 0.0329881430
Epoch:   700  |  train loss: 0.0328497354
Epoch:   800  |  train loss: 0.0329106886
Epoch:   900  |  train loss: 0.0335011795
Epoch:  1000  |  train loss: 0.0329138063
Processing class: 32
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.0288884208
Epoch:   200  |  train loss: 0.0296459313
Epoch:   300  |  train loss: 0.0295812778
Epoch:   400  |  train loss: 0.0290504254
Epoch:   500  |  train loss: 0.0303299528
Epoch:   600  |  train loss: 0.0301227640
Epoch:   700  |  train loss: 0.0311019033
Epoch:   800  |  train loss: 0.0315090887
Epoch:   900  |  train loss: 0.0313443910
Epoch:  1000  |  train loss: 0.0311406981
Processing class: 33
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.0280107711
Epoch:   200  |  train loss: 0.0296426382
Epoch:   300  |  train loss: 0.0285530154
Epoch:   400  |  train loss: 0.0288176198
Epoch:   500  |  train loss: 0.0290276889
Epoch:   600  |  train loss: 0.0300421860
Epoch:   700  |  train loss: 0.0295850795
Epoch:   800  |  train loss: 0.0298558738
Epoch:   900  |  train loss: 0.0298130970
Epoch:  1000  |  train loss: 0.0295761403
Processing class: 34
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.0292531658
Epoch:   200  |  train loss: 0.0291261278
Epoch:   300  |  train loss: 0.0297951553
Epoch:   400  |  train loss: 0.0295313857
Epoch:   500  |  train loss: 0.0295654841
Epoch:   600  |  train loss: 0.0301897425
Epoch:   700  |  train loss: 0.0297595125
Epoch:   800  |  train loss: 0.0304957822
Epoch:   900  |  train loss: 0.0307517312
Epoch:  1000  |  train loss: 0.0309164442
Processing class: 35
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.0282058265
Epoch:   200  |  train loss: 0.0290876709
Epoch:   300  |  train loss: 0.0297159299
Epoch:   400  |  train loss: 0.0300358359
Epoch:   500  |  train loss: 0.0296402588
Epoch:   600  |  train loss: 0.0300393458
Epoch:   700  |  train loss: 0.0303365543
Epoch:   800  |  train loss: 0.0302957829
Epoch:   900  |  train loss: 0.0304242197
Epoch:  1000  |  train loss: 0.0301821992
Processing class: 36
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.0297277138
Epoch:   200  |  train loss: 0.0300396737
Epoch:   300  |  train loss: 0.0306394223
Epoch:   400  |  train loss: 0.0307065856
Epoch:   500  |  train loss: 0.0316500206
Epoch:   600  |  train loss: 0.0313370802
Epoch:   700  |  train loss: 0.0309720855
Epoch:   800  |  train loss: 0.0320149198
Epoch:   900  |  train loss: 0.0312709205
Epoch:  1000  |  train loss: 0.0319043901
Processing class: 37
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.0293857157
Epoch:   200  |  train loss: 0.0293042395
Epoch:   300  |  train loss: 0.0293099675
Epoch:   400  |  train loss: 0.0297748204
Epoch:   500  |  train loss: 0.0301945832
Epoch:   600  |  train loss: 0.0298009966
Epoch:   700  |  train loss: 0.0299585089
Epoch:   800  |  train loss: 0.0302144282
Epoch:   900  |  train loss: 0.0298873097
Epoch:  1000  |  train loss: 0.0301668171
Processing class: 38
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.0291953780
Epoch:   200  |  train loss: 0.0295413025
Epoch:   300  |  train loss: 0.0303044818
Epoch:   400  |  train loss: 0.0305444583
Epoch:   500  |  train loss: 0.0308715481
Epoch:   600  |  train loss: 0.0307599645
Epoch:   700  |  train loss: 0.0309307631
Epoch:   800  |  train loss: 0.0315730248
Epoch:   900  |  train loss: 0.0308745459
Epoch:  1000  |  train loss: 0.0314301200
Processing class: 39
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.0293045837
Epoch:   200  |  train loss: 0.0291436136
Epoch:   300  |  train loss: 0.0296987887
Epoch:   400  |  train loss: 0.0301448695
Epoch:   500  |  train loss: 0.0299836434
Epoch:   600  |  train loss: 0.0302507501
Epoch:   700  |  train loss: 0.0306125566
Epoch:   800  |  train loss: 0.0310944118
Epoch:   900  |  train loss: 0.0313979328
Epoch:  1000  |  train loss: 0.0316190511
Processing class: 40
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.0294860307
Epoch:   200  |  train loss: 0.0291862186
Epoch:   300  |  train loss: 0.0304904073
Epoch:   400  |  train loss: 0.0307885170
Epoch:   500  |  train loss: 0.0312034249
Epoch:   600  |  train loss: 0.0309701942
Epoch:   700  |  train loss: 0.0313035514
Epoch:   800  |  train loss: 0.0313162245
Epoch:   900  |  train loss: 0.0310237605
Epoch:  1000  |  train loss: 0.0321768560
Processing class: 41
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.0293936837
Epoch:   200  |  train loss: 0.0294982504
Epoch:   300  |  train loss: 0.0302106507
Epoch:   400  |  train loss: 0.0301040128
Epoch:   500  |  train loss: 0.0302032385
Epoch:   600  |  train loss: 0.0302894995
Epoch:   700  |  train loss: 0.0308478937
Epoch:   800  |  train loss: 0.0305657461
Epoch:   900  |  train loss: 0.0304710291
Epoch:  1000  |  train loss: 0.0305407025
Processing class: 42
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.0285115313
Epoch:   200  |  train loss: 0.0294354863
Epoch:   300  |  train loss: 0.0300852198
Epoch:   400  |  train loss: 0.0304096449
Epoch:   500  |  train loss: 0.0308127958
Epoch:   600  |  train loss: 0.0311063372
Epoch:   700  |  train loss: 0.0315332159
Epoch:   800  |  train loss: 0.0318717655
Epoch:   900  |  train loss: 0.0316094525
Epoch:  1000  |  train loss: 0.0316032838
Processing class: 43
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.0286812734
Epoch:   200  |  train loss: 0.0287127551
Epoch:   300  |  train loss: 0.0302634828
Epoch:   400  |  train loss: 0.0314874481
Epoch:   500  |  train loss: 0.0318271518
Epoch:   600  |  train loss: 0.0315433443
Epoch:   700  |  train loss: 0.0320537504
Epoch:   800  |  train loss: 0.0318517372
Epoch:   900  |  train loss: 0.0312397208
Epoch:  1000  |  train loss: 0.0321020477
Processing class: 44
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.0284243934
Epoch:   200  |  train loss: 0.0294863954
Epoch:   300  |  train loss: 0.0292131338
Epoch:   400  |  train loss: 0.0295089714
Epoch:   500  |  train loss: 0.0299402509
Epoch:   600  |  train loss: 0.0297011077
Epoch:   700  |  train loss: 0.0303573430
Epoch:   800  |  train loss: 0.0302104730
Epoch:   900  |  train loss: 0.0299973447
Epoch:  1000  |  train loss: 0.0307188723
Processing class: 45
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.0294429153
Epoch:   200  |  train loss: 0.0307191715
Epoch:   300  |  train loss: 0.0308288697
Epoch:   400  |  train loss: 0.0315336347
Epoch:   500  |  train loss: 0.0314258058
Epoch:   600  |  train loss: 0.0324561238
Epoch:   700  |  train loss: 0.0327313744
Epoch:   800  |  train loss: 0.0325033024
Epoch:   900  |  train loss: 0.0326147318
Epoch:  1000  |  train loss: 0.0331667550
Processing class: 46
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.0299079947
Epoch:   200  |  train loss: 0.0301304765
Epoch:   300  |  train loss: 0.0307314090
Epoch:   400  |  train loss: 0.0310408138
Epoch:   500  |  train loss: 0.0311529264
Epoch:   600  |  train loss: 0.0311746459
Epoch:   700  |  train loss: 0.0313034501
Epoch:   800  |  train loss: 0.0321329601
Epoch:   900  |  train loss: 0.0317176122
Epoch:  1000  |  train loss: 0.0320826527
Processing class: 47
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.0284876112
Epoch:   200  |  train loss: 0.0292788535
Epoch:   300  |  train loss: 0.0290900584
Epoch:   400  |  train loss: 0.0299167804
Epoch:   500  |  train loss: 0.0299980108
Epoch:   600  |  train loss: 0.0300707407
Epoch:   700  |  train loss: 0.0303931370
Epoch:   800  |  train loss: 0.0300604120
Epoch:   900  |  train loss: 0.0303444650
Epoch:  1000  |  train loss: 0.0301775008
Processing class: 48
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.0289679397
Epoch:   200  |  train loss: 0.0299127400
Epoch:   300  |  train loss: 0.0299393438
Epoch:   400  |  train loss: 0.0293786611
Epoch:   500  |  train loss: 0.0303151038
Epoch:   600  |  train loss: 0.0302155785
Epoch:   700  |  train loss: 0.0298471112
Epoch:   800  |  train loss: 0.0298423056
Epoch:   900  |  train loss: 0.0307963666
Epoch:  1000  |  train loss: 0.0310963906
Processing class: 49
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.0288069434
Epoch:   200  |  train loss: 0.0291188028
Epoch:   300  |  train loss: 0.0304313067
Epoch:   400  |  train loss: 0.0307060197
Epoch:   500  |  train loss: 0.0310861629
Epoch:   600  |  train loss: 0.0311488803
Epoch:   700  |  train loss: 0.0311981022
Epoch:   800  |  train loss: 0.0310519598
Epoch:   900  |  train loss: 0.0309645232
Epoch:  1000  |  train loss: 0.0311382566
/home/z1165703/FeCAM/models/base.py:87: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  vec = torch.tensor(vec).cuda()
2024-03-05 01:24:17,345 [trainer.py] => CNN: {'total': 83.44, '00-09': 87.7, '10-19': 79.6, '20-29': 84.9, '30-39': 81.0, '40-49': 84.0, 'old': 0, 'new': 83.44}
2024-03-05 01:24:17,347 [trainer.py] => No NME accuracy
2024-03-05 01:24:17,347 [trainer.py] => FeCAM: {'total': 82.9, '00-09': 86.2, '10-19': 79.7, '20-29': 85.0, '30-39': 80.6, '40-49': 83.0, 'old': 0, 'new': 82.9}
2024-03-05 01:24:17,347 [trainer.py] => CNN top1 curve: [83.44]
2024-03-05 01:24:17,347 [trainer.py] => CNN top5 curve: [96.5]
2024-03-05 01:24:17,347 [trainer.py] => FeCAM top1 curve: [82.9]
2024-03-05 01:24:17,347 [trainer.py] => FeCAM top5 curve: [95.84]

2024-03-05 01:24:17,356 [fecam.py] => Learning on 50-60
Processing class: 50
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.0279370610
Epoch:   200  |  train loss: 0.0282734968
Epoch:   300  |  train loss: 0.0285259310
Epoch:   400  |  train loss: 0.0292466026
Epoch:   500  |  train loss: 0.0297681708
Epoch:   600  |  train loss: 0.0294681329
Epoch:   700  |  train loss: 0.0302735228
Epoch:   800  |  train loss: 0.0306932945
Epoch:   900  |  train loss: 0.0311656985
Epoch:  1000  |  train loss: 0.0314024471
Processing class: 51
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.0282336406
Epoch:   200  |  train loss: 0.0276863568
Epoch:   300  |  train loss: 0.0288102668
Epoch:   400  |  train loss: 0.0291240454
Epoch:   500  |  train loss: 0.0294285327
Epoch:   600  |  train loss: 0.0299584530
Epoch:   700  |  train loss: 0.0294814795
Epoch:   800  |  train loss: 0.0297096554
Epoch:   900  |  train loss: 0.0305923950
Epoch:  1000  |  train loss: 0.0303704161
Processing class: 52
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.0282887142
Epoch:   200  |  train loss: 0.0280884668
Epoch:   300  |  train loss: 0.0286129329
Epoch:   400  |  train loss: 0.0290098164
Epoch:   500  |  train loss: 0.0290015679
Epoch:   600  |  train loss: 0.0287008543
Epoch:   700  |  train loss: 0.0293917101
Epoch:   800  |  train loss: 0.0295014627
Epoch:   900  |  train loss: 0.0297482599
Epoch:  1000  |  train loss: 0.0293474548
Processing class: 53
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.0278678183
Epoch:   200  |  train loss: 0.0282934338
Epoch:   300  |  train loss: 0.0277598653
Epoch:   400  |  train loss: 0.0281336933
Epoch:   500  |  train loss: 0.0285369270
Epoch:   600  |  train loss: 0.0291369341
Epoch:   700  |  train loss: 0.0298413854
Epoch:   800  |  train loss: 0.0287332881
Epoch:   900  |  train loss: 0.0293537360
Epoch:  1000  |  train loss: 0.0290140826
Processing class: 54
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.0262611143
Epoch:   200  |  train loss: 0.0265922762
Epoch:   300  |  train loss: 0.0273617920
Epoch:   400  |  train loss: 0.0276975639
Epoch:   500  |  train loss: 0.0280963309
Epoch:   600  |  train loss: 0.0289111119
Epoch:   700  |  train loss: 0.0291978303
Epoch:   800  |  train loss: 0.0298667315
Epoch:   900  |  train loss: 0.0303396676
Epoch:  1000  |  train loss: 0.0302405223
Processing class: 55
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.0278349381
Epoch:   200  |  train loss: 0.0279815778
Epoch:   300  |  train loss: 0.0277067319
Epoch:   400  |  train loss: 0.0284891795
Epoch:   500  |  train loss: 0.0295193009
Epoch:   600  |  train loss: 0.0297054742
Epoch:   700  |  train loss: 0.0301008049
Epoch:   800  |  train loss: 0.0308869954
Epoch:   900  |  train loss: 0.0301310103
Epoch:  1000  |  train loss: 0.0307244472
Processing class: 56
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.0277435314
Epoch:   200  |  train loss: 0.0282079764
Epoch:   300  |  train loss: 0.0278571062
Epoch:   400  |  train loss: 0.0288654324
Epoch:   500  |  train loss: 0.0293729972
Epoch:   600  |  train loss: 0.0297585420
Epoch:   700  |  train loss: 0.0302519139
Epoch:   800  |  train loss: 0.0299753312
Epoch:   900  |  train loss: 0.0306747902
Epoch:  1000  |  train loss: 0.0311122328
Processing class: 57
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.0275079247
Epoch:   200  |  train loss: 0.0281550851
Epoch:   300  |  train loss: 0.0279706039
Epoch:   400  |  train loss: 0.0280585915
Epoch:   500  |  train loss: 0.0286179543
Epoch:   600  |  train loss: 0.0280768540
Epoch:   700  |  train loss: 0.0290389437
Epoch:   800  |  train loss: 0.0291130491
Epoch:   900  |  train loss: 0.0296699379
Epoch:  1000  |  train loss: 0.0290150903
Processing class: 58
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.0278224334
Epoch:   200  |  train loss: 0.0278137065
Epoch:   300  |  train loss: 0.0286782354
Epoch:   400  |  train loss: 0.0296969213
Epoch:   500  |  train loss: 0.0296124492
Epoch:   600  |  train loss: 0.0302630953
Epoch:   700  |  train loss: 0.0302300841
Epoch:   800  |  train loss: 0.0307588961
Epoch:   900  |  train loss: 0.0308134008
Epoch:  1000  |  train loss: 0.0309390631
Processing class: 59
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.0284456380
Epoch:   200  |  train loss: 0.0293920394
Epoch:   300  |  train loss: 0.0292276070
Epoch:   400  |  train loss: 0.0298132725
Epoch:   500  |  train loss: 0.0303960908
Epoch:   600  |  train loss: 0.0305645231
Epoch:   700  |  train loss: 0.0309852634
Epoch:   800  |  train loss: 0.0312093478
Epoch:   900  |  train loss: 0.0311825283
Epoch:  1000  |  train loss: 0.0310097158
2024-03-05 01:30:00,641 [trainer.py] => CNN: {'total': 71.75, '00-09': 82.2, '10-19': 72.8, '20-29': 78.5, '30-39': 74.9, '40-49': 68.4, '50-59': 53.7, 'old': 75.36, 'new': 53.7}
2024-03-05 01:30:00,642 [trainer.py] => No NME accuracy
2024-03-05 01:30:00,642 [trainer.py] => FeCAM: {'total': 72.78, '00-09': 82.6, '10-19': 74.0, '20-29': 81.0, '30-39': 76.4, '40-49': 76.8, '50-59': 45.9, 'old': 78.16, 'new': 45.9}
2024-03-05 01:30:00,642 [trainer.py] => CNN top1 curve: [83.44, 71.75]
2024-03-05 01:30:00,642 [trainer.py] => CNN top5 curve: [96.5, 89.6]
2024-03-05 01:30:00,642 [trainer.py] => FeCAM top1 curve: [82.9, 72.78]
2024-03-05 01:30:00,642 [trainer.py] => FeCAM top5 curve: [95.84, 91.18]

2024-03-05 01:30:00,649 [fecam.py] => Learning on 60-70
Processing class: 60
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.0288023435
Epoch:   200  |  train loss: 0.0287692055
Epoch:   300  |  train loss: 0.0284982469
Epoch:   400  |  train loss: 0.0284849618
Epoch:   500  |  train loss: 0.0294695258
Epoch:   600  |  train loss: 0.0296133358
Epoch:   700  |  train loss: 0.0315648839
Epoch:   800  |  train loss: 0.0307263631
Epoch:   900  |  train loss: 0.0309802290
Epoch:  1000  |  train loss: 0.0315566320
Processing class: 61
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.0276758354
Epoch:   200  |  train loss: 0.0273251321
Epoch:   300  |  train loss: 0.0283683389
Epoch:   400  |  train loss: 0.0286947723
Epoch:   500  |  train loss: 0.0298124306
Epoch:   600  |  train loss: 0.0292882185
Epoch:   700  |  train loss: 0.0303004228
Epoch:   800  |  train loss: 0.0309309069
Epoch:   900  |  train loss: 0.0302193794
Epoch:  1000  |  train loss: 0.0306312401
Processing class: 62
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.0283323392
Epoch:   200  |  train loss: 0.0279889148
Epoch:   300  |  train loss: 0.0286061186
Epoch:   400  |  train loss: 0.0289078161
Epoch:   500  |  train loss: 0.0285861664
Epoch:   600  |  train loss: 0.0294999868
Epoch:   700  |  train loss: 0.0298055887
Epoch:   800  |  train loss: 0.0300790507
Epoch:   900  |  train loss: 0.0299386933
Epoch:  1000  |  train loss: 0.0302238058
Processing class: 63
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.0272883169
Epoch:   200  |  train loss: 0.0275629800
Epoch:   300  |  train loss: 0.0289217114
Epoch:   400  |  train loss: 0.0292035554
Epoch:   500  |  train loss: 0.0297873944
Epoch:   600  |  train loss: 0.0306618717
Epoch:   700  |  train loss: 0.0298445623
Epoch:   800  |  train loss: 0.0312972959
Epoch:   900  |  train loss: 0.0305146914
Epoch:  1000  |  train loss: 0.0307904847
Processing class: 64
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.0289483503
Epoch:   200  |  train loss: 0.0286891855
Epoch:   300  |  train loss: 0.0302688133
Epoch:   400  |  train loss: 0.0305073239
Epoch:   500  |  train loss: 0.0308409773
Epoch:   600  |  train loss: 0.0309334453
Epoch:   700  |  train loss: 0.0304227378
Epoch:   800  |  train loss: 0.0308568709
Epoch:   900  |  train loss: 0.0316173948
Epoch:  1000  |  train loss: 0.0315032165
Processing class: 65
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.0281617254
Epoch:   200  |  train loss: 0.0283102017
Epoch:   300  |  train loss: 0.0299417958
Epoch:   400  |  train loss: 0.0306733992
Epoch:   500  |  train loss: 0.0306265440
Epoch:   600  |  train loss: 0.0319254380
Epoch:   700  |  train loss: 0.0320610303
Epoch:   800  |  train loss: 0.0332245104
Epoch:   900  |  train loss: 0.0326796725
Epoch:  1000  |  train loss: 0.0338539280
Processing class: 66
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.0286058709
Epoch:   200  |  train loss: 0.0292343326
Epoch:   300  |  train loss: 0.0299847301
Epoch:   400  |  train loss: 0.0301656775
Epoch:   500  |  train loss: 0.0309289787
Epoch:   600  |  train loss: 0.0311136916
Epoch:   700  |  train loss: 0.0319519170
Epoch:   800  |  train loss: 0.0320965998
Epoch:   900  |  train loss: 0.0321657471
Epoch:  1000  |  train loss: 0.0320929479
Processing class: 67
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.0274756495
Epoch:   200  |  train loss: 0.0278934471
Epoch:   300  |  train loss: 0.0279883131
Epoch:   400  |  train loss: 0.0282569226
Epoch:   500  |  train loss: 0.0283921201
Epoch:   600  |  train loss: 0.0284238301
Epoch:   700  |  train loss: 0.0286526166
Epoch:   800  |  train loss: 0.0291861109
Epoch:   900  |  train loss: 0.0291486464
Epoch:  1000  |  train loss: 0.0291674305
Processing class: 68
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.0273960557
Epoch:   200  |  train loss: 0.0274767675
Epoch:   300  |  train loss: 0.0275361244
Epoch:   400  |  train loss: 0.0279384755
Epoch:   500  |  train loss: 0.0281033196
Epoch:   600  |  train loss: 0.0280944820
Epoch:   700  |  train loss: 0.0284963656
Epoch:   800  |  train loss: 0.0282746244
Epoch:   900  |  train loss: 0.0286008932
Epoch:  1000  |  train loss: 0.0295430452
Processing class: 69
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.0273510151
Epoch:   200  |  train loss: 0.0281272016
Epoch:   300  |  train loss: 0.0296065047
Epoch:   400  |  train loss: 0.0303202573
Epoch:   500  |  train loss: 0.0305104300
Epoch:   600  |  train loss: 0.0297512848
Epoch:   700  |  train loss: 0.0305675894
Epoch:   800  |  train loss: 0.0305633862
Epoch:   900  |  train loss: 0.0316384386
Epoch:  1000  |  train loss: 0.0306615774
2024-03-05 01:36:40,746 [trainer.py] => CNN: {'total': 64.66, '00-09': 75.0, '10-19': 70.1, '20-29': 77.1, '30-39': 70.6, '40-49': 64.0, '50-59': 45.8, '60-69': 50.0, 'old': 67.1, 'new': 50.0}
2024-03-05 01:36:40,747 [trainer.py] => No NME accuracy
2024-03-05 01:36:40,747 [trainer.py] => FeCAM: {'total': 67.27, '00-09': 79.9, '10-19': 70.6, '20-29': 79.1, '30-39': 71.9, '40-49': 74.1, '50-59': 41.2, '60-69': 54.1, 'old': 69.47, 'new': 54.1}
2024-03-05 01:36:40,747 [trainer.py] => CNN top1 curve: [83.44, 71.75, 64.66]
2024-03-05 01:36:40,747 [trainer.py] => CNN top5 curve: [96.5, 89.6, 86.54]
2024-03-05 01:36:40,747 [trainer.py] => FeCAM top1 curve: [82.9, 72.78, 67.27]
2024-03-05 01:36:40,747 [trainer.py] => FeCAM top5 curve: [95.84, 91.18, 87.21]

2024-03-05 01:36:40,752 [fecam.py] => Learning on 70-80
Processing class: 70
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.0276328210
Epoch:   200  |  train loss: 0.0274257381
Epoch:   300  |  train loss: 0.0279639576
Epoch:   400  |  train loss: 0.0284221247
Epoch:   500  |  train loss: 0.0283157188
Epoch:   600  |  train loss: 0.0290390700
Epoch:   700  |  train loss: 0.0299646851
Epoch:   800  |  train loss: 0.0292553119
Epoch:   900  |  train loss: 0.0299527165
Epoch:  1000  |  train loss: 0.0297924578
Processing class: 71
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.0280312572
Epoch:   200  |  train loss: 0.0283801127
Epoch:   300  |  train loss: 0.0280130100
Epoch:   400  |  train loss: 0.0287048928
Epoch:   500  |  train loss: 0.0292371266
Epoch:   600  |  train loss: 0.0294409286
Epoch:   700  |  train loss: 0.0296424035
Epoch:   800  |  train loss: 0.0299271271
Epoch:   900  |  train loss: 0.0307145096
Epoch:  1000  |  train loss: 0.0306606259
Processing class: 72
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.0275802132
Epoch:   200  |  train loss: 0.0279100917
Epoch:   300  |  train loss: 0.0281659372
Epoch:   400  |  train loss: 0.0282888748
Epoch:   500  |  train loss: 0.0288530044
Epoch:   600  |  train loss: 0.0285005286
Epoch:   700  |  train loss: 0.0288486242
Epoch:   800  |  train loss: 0.0287375618
Epoch:   900  |  train loss: 0.0290297143
Epoch:  1000  |  train loss: 0.0297178213
Processing class: 73
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.0282805517
Epoch:   200  |  train loss: 0.0282469779
Epoch:   300  |  train loss: 0.0292801350
Epoch:   400  |  train loss: 0.0290456552
Epoch:   500  |  train loss: 0.0299490135
Epoch:   600  |  train loss: 0.0299793735
Epoch:   700  |  train loss: 0.0296384539
Epoch:   800  |  train loss: 0.0296927419
Epoch:   900  |  train loss: 0.0298662499
Epoch:  1000  |  train loss: 0.0303130738
Processing class: 74
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.0285584830
Epoch:   200  |  train loss: 0.0289780848
Epoch:   300  |  train loss: 0.0284095738
Epoch:   400  |  train loss: 0.0282343362
Epoch:   500  |  train loss: 0.0287336059
Epoch:   600  |  train loss: 0.0286053427
Epoch:   700  |  train loss: 0.0295800030
Epoch:   800  |  train loss: 0.0288549419
Epoch:   900  |  train loss: 0.0290364977
Epoch:  1000  |  train loss: 0.0298618890
Processing class: 75
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.0277173541
Epoch:   200  |  train loss: 0.0276717901
Epoch:   300  |  train loss: 0.0285668831
Epoch:   400  |  train loss: 0.0286119439
Epoch:   500  |  train loss: 0.0292822380
Epoch:   600  |  train loss: 0.0299427390
Epoch:   700  |  train loss: 0.0286214605
Epoch:   800  |  train loss: 0.0295962516
Epoch:   900  |  train loss: 0.0296000510
Epoch:  1000  |  train loss: 0.0303202629
Processing class: 76
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.0266094200
Epoch:   200  |  train loss: 0.0273813412
Epoch:   300  |  train loss: 0.0286531288
Epoch:   400  |  train loss: 0.0295342028
Epoch:   500  |  train loss: 0.0299091674
Epoch:   600  |  train loss: 0.0305795964
Epoch:   700  |  train loss: 0.0307493318
Epoch:   800  |  train loss: 0.0300706428
Epoch:   900  |  train loss: 0.0310376793
Epoch:  1000  |  train loss: 0.0314873252
Processing class: 77
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.0279037397
Epoch:   200  |  train loss: 0.0281509187
Epoch:   300  |  train loss: 0.0279496569
Epoch:   400  |  train loss: 0.0284531094
Epoch:   500  |  train loss: 0.0288707294
Epoch:   600  |  train loss: 0.0290903568
Epoch:   700  |  train loss: 0.0294380967
Epoch:   800  |  train loss: 0.0302073799
Epoch:   900  |  train loss: 0.0299855467
Epoch:  1000  |  train loss: 0.0300143149
Processing class: 78
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.0283270784
Epoch:   200  |  train loss: 0.0291101653
Epoch:   300  |  train loss: 0.0298769809
Epoch:   400  |  train loss: 0.0300179165
Epoch:   500  |  train loss: 0.0300723538
Epoch:   600  |  train loss: 0.0307261124
Epoch:   700  |  train loss: 0.0310238998
Epoch:   800  |  train loss: 0.0313386966
Epoch:   900  |  train loss: 0.0315036573
Epoch:  1000  |  train loss: 0.0313933704
Processing class: 79
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.0280615099
Epoch:   200  |  train loss: 0.0276849795
Epoch:   300  |  train loss: 0.0284088138
Epoch:   400  |  train loss: 0.0285891540
Epoch:   500  |  train loss: 0.0286594711
Epoch:   600  |  train loss: 0.0294932943
Epoch:   700  |  train loss: 0.0295543812
Epoch:   800  |  train loss: 0.0299918592
Epoch:   900  |  train loss: 0.0306384694
Epoch:  1000  |  train loss: 0.0302665360
2024-03-05 01:44:30,129 [trainer.py] => CNN: {'total': 59.18, '00-09': 73.6, '10-19': 68.4, '20-29': 76.9, '30-39': 69.1, '40-49': 60.6, '50-59': 37.2, '60-69': 45.7, '70-79': 41.9, 'old': 61.64, 'new': 41.9}
2024-03-05 01:44:30,130 [trainer.py] => No NME accuracy
2024-03-05 01:44:30,130 [trainer.py] => FeCAM: {'total': 61.72, '00-09': 78.1, '10-19': 69.3, '20-29': 78.0, '30-39': 69.5, '40-49': 72.1, '50-59': 35.7, '60-69': 49.1, '70-79': 42.0, 'old': 64.54, 'new': 42.0}
2024-03-05 01:44:30,131 [trainer.py] => CNN top1 curve: [83.44, 71.75, 64.66, 59.18]
2024-03-05 01:44:30,131 [trainer.py] => CNN top5 curve: [96.5, 89.6, 86.54, 84.09]
2024-03-05 01:44:30,131 [trainer.py] => FeCAM top1 curve: [82.9, 72.78, 67.27, 61.72]
2024-03-05 01:44:30,131 [trainer.py] => FeCAM top5 curve: [95.84, 91.18, 87.21, 84.09]

2024-03-05 01:44:30,136 [fecam.py] => Learning on 80-90
Processing class: 80
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.0276619777
Epoch:   200  |  train loss: 0.0294464305
Epoch:   300  |  train loss: 0.0302247167
Epoch:   400  |  train loss: 0.0306854121
Epoch:   500  |  train loss: 0.0308040686
Epoch:   600  |  train loss: 0.0309144177
Epoch:   700  |  train loss: 0.0304155979
Epoch:   800  |  train loss: 0.0314919375
Epoch:   900  |  train loss: 0.0313254025
Epoch:  1000  |  train loss: 0.0319580365
Processing class: 81
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.0276825633
Epoch:   200  |  train loss: 0.0288389880
Epoch:   300  |  train loss: 0.0285053551
Epoch:   400  |  train loss: 0.0296224851
Epoch:   500  |  train loss: 0.0295637835
Epoch:   600  |  train loss: 0.0303377319
Epoch:   700  |  train loss: 0.0303902432
Epoch:   800  |  train loss: 0.0308980536
Epoch:   900  |  train loss: 0.0311017733
Epoch:  1000  |  train loss: 0.0314517546
Processing class: 82
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.0275357686
Epoch:   200  |  train loss: 0.0283246130
Epoch:   300  |  train loss: 0.0286391012
Epoch:   400  |  train loss: 0.0287569683
Epoch:   500  |  train loss: 0.0291851833
Epoch:   600  |  train loss: 0.0291390002
Epoch:   700  |  train loss: 0.0293010347
Epoch:   800  |  train loss: 0.0295202620
Epoch:   900  |  train loss: 0.0297461674
Epoch:  1000  |  train loss: 0.0298639484
Processing class: 83
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.0280854080
Epoch:   200  |  train loss: 0.0294081360
Epoch:   300  |  train loss: 0.0296422720
Epoch:   400  |  train loss: 0.0302562062
Epoch:   500  |  train loss: 0.0312601339
Epoch:   600  |  train loss: 0.0309249412
Epoch:   700  |  train loss: 0.0318249669
Epoch:   800  |  train loss: 0.0328088261
Epoch:   900  |  train loss: 0.0325398311
Epoch:  1000  |  train loss: 0.0323846430
Processing class: 84
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.0283933718
Epoch:   200  |  train loss: 0.0285731103
Epoch:   300  |  train loss: 0.0294711944
Epoch:   400  |  train loss: 0.0297416396
Epoch:   500  |  train loss: 0.0304669201
Epoch:   600  |  train loss: 0.0300380643
Epoch:   700  |  train loss: 0.0307722051
Epoch:   800  |  train loss: 0.0314019494
Epoch:   900  |  train loss: 0.0308020119
Epoch:  1000  |  train loss: 0.0307529029
Processing class: 85
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.0280026868
Epoch:   200  |  train loss: 0.0281133320
Epoch:   300  |  train loss: 0.0290064372
Epoch:   400  |  train loss: 0.0289421346
Epoch:   500  |  train loss: 0.0295469753
Epoch:   600  |  train loss: 0.0302313048
Epoch:   700  |  train loss: 0.0311160088
Epoch:   800  |  train loss: 0.0305752680
Epoch:   900  |  train loss: 0.0307479311
Epoch:  1000  |  train loss: 0.0314919055
Processing class: 86
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.0285907976
Epoch:   200  |  train loss: 0.0281239208
Epoch:   300  |  train loss: 0.0291107345
Epoch:   400  |  train loss: 0.0290265135
Epoch:   500  |  train loss: 0.0291433539
Epoch:   600  |  train loss: 0.0296345487
Epoch:   700  |  train loss: 0.0297870450
Epoch:   800  |  train loss: 0.0298320081
Epoch:   900  |  train loss: 0.0299170341
Epoch:  1000  |  train loss: 0.0308508877
Processing class: 87
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.0274598278
Epoch:   200  |  train loss: 0.0285147209
Epoch:   300  |  train loss: 0.0287862174
Epoch:   400  |  train loss: 0.0294895954
Epoch:   500  |  train loss: 0.0295997731
Epoch:   600  |  train loss: 0.0300723501
Epoch:   700  |  train loss: 0.0304083634
Epoch:   800  |  train loss: 0.0305221077
Epoch:   900  |  train loss: 0.0302510880
Epoch:  1000  |  train loss: 0.0313410688
Processing class: 88
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.0277602475
Epoch:   200  |  train loss: 0.0276674293
Epoch:   300  |  train loss: 0.0279079735
Epoch:   400  |  train loss: 0.0282908075
Epoch:   500  |  train loss: 0.0284505330
Epoch:   600  |  train loss: 0.0284341548
Epoch:   700  |  train loss: 0.0291176241
Epoch:   800  |  train loss: 0.0293228336
Epoch:   900  |  train loss: 0.0295959000
Epoch:  1000  |  train loss: 0.0293338351
Processing class: 89
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.0281962484
Epoch:   200  |  train loss: 0.0290832371
Epoch:   300  |  train loss: 0.0297149330
Epoch:   400  |  train loss: 0.0300519664
Epoch:   500  |  train loss: 0.0308284242
Epoch:   600  |  train loss: 0.0311146256
Epoch:   700  |  train loss: 0.0306556318
Epoch:   800  |  train loss: 0.0319949515
Epoch:   900  |  train loss: 0.0318125799
Epoch:  1000  |  train loss: 0.0322781593
2024-03-05 01:53:35,894 [trainer.py] => CNN: {'total': 54.08, '00-09': 67.9, '10-19': 63.6, '20-29': 73.2, '30-39': 68.8, '40-49': 57.2, '50-59': 32.6, '60-69': 40.9, '70-79': 38.1, '80-89': 44.4, 'old': 55.29, 'new': 44.4}
2024-03-05 01:53:35,895 [trainer.py] => No NME accuracy
2024-03-05 01:53:35,895 [trainer.py] => FeCAM: {'total': 57.66, '00-09': 76.1, '10-19': 65.8, '20-29': 75.3, '30-39': 67.9, '40-49': 68.5, '50-59': 33.0, '60-69': 45.0, '70-79': 39.0, '80-89': 48.3, 'old': 58.82, 'new': 48.3}
2024-03-05 01:53:35,895 [trainer.py] => CNN top1 curve: [83.44, 71.75, 64.66, 59.18, 54.08]
2024-03-05 01:53:35,895 [trainer.py] => CNN top5 curve: [96.5, 89.6, 86.54, 84.09, 81.74]
2024-03-05 01:53:35,895 [trainer.py] => FeCAM top1 curve: [82.9, 72.78, 67.27, 61.72, 57.66]
2024-03-05 01:53:35,895 [trainer.py] => FeCAM top5 curve: [95.84, 91.18, 87.21, 84.09, 81.63]

2024-03-05 01:53:35,902 [fecam.py] => Learning on 90-100
Processing class: 90
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.0281906620
Epoch:   200  |  train loss: 0.0284170005
Epoch:   300  |  train loss: 0.0280385129
Epoch:   400  |  train loss: 0.0280987792
Epoch:   500  |  train loss: 0.0280672166
Epoch:   600  |  train loss: 0.0285642777
Epoch:   700  |  train loss: 0.0286120038
Epoch:   800  |  train loss: 0.0289294645
Epoch:   900  |  train loss: 0.0293539934
Epoch:  1000  |  train loss: 0.0301068295
Processing class: 91
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.0272601638
Epoch:   200  |  train loss: 0.0274199203
Epoch:   300  |  train loss: 0.0282786135
Epoch:   400  |  train loss: 0.0289497752
Epoch:   500  |  train loss: 0.0296907544
Epoch:   600  |  train loss: 0.0301073045
Epoch:   700  |  train loss: 0.0304729611
Epoch:   800  |  train loss: 0.0309741970
Epoch:   900  |  train loss: 0.0315495417
Epoch:  1000  |  train loss: 0.0317382779
Processing class: 92
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.0287152376
Epoch:   200  |  train loss: 0.0284803513
Epoch:   300  |  train loss: 0.0279002268
Epoch:   400  |  train loss: 0.0285064269
Epoch:   500  |  train loss: 0.0290522985
Epoch:   600  |  train loss: 0.0299337965
Epoch:   700  |  train loss: 0.0300996821
Epoch:   800  |  train loss: 0.0294471644
Epoch:   900  |  train loss: 0.0297747128
Epoch:  1000  |  train loss: 0.0300257299
Processing class: 93
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.0279536787
Epoch:   200  |  train loss: 0.0286965922
Epoch:   300  |  train loss: 0.0303728290
Epoch:   400  |  train loss: 0.0307599790
Epoch:   500  |  train loss: 0.0308807541
Epoch:   600  |  train loss: 0.0306492493
Epoch:   700  |  train loss: 0.0319719791
Epoch:   800  |  train loss: 0.0320326343
Epoch:   900  |  train loss: 0.0322800808
Epoch:  1000  |  train loss: 0.0322662596
Processing class: 94
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.0261078376
Epoch:   200  |  train loss: 0.0261907943
Epoch:   300  |  train loss: 0.0257115785
Epoch:   400  |  train loss: 0.0262976129
Epoch:   500  |  train loss: 0.0254139289
Epoch:   600  |  train loss: 0.0260817766
Epoch:   700  |  train loss: 0.0267259620
Epoch:   800  |  train loss: 0.0256010886
Epoch:   900  |  train loss: 0.0265299723
Epoch:  1000  |  train loss: 0.0263576187
Processing class: 95
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.0276711624
Epoch:   200  |  train loss: 0.0283980228
Epoch:   300  |  train loss: 0.0281532876
Epoch:   400  |  train loss: 0.0281195860
Epoch:   500  |  train loss: 0.0291997418
Epoch:   600  |  train loss: 0.0292396594
Epoch:   700  |  train loss: 0.0297185421
Epoch:   800  |  train loss: 0.0295574978
Epoch:   900  |  train loss: 0.0302512031
Epoch:  1000  |  train loss: 0.0309164383
Processing class: 96
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.0284278471
Epoch:   200  |  train loss: 0.0277248863
Epoch:   300  |  train loss: 0.0285675991
Epoch:   400  |  train loss: 0.0297565300
Epoch:   500  |  train loss: 0.0295999374
Epoch:   600  |  train loss: 0.0296224292
Epoch:   700  |  train loss: 0.0299220841
Epoch:   800  |  train loss: 0.0296585165
Epoch:   900  |  train loss: 0.0308464561
Epoch:  1000  |  train loss: 0.0309542481
Processing class: 97
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.0283367015
Epoch:   200  |  train loss: 0.0287473097
Epoch:   300  |  train loss: 0.0292379785
Epoch:   400  |  train loss: 0.0296644427
Epoch:   500  |  train loss: 0.0305315178
Epoch:   600  |  train loss: 0.0311183348
Epoch:   700  |  train loss: 0.0316856176
Epoch:   800  |  train loss: 0.0319890521
Epoch:   900  |  train loss: 0.0323574051
Epoch:  1000  |  train loss: 0.0323729031
Processing class: 98
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.0276738539
Epoch:   200  |  train loss: 0.0278468877
Epoch:   300  |  train loss: 0.0283400763
Epoch:   400  |  train loss: 0.0289098185
Epoch:   500  |  train loss: 0.0290396288
Epoch:   600  |  train loss: 0.0296109457
Epoch:   700  |  train loss: 0.0291086286
Epoch:   800  |  train loss: 0.0298141733
Epoch:   900  |  train loss: 0.0299312424
Epoch:  1000  |  train loss: 0.0297155470
Processing class: 99
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.0288399644
Epoch:   200  |  train loss: 0.0284360014
Epoch:   300  |  train loss: 0.0298276797
Epoch:   400  |  train loss: 0.0308213323
Epoch:   500  |  train loss: 0.0309403528
Epoch:   600  |  train loss: 0.0310218699
Epoch:   700  |  train loss: 0.0309530877
Epoch:   800  |  train loss: 0.0317412097
Epoch:   900  |  train loss: 0.0316085730
Epoch:  1000  |  train loss: 0.0318350010
2024-03-05 02:03:54,680 [trainer.py] => CNN: {'total': 50.36, '00-09': 57.7, '10-19': 63.6, '20-29': 71.4, '30-39': 68.5, '40-49': 56.8, '50-59': 29.6, '60-69': 38.9, '70-79': 36.6, '80-89': 42.7, '90-99': 37.8, 'old': 51.76, 'new': 37.8}
2024-03-05 02:03:54,682 [trainer.py] => No NME accuracy
2024-03-05 02:03:54,682 [trainer.py] => FeCAM: {'total': 54.74, '00-09': 73.3, '10-19': 65.1, '20-29': 74.1, '30-39': 67.2, '40-49': 67.1, '50-59': 30.8, '60-69': 43.3, '70-79': 37.2, '80-89': 46.9, '90-99': 42.4, 'old': 56.11, 'new': 42.4}
2024-03-05 02:03:54,682 [trainer.py] => CNN top1 curve: [83.44, 71.75, 64.66, 59.18, 54.08, 50.36]
2024-03-05 02:03:54,682 [trainer.py] => CNN top5 curve: [96.5, 89.6, 86.54, 84.09, 81.74, 79.63]
2024-03-05 02:03:54,682 [trainer.py] => FeCAM top1 curve: [82.9, 72.78, 67.27, 61.72, 57.66, 54.74]
2024-03-05 02:03:54,682 [trainer.py] => FeCAM top5 curve: [95.84, 91.18, 87.21, 84.09, 81.63, 79.21]

=========================================
2024-03-05 02:04:03,679 [trainer.py] => config: ./exps/FeCAM_cifar100.json
2024-03-05 02:04:03,679 [trainer.py] => prefix: train
2024-03-05 02:04:03,679 [trainer.py] => dataset: cifar100
2024-03-05 02:04:03,679 [trainer.py] => memory_size: 0
2024-03-05 02:04:03,680 [trainer.py] => shuffle: True
2024-03-05 02:04:03,680 [trainer.py] => init_cls: 50
2024-03-05 02:04:03,680 [trainer.py] => increment: 10
2024-03-05 02:04:03,680 [trainer.py] => model_name: fecam
2024-03-05 02:04:03,680 [trainer.py] => convnet_type: resnet18
2024-03-05 02:04:03,680 [trainer.py] => device: [device(type='cuda', index=0)]
2024-03-05 02:04:03,680 [trainer.py] => seed: 1993
2024-03-05 02:04:03,680 [trainer.py] => init_epochs: 200
2024-03-05 02:04:03,680 [trainer.py] => init_lr: 0.1
2024-03-05 02:04:03,680 [trainer.py] => init_weight_decay: 0.0005
2024-03-05 02:04:03,680 [trainer.py] => batch_size: 128
2024-03-05 02:04:03,680 [trainer.py] => num_workers: 8
2024-03-05 02:04:03,680 [trainer.py] => T: 5
2024-03-05 02:04:03,680 [trainer.py] => beta: 0.5
2024-03-05 02:04:03,680 [trainer.py] => alpha1: 1
2024-03-05 02:04:03,680 [trainer.py] => alpha2: 1
2024-03-05 02:04:03,680 [trainer.py] => ncm: False
2024-03-05 02:04:03,680 [trainer.py] => tukey: False
2024-03-05 02:04:03,680 [trainer.py] => diagonal: False
2024-03-05 02:04:03,680 [trainer.py] => per_class: True
2024-03-05 02:04:03,680 [trainer.py] => full_cov: True
2024-03-05 02:04:03,680 [trainer.py] => shrink: True
2024-03-05 02:04:03,680 [trainer.py] => norm_cov: False
2024-03-05 02:04:03,680 [trainer.py] => vecnorm: False
2024-03-05 02:04:03,680 [trainer.py] => ae_type: wae
2024-03-05 02:04:03,680 [trainer.py] => epochs: 1000
2024-03-05 02:04:03,680 [trainer.py] => ae_latent_dim: 32
2024-03-05 02:04:03,680 [trainer.py] => wae_sigma: 50
2024-03-05 02:04:03,680 [trainer.py] => wae_C: 0.1
Files already downloaded and verified
Files already downloaded and verified
2024-03-05 02:04:05,336 [data_manager.py] => [68, 56, 78, 8, 23, 84, 90, 65, 74, 76, 40, 89, 3, 92, 55, 9, 26, 80, 43, 38, 58, 70, 77, 1, 85, 19, 17, 50, 28, 53, 13, 81, 45, 82, 6, 59, 83, 16, 15, 44, 91, 41, 72, 60, 79, 52, 20, 10, 31, 54, 37, 95, 14, 71, 96, 98, 97, 2, 64, 66, 42, 22, 35, 86, 24, 34, 87, 21, 99, 0, 88, 27, 18, 94, 11, 12, 47, 25, 30, 46, 62, 69, 36, 61, 7, 63, 75, 5, 32, 4, 51, 48, 73, 93, 39, 67, 29, 49, 57, 33]
2024-03-05 02:04:05,640 [fecam.py] => Learning on 0-50
/home/z1165703/miniconda3/lib/python3.11/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
/home/z1165703/miniconda3/lib/python3.11/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
Processing class: 0
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.0255428132
Epoch:   200  |  train loss: 0.0263846986
Epoch:   300  |  train loss: 0.0270160183
Epoch:   400  |  train loss: 0.0273772832
Epoch:   500  |  train loss: 0.0271712035
Epoch:   600  |  train loss: 0.0271495368
Epoch:   700  |  train loss: 0.0274718303
Epoch:   800  |  train loss: 0.0275881544
Epoch:   900  |  train loss: 0.0273575265
Epoch:  1000  |  train loss: 0.0273683939
Processing class: 1
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.0246207204
Epoch:   200  |  train loss: 0.0243995611
Epoch:   300  |  train loss: 0.0259444371
Epoch:   400  |  train loss: 0.0262954552
Epoch:   500  |  train loss: 0.0258022200
Epoch:   600  |  train loss: 0.0267584305
Epoch:   700  |  train loss: 0.0264548767
Epoch:   800  |  train loss: 0.0270398680
Epoch:   900  |  train loss: 0.0278267939
Epoch:  1000  |  train loss: 0.0272329535
Processing class: 2
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.0241556685
Epoch:   200  |  train loss: 0.0242634911
Epoch:   300  |  train loss: 0.0253652949
Epoch:   400  |  train loss: 0.0254507456
Epoch:   500  |  train loss: 0.0244079631
Epoch:   600  |  train loss: 0.0256058045
Epoch:   700  |  train loss: 0.0256277680
Epoch:   800  |  train loss: 0.0265647143
Epoch:   900  |  train loss: 0.0262754817
Epoch:  1000  |  train loss: 0.0261263836
Processing class: 3
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.0272682704
Epoch:   200  |  train loss: 0.0281384695
Epoch:   300  |  train loss: 0.0286781035
Epoch:   400  |  train loss: 0.0290893860
Epoch:   500  |  train loss: 0.0288844258
Epoch:   600  |  train loss: 0.0298810288
Epoch:   700  |  train loss: 0.0306084849
Epoch:   800  |  train loss: 0.0301055152
Epoch:   900  |  train loss: 0.0307126708
Epoch:  1000  |  train loss: 0.0310248692
Processing class: 4
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.0243196215
Epoch:   200  |  train loss: 0.0240500495
Epoch:   300  |  train loss: 0.0253985412
Epoch:   400  |  train loss: 0.0256943770
Epoch:   500  |  train loss: 0.0262800921
Epoch:   600  |  train loss: 0.0263424423
Epoch:   700  |  train loss: 0.0268446956
Epoch:   800  |  train loss: 0.0270447455
Epoch:   900  |  train loss: 0.0273651581
Epoch:  1000  |  train loss: 0.0281195123
Processing class: 5
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.0241533503
Epoch:   200  |  train loss: 0.0242484871
Epoch:   300  |  train loss: 0.0248463266
Epoch:   400  |  train loss: 0.0250851445
Epoch:   500  |  train loss: 0.0250807766
Epoch:   600  |  train loss: 0.0263693631
Epoch:   700  |  train loss: 0.0267615117
Epoch:   800  |  train loss: 0.0263844065
Epoch:   900  |  train loss: 0.0264255624
Epoch:  1000  |  train loss: 0.0270556934
Processing class: 6
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.0247740433
Epoch:   200  |  train loss: 0.0252041597
Epoch:   300  |  train loss: 0.0265629482
Epoch:   400  |  train loss: 0.0266155355
Epoch:   500  |  train loss: 0.0276720274
Epoch:   600  |  train loss: 0.0274008233
Epoch:   700  |  train loss: 0.0271885965
Epoch:   800  |  train loss: 0.0275274307
Epoch:   900  |  train loss: 0.0273746900
Epoch:  1000  |  train loss: 0.0277911134
Processing class: 7
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.0236888323
Epoch:   200  |  train loss: 0.0241114881
Epoch:   300  |  train loss: 0.0245767877
Epoch:   400  |  train loss: 0.0250401970
Epoch:   500  |  train loss: 0.0257356059
Epoch:   600  |  train loss: 0.0255247097
Epoch:   700  |  train loss: 0.0260997336
Epoch:   800  |  train loss: 0.0270063080
Epoch:   900  |  train loss: 0.0264663335
Epoch:  1000  |  train loss: 0.0272532362
Processing class: 8
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.0242154285
Epoch:   200  |  train loss: 0.0249114357
Epoch:   300  |  train loss: 0.0249268845
Epoch:   400  |  train loss: 0.0261108797
Epoch:   500  |  train loss: 0.0257550273
Epoch:   600  |  train loss: 0.0262227885
Epoch:   700  |  train loss: 0.0270267636
Epoch:   800  |  train loss: 0.0265536867
Epoch:   900  |  train loss: 0.0270482816
Epoch:  1000  |  train loss: 0.0267893083
Processing class: 9
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.0236803088
Epoch:   200  |  train loss: 0.0242711086
Epoch:   300  |  train loss: 0.0251318026
Epoch:   400  |  train loss: 0.0245179210
Epoch:   500  |  train loss: 0.0253881503
Epoch:   600  |  train loss: 0.0249708299
Epoch:   700  |  train loss: 0.0252835341
Epoch:   800  |  train loss: 0.0255563665
Epoch:   900  |  train loss: 0.0256272059
Epoch:  1000  |  train loss: 0.0252265889
Processing class: 10
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.0228350740
Epoch:   200  |  train loss: 0.0230082300
Epoch:   300  |  train loss: 0.0234466698
Epoch:   400  |  train loss: 0.0244407937
Epoch:   500  |  train loss: 0.0235508759
Epoch:   600  |  train loss: 0.0241154484
Epoch:   700  |  train loss: 0.0245475780
Epoch:   800  |  train loss: 0.0244519643
Epoch:   900  |  train loss: 0.0247669049
Epoch:  1000  |  train loss: 0.0247782320
Processing class: 11
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.0242807977
Epoch:   200  |  train loss: 0.0249061108
Epoch:   300  |  train loss: 0.0247078303
Epoch:   400  |  train loss: 0.0252656009
Epoch:   500  |  train loss: 0.0255384807
Epoch:   600  |  train loss: 0.0262623336
Epoch:   700  |  train loss: 0.0272236533
Epoch:   800  |  train loss: 0.0271091312
Epoch:   900  |  train loss: 0.0272855733
Epoch:  1000  |  train loss: 0.0278489809
Processing class: 12
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.0232285783
Epoch:   200  |  train loss: 0.0246449593
Epoch:   300  |  train loss: 0.0249610227
Epoch:   400  |  train loss: 0.0259435751
Epoch:   500  |  train loss: 0.0261564597
Epoch:   600  |  train loss: 0.0259304181
Epoch:   700  |  train loss: 0.0263358377
Epoch:   800  |  train loss: 0.0266314309
Epoch:   900  |  train loss: 0.0271466605
Epoch:  1000  |  train loss: 0.0269256476
Processing class: 13
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.0241430357
Epoch:   200  |  train loss: 0.0254210908
Epoch:   300  |  train loss: 0.0264140032
Epoch:   400  |  train loss: 0.0268596981
Epoch:   500  |  train loss: 0.0277728483
Epoch:   600  |  train loss: 0.0272789881
Epoch:   700  |  train loss: 0.0278387614
Epoch:   800  |  train loss: 0.0282818314
Epoch:   900  |  train loss: 0.0285646472
Epoch:  1000  |  train loss: 0.0289378345
Processing class: 14
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.0237672750
Epoch:   200  |  train loss: 0.0241672281
Epoch:   300  |  train loss: 0.0245888483
Epoch:   400  |  train loss: 0.0254939027
Epoch:   500  |  train loss: 0.0255093653
Epoch:   600  |  train loss: 0.0256535016
Epoch:   700  |  train loss: 0.0260656707
Epoch:   800  |  train loss: 0.0270638112
Epoch:   900  |  train loss: 0.0268031042
Epoch:  1000  |  train loss: 0.0262404934
Processing class: 15
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.0244435806
Epoch:   200  |  train loss: 0.0250025224
Epoch:   300  |  train loss: 0.0262912001
Epoch:   400  |  train loss: 0.0268801186
Epoch:   500  |  train loss: 0.0274239335
Epoch:   600  |  train loss: 0.0270991698
Epoch:   700  |  train loss: 0.0279420689
Epoch:   800  |  train loss: 0.0272896729
Epoch:   900  |  train loss: 0.0280546438
Epoch:  1000  |  train loss: 0.0277507376
Processing class: 16
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.0246325228
Epoch:   200  |  train loss: 0.0267409813
Epoch:   300  |  train loss: 0.0276686769
Epoch:   400  |  train loss: 0.0273726691
Epoch:   500  |  train loss: 0.0281082854
Epoch:   600  |  train loss: 0.0275821745
Epoch:   700  |  train loss: 0.0275097474
Epoch:   800  |  train loss: 0.0285578884
Epoch:   900  |  train loss: 0.0277788136
Epoch:  1000  |  train loss: 0.0286355324
Processing class: 17
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.0232076835
Epoch:   200  |  train loss: 0.0237006467
Epoch:   300  |  train loss: 0.0244935062
Epoch:   400  |  train loss: 0.0247426383
Epoch:   500  |  train loss: 0.0242030263
Epoch:   600  |  train loss: 0.0248799611
Epoch:   700  |  train loss: 0.0248296812
Epoch:   800  |  train loss: 0.0249934543
Epoch:   900  |  train loss: 0.0253077999
Epoch:  1000  |  train loss: 0.0252337884
Processing class: 18
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.0241094850
Epoch:   200  |  train loss: 0.0243528791
Epoch:   300  |  train loss: 0.0254301853
Epoch:   400  |  train loss: 0.0255048174
Epoch:   500  |  train loss: 0.0266091451
Epoch:   600  |  train loss: 0.0265238088
Epoch:   700  |  train loss: 0.0278480481
Epoch:   800  |  train loss: 0.0279439155
Epoch:   900  |  train loss: 0.0283061139
Epoch:  1000  |  train loss: 0.0291098446
Processing class: 19
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.0247093115
Epoch:   200  |  train loss: 0.0263896808
Epoch:   300  |  train loss: 0.0265598770
Epoch:   400  |  train loss: 0.0268468596
Epoch:   500  |  train loss: 0.0265629385
Epoch:   600  |  train loss: 0.0267268136
Epoch:   700  |  train loss: 0.0271855071
Epoch:   800  |  train loss: 0.0270680550
Epoch:   900  |  train loss: 0.0285481792
Epoch:  1000  |  train loss: 0.0288948525
Processing class: 20
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.0245303515
Epoch:   200  |  train loss: 0.0252182867
Epoch:   300  |  train loss: 0.0257927358
Epoch:   400  |  train loss: 0.0264026254
Epoch:   500  |  train loss: 0.0271582894
Epoch:   600  |  train loss: 0.0268218391
Epoch:   700  |  train loss: 0.0270835157
Epoch:   800  |  train loss: 0.0271227662
Epoch:   900  |  train loss: 0.0275235079
Epoch:  1000  |  train loss: 0.0280793894
Processing class: 21
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.0246104963
Epoch:   200  |  train loss: 0.0253795866
Epoch:   300  |  train loss: 0.0261957776
Epoch:   400  |  train loss: 0.0267162807
Epoch:   500  |  train loss: 0.0271755904
Epoch:   600  |  train loss: 0.0277401093
Epoch:   700  |  train loss: 0.0276541971
Epoch:   800  |  train loss: 0.0276108168
Epoch:   900  |  train loss: 0.0283610798
Epoch:  1000  |  train loss: 0.0283391770
Processing class: 22
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.0237444088
Epoch:   200  |  train loss: 0.0240852050
Epoch:   300  |  train loss: 0.0247243471
Epoch:   400  |  train loss: 0.0257104147
Epoch:   500  |  train loss: 0.0261220951
Epoch:   600  |  train loss: 0.0258922163
Epoch:   700  |  train loss: 0.0263686400
Epoch:   800  |  train loss: 0.0263115026
Epoch:   900  |  train loss: 0.0260718167
Epoch:  1000  |  train loss: 0.0271311302
Processing class: 23
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.0252774872
Epoch:   200  |  train loss: 0.0259770349
Epoch:   300  |  train loss: 0.0267042276
Epoch:   400  |  train loss: 0.0261708185
Epoch:   500  |  train loss: 0.0263021260
Epoch:   600  |  train loss: 0.0261117913
Epoch:   700  |  train loss: 0.0267154690
Epoch:   800  |  train loss: 0.0264707092
Epoch:   900  |  train loss: 0.0270137958
Epoch:  1000  |  train loss: 0.0267510787
Processing class: 24
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.0245409127
Epoch:   200  |  train loss: 0.0246257149
Epoch:   300  |  train loss: 0.0240022309
Epoch:   400  |  train loss: 0.0248352401
Epoch:   500  |  train loss: 0.0246459197
Epoch:   600  |  train loss: 0.0251486950
Epoch:   700  |  train loss: 0.0243955333
Epoch:   800  |  train loss: 0.0247569311
Epoch:   900  |  train loss: 0.0253623176
Epoch:  1000  |  train loss: 0.0252888076
Processing class: 25
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.0240328308
Epoch:   200  |  train loss: 0.0246992491
Epoch:   300  |  train loss: 0.0255899359
Epoch:   400  |  train loss: 0.0253495093
Epoch:   500  |  train loss: 0.0264356643
Epoch:   600  |  train loss: 0.0261346441
Epoch:   700  |  train loss: 0.0269841418
Epoch:   800  |  train loss: 0.0267796528
Epoch:   900  |  train loss: 0.0267170642
Epoch:  1000  |  train loss: 0.0270187225
Processing class: 26
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.0255845264
Epoch:   200  |  train loss: 0.0265139550
Epoch:   300  |  train loss: 0.0259071875
Epoch:   400  |  train loss: 0.0267275475
Epoch:   500  |  train loss: 0.0275487177
Epoch:   600  |  train loss: 0.0268188801
Epoch:   700  |  train loss: 0.0272956152
Epoch:   800  |  train loss: 0.0276407901
Epoch:   900  |  train loss: 0.0276181538
Epoch:  1000  |  train loss: 0.0282609187
Processing class: 27
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.0239566576
Epoch:   200  |  train loss: 0.0239946183
Epoch:   300  |  train loss: 0.0243352514
Epoch:   400  |  train loss: 0.0247609641
Epoch:   500  |  train loss: 0.0253935345
Epoch:   600  |  train loss: 0.0254629668
Epoch:   700  |  train loss: 0.0258392189
Epoch:   800  |  train loss: 0.0258812729
Epoch:   900  |  train loss: 0.0262143765
Epoch:  1000  |  train loss: 0.0257160548
Processing class: 28
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.0249614228
Epoch:   200  |  train loss: 0.0257436633
Epoch:   300  |  train loss: 0.0268226225
Epoch:   400  |  train loss: 0.0274046395
Epoch:   500  |  train loss: 0.0277341340
Epoch:   600  |  train loss: 0.0278954320
Epoch:   700  |  train loss: 0.0280715588
Epoch:   800  |  train loss: 0.0283598911
Epoch:   900  |  train loss: 0.0285463285
Epoch:  1000  |  train loss: 0.0280050516
Processing class: 29
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.0243818220
Epoch:   200  |  train loss: 0.0251527295
Epoch:   300  |  train loss: 0.0253027938
Epoch:   400  |  train loss: 0.0252386458
Epoch:   500  |  train loss: 0.0259469122
Epoch:   600  |  train loss: 0.0263903484
Epoch:   700  |  train loss: 0.0265306730
Epoch:   800  |  train loss: 0.0262516573
Epoch:   900  |  train loss: 0.0264711935
Epoch:  1000  |  train loss: 0.0264053755
Processing class: 30
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.0255646244
Epoch:   200  |  train loss: 0.0264467914
Epoch:   300  |  train loss: 0.0278680641
Epoch:   400  |  train loss: 0.0289513364
Epoch:   500  |  train loss: 0.0285094388
Epoch:   600  |  train loss: 0.0286338788
Epoch:   700  |  train loss: 0.0286280070
Epoch:   800  |  train loss: 0.0293438178
Epoch:   900  |  train loss: 0.0298167381
Epoch:  1000  |  train loss: 0.0294398516
Processing class: 31
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.0249925196
Epoch:   200  |  train loss: 0.0260966882
Epoch:   300  |  train loss: 0.0264687464
Epoch:   400  |  train loss: 0.0273983039
Epoch:   500  |  train loss: 0.0277959611
Epoch:   600  |  train loss: 0.0286344182
Epoch:   700  |  train loss: 0.0285481602
Epoch:   800  |  train loss: 0.0286280926
Epoch:   900  |  train loss: 0.0292576339
Epoch:  1000  |  train loss: 0.0287461244
Processing class: 32
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.0239481021
Epoch:   200  |  train loss: 0.0247243755
Epoch:   300  |  train loss: 0.0247714762
Epoch:   400  |  train loss: 0.0243797481
Epoch:   500  |  train loss: 0.0257020719
Epoch:   600  |  train loss: 0.0256561134
Epoch:   700  |  train loss: 0.0266717996
Epoch:   800  |  train loss: 0.0271479677
Epoch:   900  |  train loss: 0.0270361461
Epoch:  1000  |  train loss: 0.0269050021
Processing class: 33
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.0232765052
Epoch:   200  |  train loss: 0.0248115201
Epoch:   300  |  train loss: 0.0239589419
Epoch:   400  |  train loss: 0.0243540809
Epoch:   500  |  train loss: 0.0246617872
Epoch:   600  |  train loss: 0.0257113293
Epoch:   700  |  train loss: 0.0253403585
Epoch:   800  |  train loss: 0.0256581590
Epoch:   900  |  train loss: 0.0256773520
Epoch:  1000  |  train loss: 0.0254953209
Processing class: 34
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.0244016171
Epoch:   200  |  train loss: 0.0243455444
Epoch:   300  |  train loss: 0.0251305722
Epoch:   400  |  train loss: 0.0249723051
Epoch:   500  |  train loss: 0.0251160067
Epoch:   600  |  train loss: 0.0257825505
Epoch:   700  |  train loss: 0.0254750803
Epoch:   800  |  train loss: 0.0262613937
Epoch:   900  |  train loss: 0.0265655924
Epoch:  1000  |  train loss: 0.0267576475
Processing class: 35
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.0231468320
Epoch:   200  |  train loss: 0.0241830811
Epoch:   300  |  train loss: 0.0248716071
Epoch:   400  |  train loss: 0.0252620686
Epoch:   500  |  train loss: 0.0249740828
Epoch:   600  |  train loss: 0.0253615163
Epoch:   700  |  train loss: 0.0257386282
Epoch:   800  |  train loss: 0.0257453501
Epoch:   900  |  train loss: 0.0258943953
Epoch:  1000  |  train loss: 0.0257094551
Processing class: 36
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.0252693057
Epoch:   200  |  train loss: 0.0256971058
Epoch:   300  |  train loss: 0.0264042474
Epoch:   400  |  train loss: 0.0265315060
Epoch:   500  |  train loss: 0.0275585383
Epoch:   600  |  train loss: 0.0273656134
Epoch:   700  |  train loss: 0.0270943500
Epoch:   800  |  train loss: 0.0282006886
Epoch:   900  |  train loss: 0.0275472779
Epoch:  1000  |  train loss: 0.0282365333
Processing class: 37
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.0244354874
Epoch:   200  |  train loss: 0.0244311951
Epoch:   300  |  train loss: 0.0245622505
Epoch:   400  |  train loss: 0.0251086280
Epoch:   500  |  train loss: 0.0255972821
Epoch:   600  |  train loss: 0.0252970453
Epoch:   700  |  train loss: 0.0255225766
Epoch:   800  |  train loss: 0.0257994592
Epoch:   900  |  train loss: 0.0255681265
Epoch:  1000  |  train loss: 0.0258589912
Processing class: 38
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.0245862506
Epoch:   200  |  train loss: 0.0249643143
Epoch:   300  |  train loss: 0.0258014664
Epoch:   400  |  train loss: 0.0260234125
Epoch:   500  |  train loss: 0.0264824737
Epoch:   600  |  train loss: 0.0264830049
Epoch:   700  |  train loss: 0.0267415173
Epoch:   800  |  train loss: 0.0273962077
Epoch:   900  |  train loss: 0.0267822146
Epoch:  1000  |  train loss: 0.0273569260
Processing class: 39
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.0245775275
Epoch:   200  |  train loss: 0.0244994249
Epoch:   300  |  train loss: 0.0251297709
Epoch:   400  |  train loss: 0.0256569456
Epoch:   500  |  train loss: 0.0255961504
Epoch:   600  |  train loss: 0.0259110268
Epoch:   700  |  train loss: 0.0263484832
Epoch:   800  |  train loss: 0.0268834587
Epoch:   900  |  train loss: 0.0272728633
Epoch:  1000  |  train loss: 0.0275547158
Processing class: 40
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.0248051919
Epoch:   200  |  train loss: 0.0247079644
Epoch:   300  |  train loss: 0.0260391634
Epoch:   400  |  train loss: 0.0263745580
Epoch:   500  |  train loss: 0.0268484022
Epoch:   600  |  train loss: 0.0267717559
Epoch:   700  |  train loss: 0.0271164030
Epoch:   800  |  train loss: 0.0271865442
Epoch:   900  |  train loss: 0.0269389849
Epoch:  1000  |  train loss: 0.0281110037
Processing class: 41
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.0248836890
Epoch:   200  |  train loss: 0.0249670058
Epoch:   300  |  train loss: 0.0258483410
Epoch:   400  |  train loss: 0.0259140950
Epoch:   500  |  train loss: 0.0261100918
Epoch:   600  |  train loss: 0.0262418624
Epoch:   700  |  train loss: 0.0268326100
Epoch:   800  |  train loss: 0.0265950814
Epoch:   900  |  train loss: 0.0265605118
Epoch:  1000  |  train loss: 0.0266806815
Processing class: 42
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.0236767050
Epoch:   200  |  train loss: 0.0246878978
Epoch:   300  |  train loss: 0.0254498452
Epoch:   400  |  train loss: 0.0259129252
Epoch:   500  |  train loss: 0.0264024574
Epoch:   600  |  train loss: 0.0267819870
Epoch:   700  |  train loss: 0.0272849236
Epoch:   800  |  train loss: 0.0277319029
Epoch:   900  |  train loss: 0.0275579944
Epoch:  1000  |  train loss: 0.0276136294
Processing class: 43
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.0237634711
Epoch:   200  |  train loss: 0.0239171106
Epoch:   300  |  train loss: 0.0256433718
Epoch:   400  |  train loss: 0.0268713612
Epoch:   500  |  train loss: 0.0272519488
Epoch:   600  |  train loss: 0.0269902080
Epoch:   700  |  train loss: 0.0275216419
Epoch:   800  |  train loss: 0.0273467667
Epoch:   900  |  train loss: 0.0268092569
Epoch:  1000  |  train loss: 0.0276914626
Processing class: 44
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.0235156260
Epoch:   200  |  train loss: 0.0246985186
Epoch:   300  |  train loss: 0.0244912580
Epoch:   400  |  train loss: 0.0249050301
Epoch:   500  |  train loss: 0.0253764898
Epoch:   600  |  train loss: 0.0251993071
Epoch:   700  |  train loss: 0.0258762490
Epoch:   800  |  train loss: 0.0257877707
Epoch:   900  |  train loss: 0.0256429583
Epoch:  1000  |  train loss: 0.0263950765
Processing class: 45
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.0247367259
Epoch:   200  |  train loss: 0.0260799699
Epoch:   300  |  train loss: 0.0264169384
Epoch:   400  |  train loss: 0.0270595089
Epoch:   500  |  train loss: 0.0269889552
Epoch:   600  |  train loss: 0.0280518334
Epoch:   700  |  train loss: 0.0283803582
Epoch:   800  |  train loss: 0.0281931583
Epoch:   900  |  train loss: 0.0283081781
Epoch:  1000  |  train loss: 0.0288862418
Processing class: 46
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.0253200129
Epoch:   200  |  train loss: 0.0256466050
Epoch:   300  |  train loss: 0.0263983890
Epoch:   400  |  train loss: 0.0267129607
Epoch:   500  |  train loss: 0.0268648177
Epoch:   600  |  train loss: 0.0268899959
Epoch:   700  |  train loss: 0.0270533931
Epoch:   800  |  train loss: 0.0279192045
Epoch:   900  |  train loss: 0.0275712181
Epoch:  1000  |  train loss: 0.0279417489
Processing class: 47
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.0236057140
Epoch:   200  |  train loss: 0.0244752798
Epoch:   300  |  train loss: 0.0245005175
Epoch:   400  |  train loss: 0.0254017808
Epoch:   500  |  train loss: 0.0255638689
Epoch:   600  |  train loss: 0.0257390495
Epoch:   700  |  train loss: 0.0261579454
Epoch:   800  |  train loss: 0.0259084977
Epoch:   900  |  train loss: 0.0262477711
Epoch:  1000  |  train loss: 0.0261531550
Processing class: 48
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.0241328061
Epoch:   200  |  train loss: 0.0251176804
Epoch:   300  |  train loss: 0.0253688022
Epoch:   400  |  train loss: 0.0249413364
Epoch:   500  |  train loss: 0.0258399967
Epoch:   600  |  train loss: 0.0257967047
Epoch:   700  |  train loss: 0.0255290743
Epoch:   800  |  train loss: 0.0255373999
Epoch:   900  |  train loss: 0.0264647555
Epoch:  1000  |  train loss: 0.0268155996
Processing class: 49
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.0239266984
Epoch:   200  |  train loss: 0.0243166499
Epoch:   300  |  train loss: 0.0257279642
Epoch:   400  |  train loss: 0.0260824330
Epoch:   500  |  train loss: 0.0265653152
Epoch:   600  |  train loss: 0.0266725048
Epoch:   700  |  train loss: 0.0267938845
Epoch:   800  |  train loss: 0.0267180726
Epoch:   900  |  train loss: 0.0266805653
Epoch:  1000  |  train loss: 0.0268814616
/home/z1165703/FeCAM/models/base.py:87: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  vec = torch.tensor(vec).cuda()
2024-03-05 02:21:50,183 [trainer.py] => CNN: {'total': 83.44, '00-09': 87.7, '10-19': 79.6, '20-29': 84.9, '30-39': 81.0, '40-49': 84.0, 'old': 0, 'new': 83.44}
2024-03-05 02:21:50,190 [trainer.py] => No NME accuracy
2024-03-05 02:21:50,190 [trainer.py] => FeCAM: {'total': 82.9, '00-09': 86.2, '10-19': 79.7, '20-29': 85.0, '30-39': 80.6, '40-49': 83.0, 'old': 0, 'new': 82.9}
2024-03-05 02:21:50,190 [trainer.py] => CNN top1 curve: [83.44]
2024-03-05 02:21:50,190 [trainer.py] => CNN top5 curve: [96.5]
2024-03-05 02:21:50,190 [trainer.py] => FeCAM top1 curve: [82.9]
2024-03-05 02:21:50,190 [trainer.py] => FeCAM top5 curve: [95.84]

2024-03-05 02:21:50,213 [fecam.py] => Learning on 50-60
Processing class: 50
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.0227788229
Epoch:   200  |  train loss: 0.0232323151
Epoch:   300  |  train loss: 0.0235923354
Epoch:   400  |  train loss: 0.0243695352
Epoch:   500  |  train loss: 0.0250160515
Epoch:   600  |  train loss: 0.0248312902
Epoch:   700  |  train loss: 0.0257063210
Epoch:   800  |  train loss: 0.0261936493
Epoch:   900  |  train loss: 0.0266969219
Epoch:  1000  |  train loss: 0.0269963987
Processing class: 51
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.0230383478
Epoch:   200  |  train loss: 0.0226690762
Epoch:   300  |  train loss: 0.0238178320
Epoch:   400  |  train loss: 0.0242100373
Epoch:   500  |  train loss: 0.0246464469
Epoch:   600  |  train loss: 0.0252888292
Epoch:   700  |  train loss: 0.0249131627
Epoch:   800  |  train loss: 0.0252021160
Epoch:   900  |  train loss: 0.0261064421
Epoch:  1000  |  train loss: 0.0260056373
Processing class: 52
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.0229653999
Epoch:   200  |  train loss: 0.0228357334
Epoch:   300  |  train loss: 0.0234303519
Epoch:   400  |  train loss: 0.0238781378
Epoch:   500  |  train loss: 0.0239538986
Epoch:   600  |  train loss: 0.0237684909
Epoch:   700  |  train loss: 0.0244754378
Epoch:   800  |  train loss: 0.0246337254
Epoch:   900  |  train loss: 0.0249394786
Epoch:  1000  |  train loss: 0.0246129964
Processing class: 53
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.0228320148
Epoch:   200  |  train loss: 0.0234050043
Epoch:   300  |  train loss: 0.0230029382
Epoch:   400  |  train loss: 0.0234579597
Epoch:   500  |  train loss: 0.0238985743
Epoch:   600  |  train loss: 0.0245112728
Epoch:   700  |  train loss: 0.0252601352
Epoch:   800  |  train loss: 0.0242779505
Epoch:   900  |  train loss: 0.0248921666
Epoch:  1000  |  train loss: 0.0246489130
Processing class: 54
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.0215016812
Epoch:   200  |  train loss: 0.0219563313
Epoch:   300  |  train loss: 0.0228747014
Epoch:   400  |  train loss: 0.0233173616
Epoch:   500  |  train loss: 0.0237910304
Epoch:   600  |  train loss: 0.0246563964
Epoch:   700  |  train loss: 0.0250095442
Epoch:   800  |  train loss: 0.0257275246
Epoch:   900  |  train loss: 0.0262432694
Epoch:  1000  |  train loss: 0.0262384005
Processing class: 55
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.0225025948
Epoch:   200  |  train loss: 0.0226837911
Epoch:   300  |  train loss: 0.0224891867
Epoch:   400  |  train loss: 0.0232820030
Epoch:   500  |  train loss: 0.0243238110
Epoch:   600  |  train loss: 0.0245602846
Epoch:   700  |  train loss: 0.0250071414
Epoch:   800  |  train loss: 0.0258044917
Epoch:   900  |  train loss: 0.0251893371
Epoch:  1000  |  train loss: 0.0258029252
Processing class: 56
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.0226329528
Epoch:   200  |  train loss: 0.0232620735
Epoch:   300  |  train loss: 0.0232118826
Epoch:   400  |  train loss: 0.0242872063
Epoch:   500  |  train loss: 0.0249531269
Epoch:   600  |  train loss: 0.0255011089
Epoch:   700  |  train loss: 0.0261084117
Epoch:   800  |  train loss: 0.0259351112
Epoch:   900  |  train loss: 0.0267016530
Epoch:  1000  |  train loss: 0.0272008661
Processing class: 57
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.0222227328
Epoch:   200  |  train loss: 0.0228313554
Epoch:   300  |  train loss: 0.0227212965
Epoch:   400  |  train loss: 0.0228679348
Epoch:   500  |  train loss: 0.0234521266
Epoch:   600  |  train loss: 0.0230076961
Epoch:   700  |  train loss: 0.0239595175
Epoch:   800  |  train loss: 0.0241083141
Epoch:   900  |  train loss: 0.0246769402
Epoch:  1000  |  train loss: 0.0241486616
Processing class: 58
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.0226285085
Epoch:   200  |  train loss: 0.0229108166
Epoch:   300  |  train loss: 0.0239204779
Epoch:   400  |  train loss: 0.0250967354
Epoch:   500  |  train loss: 0.0251312267
Epoch:   600  |  train loss: 0.0258303981
Epoch:   700  |  train loss: 0.0259199344
Epoch:   800  |  train loss: 0.0264875367
Epoch:   900  |  train loss: 0.0266018774
Epoch:  1000  |  train loss: 0.0268165611
Processing class: 59
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.0231014408
Epoch:   200  |  train loss: 0.0240721889
Epoch:   300  |  train loss: 0.0241012350
Epoch:   400  |  train loss: 0.0248204608
Epoch:   500  |  train loss: 0.0254670296
Epoch:   600  |  train loss: 0.0256918218
Epoch:   700  |  train loss: 0.0261625778
Epoch:   800  |  train loss: 0.0264464885
Epoch:   900  |  train loss: 0.0264977027
Epoch:  1000  |  train loss: 0.0263455328
2024-03-05 02:27:53,491 [trainer.py] => CNN: {'total': 71.75, '00-09': 82.2, '10-19': 72.8, '20-29': 78.5, '30-39': 74.9, '40-49': 68.4, '50-59': 53.7, 'old': 75.36, 'new': 53.7}
2024-03-05 02:27:53,491 [trainer.py] => No NME accuracy
2024-03-05 02:27:53,491 [trainer.py] => FeCAM: {'total': 72.78, '00-09': 82.6, '10-19': 74.0, '20-29': 81.0, '30-39': 76.4, '40-49': 76.8, '50-59': 45.9, 'old': 78.16, 'new': 45.9}
2024-03-05 02:27:53,492 [trainer.py] => CNN top1 curve: [83.44, 71.75]
2024-03-05 02:27:53,492 [trainer.py] => CNN top5 curve: [96.5, 89.6]
2024-03-05 02:27:53,492 [trainer.py] => FeCAM top1 curve: [82.9, 72.78]
2024-03-05 02:27:53,492 [trainer.py] => FeCAM top5 curve: [95.84, 91.18]

2024-03-05 02:27:53,497 [fecam.py] => Learning on 60-70
Processing class: 60
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.0234863672
Epoch:   200  |  train loss: 0.0235201038
Epoch:   300  |  train loss: 0.0233936459
Epoch:   400  |  train loss: 0.0235060547
Epoch:   500  |  train loss: 0.0245826483
Epoch:   600  |  train loss: 0.0248217136
Epoch:   700  |  train loss: 0.0268340308
Epoch:   800  |  train loss: 0.0261450317
Epoch:   900  |  train loss: 0.0264750373
Epoch:  1000  |  train loss: 0.0271130357
Processing class: 61
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.0225749366
Epoch:   200  |  train loss: 0.0224458363
Epoch:   300  |  train loss: 0.0236588329
Epoch:   400  |  train loss: 0.0241612740
Epoch:   500  |  train loss: 0.0253517073
Epoch:   600  |  train loss: 0.0249788817
Epoch:   700  |  train loss: 0.0260869674
Epoch:   800  |  train loss: 0.0267829526
Epoch:   900  |  train loss: 0.0261868570
Epoch:  1000  |  train loss: 0.0266290009
Processing class: 62
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.0229486730
Epoch:   200  |  train loss: 0.0226841949
Epoch:   300  |  train loss: 0.0233104944
Epoch:   400  |  train loss: 0.0236330677
Epoch:   500  |  train loss: 0.0234035138
Epoch:   600  |  train loss: 0.0243197352
Epoch:   700  |  train loss: 0.0246424180
Epoch:   800  |  train loss: 0.0249775086
Epoch:   900  |  train loss: 0.0249182634
Epoch:  1000  |  train loss: 0.0252208993
Processing class: 63
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.0221040484
Epoch:   200  |  train loss: 0.0224176999
Epoch:   300  |  train loss: 0.0238895152
Epoch:   400  |  train loss: 0.0243309688
Epoch:   500  |  train loss: 0.0250577461
Epoch:   600  |  train loss: 0.0259551741
Epoch:   700  |  train loss: 0.0252691329
Epoch:   800  |  train loss: 0.0267413791
Epoch:   900  |  train loss: 0.0260574106
Epoch:  1000  |  train loss: 0.0263616566
Processing class: 64
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.0237536002
Epoch:   200  |  train loss: 0.0238650076
Epoch:   300  |  train loss: 0.0255828414
Epoch:   400  |  train loss: 0.0259409212
Epoch:   500  |  train loss: 0.0264776800
Epoch:   600  |  train loss: 0.0266864937
Epoch:   700  |  train loss: 0.0263065837
Epoch:   800  |  train loss: 0.0267790806
Epoch:   900  |  train loss: 0.0275842965
Epoch:  1000  |  train loss: 0.0275867213
Processing class: 65
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.0230426569
Epoch:   200  |  train loss: 0.0234604929
Epoch:   300  |  train loss: 0.0252965100
Epoch:   400  |  train loss: 0.0261692643
Epoch:   500  |  train loss: 0.0262708027
Epoch:   600  |  train loss: 0.0276705034
Epoch:   700  |  train loss: 0.0278987184
Epoch:   800  |  train loss: 0.0291323494
Epoch:   900  |  train loss: 0.0286862064
Epoch:  1000  |  train loss: 0.0299223356
Processing class: 66
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.0233320661
Epoch:   200  |  train loss: 0.0240557261
Epoch:   300  |  train loss: 0.0249260627
Epoch:   400  |  train loss: 0.0252339344
Epoch:   500  |  train loss: 0.0260291502
Epoch:   600  |  train loss: 0.0262862589
Epoch:   700  |  train loss: 0.0271600202
Epoch:   800  |  train loss: 0.0273426924
Epoch:   900  |  train loss: 0.0274940297
Epoch:  1000  |  train loss: 0.0274371035
Processing class: 67
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.0222696867
Epoch:   200  |  train loss: 0.0227110475
Epoch:   300  |  train loss: 0.0229020581
Epoch:   400  |  train loss: 0.0232473761
Epoch:   500  |  train loss: 0.0234676536
Epoch:   600  |  train loss: 0.0235431783
Epoch:   700  |  train loss: 0.0238095935
Epoch:   800  |  train loss: 0.0243817404
Epoch:   900  |  train loss: 0.0244030800
Epoch:  1000  |  train loss: 0.0244706392
Processing class: 68
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.0223437157
Epoch:   200  |  train loss: 0.0224926822
Epoch:   300  |  train loss: 0.0228115667
Epoch:   400  |  train loss: 0.0233109888
Epoch:   500  |  train loss: 0.0235427994
Epoch:   600  |  train loss: 0.0236325998
Epoch:   700  |  train loss: 0.0240740869
Epoch:   800  |  train loss: 0.0239541221
Epoch:   900  |  train loss: 0.0243515253
Epoch:  1000  |  train loss: 0.0253018454
Processing class: 69
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.0229639556
Epoch:   200  |  train loss: 0.0239015274
Epoch:   300  |  train loss: 0.0255926643
Epoch:   400  |  train loss: 0.0264574587
Epoch:   500  |  train loss: 0.0267575637
Epoch:   600  |  train loss: 0.0260640252
Epoch:   700  |  train loss: 0.0269441053
Epoch:   800  |  train loss: 0.0269913990
Epoch:   900  |  train loss: 0.0280532766
Epoch:  1000  |  train loss: 0.0271572623
2024-03-05 02:34:30,218 [trainer.py] => CNN: {'total': 64.66, '00-09': 75.0, '10-19': 70.1, '20-29': 77.1, '30-39': 70.6, '40-49': 64.0, '50-59': 45.8, '60-69': 50.0, 'old': 67.1, 'new': 50.0}
2024-03-05 02:34:30,218 [trainer.py] => No NME accuracy
2024-03-05 02:34:30,218 [trainer.py] => FeCAM: {'total': 67.27, '00-09': 79.9, '10-19': 70.6, '20-29': 79.1, '30-39': 71.9, '40-49': 74.1, '50-59': 41.2, '60-69': 54.1, 'old': 69.47, 'new': 54.1}
2024-03-05 02:34:30,218 [trainer.py] => CNN top1 curve: [83.44, 71.75, 64.66]
2024-03-05 02:34:30,218 [trainer.py] => CNN top5 curve: [96.5, 89.6, 86.54]
2024-03-05 02:34:30,218 [trainer.py] => FeCAM top1 curve: [82.9, 72.78, 67.27]
2024-03-05 02:34:30,218 [trainer.py] => FeCAM top5 curve: [95.84, 91.18, 87.21]

2024-03-05 02:34:30,222 [fecam.py] => Learning on 70-80
Processing class: 70
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.0224766877
Epoch:   200  |  train loss: 0.0223904308
Epoch:   300  |  train loss: 0.0230925579
Epoch:   400  |  train loss: 0.0236148309
Epoch:   500  |  train loss: 0.0236874305
Epoch:   600  |  train loss: 0.0244440045
Epoch:   700  |  train loss: 0.0254469603
Epoch:   800  |  train loss: 0.0248989772
Epoch:   900  |  train loss: 0.0256200038
Epoch:  1000  |  train loss: 0.0255342044
Processing class: 71
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.0227424014
Epoch:   200  |  train loss: 0.0231317446
Epoch:   300  |  train loss: 0.0229550306
Epoch:   400  |  train loss: 0.0237265978
Epoch:   500  |  train loss: 0.0243433777
Epoch:   600  |  train loss: 0.0246334802
Epoch:   700  |  train loss: 0.0249090940
Epoch:   800  |  train loss: 0.0252640124
Epoch:   900  |  train loss: 0.0260995507
Epoch:  1000  |  train loss: 0.0261283938
Processing class: 72
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.0223879665
Epoch:   200  |  train loss: 0.0227799267
Epoch:   300  |  train loss: 0.0231244195
Epoch:   400  |  train loss: 0.0233628511
Epoch:   500  |  train loss: 0.0240000106
Epoch:   600  |  train loss: 0.0237464171
Epoch:   700  |  train loss: 0.0241294190
Epoch:   800  |  train loss: 0.0240989879
Epoch:   900  |  train loss: 0.0244525071
Epoch:  1000  |  train loss: 0.0251810592
Processing class: 73
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.0230214864
Epoch:   200  |  train loss: 0.0231210664
Epoch:   300  |  train loss: 0.0242761537
Epoch:   400  |  train loss: 0.0241702430
Epoch:   500  |  train loss: 0.0250315066
Epoch:   600  |  train loss: 0.0251126572
Epoch:   700  |  train loss: 0.0248865332
Epoch:   800  |  train loss: 0.0249692786
Epoch:   900  |  train loss: 0.0252047803
Epoch:  1000  |  train loss: 0.0256949414
Processing class: 74
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.0231871821
Epoch:   200  |  train loss: 0.0235880941
Epoch:   300  |  train loss: 0.0231561948
Epoch:   400  |  train loss: 0.0230971862
Epoch:   500  |  train loss: 0.0236469358
Epoch:   600  |  train loss: 0.0236130510
Epoch:   700  |  train loss: 0.0245616373
Epoch:   800  |  train loss: 0.0239821836
Epoch:   900  |  train loss: 0.0242067784
Epoch:  1000  |  train loss: 0.0250463963
Processing class: 75
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.0224695180
Epoch:   200  |  train loss: 0.0225169607
Epoch:   300  |  train loss: 0.0234826475
Epoch:   400  |  train loss: 0.0236373138
Epoch:   500  |  train loss: 0.0243536849
Epoch:   600  |  train loss: 0.0250514146
Epoch:   700  |  train loss: 0.0239022482
Epoch:   800  |  train loss: 0.0248570986
Epoch:   900  |  train loss: 0.0249211576
Epoch:  1000  |  train loss: 0.0256674815
Processing class: 76
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.0222678322
Epoch:   200  |  train loss: 0.0233447902
Epoch:   300  |  train loss: 0.0247869693
Epoch:   400  |  train loss: 0.0257860128
Epoch:   500  |  train loss: 0.0262185048
Epoch:   600  |  train loss: 0.0269607354
Epoch:   700  |  train loss: 0.0271437678
Epoch:   800  |  train loss: 0.0265677396
Epoch:   900  |  train loss: 0.0275588904
Epoch:  1000  |  train loss: 0.0280290611
Processing class: 77
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.0228286717
Epoch:   200  |  train loss: 0.0231828425
Epoch:   300  |  train loss: 0.0231943551
Epoch:   400  |  train loss: 0.0237971213
Epoch:   500  |  train loss: 0.0243805803
Epoch:   600  |  train loss: 0.0247516971
Epoch:   700  |  train loss: 0.0251950420
Epoch:   800  |  train loss: 0.0260113936
Epoch:   900  |  train loss: 0.0258792266
Epoch:  1000  |  train loss: 0.0259899199
Processing class: 78
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.0231303137
Epoch:   200  |  train loss: 0.0240734104
Epoch:   300  |  train loss: 0.0249419402
Epoch:   400  |  train loss: 0.0252003521
Epoch:   500  |  train loss: 0.0253940206
Epoch:   600  |  train loss: 0.0261006873
Epoch:   700  |  train loss: 0.0264900472
Epoch:   800  |  train loss: 0.0268846922
Epoch:   900  |  train loss: 0.0271091547
Epoch:  1000  |  train loss: 0.0270431466
Processing class: 79
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.0227142867
Epoch:   200  |  train loss: 0.0224126820
Epoch:   300  |  train loss: 0.0231407240
Epoch:   400  |  train loss: 0.0234247282
Epoch:   500  |  train loss: 0.0235357627
Epoch:   600  |  train loss: 0.0244061496
Epoch:   700  |  train loss: 0.0245195866
Epoch:   800  |  train loss: 0.0250088234
Epoch:   900  |  train loss: 0.0256866962
Epoch:  1000  |  train loss: 0.0254114334
2024-03-05 02:42:31,727 [trainer.py] => CNN: {'total': 59.18, '00-09': 73.6, '10-19': 68.4, '20-29': 76.9, '30-39': 69.1, '40-49': 60.6, '50-59': 37.2, '60-69': 45.7, '70-79': 41.9, 'old': 61.64, 'new': 41.9}
2024-03-05 02:42:33,827 [trainer.py] => No NME accuracy
2024-03-05 02:42:33,827 [trainer.py] => FeCAM: {'total': 61.72, '00-09': 78.1, '10-19': 69.3, '20-29': 78.0, '30-39': 69.5, '40-49': 72.1, '50-59': 35.7, '60-69': 49.1, '70-79': 42.0, 'old': 64.54, 'new': 42.0}
2024-03-05 02:42:33,827 [trainer.py] => CNN top1 curve: [83.44, 71.75, 64.66, 59.18]
2024-03-05 02:42:33,827 [trainer.py] => CNN top5 curve: [96.5, 89.6, 86.54, 84.09]
2024-03-05 02:42:33,827 [trainer.py] => FeCAM top1 curve: [82.9, 72.78, 67.27, 61.72]
2024-03-05 02:42:33,827 [trainer.py] => FeCAM top5 curve: [95.84, 91.18, 87.21, 84.09]

2024-03-05 02:42:33,833 [fecam.py] => Learning on 80-90
Processing class: 80
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.0228890926
Epoch:   200  |  train loss: 0.0250644542
Epoch:   300  |  train loss: 0.0260389347
Epoch:   400  |  train loss: 0.0266077813
Epoch:   500  |  train loss: 0.0268357426
Epoch:   600  |  train loss: 0.0270111281
Epoch:   700  |  train loss: 0.0266219862
Epoch:   800  |  train loss: 0.0277109150
Epoch:   900  |  train loss: 0.0275971860
Epoch:  1000  |  train loss: 0.0282741196
Processing class: 81
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.0225547325
Epoch:   200  |  train loss: 0.0237891354
Epoch:   300  |  train loss: 0.0236442704
Epoch:   400  |  train loss: 0.0248323593
Epoch:   500  |  train loss: 0.0248826686
Epoch:   600  |  train loss: 0.0256920833
Epoch:   700  |  train loss: 0.0258427519
Epoch:   800  |  train loss: 0.0263705444
Epoch:   900  |  train loss: 0.0266670447
Epoch:  1000  |  train loss: 0.0270949122
Processing class: 82
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.0225722086
Epoch:   200  |  train loss: 0.0235061072
Epoch:   300  |  train loss: 0.0239525076
Epoch:   400  |  train loss: 0.0241992064
Epoch:   500  |  train loss: 0.0247589923
Epoch:   600  |  train loss: 0.0247970875
Epoch:   700  |  train loss: 0.0250466101
Epoch:   800  |  train loss: 0.0252796061
Epoch:   900  |  train loss: 0.0255932208
Epoch:  1000  |  train loss: 0.0257672645
Processing class: 83
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.0231580518
Epoch:   200  |  train loss: 0.0247861102
Epoch:   300  |  train loss: 0.0253002577
Epoch:   400  |  train loss: 0.0260802191
Epoch:   500  |  train loss: 0.0271956552
Epoch:   600  |  train loss: 0.0269842081
Epoch:   700  |  train loss: 0.0279421534
Epoch:   800  |  train loss: 0.0289590288
Epoch:   900  |  train loss: 0.0287545398
Epoch:  1000  |  train loss: 0.0287132602
Processing class: 84
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.0231684428
Epoch:   200  |  train loss: 0.0235713433
Epoch:   300  |  train loss: 0.0245941013
Epoch:   400  |  train loss: 0.0250034377
Epoch:   500  |  train loss: 0.0258330625
Epoch:   600  |  train loss: 0.0254974112
Epoch:   700  |  train loss: 0.0262825515
Epoch:   800  |  train loss: 0.0269512050
Epoch:   900  |  train loss: 0.0264584202
Epoch:  1000  |  train loss: 0.0264564712
Processing class: 85
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.0227616031
Epoch:   200  |  train loss: 0.0230660297
Epoch:   300  |  train loss: 0.0240804140
Epoch:   400  |  train loss: 0.0241614532
Epoch:   500  |  train loss: 0.0248525281
Epoch:   600  |  train loss: 0.0256116185
Epoch:   700  |  train loss: 0.0265244260
Epoch:   800  |  train loss: 0.0260772195
Epoch:   900  |  train loss: 0.0262899499
Epoch:  1000  |  train loss: 0.0270860460
Processing class: 86
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.0232148666
Epoch:   200  |  train loss: 0.0228654917
Epoch:   300  |  train loss: 0.0239006400
Epoch:   400  |  train loss: 0.0239660364
Epoch:   500  |  train loss: 0.0241389669
Epoch:   600  |  train loss: 0.0247158036
Epoch:   700  |  train loss: 0.0249227263
Epoch:   800  |  train loss: 0.0250460371
Epoch:   900  |  train loss: 0.0251830380
Epoch:  1000  |  train loss: 0.0261344060
Processing class: 87
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.0224392831
Epoch:   200  |  train loss: 0.0237759136
Epoch:   300  |  train loss: 0.0241907611
Epoch:   400  |  train loss: 0.0250528239
Epoch:   500  |  train loss: 0.0252776261
Epoch:   600  |  train loss: 0.0257850211
Epoch:   700  |  train loss: 0.0261935435
Epoch:   800  |  train loss: 0.0263718653
Epoch:   900  |  train loss: 0.0261999901
Epoch:  1000  |  train loss: 0.0273192767
Processing class: 88
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.0225991171
Epoch:   200  |  train loss: 0.0226307798
Epoch:   300  |  train loss: 0.0230229389
Epoch:   400  |  train loss: 0.0235142916
Epoch:   500  |  train loss: 0.0238011748
Epoch:   600  |  train loss: 0.0239498660
Epoch:   700  |  train loss: 0.0246954400
Epoch:   800  |  train loss: 0.0250075288
Epoch:   900  |  train loss: 0.0253393769
Epoch:  1000  |  train loss: 0.0251478370
Processing class: 89
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.0230425019
Epoch:   200  |  train loss: 0.0241010066
Epoch:   300  |  train loss: 0.0249232247
Epoch:   400  |  train loss: 0.0253528643
Epoch:   500  |  train loss: 0.0262055904
Epoch:   600  |  train loss: 0.0266336098
Epoch:   700  |  train loss: 0.0262506153
Epoch:   800  |  train loss: 0.0276421797
Epoch:   900  |  train loss: 0.0275409356
Epoch:  1000  |  train loss: 0.0280720871
2024-03-05 02:51:33,580 [trainer.py] => CNN: {'total': 54.08, '00-09': 67.9, '10-19': 63.6, '20-29': 73.2, '30-39': 68.8, '40-49': 57.2, '50-59': 32.6, '60-69': 40.9, '70-79': 38.1, '80-89': 44.4, 'old': 55.29, 'new': 44.4}
2024-03-05 02:51:33,580 [trainer.py] => No NME accuracy
2024-03-05 02:51:33,580 [trainer.py] => FeCAM: {'total': 57.66, '00-09': 76.1, '10-19': 65.8, '20-29': 75.3, '30-39': 67.9, '40-49': 68.5, '50-59': 33.0, '60-69': 45.0, '70-79': 39.0, '80-89': 48.3, 'old': 58.82, 'new': 48.3}
2024-03-05 02:51:33,580 [trainer.py] => CNN top1 curve: [83.44, 71.75, 64.66, 59.18, 54.08]
2024-03-05 02:51:33,581 [trainer.py] => CNN top5 curve: [96.5, 89.6, 86.54, 84.09, 81.74]
2024-03-05 02:51:33,581 [trainer.py] => FeCAM top1 curve: [82.9, 72.78, 67.27, 61.72, 57.66]
2024-03-05 02:51:33,581 [trainer.py] => FeCAM top5 curve: [95.84, 91.18, 87.21, 84.09, 81.63]

2024-03-05 02:51:33,588 [fecam.py] => Learning on 90-100
Processing class: 90
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.0229725759
Epoch:   200  |  train loss: 0.0232705493
Epoch:   300  |  train loss: 0.0230871983
Epoch:   400  |  train loss: 0.0233211953
Epoch:   500  |  train loss: 0.0234193012
Epoch:   600  |  train loss: 0.0240299854
Epoch:   700  |  train loss: 0.0242174134
Epoch:   800  |  train loss: 0.0246152971
Epoch:   900  |  train loss: 0.0250801053
Epoch:  1000  |  train loss: 0.0258771837
Processing class: 91
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.0228229102
Epoch:   200  |  train loss: 0.0231854968
Epoch:   300  |  train loss: 0.0241746031
Epoch:   400  |  train loss: 0.0248921521
Epoch:   500  |  train loss: 0.0257779013
Epoch:   600  |  train loss: 0.0262689866
Epoch:   700  |  train loss: 0.0267714269
Epoch:   800  |  train loss: 0.0273841921
Epoch:   900  |  train loss: 0.0280597042
Epoch:  1000  |  train loss: 0.0282905329
Processing class: 92
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.0235482566
Epoch:   200  |  train loss: 0.0236506328
Epoch:   300  |  train loss: 0.0232074097
Epoch:   400  |  train loss: 0.0238844782
Epoch:   500  |  train loss: 0.0245519120
Epoch:   600  |  train loss: 0.0256101131
Epoch:   700  |  train loss: 0.0258780904
Epoch:   800  |  train loss: 0.0253733899
Epoch:   900  |  train loss: 0.0257624719
Epoch:  1000  |  train loss: 0.0260849997
Processing class: 93
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.0227938358
Epoch:   200  |  train loss: 0.0237367187
Epoch:   300  |  train loss: 0.0255502287
Epoch:   400  |  train loss: 0.0260171283
Epoch:   500  |  train loss: 0.0262708236
Epoch:   600  |  train loss: 0.0261946056
Epoch:   700  |  train loss: 0.0275799152
Epoch:   800  |  train loss: 0.0277458563
Epoch:   900  |  train loss: 0.0280723397
Epoch:  1000  |  train loss: 0.0281409152
Processing class: 94
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.0213211458
Epoch:   200  |  train loss: 0.0215590511
Epoch:   300  |  train loss: 0.0213590380
Epoch:   400  |  train loss: 0.0221078616
Epoch:   500  |  train loss: 0.0213893946
Epoch:   600  |  train loss: 0.0221264075
Epoch:   700  |  train loss: 0.0228226349
Epoch:   800  |  train loss: 0.0218428634
Epoch:   900  |  train loss: 0.0228019014
Epoch:  1000  |  train loss: 0.0226921882
Processing class: 95
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.0224123757
Epoch:   200  |  train loss: 0.0231364727
Epoch:   300  |  train loss: 0.0229917780
Epoch:   400  |  train loss: 0.0230717521
Epoch:   500  |  train loss: 0.0242208656
Epoch:   600  |  train loss: 0.0243441828
Epoch:   700  |  train loss: 0.0248901363
Epoch:   800  |  train loss: 0.0248127915
Epoch:   900  |  train loss: 0.0255440455
Epoch:  1000  |  train loss: 0.0262675449
Processing class: 96
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.0230920441
Epoch:   200  |  train loss: 0.0225664549
Epoch:   300  |  train loss: 0.0235014137
Epoch:   400  |  train loss: 0.0247327950
Epoch:   500  |  train loss: 0.0246951718
Epoch:   600  |  train loss: 0.0247787267
Epoch:   700  |  train loss: 0.0251694556
Epoch:   800  |  train loss: 0.0250018895
Epoch:   900  |  train loss: 0.0262458194
Epoch:  1000  |  train loss: 0.0264337767
Processing class: 97
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.0231140517
Epoch:   200  |  train loss: 0.0237601966
Epoch:   300  |  train loss: 0.0244400173
Epoch:   400  |  train loss: 0.0249635443
Epoch:   500  |  train loss: 0.0259340614
Epoch:   600  |  train loss: 0.0266090456
Epoch:   700  |  train loss: 0.0272417877
Epoch:   800  |  train loss: 0.0276623055
Epoch:   900  |  train loss: 0.0280949652
Epoch:  1000  |  train loss: 0.0281592436
Processing class: 98
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.0227274887
Epoch:   200  |  train loss: 0.0231046777
Epoch:   300  |  train loss: 0.0238153689
Epoch:   400  |  train loss: 0.0245361488
Epoch:   500  |  train loss: 0.0248301148
Epoch:   600  |  train loss: 0.0254974261
Epoch:   700  |  train loss: 0.0251102168
Epoch:   800  |  train loss: 0.0258860439
Epoch:   900  |  train loss: 0.0260580610
Epoch:  1000  |  train loss: 0.0259028118
Processing class: 99
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.0235445682
Epoch:   200  |  train loss: 0.0233169902
Epoch:   300  |  train loss: 0.0248816077
Epoch:   400  |  train loss: 0.0259577446
Epoch:   500  |  train loss: 0.0262560878
Epoch:   600  |  train loss: 0.0264706112
Epoch:   700  |  train loss: 0.0264644906
Epoch:   800  |  train loss: 0.0273030609
Epoch:   900  |  train loss: 0.0272961803
Epoch:  1000  |  train loss: 0.0275652729
2024-03-05 03:01:56,361 [trainer.py] => CNN: {'total': 50.36, '00-09': 57.7, '10-19': 63.6, '20-29': 71.4, '30-39': 68.5, '40-49': 56.8, '50-59': 29.6, '60-69': 38.9, '70-79': 36.6, '80-89': 42.7, '90-99': 37.8, 'old': 51.76, 'new': 37.8}
2024-03-05 03:01:56,361 [trainer.py] => No NME accuracy
2024-03-05 03:01:56,361 [trainer.py] => FeCAM: {'total': 54.74, '00-09': 73.3, '10-19': 65.1, '20-29': 74.1, '30-39': 67.2, '40-49': 67.1, '50-59': 30.8, '60-69': 43.3, '70-79': 37.2, '80-89': 46.9, '90-99': 42.4, 'old': 56.11, 'new': 42.4}
2024-03-05 03:01:56,362 [trainer.py] => CNN top1 curve: [83.44, 71.75, 64.66, 59.18, 54.08, 50.36]
2024-03-05 03:01:56,362 [trainer.py] => CNN top5 curve: [96.5, 89.6, 86.54, 84.09, 81.74, 79.63]
2024-03-05 03:01:56,362 [trainer.py] => FeCAM top1 curve: [82.9, 72.78, 67.27, 61.72, 57.66, 54.74]
2024-03-05 03:01:56,362 [trainer.py] => FeCAM top5 curve: [95.84, 91.18, 87.21, 84.09, 81.63, 79.21]

=========================================
2024-03-05 03:02:11,600 [trainer.py] => config: ./exps/FeCAM_cifar100.json
2024-03-05 03:02:11,600 [trainer.py] => prefix: train
2024-03-05 03:02:11,600 [trainer.py] => dataset: cifar100
2024-03-05 03:02:11,601 [trainer.py] => memory_size: 0
2024-03-05 03:02:11,601 [trainer.py] => shuffle: True
2024-03-05 03:02:11,601 [trainer.py] => init_cls: 50
2024-03-05 03:02:11,601 [trainer.py] => increment: 10
2024-03-05 03:02:11,601 [trainer.py] => model_name: fecam
2024-03-05 03:02:11,601 [trainer.py] => convnet_type: resnet18
2024-03-05 03:02:11,601 [trainer.py] => device: [device(type='cuda', index=0)]
2024-03-05 03:02:11,601 [trainer.py] => seed: 1993
2024-03-05 03:02:11,601 [trainer.py] => init_epochs: 200
2024-03-05 03:02:11,601 [trainer.py] => init_lr: 0.1
2024-03-05 03:02:11,601 [trainer.py] => init_weight_decay: 0.0005
2024-03-05 03:02:11,601 [trainer.py] => batch_size: 128
2024-03-05 03:02:11,601 [trainer.py] => num_workers: 8
2024-03-05 03:02:11,601 [trainer.py] => T: 5
2024-03-05 03:02:11,601 [trainer.py] => beta: 0.5
2024-03-05 03:02:11,601 [trainer.py] => alpha1: 1
2024-03-05 03:02:11,601 [trainer.py] => alpha2: 1
2024-03-05 03:02:11,601 [trainer.py] => ncm: False
2024-03-05 03:02:11,601 [trainer.py] => tukey: False
2024-03-05 03:02:11,601 [trainer.py] => diagonal: False
2024-03-05 03:02:11,601 [trainer.py] => per_class: True
2024-03-05 03:02:11,601 [trainer.py] => full_cov: True
2024-03-05 03:02:11,601 [trainer.py] => shrink: True
2024-03-05 03:02:11,601 [trainer.py] => norm_cov: False
2024-03-05 03:02:11,601 [trainer.py] => vecnorm: False
2024-03-05 03:02:11,601 [trainer.py] => ae_type: wae
2024-03-05 03:02:11,601 [trainer.py] => epochs: 1000
2024-03-05 03:02:11,601 [trainer.py] => ae_latent_dim: 32
2024-03-05 03:02:11,601 [trainer.py] => wae_sigma: 1
2024-03-05 03:02:11,601 [trainer.py] => wae_C: 0.5
Files already downloaded and verified
Files already downloaded and verified
2024-03-05 03:02:13,261 [data_manager.py] => [68, 56, 78, 8, 23, 84, 90, 65, 74, 76, 40, 89, 3, 92, 55, 9, 26, 80, 43, 38, 58, 70, 77, 1, 85, 19, 17, 50, 28, 53, 13, 81, 45, 82, 6, 59, 83, 16, 15, 44, 91, 41, 72, 60, 79, 52, 20, 10, 31, 54, 37, 95, 14, 71, 96, 98, 97, 2, 64, 66, 42, 22, 35, 86, 24, 34, 87, 21, 99, 0, 88, 27, 18, 94, 11, 12, 47, 25, 30, 46, 62, 69, 36, 61, 7, 63, 75, 5, 32, 4, 51, 48, 73, 93, 39, 67, 29, 49, 57, 33]
2024-03-05 03:02:13,524 [fecam.py] => Learning on 0-50
/home/z1165703/miniconda3/lib/python3.11/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
/home/z1165703/miniconda3/lib/python3.11/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
Processing class: 0
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.2218992740
Epoch:   200  |  train loss: 0.1853757977
Epoch:   300  |  train loss: 0.1793620557
Epoch:   400  |  train loss: 0.1639391065
Epoch:   500  |  train loss: 0.1571241945
Epoch:   600  |  train loss: 0.1541722983
Epoch:   700  |  train loss: 0.1493919134
Epoch:   800  |  train loss: 0.1476750076
Epoch:   900  |  train loss: 0.1438659251
Epoch:  1000  |  train loss: 0.1404607624
Processing class: 1
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.2625095844
Epoch:   200  |  train loss: 0.2566504538
Epoch:   300  |  train loss: 0.2232522309
Epoch:   400  |  train loss: 0.2074273050
Epoch:   500  |  train loss: 0.1930326730
Epoch:   600  |  train loss: 0.1871995240
Epoch:   700  |  train loss: 0.1797477633
Epoch:   800  |  train loss: 0.1721927762
Epoch:   900  |  train loss: 0.1672434419
Epoch:  1000  |  train loss: 0.1620136082
Processing class: 2
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.2865509331
Epoch:   200  |  train loss: 0.2537194103
Epoch:   300  |  train loss: 0.2195730746
Epoch:   400  |  train loss: 0.2000285447
Epoch:   500  |  train loss: 0.1846797287
Epoch:   600  |  train loss: 0.1702791780
Epoch:   700  |  train loss: 0.1606968611
Epoch:   800  |  train loss: 0.1536048919
Epoch:   900  |  train loss: 0.1464668840
Epoch:  1000  |  train loss: 0.1401389748
Processing class: 3
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.2161800742
Epoch:   200  |  train loss: 0.2163192242
Epoch:   300  |  train loss: 0.1957356632
Epoch:   400  |  train loss: 0.1707325161
Epoch:   500  |  train loss: 0.1591674477
Epoch:   600  |  train loss: 0.1534578204
Epoch:   700  |  train loss: 0.1414384544
Epoch:   800  |  train loss: 0.1361003205
Epoch:   900  |  train loss: 0.1282127649
Epoch:  1000  |  train loss: 0.1216424972
Processing class: 4
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.2625568122
Epoch:   200  |  train loss: 0.2548825294
Epoch:   300  |  train loss: 0.2162296146
Epoch:   400  |  train loss: 0.1984432489
Epoch:   500  |  train loss: 0.1841233373
Epoch:   600  |  train loss: 0.1701489329
Epoch:   700  |  train loss: 0.1643904001
Epoch:   800  |  train loss: 0.1595535398
Epoch:   900  |  train loss: 0.1539039880
Epoch:  1000  |  train loss: 0.1479402661
Processing class: 5
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.2928393424
Epoch:   200  |  train loss: 0.2449416995
Epoch:   300  |  train loss: 0.2256030858
Epoch:   400  |  train loss: 0.1970356226
Epoch:   500  |  train loss: 0.1885087997
Epoch:   600  |  train loss: 0.1777802229
Epoch:   700  |  train loss: 0.1639879078
Epoch:   800  |  train loss: 0.1559802562
Epoch:   900  |  train loss: 0.1491607577
Epoch:  1000  |  train loss: 0.1439159393
Processing class: 6
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.2576780021
Epoch:   200  |  train loss: 0.2561111957
Epoch:   300  |  train loss: 0.2219617158
Epoch:   400  |  train loss: 0.1973974675
Epoch:   500  |  train loss: 0.1861760318
Epoch:   600  |  train loss: 0.1808302790
Epoch:   700  |  train loss: 0.1752920181
Epoch:   800  |  train loss: 0.1667115390
Epoch:   900  |  train loss: 0.1574155390
Epoch:  1000  |  train loss: 0.1523983836
Processing class: 7
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.2906821787
Epoch:   200  |  train loss: 0.2545478612
Epoch:   300  |  train loss: 0.2373057604
Epoch:   400  |  train loss: 0.2097263813
Epoch:   500  |  train loss: 0.1917878598
Epoch:   600  |  train loss: 0.1822385818
Epoch:   700  |  train loss: 0.1727212429
Epoch:   800  |  train loss: 0.1659192532
Epoch:   900  |  train loss: 0.1583186030
Epoch:  1000  |  train loss: 0.1524969250
Processing class: 8
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.2745169878
Epoch:   200  |  train loss: 0.2487542540
Epoch:   300  |  train loss: 0.2081409782
Epoch:   400  |  train loss: 0.1916236460
Epoch:   500  |  train loss: 0.1808061868
Epoch:   600  |  train loss: 0.1676521599
Epoch:   700  |  train loss: 0.1596970052
Epoch:   800  |  train loss: 0.1534589171
Epoch:   900  |  train loss: 0.1477779478
Epoch:  1000  |  train loss: 0.1413871408
Processing class: 9
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.2545905471
Epoch:   200  |  train loss: 0.2516595811
Epoch:   300  |  train loss: 0.2323778421
Epoch:   400  |  train loss: 0.2141114265
Epoch:   500  |  train loss: 0.2043141693
Epoch:   600  |  train loss: 0.1986846626
Epoch:   700  |  train loss: 0.1933805674
Epoch:   800  |  train loss: 0.1881157935
Epoch:   900  |  train loss: 0.1832311183
Epoch:  1000  |  train loss: 0.1776964933
Processing class: 10
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.2916441321
Epoch:   200  |  train loss: 0.2577468544
Epoch:   300  |  train loss: 0.2398517042
Epoch:   400  |  train loss: 0.2215285689
Epoch:   500  |  train loss: 0.2062865525
Epoch:   600  |  train loss: 0.1954249412
Epoch:   700  |  train loss: 0.1856366277
Epoch:   800  |  train loss: 0.1789688200
Epoch:   900  |  train loss: 0.1731048942
Epoch:  1000  |  train loss: 0.1686825573
Processing class: 11
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.2945776403
Epoch:   200  |  train loss: 0.2453953832
Epoch:   300  |  train loss: 0.2297707915
Epoch:   400  |  train loss: 0.2083889306
Epoch:   500  |  train loss: 0.1915291965
Epoch:   600  |  train loss: 0.1827243149
Epoch:   700  |  train loss: 0.1739097029
Epoch:   800  |  train loss: 0.1663100660
Epoch:   900  |  train loss: 0.1596613884
Epoch:  1000  |  train loss: 0.1532508790
Processing class: 12
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.2752358496
Epoch:   200  |  train loss: 0.2523471504
Epoch:   300  |  train loss: 0.2266055763
Epoch:   400  |  train loss: 0.2017801404
Epoch:   500  |  train loss: 0.1873465002
Epoch:   600  |  train loss: 0.1738438249
Epoch:   700  |  train loss: 0.1648832262
Epoch:   800  |  train loss: 0.1596608609
Epoch:   900  |  train loss: 0.1536451578
Epoch:  1000  |  train loss: 0.1487424225
Processing class: 13
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.2631890655
Epoch:   200  |  train loss: 0.2277899534
Epoch:   300  |  train loss: 0.2015137315
Epoch:   400  |  train loss: 0.1817430258
Epoch:   500  |  train loss: 0.1687664121
Epoch:   600  |  train loss: 0.1627448291
Epoch:   700  |  train loss: 0.1555345267
Epoch:   800  |  train loss: 0.1507124633
Epoch:   900  |  train loss: 0.1463732809
Epoch:  1000  |  train loss: 0.1414336219
Processing class: 14
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.3128582001
Epoch:   200  |  train loss: 0.3084524095
Epoch:   300  |  train loss: 0.2736887097
Epoch:   400  |  train loss: 0.2431791067
Epoch:   500  |  train loss: 0.2293851852
Epoch:   600  |  train loss: 0.2160374999
Epoch:   700  |  train loss: 0.2071763396
Epoch:   800  |  train loss: 0.1997277200
Epoch:   900  |  train loss: 0.1944502831
Epoch:  1000  |  train loss: 0.1896886647
Processing class: 15
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.2490782797
Epoch:   200  |  train loss: 0.2302245021
Epoch:   300  |  train loss: 0.2012742817
Epoch:   400  |  train loss: 0.1871799856
Epoch:   500  |  train loss: 0.1756497025
Epoch:   600  |  train loss: 0.1694440633
Epoch:   700  |  train loss: 0.1646839559
Epoch:   800  |  train loss: 0.1584612966
Epoch:   900  |  train loss: 0.1559775710
Epoch:  1000  |  train loss: 0.1512473196
Processing class: 16
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.2656708658
Epoch:   200  |  train loss: 0.2203145385
Epoch:   300  |  train loss: 0.2024271160
Epoch:   400  |  train loss: 0.1898041248
Epoch:   500  |  train loss: 0.1849743575
Epoch:   600  |  train loss: 0.1765760154
Epoch:   700  |  train loss: 0.1680439830
Epoch:   800  |  train loss: 0.1619591266
Epoch:   900  |  train loss: 0.1566304654
Epoch:  1000  |  train loss: 0.1543309987
Processing class: 17
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.2647502869
Epoch:   200  |  train loss: 0.2603783786
Epoch:   300  |  train loss: 0.2487050861
Epoch:   400  |  train loss: 0.2231534809
Epoch:   500  |  train loss: 0.1992323130
Epoch:   600  |  train loss: 0.1882148981
Epoch:   700  |  train loss: 0.1809240490
Epoch:   800  |  train loss: 0.1743740708
Epoch:   900  |  train loss: 0.1684224486
Epoch:  1000  |  train loss: 0.1637447894
Processing class: 18
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.2595859319
Epoch:   200  |  train loss: 0.2549558014
Epoch:   300  |  train loss: 0.1964101076
Epoch:   400  |  train loss: 0.1786843121
Epoch:   500  |  train loss: 0.1597450465
Epoch:   600  |  train loss: 0.1479566395
Epoch:   700  |  train loss: 0.1416437000
Epoch:   800  |  train loss: 0.1347458541
Epoch:   900  |  train loss: 0.1297180191
Epoch:  1000  |  train loss: 0.1254141748
Processing class: 19
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.2382138312
Epoch:   200  |  train loss: 0.2145679265
Epoch:   300  |  train loss: 0.1912095547
Epoch:   400  |  train loss: 0.1858199447
Epoch:   500  |  train loss: 0.1688924551
Epoch:   600  |  train loss: 0.1612757087
Epoch:   700  |  train loss: 0.1550601214
Epoch:   800  |  train loss: 0.1477959424
Epoch:   900  |  train loss: 0.1416661531
Epoch:  1000  |  train loss: 0.1368132472
Processing class: 20
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.2587527573
Epoch:   200  |  train loss: 0.2246892542
Epoch:   300  |  train loss: 0.2164802045
Epoch:   400  |  train loss: 0.1920147538
Epoch:   500  |  train loss: 0.1863823861
Epoch:   600  |  train loss: 0.1760558337
Epoch:   700  |  train loss: 0.1666846991
Epoch:   800  |  train loss: 0.1631943494
Epoch:   900  |  train loss: 0.1580519080
Epoch:  1000  |  train loss: 0.1549427658
Processing class: 21
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.2652727008
Epoch:   200  |  train loss: 0.2279311001
Epoch:   300  |  train loss: 0.1818190426
Epoch:   400  |  train loss: 0.1635848403
Epoch:   500  |  train loss: 0.1523344785
Epoch:   600  |  train loss: 0.1455309778
Epoch:   700  |  train loss: 0.1400740772
Epoch:   800  |  train loss: 0.1351194113
Epoch:   900  |  train loss: 0.1297152728
Epoch:  1000  |  train loss: 0.1243100524
Processing class: 22
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.2868495166
Epoch:   200  |  train loss: 0.2817282915
Epoch:   300  |  train loss: 0.2587994039
Epoch:   400  |  train loss: 0.2213542163
Epoch:   500  |  train loss: 0.2080741376
Epoch:   600  |  train loss: 0.1938231975
Epoch:   700  |  train loss: 0.1817024589
Epoch:   800  |  train loss: 0.1728794247
Epoch:   900  |  train loss: 0.1664039791
Epoch:  1000  |  train loss: 0.1615766764
Processing class: 23
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.2469626486
Epoch:   200  |  train loss: 0.2422131270
Epoch:   300  |  train loss: 0.2144632787
Epoch:   400  |  train loss: 0.1958748668
Epoch:   500  |  train loss: 0.1865583479
Epoch:   600  |  train loss: 0.1718491673
Epoch:   700  |  train loss: 0.1625527322
Epoch:   800  |  train loss: 0.1535793662
Epoch:   900  |  train loss: 0.1471745789
Epoch:  1000  |  train loss: 0.1405836493
Processing class: 24
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.2601291448
Epoch:   200  |  train loss: 0.2587860346
Epoch:   300  |  train loss: 0.2555706471
Epoch:   400  |  train loss: 0.2199538648
Epoch:   500  |  train loss: 0.2075580418
Epoch:   600  |  train loss: 0.1968566865
Epoch:   700  |  train loss: 0.1876726210
Epoch:   800  |  train loss: 0.1809652328
Epoch:   900  |  train loss: 0.1735763609
Epoch:  1000  |  train loss: 0.1672814488
Processing class: 25
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.2743078768
Epoch:   200  |  train loss: 0.2470703155
Epoch:   300  |  train loss: 0.2287513614
Epoch:   400  |  train loss: 0.2014457673
Epoch:   500  |  train loss: 0.1850875884
Epoch:   600  |  train loss: 0.1759997517
Epoch:   700  |  train loss: 0.1688057095
Epoch:   800  |  train loss: 0.1618090719
Epoch:   900  |  train loss: 0.1553236574
Epoch:  1000  |  train loss: 0.1519737661
Processing class: 26
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.2442937076
Epoch:   200  |  train loss: 0.2423309177
Epoch:   300  |  train loss: 0.2455341876
Epoch:   400  |  train loss: 0.2220605999
Epoch:   500  |  train loss: 0.2030131251
Epoch:   600  |  train loss: 0.1826233864
Epoch:   700  |  train loss: 0.1760604590
Epoch:   800  |  train loss: 0.1693948179
Epoch:   900  |  train loss: 0.1619112879
Epoch:  1000  |  train loss: 0.1569587708
Processing class: 27
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.2864578843
Epoch:   200  |  train loss: 0.2499810755
Epoch:   300  |  train loss: 0.2371720672
Epoch:   400  |  train loss: 0.2101049960
Epoch:   500  |  train loss: 0.1899999291
Epoch:   600  |  train loss: 0.1826586664
Epoch:   700  |  train loss: 0.1777511775
Epoch:   800  |  train loss: 0.1710868984
Epoch:   900  |  train loss: 0.1625759661
Epoch:  1000  |  train loss: 0.1543617696
Processing class: 28
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.2641690016
Epoch:   200  |  train loss: 0.2584637046
Epoch:   300  |  train loss: 0.2268698931
Epoch:   400  |  train loss: 0.2053251624
Epoch:   500  |  train loss: 0.1885758370
Epoch:   600  |  train loss: 0.1828239858
Epoch:   700  |  train loss: 0.1772391289
Epoch:   800  |  train loss: 0.1707339048
Epoch:   900  |  train loss: 0.1668187797
Epoch:  1000  |  train loss: 0.1616776407
Processing class: 29
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.2668825150
Epoch:   200  |  train loss: 0.2303715050
Epoch:   300  |  train loss: 0.2060820788
Epoch:   400  |  train loss: 0.1841543943
Epoch:   500  |  train loss: 0.1733375579
Epoch:   600  |  train loss: 0.1659389168
Epoch:   700  |  train loss: 0.1558050603
Epoch:   800  |  train loss: 0.1469171464
Epoch:   900  |  train loss: 0.1416804075
Epoch:  1000  |  train loss: 0.1376508772
Processing class: 30
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.2402234107
Epoch:   200  |  train loss: 0.2346103519
Epoch:   300  |  train loss: 0.1920994252
Epoch:   400  |  train loss: 0.1823235005
Epoch:   500  |  train loss: 0.1795437276
Epoch:   600  |  train loss: 0.1711802334
Epoch:   700  |  train loss: 0.1613182336
Epoch:   800  |  train loss: 0.1549263239
Epoch:   900  |  train loss: 0.1497432411
Epoch:  1000  |  train loss: 0.1464605063
Processing class: 31
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.2705286562
Epoch:   200  |  train loss: 0.2363124311
Epoch:   300  |  train loss: 0.2118837953
Epoch:   400  |  train loss: 0.2065250486
Epoch:   500  |  train loss: 0.1913184017
Epoch:   600  |  train loss: 0.1838502318
Epoch:   700  |  train loss: 0.1765926570
Epoch:   800  |  train loss: 0.1716820180
Epoch:   900  |  train loss: 0.1642400920
Epoch:  1000  |  train loss: 0.1587910056
Processing class: 32
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.3002030432
Epoch:   200  |  train loss: 0.2843564749
Epoch:   300  |  train loss: 0.2588876784
Epoch:   400  |  train loss: 0.2350221992
Epoch:   500  |  train loss: 0.2122049183
Epoch:   600  |  train loss: 0.1957193315
Epoch:   700  |  train loss: 0.1836848587
Epoch:   800  |  train loss: 0.1768620193
Epoch:   900  |  train loss: 0.1704443693
Epoch:  1000  |  train loss: 0.1652146846
Processing class: 33
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.2756856620
Epoch:   200  |  train loss: 0.2511486292
Epoch:   300  |  train loss: 0.2160342425
Epoch:   400  |  train loss: 0.1948018163
Epoch:   500  |  train loss: 0.1836321265
Epoch:   600  |  train loss: 0.1709950268
Epoch:   700  |  train loss: 0.1642560452
Epoch:   800  |  train loss: 0.1583266050
Epoch:   900  |  train loss: 0.1524138629
Epoch:  1000  |  train loss: 0.1461524576
Processing class: 34
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.2825483799
Epoch:   200  |  train loss: 0.2718794405
Epoch:   300  |  train loss: 0.2267951429
Epoch:   400  |  train loss: 0.2128277361
Epoch:   500  |  train loss: 0.1949866652
Epoch:   600  |  train loss: 0.1810885578
Epoch:   700  |  train loss: 0.1719824344
Epoch:   800  |  train loss: 0.1645505995
Epoch:   900  |  train loss: 0.1579885066
Epoch:  1000  |  train loss: 0.1523508221
Processing class: 35
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.3333628118
Epoch:   200  |  train loss: 0.2723026574
Epoch:   300  |  train loss: 0.2481347829
Epoch:   400  |  train loss: 0.2363064975
Epoch:   500  |  train loss: 0.2193583637
Epoch:   600  |  train loss: 0.2132046789
Epoch:   700  |  train loss: 0.2032337040
Epoch:   800  |  train loss: 0.1951307029
Epoch:   900  |  train loss: 0.1881363928
Epoch:  1000  |  train loss: 0.1818231046
Processing class: 36
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.2357417583
Epoch:   200  |  train loss: 0.2159613878
Epoch:   300  |  train loss: 0.1959410518
Epoch:   400  |  train loss: 0.1884537011
Epoch:   500  |  train loss: 0.1758319318
Epoch:   600  |  train loss: 0.1631768674
Epoch:   700  |  train loss: 0.1539356530
Epoch:   800  |  train loss: 0.1471314818
Epoch:   900  |  train loss: 0.1409172654
Epoch:  1000  |  train loss: 0.1357959151
Processing class: 37
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.2952987373
Epoch:   200  |  train loss: 0.2768375218
Epoch:   300  |  train loss: 0.2454620421
Epoch:   400  |  train loss: 0.2212160528
Epoch:   500  |  train loss: 0.2069619298
Epoch:   600  |  train loss: 0.1967176855
Epoch:   700  |  train loss: 0.1884160906
Epoch:   800  |  train loss: 0.1822666258
Epoch:   900  |  train loss: 0.1740912259
Epoch:  1000  |  train loss: 0.1684704095
Processing class: 38
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.2467770338
Epoch:   200  |  train loss: 0.2400094420
Epoch:   300  |  train loss: 0.2242708951
Epoch:   400  |  train loss: 0.2124343395
Epoch:   500  |  train loss: 0.1947118461
Epoch:   600  |  train loss: 0.1817271262
Epoch:   700  |  train loss: 0.1691933244
Epoch:   800  |  train loss: 0.1645154506
Epoch:   900  |  train loss: 0.1600382119
Epoch:  1000  |  train loss: 0.1564151019
Processing class: 39
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.2712099969
Epoch:   200  |  train loss: 0.2550755769
Epoch:   300  |  train loss: 0.2343997806
Epoch:   400  |  train loss: 0.2157822520
Epoch:   500  |  train loss: 0.1970960379
Epoch:   600  |  train loss: 0.1876199812
Epoch:   700  |  train loss: 0.1782628953
Epoch:   800  |  train loss: 0.1701065451
Epoch:   900  |  train loss: 0.1624905884
Epoch:  1000  |  train loss: 0.1560358733
Processing class: 40
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.2584639132
Epoch:   200  |  train loss: 0.2271760076
Epoch:   300  |  train loss: 0.2157703042
Epoch:   400  |  train loss: 0.2012949973
Epoch:   500  |  train loss: 0.1919786781
Epoch:   600  |  train loss: 0.1757085741
Epoch:   700  |  train loss: 0.1702133626
Epoch:   800  |  train loss: 0.1637980193
Epoch:   900  |  train loss: 0.1589525431
Epoch:  1000  |  train loss: 0.1533510625
Processing class: 41
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.2328563511
Epoch:   200  |  train loss: 0.2341667980
Epoch:   300  |  train loss: 0.1965061754
Epoch:   400  |  train loss: 0.1783475816
Epoch:   500  |  train loss: 0.1690753937
Epoch:   600  |  train loss: 0.1629626393
Epoch:   700  |  train loss: 0.1566064149
Epoch:   800  |  train loss: 0.1516544521
Epoch:   900  |  train loss: 0.1471247435
Epoch:  1000  |  train loss: 0.1435871422
Processing class: 42
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.2840052366
Epoch:   200  |  train loss: 0.2499217242
Epoch:   300  |  train loss: 0.2230692923
Epoch:   400  |  train loss: 0.2027724504
Epoch:   500  |  train loss: 0.1872166306
Epoch:   600  |  train loss: 0.1727644563
Epoch:   700  |  train loss: 0.1642444342
Epoch:   800  |  train loss: 0.1560957104
Epoch:   900  |  train loss: 0.1490065485
Epoch:  1000  |  train loss: 0.1430399299
Processing class: 43
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.2959817767
Epoch:   200  |  train loss: 0.2661080360
Epoch:   300  |  train loss: 0.2302739024
Epoch:   400  |  train loss: 0.2200892299
Epoch:   500  |  train loss: 0.2115582377
Epoch:   600  |  train loss: 0.2081801057
Epoch:   700  |  train loss: 0.2043609858
Epoch:   800  |  train loss: 0.2000485092
Epoch:   900  |  train loss: 0.1945207685
Epoch:  1000  |  train loss: 0.1891247541
Processing class: 44
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.3020401776
Epoch:   200  |  train loss: 0.2573860139
Epoch:   300  |  train loss: 0.2437053829
Epoch:   400  |  train loss: 0.2197369963
Epoch:   500  |  train loss: 0.2090727150
Epoch:   600  |  train loss: 0.2004945934
Epoch:   700  |  train loss: 0.1927240789
Epoch:   800  |  train loss: 0.1852895379
Epoch:   900  |  train loss: 0.1770905167
Epoch:  1000  |  train loss: 0.1703486532
Processing class: 45
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.2606055886
Epoch:   200  |  train loss: 0.2304060966
Epoch:   300  |  train loss: 0.2084372252
Epoch:   400  |  train loss: 0.2098810792
Epoch:   500  |  train loss: 0.2059714854
Epoch:   600  |  train loss: 0.1952702761
Epoch:   700  |  train loss: 0.1866995454
Epoch:   800  |  train loss: 0.1824288875
Epoch:   900  |  train loss: 0.1798817903
Epoch:  1000  |  train loss: 0.1764089137
Processing class: 46
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.2398818344
Epoch:   200  |  train loss: 0.2310898721
Epoch:   300  |  train loss: 0.2044602513
Epoch:   400  |  train loss: 0.2011518359
Epoch:   500  |  train loss: 0.1943549275
Epoch:   600  |  train loss: 0.1893172055
Epoch:   700  |  train loss: 0.1816000432
Epoch:   800  |  train loss: 0.1761106908
Epoch:   900  |  train loss: 0.1702810645
Epoch:  1000  |  train loss: 0.1655186027
Processing class: 47
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.2893737495
Epoch:   200  |  train loss: 0.2586565435
Epoch:   300  |  train loss: 0.2195874184
Epoch:   400  |  train loss: 0.2025757402
Epoch:   500  |  train loss: 0.1909057885
Epoch:   600  |  train loss: 0.1783943623
Epoch:   700  |  train loss: 0.1683505416
Epoch:   800  |  train loss: 0.1613886029
Epoch:   900  |  train loss: 0.1559659004
Epoch:  1000  |  train loss: 0.1518095195
Processing class: 48
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.2813374043
Epoch:   200  |  train loss: 0.2695869684
Epoch:   300  |  train loss: 0.2165469378
Epoch:   400  |  train loss: 0.2007035375
Epoch:   500  |  train loss: 0.1967437088
Epoch:   600  |  train loss: 0.1839041024
Epoch:   700  |  train loss: 0.1782715261
Epoch:   800  |  train loss: 0.1741276592
Epoch:   900  |  train loss: 0.1696173251
Epoch:  1000  |  train loss: 0.1646264434
Processing class: 49
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.2913373351
Epoch:   200  |  train loss: 0.2630863935
Epoch:   300  |  train loss: 0.2313554198
Epoch:   400  |  train loss: 0.2156411350
Epoch:   500  |  train loss: 0.1969332427
Epoch:   600  |  train loss: 0.1878202319
Epoch:   700  |  train loss: 0.1773489028
Epoch:   800  |  train loss: 0.1720852286
Epoch:   900  |  train loss: 0.1663887709
Epoch:  1000  |  train loss: 0.1603728145
/home/z1165703/FeCAM/models/base.py:87: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  vec = torch.tensor(vec).cuda()
2024-03-05 03:20:18,990 [trainer.py] => CNN: {'total': 83.44, '00-09': 87.7, '10-19': 79.6, '20-29': 84.9, '30-39': 81.0, '40-49': 84.0, 'old': 0, 'new': 83.44}
2024-03-05 03:20:19,170 [trainer.py] => No NME accuracy
2024-03-05 03:20:19,170 [trainer.py] => FeCAM: {'total': 82.9, '00-09': 86.2, '10-19': 79.7, '20-29': 85.0, '30-39': 80.6, '40-49': 83.0, 'old': 0, 'new': 82.9}
2024-03-05 03:20:19,170 [trainer.py] => CNN top1 curve: [83.44]
2024-03-05 03:20:19,170 [trainer.py] => CNN top5 curve: [96.5]
2024-03-05 03:20:19,170 [trainer.py] => FeCAM top1 curve: [82.9]
2024-03-05 03:20:19,170 [trainer.py] => FeCAM top5 curve: [95.84]

2024-03-05 03:20:19,185 [fecam.py] => Learning on 50-60
Processing class: 50
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.3826200902
Epoch:   200  |  train loss: 0.3253310382
Epoch:   300  |  train loss: 0.2962796390
Epoch:   400  |  train loss: 0.2684257209
Epoch:   500  |  train loss: 0.2428732157
Epoch:   600  |  train loss: 0.2257256746
Epoch:   700  |  train loss: 0.2124596059
Epoch:   800  |  train loss: 0.2018547863
Epoch:   900  |  train loss: 0.1941235602
Epoch:  1000  |  train loss: 0.1876025617
Processing class: 51
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.3867633224
Epoch:   200  |  train loss: 0.3304868519
Epoch:   300  |  train loss: 0.2942989767
Epoch:   400  |  train loss: 0.2737002432
Epoch:   500  |  train loss: 0.2451648265
Epoch:   600  |  train loss: 0.2239138603
Epoch:   700  |  train loss: 0.2104110599
Epoch:   800  |  train loss: 0.2021738529
Epoch:   900  |  train loss: 0.1943671644
Epoch:  1000  |  train loss: 0.1860353410
Processing class: 52
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.4451835096
Epoch:   200  |  train loss: 0.4161330521
Epoch:   300  |  train loss: 0.3666309178
Epoch:   400  |  train loss: 0.3342397213
Epoch:   500  |  train loss: 0.3086554587
Epoch:   600  |  train loss: 0.2861399591
Epoch:   700  |  train loss: 0.2685469449
Epoch:   800  |  train loss: 0.2542312026
Epoch:   900  |  train loss: 0.2404367089
Epoch:  1000  |  train loss: 0.2287046611
Processing class: 53
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.3220432818
Epoch:   200  |  train loss: 0.2764213443
Epoch:   300  |  train loss: 0.2574452877
Epoch:   400  |  train loss: 0.2359797060
Epoch:   500  |  train loss: 0.2265500486
Epoch:   600  |  train loss: 0.2183847517
Epoch:   700  |  train loss: 0.2091558516
Epoch:   800  |  train loss: 0.2013797760
Epoch:   900  |  train loss: 0.1953248829
Epoch:  1000  |  train loss: 0.1896848500
Processing class: 54
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.2960769594
Epoch:   200  |  train loss: 0.2617016494
Epoch:   300  |  train loss: 0.2292888314
Epoch:   400  |  train loss: 0.2140956104
Epoch:   500  |  train loss: 0.2007822812
Epoch:   600  |  train loss: 0.1902145714
Epoch:   700  |  train loss: 0.1809317917
Epoch:   800  |  train loss: 0.1736291498
Epoch:   900  |  train loss: 0.1675384820
Epoch:  1000  |  train loss: 0.1610584885
Processing class: 55
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.4810900748
Epoch:   200  |  train loss: 0.4496878743
Epoch:   300  |  train loss: 0.4163726866
Epoch:   400  |  train loss: 0.3839200497
Epoch:   500  |  train loss: 0.3548131108
Epoch:   600  |  train loss: 0.3265577555
Epoch:   700  |  train loss: 0.3058264852
Epoch:   800  |  train loss: 0.2878907621
Epoch:   900  |  train loss: 0.2720756829
Epoch:  1000  |  train loss: 0.2592303514
Processing class: 56
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.3687434256
Epoch:   200  |  train loss: 0.2996730506
Epoch:   300  |  train loss: 0.2491807878
Epoch:   400  |  train loss: 0.2210517824
Epoch:   500  |  train loss: 0.2010395378
Epoch:   600  |  train loss: 0.1856186867
Epoch:   700  |  train loss: 0.1746512085
Epoch:   800  |  train loss: 0.1649675578
Epoch:   900  |  train loss: 0.1581553042
Epoch:  1000  |  train loss: 0.1520582497
Processing class: 57
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.4800757408
Epoch:   200  |  train loss: 0.4405520856
Epoch:   300  |  train loss: 0.4063675225
Epoch:   400  |  train loss: 0.3731572390
Epoch:   500  |  train loss: 0.3434394419
Epoch:   600  |  train loss: 0.3241772175
Epoch:   700  |  train loss: 0.3079367220
Epoch:   800  |  train loss: 0.2917550743
Epoch:   900  |  train loss: 0.2764472783
Epoch:  1000  |  train loss: 0.2613305181
Processing class: 58
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.3962293684
Epoch:   200  |  train loss: 0.2947478235
Epoch:   300  |  train loss: 0.2585848987
Epoch:   400  |  train loss: 0.2254873097
Epoch:   500  |  train loss: 0.2104291826
Epoch:   600  |  train loss: 0.1988401800
Epoch:   700  |  train loss: 0.1879498899
Epoch:   800  |  train loss: 0.1795635700
Epoch:   900  |  train loss: 0.1715590835
Epoch:  1000  |  train loss: 0.1644986480
Processing class: 59
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.4401623011
Epoch:   200  |  train loss: 0.3849410236
Epoch:   300  |  train loss: 0.3359355509
Epoch:   400  |  train loss: 0.2986339390
Epoch:   500  |  train loss: 0.2723236024
Epoch:   600  |  train loss: 0.2568538249
Epoch:   700  |  train loss: 0.2432367206
Epoch:   800  |  train loss: 0.2331569135
Epoch:   900  |  train loss: 0.2231620461
Epoch:  1000  |  train loss: 0.2162001878
2024-03-05 03:26:04,509 [trainer.py] => CNN: {'total': 71.75, '00-09': 82.2, '10-19': 72.8, '20-29': 78.5, '30-39': 74.9, '40-49': 68.4, '50-59': 53.7, 'old': 75.36, 'new': 53.7}
2024-03-05 03:26:05,455 [trainer.py] => No NME accuracy
2024-03-05 03:26:05,455 [trainer.py] => FeCAM: {'total': 72.78, '00-09': 82.6, '10-19': 74.0, '20-29': 81.0, '30-39': 76.4, '40-49': 76.8, '50-59': 45.9, 'old': 78.16, 'new': 45.9}
2024-03-05 03:26:05,455 [trainer.py] => CNN top1 curve: [83.44, 71.75]
2024-03-05 03:26:05,455 [trainer.py] => CNN top5 curve: [96.5, 89.6]
2024-03-05 03:26:05,455 [trainer.py] => FeCAM top1 curve: [82.9, 72.78]
2024-03-05 03:26:05,455 [trainer.py] => FeCAM top5 curve: [95.84, 91.18]

2024-03-05 03:26:05,462 [fecam.py] => Learning on 60-70
Processing class: 60
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.4285791993
Epoch:   200  |  train loss: 0.3813509107
Epoch:   300  |  train loss: 0.3395575583
Epoch:   400  |  train loss: 0.3040211380
Epoch:   500  |  train loss: 0.2755815655
Epoch:   600  |  train loss: 0.2535468489
Epoch:   700  |  train loss: 0.2357457608
Epoch:   800  |  train loss: 0.2198586732
Epoch:   900  |  train loss: 0.2096330523
Epoch:  1000  |  train loss: 0.1995588213
Processing class: 61
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.3662742972
Epoch:   200  |  train loss: 0.2986006260
Epoch:   300  |  train loss: 0.2543152958
Epoch:   400  |  train loss: 0.2240793258
Epoch:   500  |  train loss: 0.2064647198
Epoch:   600  |  train loss: 0.1929638654
Epoch:   700  |  train loss: 0.1814610153
Epoch:   800  |  train loss: 0.1713465869
Epoch:   900  |  train loss: 0.1644958079
Epoch:  1000  |  train loss: 0.1582458526
Processing class: 62
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.4809622407
Epoch:   200  |  train loss: 0.4432189584
Epoch:   300  |  train loss: 0.4137572765
Epoch:   400  |  train loss: 0.3904670060
Epoch:   500  |  train loss: 0.3698153079
Epoch:   600  |  train loss: 0.3512061477
Epoch:   700  |  train loss: 0.3325753510
Epoch:   800  |  train loss: 0.3156429708
Epoch:   900  |  train loss: 0.3008480728
Epoch:  1000  |  train loss: 0.2874169767
Processing class: 63
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.4075586021
Epoch:   200  |  train loss: 0.3685263276
Epoch:   300  |  train loss: 0.2953879356
Epoch:   400  |  train loss: 0.2582676858
Epoch:   500  |  train loss: 0.2340489537
Epoch:   600  |  train loss: 0.2179019123
Epoch:   700  |  train loss: 0.2059821546
Epoch:   800  |  train loss: 0.1971868813
Epoch:   900  |  train loss: 0.1881311655
Epoch:  1000  |  train loss: 0.1806826293
Processing class: 64
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.3680682659
Epoch:   200  |  train loss: 0.2715559512
Epoch:   300  |  train loss: 0.2345305443
Epoch:   400  |  train loss: 0.2125241727
Epoch:   500  |  train loss: 0.1913805217
Epoch:   600  |  train loss: 0.1767626584
Epoch:   700  |  train loss: 0.1667224705
Epoch:   800  |  train loss: 0.1588502854
Epoch:   900  |  train loss: 0.1522500187
Epoch:  1000  |  train loss: 0.1450504392
Processing class: 65
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.3686010599
Epoch:   200  |  train loss: 0.2972682178
Epoch:   300  |  train loss: 0.2377979904
Epoch:   400  |  train loss: 0.2129933029
Epoch:   500  |  train loss: 0.1967094630
Epoch:   600  |  train loss: 0.1825533897
Epoch:   700  |  train loss: 0.1711327285
Epoch:   800  |  train loss: 0.1626330316
Epoch:   900  |  train loss: 0.1541146427
Epoch:  1000  |  train loss: 0.1479343295
Processing class: 66
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.4072078705
Epoch:   200  |  train loss: 0.3554205239
Epoch:   300  |  train loss: 0.3118296325
Epoch:   400  |  train loss: 0.2851133823
Epoch:   500  |  train loss: 0.2696758091
Epoch:   600  |  train loss: 0.2559589446
Epoch:   700  |  train loss: 0.2461046517
Epoch:   800  |  train loss: 0.2383868188
Epoch:   900  |  train loss: 0.2304836780
Epoch:  1000  |  train loss: 0.2240907371
Processing class: 67
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.4263281345
Epoch:   200  |  train loss: 0.3727607131
Epoch:   300  |  train loss: 0.3353927016
Epoch:   400  |  train loss: 0.3092404604
Epoch:   500  |  train loss: 0.2903946936
Epoch:   600  |  train loss: 0.2741144538
Epoch:   700  |  train loss: 0.2590940416
Epoch:   800  |  train loss: 0.2476397276
Epoch:   900  |  train loss: 0.2370168030
Epoch:  1000  |  train loss: 0.2284364104
Processing class: 68
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.3605423450
Epoch:   200  |  train loss: 0.3246028721
Epoch:   300  |  train loss: 0.2708751798
Epoch:   400  |  train loss: 0.2497159839
Epoch:   500  |  train loss: 0.2343429267
Epoch:   600  |  train loss: 0.2207619369
Epoch:   700  |  train loss: 0.2104328156
Epoch:   800  |  train loss: 0.2011330783
Epoch:   900  |  train loss: 0.1932136089
Epoch:  1000  |  train loss: 0.1867901951
Processing class: 69
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.2117933959
Epoch:   200  |  train loss: 0.1838468790
Epoch:   300  |  train loss: 0.1596483976
Epoch:   400  |  train loss: 0.1452772200
Epoch:   500  |  train loss: 0.1365950614
Epoch:   600  |  train loss: 0.1318096846
Epoch:   700  |  train loss: 0.1285043642
Epoch:   800  |  train loss: 0.1246843964
Epoch:   900  |  train loss: 0.1227119237
Epoch:  1000  |  train loss: 0.1197734281
2024-03-05 03:32:41,463 [trainer.py] => CNN: {'total': 64.66, '00-09': 75.0, '10-19': 70.1, '20-29': 77.1, '30-39': 70.6, '40-49': 64.0, '50-59': 45.8, '60-69': 50.0, 'old': 67.1, 'new': 50.0}
2024-03-05 03:32:41,463 [trainer.py] => No NME accuracy
2024-03-05 03:32:41,463 [trainer.py] => FeCAM: {'total': 67.27, '00-09': 79.9, '10-19': 70.6, '20-29': 79.1, '30-39': 71.9, '40-49': 74.1, '50-59': 41.2, '60-69': 54.1, 'old': 69.47, 'new': 54.1}
2024-03-05 03:32:41,463 [trainer.py] => CNN top1 curve: [83.44, 71.75, 64.66]
2024-03-05 03:32:41,463 [trainer.py] => CNN top5 curve: [96.5, 89.6, 86.54]
2024-03-05 03:32:41,463 [trainer.py] => FeCAM top1 curve: [82.9, 72.78, 67.27]
2024-03-05 03:32:41,464 [trainer.py] => FeCAM top5 curve: [95.84, 91.18, 87.21]

2024-03-05 03:32:41,467 [fecam.py] => Learning on 70-80
Processing class: 70
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.3881809652
Epoch:   200  |  train loss: 0.3440859675
Epoch:   300  |  train loss: 0.2899095714
Epoch:   400  |  train loss: 0.2635017872
Epoch:   500  |  train loss: 0.2413762718
Epoch:   600  |  train loss: 0.2227933526
Epoch:   700  |  train loss: 0.2065466255
Epoch:   800  |  train loss: 0.1935569406
Epoch:   900  |  train loss: 0.1864375502
Epoch:  1000  |  train loss: 0.1800132602
Processing class: 71
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.4360609591
Epoch:   200  |  train loss: 0.3880761623
Epoch:   300  |  train loss: 0.3255513608
Epoch:   400  |  train loss: 0.2917918444
Epoch:   500  |  train loss: 0.2658661962
Epoch:   600  |  train loss: 0.2458589554
Epoch:   700  |  train loss: 0.2282211661
Epoch:   800  |  train loss: 0.2147484481
Epoch:   900  |  train loss: 0.2028476149
Epoch:  1000  |  train loss: 0.1926083475
Processing class: 72
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.4200215876
Epoch:   200  |  train loss: 0.3731657624
Epoch:   300  |  train loss: 0.3267787158
Epoch:   400  |  train loss: 0.2959789515
Epoch:   500  |  train loss: 0.2763957679
Epoch:   600  |  train loss: 0.2579838574
Epoch:   700  |  train loss: 0.2433851659
Epoch:   800  |  train loss: 0.2300118119
Epoch:   900  |  train loss: 0.2184597731
Epoch:  1000  |  train loss: 0.2093999654
Processing class: 73
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.4076986909
Epoch:   200  |  train loss: 0.3502251625
Epoch:   300  |  train loss: 0.2891250432
Epoch:   400  |  train loss: 0.2662862659
Epoch:   500  |  train loss: 0.2529993743
Epoch:   600  |  train loss: 0.2411808670
Epoch:   700  |  train loss: 0.2302601993
Epoch:   800  |  train loss: 0.2202606201
Epoch:   900  |  train loss: 0.2112882316
Epoch:  1000  |  train loss: 0.2040253907
Processing class: 74
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.4674383044
Epoch:   200  |  train loss: 0.4460994720
Epoch:   300  |  train loss: 0.3974303424
Epoch:   400  |  train loss: 0.3583596349
Epoch:   500  |  train loss: 0.3251193583
Epoch:   600  |  train loss: 0.3015952766
Epoch:   700  |  train loss: 0.2843434513
Epoch:   800  |  train loss: 0.2678508490
Epoch:   900  |  train loss: 0.2548820436
Epoch:  1000  |  train loss: 0.2441319764
Processing class: 75
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.4182004809
Epoch:   200  |  train loss: 0.3717200398
Epoch:   300  |  train loss: 0.3211691260
Epoch:   400  |  train loss: 0.2963994443
Epoch:   500  |  train loss: 0.2743783236
Epoch:   600  |  train loss: 0.2567847013
Epoch:   700  |  train loss: 0.2427307904
Epoch:   800  |  train loss: 0.2317429841
Epoch:   900  |  train loss: 0.2229244530
Epoch:  1000  |  train loss: 0.2143221587
Processing class: 76
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.2394920230
Epoch:   200  |  train loss: 0.1994657129
Epoch:   300  |  train loss: 0.1704818755
Epoch:   400  |  train loss: 0.1574097484
Epoch:   500  |  train loss: 0.1501843810
Epoch:   600  |  train loss: 0.1426441595
Epoch:   700  |  train loss: 0.1387327969
Epoch:   800  |  train loss: 0.1321832329
Epoch:   900  |  train loss: 0.1286502793
Epoch:  1000  |  train loss: 0.1252722338
Processing class: 77
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.3668310523
Epoch:   200  |  train loss: 0.3220539689
Epoch:   300  |  train loss: 0.2784432650
Epoch:   400  |  train loss: 0.2520421803
Epoch:   500  |  train loss: 0.2249002546
Epoch:   600  |  train loss: 0.2043133765
Epoch:   700  |  train loss: 0.1902031541
Epoch:   800  |  train loss: 0.1792931646
Epoch:   900  |  train loss: 0.1711452812
Epoch:  1000  |  train loss: 0.1625378579
Processing class: 78
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.3852500260
Epoch:   200  |  train loss: 0.3085341454
Epoch:   300  |  train loss: 0.2822526515
Epoch:   400  |  train loss: 0.2552539647
Epoch:   500  |  train loss: 0.2312248349
Epoch:   600  |  train loss: 0.2157109112
Epoch:   700  |  train loss: 0.2047524422
Epoch:   800  |  train loss: 0.1944374770
Epoch:   900  |  train loss: 0.1878539920
Epoch:  1000  |  train loss: 0.1813631713
Processing class: 79
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.4808849394
Epoch:   200  |  train loss: 0.4283652365
Epoch:   300  |  train loss: 0.3917842329
Epoch:   400  |  train loss: 0.3558801889
Epoch:   500  |  train loss: 0.3310235620
Epoch:   600  |  train loss: 0.3090766668
Epoch:   700  |  train loss: 0.2903544009
Epoch:   800  |  train loss: 0.2724941015
Epoch:   900  |  train loss: 0.2577406704
Epoch:  1000  |  train loss: 0.2447813958
2024-03-05 03:40:21,553 [trainer.py] => CNN: {'total': 59.18, '00-09': 73.6, '10-19': 68.4, '20-29': 76.9, '30-39': 69.1, '40-49': 60.6, '50-59': 37.2, '60-69': 45.7, '70-79': 41.9, 'old': 61.64, 'new': 41.9}
2024-03-05 03:40:27,361 [trainer.py] => No NME accuracy
2024-03-05 03:40:27,361 [trainer.py] => FeCAM: {'total': 61.72, '00-09': 78.1, '10-19': 69.3, '20-29': 78.0, '30-39': 69.5, '40-49': 72.1, '50-59': 35.7, '60-69': 49.1, '70-79': 42.0, 'old': 64.54, 'new': 42.0}
2024-03-05 03:40:27,361 [trainer.py] => CNN top1 curve: [83.44, 71.75, 64.66, 59.18]
2024-03-05 03:40:27,361 [trainer.py] => CNN top5 curve: [96.5, 89.6, 86.54, 84.09]
2024-03-05 03:40:27,361 [trainer.py] => FeCAM top1 curve: [82.9, 72.78, 67.27, 61.72]
2024-03-05 03:40:27,362 [trainer.py] => FeCAM top5 curve: [95.84, 91.18, 87.21, 84.09]

2024-03-05 03:40:27,373 [fecam.py] => Learning on 80-90
Processing class: 80
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.2824213266
Epoch:   200  |  train loss: 0.2049699247
Epoch:   300  |  train loss: 0.1824707955
Epoch:   400  |  train loss: 0.1684794158
Epoch:   500  |  train loss: 0.1571353912
Epoch:   600  |  train loss: 0.1510379806
Epoch:   700  |  train loss: 0.1446215361
Epoch:   800  |  train loss: 0.1413477331
Epoch:   900  |  train loss: 0.1371941179
Epoch:  1000  |  train loss: 0.1334278196
Processing class: 81
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.3665814042
Epoch:   200  |  train loss: 0.3019653201
Epoch:   300  |  train loss: 0.2699070871
Epoch:   400  |  train loss: 0.2484523207
Epoch:   500  |  train loss: 0.2305456460
Epoch:   600  |  train loss: 0.2176732898
Epoch:   700  |  train loss: 0.2068434507
Epoch:   800  |  train loss: 0.1991933107
Epoch:   900  |  train loss: 0.1898957431
Epoch:  1000  |  train loss: 0.1816631317
Processing class: 82
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.3323974252
Epoch:   200  |  train loss: 0.2722093880
Epoch:   300  |  train loss: 0.2429595172
Epoch:   400  |  train loss: 0.2193692774
Epoch:   500  |  train loss: 0.2009799033
Epoch:   600  |  train loss: 0.1894546956
Epoch:   700  |  train loss: 0.1808409810
Epoch:   800  |  train loss: 0.1731017172
Epoch:   900  |  train loss: 0.1665651172
Epoch:  1000  |  train loss: 0.1602606654
Processing class: 83
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.3052353024
Epoch:   200  |  train loss: 0.2221651167
Epoch:   300  |  train loss: 0.1904248685
Epoch:   400  |  train loss: 0.1693310589
Epoch:   500  |  train loss: 0.1569076806
Epoch:   600  |  train loss: 0.1487607211
Epoch:   700  |  train loss: 0.1435214430
Epoch:   800  |  train loss: 0.1381086081
Epoch:   900  |  train loss: 0.1332151711
Epoch:  1000  |  train loss: 0.1295244232
Processing class: 84
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.4005635798
Epoch:   200  |  train loss: 0.3112568855
Epoch:   300  |  train loss: 0.2727583051
Epoch:   400  |  train loss: 0.2412517428
Epoch:   500  |  train loss: 0.2222312838
Epoch:   600  |  train loss: 0.2082758576
Epoch:   700  |  train loss: 0.1973959684
Epoch:   800  |  train loss: 0.1891295493
Epoch:   900  |  train loss: 0.1803334206
Epoch:  1000  |  train loss: 0.1726705819
Processing class: 85
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.4126386285
Epoch:   200  |  train loss: 0.3260137141
Epoch:   300  |  train loss: 0.2782194912
Epoch:   400  |  train loss: 0.2498434782
Epoch:   500  |  train loss: 0.2327444017
Epoch:   600  |  train loss: 0.2177525192
Epoch:   700  |  train loss: 0.2075616032
Epoch:   800  |  train loss: 0.1982280791
Epoch:   900  |  train loss: 0.1896637768
Epoch:  1000  |  train loss: 0.1827696502
Processing class: 86
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.4754188895
Epoch:   200  |  train loss: 0.4140568137
Epoch:   300  |  train loss: 0.3453976452
Epoch:   400  |  train loss: 0.3076152861
Epoch:   500  |  train loss: 0.2824259043
Epoch:   600  |  train loss: 0.2622302026
Epoch:   700  |  train loss: 0.2460377336
Epoch:   800  |  train loss: 0.2330610782
Epoch:   900  |  train loss: 0.2231235206
Epoch:  1000  |  train loss: 0.2150786489
Processing class: 87
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.3322222531
Epoch:   200  |  train loss: 0.2637115449
Epoch:   300  |  train loss: 0.2421894968
Epoch:   400  |  train loss: 0.2161126584
Epoch:   500  |  train loss: 0.2013788462
Epoch:   600  |  train loss: 0.1934806585
Epoch:   700  |  train loss: 0.1857164085
Epoch:   800  |  train loss: 0.1781851172
Epoch:   900  |  train loss: 0.1716435343
Epoch:  1000  |  train loss: 0.1666540921
Processing class: 88
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.3985240102
Epoch:   200  |  train loss: 0.3468439400
Epoch:   300  |  train loss: 0.3056522489
Epoch:   400  |  train loss: 0.2744196862
Epoch:   500  |  train loss: 0.2471850634
Epoch:   600  |  train loss: 0.2269538522
Epoch:   700  |  train loss: 0.2142262220
Epoch:   800  |  train loss: 0.2035590857
Epoch:   900  |  train loss: 0.1953883648
Epoch:  1000  |  train loss: 0.1869174004
Processing class: 89
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.3746110260
Epoch:   200  |  train loss: 0.3146668196
Epoch:   300  |  train loss: 0.2653448671
Epoch:   400  |  train loss: 0.2430581778
Epoch:   500  |  train loss: 0.2219254076
Epoch:   600  |  train loss: 0.2046901166
Epoch:   700  |  train loss: 0.1921358824
Epoch:   800  |  train loss: 0.1826249927
Epoch:   900  |  train loss: 0.1734409451
Epoch:  1000  |  train loss: 0.1658136308
2024-03-05 03:49:21,708 [trainer.py] => CNN: {'total': 54.08, '00-09': 67.9, '10-19': 63.6, '20-29': 73.2, '30-39': 68.8, '40-49': 57.2, '50-59': 32.6, '60-69': 40.9, '70-79': 38.1, '80-89': 44.4, 'old': 55.29, 'new': 44.4}
2024-03-05 03:49:21,708 [trainer.py] => No NME accuracy
2024-03-05 03:49:21,708 [trainer.py] => FeCAM: {'total': 57.66, '00-09': 76.1, '10-19': 65.8, '20-29': 75.3, '30-39': 67.9, '40-49': 68.5, '50-59': 33.0, '60-69': 45.0, '70-79': 39.0, '80-89': 48.3, 'old': 58.82, 'new': 48.3}
2024-03-05 03:49:21,708 [trainer.py] => CNN top1 curve: [83.44, 71.75, 64.66, 59.18, 54.08]
2024-03-05 03:49:21,709 [trainer.py] => CNN top5 curve: [96.5, 89.6, 86.54, 84.09, 81.74]
2024-03-05 03:49:21,709 [trainer.py] => FeCAM top1 curve: [82.9, 72.78, 67.27, 61.72, 57.66]
2024-03-05 03:49:21,709 [trainer.py] => FeCAM top5 curve: [95.84, 91.18, 87.21, 84.09, 81.63]

2024-03-05 03:49:21,719 [fecam.py] => Learning on 90-100
Processing class: 90
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.4107612014
Epoch:   200  |  train loss: 0.3677364230
Epoch:   300  |  train loss: 0.3130149961
Epoch:   400  |  train loss: 0.2737363458
Epoch:   500  |  train loss: 0.2464460105
Epoch:   600  |  train loss: 0.2243460864
Epoch:   700  |  train loss: 0.2070989370
Epoch:   800  |  train loss: 0.1963988185
Epoch:   900  |  train loss: 0.1883136243
Epoch:  1000  |  train loss: 0.1805725902
Processing class: 91
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.2314607292
Epoch:   200  |  train loss: 0.2084782511
Epoch:   300  |  train loss: 0.1841085285
Epoch:   400  |  train loss: 0.1740750432
Epoch:   500  |  train loss: 0.1609174222
Epoch:   600  |  train loss: 0.1518809199
Epoch:   700  |  train loss: 0.1410933703
Epoch:   800  |  train loss: 0.1325990245
Epoch:   900  |  train loss: 0.1265965894
Epoch:  1000  |  train loss: 0.1228792429
Processing class: 92
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.3616893768
Epoch:   200  |  train loss: 0.2785341680
Epoch:   300  |  train loss: 0.2567116708
Epoch:   400  |  train loss: 0.2367957920
Epoch:   500  |  train loss: 0.2134723872
Epoch:   600  |  train loss: 0.1920721591
Epoch:   700  |  train loss: 0.1786638290
Epoch:   800  |  train loss: 0.1696697086
Epoch:   900  |  train loss: 0.1624872088
Epoch:  1000  |  train loss: 0.1560914576
Processing class: 93
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.3882818699
Epoch:   200  |  train loss: 0.3008692741
Epoch:   300  |  train loss: 0.2581080377
Epoch:   400  |  train loss: 0.2356037706
Epoch:   500  |  train loss: 0.2155374736
Epoch:   600  |  train loss: 0.1992618114
Epoch:   700  |  train loss: 0.1877789825
Epoch:   800  |  train loss: 0.1782705486
Epoch:   900  |  train loss: 0.1704270929
Epoch:  1000  |  train loss: 0.1620546013
Processing class: 94
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.3481794715
Epoch:   200  |  train loss: 0.2984190166
Epoch:   300  |  train loss: 0.2476627737
Epoch:   400  |  train loss: 0.2220622331
Epoch:   500  |  train loss: 0.2047293991
Epoch:   600  |  train loss: 0.1927146763
Epoch:   700  |  train loss: 0.1828442007
Epoch:   800  |  train loss: 0.1726360589
Epoch:   900  |  train loss: 0.1656263053
Epoch:  1000  |  train loss: 0.1588439435
Processing class: 95
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.4324854136
Epoch:   200  |  train loss: 0.3911797523
Epoch:   300  |  train loss: 0.3480390966
Epoch:   400  |  train loss: 0.3060522914
Epoch:   500  |  train loss: 0.2745643914
Epoch:   600  |  train loss: 0.2549186140
Epoch:   700  |  train loss: 0.2377558798
Epoch:   800  |  train loss: 0.2243067205
Epoch:   900  |  train loss: 0.2129502445
Epoch:  1000  |  train loss: 0.2036048949
Processing class: 96
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.4408787072
Epoch:   200  |  train loss: 0.3891951859
Epoch:   300  |  train loss: 0.3317702174
Epoch:   400  |  train loss: 0.3016980231
Epoch:   500  |  train loss: 0.2760712624
Epoch:   600  |  train loss: 0.2580976248
Epoch:   700  |  train loss: 0.2429287195
Epoch:   800  |  train loss: 0.2292640418
Epoch:   900  |  train loss: 0.2161924422
Epoch:  1000  |  train loss: 0.2067355633
Processing class: 97
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.3835147083
Epoch:   200  |  train loss: 0.3033703685
Epoch:   300  |  train loss: 0.2566830188
Epoch:   400  |  train loss: 0.2353130221
Epoch:   500  |  train loss: 0.2167643219
Epoch:   600  |  train loss: 0.2031665534
Epoch:   700  |  train loss: 0.1914214283
Epoch:   800  |  train loss: 0.1827079982
Epoch:   900  |  train loss: 0.1741597712
Epoch:  1000  |  train loss: 0.1662829697
Processing class: 98
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.3261011720
Epoch:   200  |  train loss: 0.2778551996
Epoch:   300  |  train loss: 0.2326993257
Epoch:   400  |  train loss: 0.2089955598
Epoch:   500  |  train loss: 0.1922802776
Epoch:   600  |  train loss: 0.1791295946
Epoch:   700  |  train loss: 0.1698318839
Epoch:   800  |  train loss: 0.1632072777
Epoch:   900  |  train loss: 0.1571554363
Epoch:  1000  |  train loss: 0.1534565181
Processing class: 99
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.4046233594
Epoch:   200  |  train loss: 0.3474821091
Epoch:   300  |  train loss: 0.2842524827
Epoch:   400  |  train loss: 0.2501848280
Epoch:   500  |  train loss: 0.2254799545
Epoch:   600  |  train loss: 0.2103590965
Epoch:   700  |  train loss: 0.1991592765
Epoch:   800  |  train loss: 0.1903020412
Epoch:   900  |  train loss: 0.1822112441
Epoch:  1000  |  train loss: 0.1748610497
2024-03-05 03:59:51,217 [trainer.py] => CNN: {'total': 50.36, '00-09': 57.7, '10-19': 63.6, '20-29': 71.4, '30-39': 68.5, '40-49': 56.8, '50-59': 29.6, '60-69': 38.9, '70-79': 36.6, '80-89': 42.7, '90-99': 37.8, 'old': 51.76, 'new': 37.8}
2024-03-05 03:59:51,946 [trainer.py] => No NME accuracy
2024-03-05 03:59:51,946 [trainer.py] => FeCAM: {'total': 54.74, '00-09': 73.3, '10-19': 65.1, '20-29': 74.1, '30-39': 67.2, '40-49': 67.1, '50-59': 30.8, '60-69': 43.3, '70-79': 37.2, '80-89': 46.9, '90-99': 42.4, 'old': 56.11, 'new': 42.4}
2024-03-05 03:59:51,948 [trainer.py] => CNN top1 curve: [83.44, 71.75, 64.66, 59.18, 54.08, 50.36]
2024-03-05 03:59:51,948 [trainer.py] => CNN top5 curve: [96.5, 89.6, 86.54, 84.09, 81.74, 79.63]
2024-03-05 03:59:51,949 [trainer.py] => FeCAM top1 curve: [82.9, 72.78, 67.27, 61.72, 57.66, 54.74]
2024-03-05 03:59:51,949 [trainer.py] => FeCAM top5 curve: [95.84, 91.18, 87.21, 84.09, 81.63, 79.21]

=========================================
2024-03-05 04:00:08,472 [trainer.py] => config: ./exps/FeCAM_cifar100.json
2024-03-05 04:00:08,472 [trainer.py] => prefix: train
2024-03-05 04:00:08,472 [trainer.py] => dataset: cifar100
2024-03-05 04:00:08,472 [trainer.py] => memory_size: 0
2024-03-05 04:00:08,472 [trainer.py] => shuffle: True
2024-03-05 04:00:08,472 [trainer.py] => init_cls: 50
2024-03-05 04:00:08,472 [trainer.py] => increment: 10
2024-03-05 04:00:08,473 [trainer.py] => model_name: fecam
2024-03-05 04:00:08,473 [trainer.py] => convnet_type: resnet18
2024-03-05 04:00:08,473 [trainer.py] => device: [device(type='cuda', index=0)]
2024-03-05 04:00:08,473 [trainer.py] => seed: 1993
2024-03-05 04:00:08,473 [trainer.py] => init_epochs: 200
2024-03-05 04:00:08,473 [trainer.py] => init_lr: 0.1
2024-03-05 04:00:08,473 [trainer.py] => init_weight_decay: 0.0005
2024-03-05 04:00:08,473 [trainer.py] => batch_size: 128
2024-03-05 04:00:08,473 [trainer.py] => num_workers: 8
2024-03-05 04:00:08,473 [trainer.py] => T: 5
2024-03-05 04:00:08,473 [trainer.py] => beta: 0.5
2024-03-05 04:00:08,473 [trainer.py] => alpha1: 1
2024-03-05 04:00:08,473 [trainer.py] => alpha2: 1
2024-03-05 04:00:08,473 [trainer.py] => ncm: False
2024-03-05 04:00:08,473 [trainer.py] => tukey: False
2024-03-05 04:00:08,473 [trainer.py] => diagonal: False
2024-03-05 04:00:08,473 [trainer.py] => per_class: True
2024-03-05 04:00:08,473 [trainer.py] => full_cov: True
2024-03-05 04:00:08,473 [trainer.py] => shrink: True
2024-03-05 04:00:08,473 [trainer.py] => norm_cov: False
2024-03-05 04:00:08,473 [trainer.py] => vecnorm: False
2024-03-05 04:00:08,473 [trainer.py] => ae_type: wae
2024-03-05 04:00:08,473 [trainer.py] => epochs: 1000
2024-03-05 04:00:08,473 [trainer.py] => ae_latent_dim: 32
2024-03-05 04:00:08,474 [trainer.py] => wae_sigma: 5
2024-03-05 04:00:08,474 [trainer.py] => wae_C: 0.5
Files already downloaded and verified
Files already downloaded and verified
2024-03-05 04:00:10,127 [data_manager.py] => [68, 56, 78, 8, 23, 84, 90, 65, 74, 76, 40, 89, 3, 92, 55, 9, 26, 80, 43, 38, 58, 70, 77, 1, 85, 19, 17, 50, 28, 53, 13, 81, 45, 82, 6, 59, 83, 16, 15, 44, 91, 41, 72, 60, 79, 52, 20, 10, 31, 54, 37, 95, 14, 71, 96, 98, 97, 2, 64, 66, 42, 22, 35, 86, 24, 34, 87, 21, 99, 0, 88, 27, 18, 94, 11, 12, 47, 25, 30, 46, 62, 69, 36, 61, 7, 63, 75, 5, 32, 4, 51, 48, 73, 93, 39, 67, 29, 49, 57, 33]
2024-03-05 04:00:10,434 [fecam.py] => Learning on 0-50
/home/z1165703/miniconda3/lib/python3.11/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
/home/z1165703/miniconda3/lib/python3.11/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
Processing class: 0
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.2823993623
Epoch:   200  |  train loss: 0.2610126674
Epoch:   300  |  train loss: 0.2599812269
Epoch:   400  |  train loss: 0.2516556919
Epoch:   500  |  train loss: 0.2466966599
Epoch:   600  |  train loss: 0.2451525241
Epoch:   700  |  train loss: 0.2424269557
Epoch:   800  |  train loss: 0.2413014233
Epoch:   900  |  train loss: 0.2379300147
Epoch:  1000  |  train loss: 0.2346421719
Processing class: 1
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.3109379947
Epoch:   200  |  train loss: 0.3068177521
Epoch:   300  |  train loss: 0.2946201026
Epoch:   400  |  train loss: 0.2864451289
Epoch:   500  |  train loss: 0.2762223721
Epoch:   600  |  train loss: 0.2746214569
Epoch:   700  |  train loss: 0.2689832866
Epoch:   800  |  train loss: 0.2649354756
Epoch:   900  |  train loss: 0.2624117911
Epoch:  1000  |  train loss: 0.2569528669
Processing class: 2
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.3269503951
Epoch:   200  |  train loss: 0.3106831670
Epoch:   300  |  train loss: 0.2955783963
Epoch:   400  |  train loss: 0.2817825854
Epoch:   500  |  train loss: 0.2687804073
Epoch:   600  |  train loss: 0.2597942948
Epoch:   700  |  train loss: 0.2514446348
Epoch:   800  |  train loss: 0.2468491077
Epoch:   900  |  train loss: 0.2395295709
Epoch:  1000  |  train loss: 0.2333931446
Processing class: 3
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.2792312682
Epoch:   200  |  train loss: 0.2803884983
Epoch:   300  |  train loss: 0.2720074594
Epoch:   400  |  train loss: 0.2576802135
Epoch:   500  |  train loss: 0.2491234928
Epoch:   600  |  train loss: 0.2466872543
Epoch:   700  |  train loss: 0.2385156214
Epoch:   800  |  train loss: 0.2333143175
Epoch:   900  |  train loss: 0.2269984782
Epoch:  1000  |  train loss: 0.2214417040
Processing class: 4
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.3102838993
Epoch:   200  |  train loss: 0.3045185089
Epoch:   300  |  train loss: 0.2871047676
Epoch:   400  |  train loss: 0.2758346736
Epoch:   500  |  train loss: 0.2674532712
Epoch:   600  |  train loss: 0.2569153100
Epoch:   700  |  train loss: 0.2541281939
Epoch:   800  |  train loss: 0.2506107032
Epoch:   900  |  train loss: 0.2465403974
Epoch:  1000  |  train loss: 0.2428434253
Processing class: 5
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.3309724152
Epoch:   200  |  train loss: 0.3069190621
Epoch:   300  |  train loss: 0.2953827560
Epoch:   400  |  train loss: 0.2800562620
Epoch:   500  |  train loss: 0.2742782891
Epoch:   600  |  train loss: 0.2690602183
Epoch:   700  |  train loss: 0.2596927673
Epoch:   800  |  train loss: 0.2519095540
Epoch:   900  |  train loss: 0.2454615086
Epoch:  1000  |  train loss: 0.2417635918
Processing class: 6
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.3075064540
Epoch:   200  |  train loss: 0.3074091077
Epoch:   300  |  train loss: 0.2920902610
Epoch:   400  |  train loss: 0.2781754375
Epoch:   500  |  train loss: 0.2721891701
Epoch:   600  |  train loss: 0.2679810226
Epoch:   700  |  train loss: 0.2635413945
Epoch:   800  |  train loss: 0.2580576360
Epoch:   900  |  train loss: 0.2499225706
Epoch:  1000  |  train loss: 0.2466976494
Processing class: 7
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.3282253802
Epoch:   200  |  train loss: 0.3120401561
Epoch:   300  |  train loss: 0.3029282033
Epoch:   400  |  train loss: 0.2876080871
Epoch:   500  |  train loss: 0.2762989402
Epoch:   600  |  train loss: 0.2693396628
Epoch:   700  |  train loss: 0.2635350168
Epoch:   800  |  train loss: 0.2596607655
Epoch:   900  |  train loss: 0.2522006303
Epoch:  1000  |  train loss: 0.2480257779
Processing class: 8
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.3180085957
Epoch:   200  |  train loss: 0.3075926781
Epoch:   300  |  train loss: 0.2859117627
Epoch:   400  |  train loss: 0.2775947273
Epoch:   500  |  train loss: 0.2695505142
Epoch:   600  |  train loss: 0.2607174754
Epoch:   700  |  train loss: 0.2561669022
Epoch:   800  |  train loss: 0.2496707737
Epoch:   900  |  train loss: 0.2456791759
Epoch:  1000  |  train loss: 0.2393850297
Processing class: 9
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.3044194579
Epoch:   200  |  train loss: 0.3037395537
Epoch:   300  |  train loss: 0.2980971158
Epoch:   400  |  train loss: 0.2863180995
Epoch:   500  |  train loss: 0.2825112581
Epoch:   600  |  train loss: 0.2780152977
Epoch:   700  |  train loss: 0.2760159671
Epoch:   800  |  train loss: 0.2733136654
Epoch:   900  |  train loss: 0.2702721298
Epoch:  1000  |  train loss: 0.2653599799
Processing class: 10
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.3271362722
Epoch:   200  |  train loss: 0.3100150704
Epoch:   300  |  train loss: 0.3022235811
Epoch:   400  |  train loss: 0.2943234563
Epoch:   500  |  train loss: 0.2820358276
Epoch:   600  |  train loss: 0.2757147014
Epoch:   700  |  train loss: 0.2700494945
Epoch:   800  |  train loss: 0.2642633796
Epoch:   900  |  train loss: 0.2606742114
Epoch:  1000  |  train loss: 0.2569041491
Processing class: 11
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.3317045033
Epoch:   200  |  train loss: 0.3085927427
Epoch:   300  |  train loss: 0.2984055817
Epoch:   400  |  train loss: 0.2880066931
Epoch:   500  |  train loss: 0.2765412748
Epoch:   600  |  train loss: 0.2715502203
Epoch:   700  |  train loss: 0.2671523213
Epoch:   800  |  train loss: 0.2616793603
Epoch:   900  |  train loss: 0.2563523650
Epoch:  1000  |  train loss: 0.2517068386
Processing class: 12
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.3178490460
Epoch:   200  |  train loss: 0.3104799390
Epoch:   300  |  train loss: 0.2970133364
Epoch:   400  |  train loss: 0.2825726092
Epoch:   500  |  train loss: 0.2734483123
Epoch:   600  |  train loss: 0.2635207832
Epoch:   700  |  train loss: 0.2575721651
Epoch:   800  |  train loss: 0.2540914416
Epoch:   900  |  train loss: 0.2502243906
Epoch:  1000  |  train loss: 0.2452877551
Processing class: 13
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.3094301462
Epoch:   200  |  train loss: 0.2927818537
Epoch:   300  |  train loss: 0.2788679063
Epoch:   400  |  train loss: 0.2659306169
Epoch:   500  |  train loss: 0.2577469915
Epoch:   600  |  train loss: 0.2524119526
Epoch:   700  |  train loss: 0.2474985749
Epoch:   800  |  train loss: 0.2447563827
Epoch:   900  |  train loss: 0.2420346349
Epoch:  1000  |  train loss: 0.2384945750
Processing class: 14
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.3425461054
Epoch:   200  |  train loss: 0.3413738012
Epoch:   300  |  train loss: 0.3274140835
Epoch:   400  |  train loss: 0.3137909353
Epoch:   500  |  train loss: 0.3050657868
Epoch:   600  |  train loss: 0.2972454250
Epoch:   700  |  train loss: 0.2931294143
Epoch:   800  |  train loss: 0.2901281059
Epoch:   900  |  train loss: 0.2862817943
Epoch:  1000  |  train loss: 0.2819439530
Processing class: 15
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.3022802591
Epoch:   200  |  train loss: 0.2947620213
Epoch:   300  |  train loss: 0.2762911856
Epoch:   400  |  train loss: 0.2682437301
Epoch:   500  |  train loss: 0.2615975380
Epoch:   600  |  train loss: 0.2559015214
Epoch:   700  |  train loss: 0.2539393306
Epoch:   800  |  train loss: 0.2489586443
Epoch:   900  |  train loss: 0.2484161168
Epoch:  1000  |  train loss: 0.2447448790
Processing class: 16
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.3119991541
Epoch:   200  |  train loss: 0.2899501383
Epoch:   300  |  train loss: 0.2810516298
Epoch:   400  |  train loss: 0.2732318878
Epoch:   500  |  train loss: 0.2712860346
Epoch:   600  |  train loss: 0.2656557262
Epoch:   700  |  train loss: 0.2597627163
Epoch:   800  |  train loss: 0.2568943173
Epoch:   900  |  train loss: 0.2510358155
Epoch:  1000  |  train loss: 0.2507372975
Processing class: 17
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.3089259446
Epoch:   200  |  train loss: 0.3077641726
Epoch:   300  |  train loss: 0.3050390899
Epoch:   400  |  train loss: 0.2936305046
Epoch:   500  |  train loss: 0.2768424213
Epoch:   600  |  train loss: 0.2708543479
Epoch:   700  |  train loss: 0.2654908597
Epoch:   800  |  train loss: 0.2607125938
Epoch:   900  |  train loss: 0.2565895706
Epoch:  1000  |  train loss: 0.2532243967
Processing class: 18
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.3076676071
Epoch:   200  |  train loss: 0.3063486636
Epoch:   300  |  train loss: 0.2764968872
Epoch:   400  |  train loss: 0.2654869318
Epoch:   500  |  train loss: 0.2529413551
Epoch:   600  |  train loss: 0.2424310714
Epoch:   700  |  train loss: 0.2392163903
Epoch:   800  |  train loss: 0.2329663068
Epoch:   900  |  train loss: 0.2290693849
Epoch:  1000  |  train loss: 0.2255863458
Processing class: 19
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.2918567061
Epoch:   200  |  train loss: 0.2822086453
Epoch:   300  |  train loss: 0.2703161001
Epoch:   400  |  train loss: 0.2664179683
Epoch:   500  |  train loss: 0.2547882766
Epoch:   600  |  train loss: 0.2504966199
Epoch:   700  |  train loss: 0.2463964283
Epoch:   800  |  train loss: 0.2404014826
Epoch:   900  |  train loss: 0.2373687714
Epoch:  1000  |  train loss: 0.2337160826
Processing class: 20
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.3086226404
Epoch:   200  |  train loss: 0.2929659009
Epoch:   300  |  train loss: 0.2888874054
Epoch:   400  |  train loss: 0.2759886861
Epoch:   500  |  train loss: 0.2741379678
Epoch:   600  |  train loss: 0.2668103993
Epoch:   700  |  train loss: 0.2605792582
Epoch:   800  |  train loss: 0.2582511604
Epoch:   900  |  train loss: 0.2543717951
Epoch:  1000  |  train loss: 0.2532623589
Processing class: 21
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.3126097322
Epoch:   200  |  train loss: 0.2965759337
Epoch:   300  |  train loss: 0.2699626327
Epoch:   400  |  train loss: 0.2567369819
Epoch:   500  |  train loss: 0.2482169241
Epoch:   600  |  train loss: 0.2432457179
Epoch:   700  |  train loss: 0.2388011277
Epoch:   800  |  train loss: 0.2334966183
Epoch:   900  |  train loss: 0.2296590269
Epoch:  1000  |  train loss: 0.2238323599
Processing class: 22
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.3268832028
Epoch:   200  |  train loss: 0.3242060065
Epoch:   300  |  train loss: 0.3173188329
Epoch:   400  |  train loss: 0.2991266549
Epoch:   500  |  train loss: 0.2906784296
Epoch:   600  |  train loss: 0.2810317457
Epoch:   700  |  train loss: 0.2737421811
Epoch:   800  |  train loss: 0.2664404809
Epoch:   900  |  train loss: 0.2612087190
Epoch:  1000  |  train loss: 0.2595819592
Processing class: 23
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.3016305923
Epoch:   200  |  train loss: 0.3002490699
Epoch:   300  |  train loss: 0.2891865313
Epoch:   400  |  train loss: 0.2775448740
Epoch:   500  |  train loss: 0.2709236562
Epoch:   600  |  train loss: 0.2597827256
Epoch:   700  |  train loss: 0.2532778770
Epoch:   800  |  train loss: 0.2461524129
Epoch:   900  |  train loss: 0.2419914335
Epoch:  1000  |  train loss: 0.2354934216
Processing class: 24
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.3097882509
Epoch:   200  |  train loss: 0.3083209991
Epoch:   300  |  train loss: 0.3078410208
Epoch:   400  |  train loss: 0.2930649519
Epoch:   500  |  train loss: 0.2851076901
Epoch:   600  |  train loss: 0.2797102153
Epoch:   700  |  train loss: 0.2719419539
Epoch:   800  |  train loss: 0.2685159624
Epoch:   900  |  train loss: 0.2646494389
Epoch:  1000  |  train loss: 0.2591504455
Processing class: 25
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.3179984808
Epoch:   200  |  train loss: 0.3055390060
Epoch:   300  |  train loss: 0.2966186523
Epoch:   400  |  train loss: 0.2795913279
Epoch:   500  |  train loss: 0.2698375165
Epoch:   600  |  train loss: 0.2624999285
Epoch:   700  |  train loss: 0.2591890842
Epoch:   800  |  train loss: 0.2532107711
Epoch:   900  |  train loss: 0.2477178901
Epoch:  1000  |  train loss: 0.2454706520
Processing class: 26
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.2998873711
Epoch:   200  |  train loss: 0.2998127937
Epoch:   300  |  train loss: 0.3017852485
Epoch:   400  |  train loss: 0.2930913985
Epoch:   500  |  train loss: 0.2846237540
Epoch:   600  |  train loss: 0.2701196373
Epoch:   700  |  train loss: 0.2660122395
Epoch:   800  |  train loss: 0.2625682712
Epoch:   900  |  train loss: 0.2567293465
Epoch:  1000  |  train loss: 0.2542839527
Processing class: 27
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.3274029315
Epoch:   200  |  train loss: 0.3085145712
Epoch:   300  |  train loss: 0.3032095253
Epoch:   400  |  train loss: 0.2884530663
Epoch:   500  |  train loss: 0.2768417835
Epoch:   600  |  train loss: 0.2723160625
Epoch:   700  |  train loss: 0.2690257072
Epoch:   800  |  train loss: 0.2644855440
Epoch:   900  |  train loss: 0.2593129694
Epoch:  1000  |  train loss: 0.2511861742
Processing class: 28
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.3145239115
Epoch:   200  |  train loss: 0.3121372461
Epoch:   300  |  train loss: 0.2966969430
Epoch:   400  |  train loss: 0.2851251721
Epoch:   500  |  train loss: 0.2761632562
Epoch:   600  |  train loss: 0.2723653734
Epoch:   700  |  train loss: 0.2697713673
Epoch:   800  |  train loss: 0.2663308561
Epoch:   900  |  train loss: 0.2637763202
Epoch:  1000  |  train loss: 0.2595362186
Processing class: 29
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.3147520840
Epoch:   200  |  train loss: 0.2978881180
Epoch:   300  |  train loss: 0.2834749877
Epoch:   400  |  train loss: 0.2686574459
Epoch:   500  |  train loss: 0.2619504988
Epoch:   600  |  train loss: 0.2563761055
Epoch:   700  |  train loss: 0.2487359375
Epoch:   800  |  train loss: 0.2409171373
Epoch:   900  |  train loss: 0.2366418034
Epoch:  1000  |  train loss: 0.2325660497
Processing class: 30
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.2981785297
Epoch:   200  |  train loss: 0.2964633822
Epoch:   300  |  train loss: 0.2768481195
Epoch:   400  |  train loss: 0.2723883510
Epoch:   500  |  train loss: 0.2701521635
Epoch:   600  |  train loss: 0.2645840824
Epoch:   700  |  train loss: 0.2569899797
Epoch:   800  |  train loss: 0.2535880536
Epoch:   900  |  train loss: 0.2504181713
Epoch:  1000  |  train loss: 0.2463085562
Processing class: 31
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.3173129678
Epoch:   200  |  train loss: 0.3021236360
Epoch:   300  |  train loss: 0.2888322234
Epoch:   400  |  train loss: 0.2865837991
Epoch:   500  |  train loss: 0.2786486924
Epoch:   600  |  train loss: 0.2748819053
Epoch:   700  |  train loss: 0.2692967653
Epoch:   800  |  train loss: 0.2662492692
Epoch:   900  |  train loss: 0.2621090055
Epoch:  1000  |  train loss: 0.2569346398
Processing class: 32
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.3345585525
Epoch:   200  |  train loss: 0.3300265312
Epoch:   300  |  train loss: 0.3182372093
Epoch:   400  |  train loss: 0.3044127882
Epoch:   500  |  train loss: 0.2936040580
Epoch:   600  |  train loss: 0.2811370909
Epoch:   700  |  train loss: 0.2744815528
Epoch:   800  |  train loss: 0.2697551161
Epoch:   900  |  train loss: 0.2645217597
Epoch:  1000  |  train loss: 0.2598628432
Processing class: 33
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.3178601921
Epoch:   200  |  train loss: 0.3133394182
Epoch:   300  |  train loss: 0.2919264019
Epoch:   400  |  train loss: 0.2786232412
Epoch:   500  |  train loss: 0.2702338457
Epoch:   600  |  train loss: 0.2632258117
Epoch:   700  |  train loss: 0.2569794267
Epoch:   800  |  train loss: 0.2526393265
Epoch:   900  |  train loss: 0.2475766182
Epoch:  1000  |  train loss: 0.2421557248
Processing class: 34
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.3238064885
Epoch:   200  |  train loss: 0.3180802226
Epoch:   300  |  train loss: 0.2987277687
Epoch:   400  |  train loss: 0.2899324000
Epoch:   500  |  train loss: 0.2790749609
Epoch:   600  |  train loss: 0.2713849545
Epoch:   700  |  train loss: 0.2634681344
Epoch:   800  |  train loss: 0.2593320221
Epoch:   900  |  train loss: 0.2548104197
Epoch:  1000  |  train loss: 0.2503752619
Processing class: 35
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.3521866977
Epoch:   200  |  train loss: 0.3259633839
Epoch:   300  |  train loss: 0.3155551732
Epoch:   400  |  train loss: 0.3088518500
Epoch:   500  |  train loss: 0.2986008525
Epoch:   600  |  train loss: 0.2959418952
Epoch:   700  |  train loss: 0.2899936736
Epoch:   800  |  train loss: 0.2844756663
Epoch:   900  |  train loss: 0.2801989913
Epoch:  1000  |  train loss: 0.2750942469
Processing class: 36
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.2894529939
Epoch:   200  |  train loss: 0.2803067923
Epoch:   300  |  train loss: 0.2697558880
Epoch:   400  |  train loss: 0.2649897426
Epoch:   500  |  train loss: 0.2582592875
Epoch:   600  |  train loss: 0.2492597729
Epoch:   700  |  train loss: 0.2416180640
Epoch:   800  |  train loss: 0.2378693730
Epoch:   900  |  train loss: 0.2312457740
Epoch:  1000  |  train loss: 0.2276400119
Processing class: 37
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.3340462744
Epoch:   200  |  train loss: 0.3259500504
Epoch:   300  |  train loss: 0.3099135339
Epoch:   400  |  train loss: 0.2977745295
Epoch:   500  |  train loss: 0.2897303700
Epoch:   600  |  train loss: 0.2818884134
Epoch:   700  |  train loss: 0.2759640455
Epoch:   800  |  train loss: 0.2722759664
Epoch:   900  |  train loss: 0.2652078331
Epoch:  1000  |  train loss: 0.2617680609
Processing class: 38
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.3000444531
Epoch:   200  |  train loss: 0.2975665331
Epoch:   300  |  train loss: 0.2910121560
Epoch:   400  |  train loss: 0.2865956128
Epoch:   500  |  train loss: 0.2761191249
Epoch:   600  |  train loss: 0.2667278886
Epoch:   700  |  train loss: 0.2580764472
Epoch:   800  |  train loss: 0.2559603989
Epoch:   900  |  train loss: 0.2510124713
Epoch:  1000  |  train loss: 0.2491798103
Processing class: 39
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.3151072621
Epoch:   200  |  train loss: 0.3072147191
Epoch:   300  |  train loss: 0.2981592715
Epoch:   400  |  train loss: 0.2889054656
Epoch:   500  |  train loss: 0.2779392183
Epoch:   600  |  train loss: 0.2723887801
Epoch:   700  |  train loss: 0.2663239568
Epoch:   800  |  train loss: 0.2611659944
Epoch:   900  |  train loss: 0.2553338408
Epoch:  1000  |  train loss: 0.2505174160
Processing class: 40
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.3078036904
Epoch:   200  |  train loss: 0.2894393325
Epoch:   300  |  train loss: 0.2857257187
Epoch:   400  |  train loss: 0.2786455095
Epoch:   500  |  train loss: 0.2730907559
Epoch:   600  |  train loss: 0.2615759671
Epoch:   700  |  train loss: 0.2587501526
Epoch:   800  |  train loss: 0.2543969244
Epoch:   900  |  train loss: 0.2504480928
Epoch:  1000  |  train loss: 0.2484160185
Processing class: 41
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.2901278257
Epoch:   200  |  train loss: 0.2914567649
Epoch:   300  |  train loss: 0.2722926736
Epoch:   400  |  train loss: 0.2593046755
Epoch:   500  |  train loss: 0.2526410997
Epoch:   600  |  train loss: 0.2479316890
Epoch:   700  |  train loss: 0.2443350405
Epoch:   800  |  train loss: 0.2401328504
Epoch:   900  |  train loss: 0.2364699125
Epoch:  1000  |  train loss: 0.2334836364
Processing class: 42
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.3245023549
Epoch:   200  |  train loss: 0.3109355688
Epoch:   300  |  train loss: 0.2974214733
Epoch:   400  |  train loss: 0.2848076642
Epoch:   500  |  train loss: 0.2750566483
Epoch:   600  |  train loss: 0.2651460588
Epoch:   700  |  train loss: 0.2589929461
Epoch:   800  |  train loss: 0.2523951262
Epoch:   900  |  train loss: 0.2457673699
Epoch:  1000  |  train loss: 0.2407185823
Processing class: 43
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.3326701641
Epoch:   200  |  train loss: 0.3196559191
Epoch:   300  |  train loss: 0.3004709125
Epoch:   400  |  train loss: 0.2960373878
Epoch:   500  |  train loss: 0.2915686071
Epoch:   600  |  train loss: 0.2893218875
Epoch:   700  |  train loss: 0.2880546510
Epoch:   800  |  train loss: 0.2850444853
Epoch:   900  |  train loss: 0.2799087048
Epoch:  1000  |  train loss: 0.2779223502
Processing class: 44
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.3343323171
Epoch:   200  |  train loss: 0.3150866807
Epoch:   300  |  train loss: 0.3074951470
Epoch:   400  |  train loss: 0.2949289560
Epoch:   500  |  train loss: 0.2891836762
Epoch:   600  |  train loss: 0.2838593721
Epoch:   700  |  train loss: 0.2799683213
Epoch:   800  |  train loss: 0.2745620370
Epoch:   900  |  train loss: 0.2684622109
Epoch:  1000  |  train loss: 0.2646478057
Processing class: 45
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.3098225951
Epoch:   200  |  train loss: 0.2985377431
Epoch:   300  |  train loss: 0.2833724439
Epoch:   400  |  train loss: 0.2860614479
Epoch:   500  |  train loss: 0.2834252059
Epoch:   600  |  train loss: 0.2792322636
Epoch:   700  |  train loss: 0.2740343690
Epoch:   800  |  train loss: 0.2709559977
Epoch:   900  |  train loss: 0.2699743152
Epoch:  1000  |  train loss: 0.2681033254
Processing class: 46
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.2961261332
Epoch:   200  |  train loss: 0.2895310283
Epoch:   300  |  train loss: 0.2762427926
Epoch:   400  |  train loss: 0.2752086341
Epoch:   500  |  train loss: 0.2719714820
Epoch:   600  |  train loss: 0.2698324561
Epoch:   700  |  train loss: 0.2664620131
Epoch:   800  |  train loss: 0.2638504028
Epoch:   900  |  train loss: 0.2590002894
Epoch:  1000  |  train loss: 0.2569363028
Processing class: 47
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.3281063795
Epoch:   200  |  train loss: 0.3156150579
Epoch:   300  |  train loss: 0.2940187514
Epoch:   400  |  train loss: 0.2841298640
Epoch:   500  |  train loss: 0.2759063125
Epoch:   600  |  train loss: 0.2667443037
Epoch:   700  |  train loss: 0.2596947223
Epoch:   800  |  train loss: 0.2532789856
Epoch:   900  |  train loss: 0.2489072144
Epoch:  1000  |  train loss: 0.2447358847
Processing class: 48
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.3232025504
Epoch:   200  |  train loss: 0.3181192160
Epoch:   300  |  train loss: 0.2914351463
Epoch:   400  |  train loss: 0.2807032645
Epoch:   500  |  train loss: 0.2806677401
Epoch:   600  |  train loss: 0.2730370343
Epoch:   700  |  train loss: 0.2679639220
Epoch:   800  |  train loss: 0.2652648091
Epoch:   900  |  train loss: 0.2642565489
Epoch:  1000  |  train loss: 0.2607996225
Processing class: 49
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.3291884065
Epoch:   200  |  train loss: 0.3176399708
Epoch:   300  |  train loss: 0.3032999873
Epoch:   400  |  train loss: 0.2938609898
Epoch:   500  |  train loss: 0.2834779203
Epoch:   600  |  train loss: 0.2780328691
Epoch:   700  |  train loss: 0.2711397469
Epoch:   800  |  train loss: 0.2666293830
Epoch:   900  |  train loss: 0.2623470128
Epoch:  1000  |  train loss: 0.2582192540
/home/z1165703/FeCAM/models/base.py:87: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  vec = torch.tensor(vec).cuda()
2024-03-05 04:18:04,775 [trainer.py] => CNN: {'total': 83.44, '00-09': 87.7, '10-19': 79.6, '20-29': 84.9, '30-39': 81.0, '40-49': 84.0, 'old': 0, 'new': 83.44}
2024-03-05 04:18:04,776 [trainer.py] => No NME accuracy
2024-03-05 04:18:04,776 [trainer.py] => FeCAM: {'total': 82.9, '00-09': 86.2, '10-19': 79.7, '20-29': 85.0, '30-39': 80.6, '40-49': 83.0, 'old': 0, 'new': 82.9}
2024-03-05 04:18:04,777 [trainer.py] => CNN top1 curve: [83.44]
2024-03-05 04:18:04,777 [trainer.py] => CNN top5 curve: [96.5]
2024-03-05 04:18:04,777 [trainer.py] => FeCAM top1 curve: [82.9]
2024-03-05 04:18:04,777 [trainer.py] => FeCAM top5 curve: [95.84]

2024-03-05 04:18:04,789 [fecam.py] => Learning on 50-60
Processing class: 50
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.3705344200
Epoch:   200  |  train loss: 0.3493279397
Epoch:   300  |  train loss: 0.3365989745
Epoch:   400  |  train loss: 0.3243892610
Epoch:   500  |  train loss: 0.3108224571
Epoch:   600  |  train loss: 0.2996786177
Epoch:   700  |  train loss: 0.2923848331
Epoch:   800  |  train loss: 0.2859952569
Epoch:   900  |  train loss: 0.2815717340
Epoch:  1000  |  train loss: 0.2770293415
Processing class: 51
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.3739120364
Epoch:   200  |  train loss: 0.3507164001
Epoch:   300  |  train loss: 0.3373589814
Epoch:   400  |  train loss: 0.3272774816
Epoch:   500  |  train loss: 0.3121391237
Epoch:   600  |  train loss: 0.2999098539
Epoch:   700  |  train loss: 0.2900867939
Epoch:   800  |  train loss: 0.2845072031
Epoch:   900  |  train loss: 0.2806317270
Epoch:  1000  |  train loss: 0.2732744813
Processing class: 52
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.3943106294
Epoch:   200  |  train loss: 0.3843698442
Epoch:   300  |  train loss: 0.3699466407
Epoch:   400  |  train loss: 0.3579653919
Epoch:   500  |  train loss: 0.3464921236
Epoch:   600  |  train loss: 0.3352334440
Epoch:   700  |  train loss: 0.3279915333
Epoch:   800  |  train loss: 0.3201534867
Epoch:   900  |  train loss: 0.3125980437
Epoch:  1000  |  train loss: 0.3044579923
Processing class: 53
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.3467219949
Epoch:   200  |  train loss: 0.3245754659
Epoch:   300  |  train loss: 0.3140215516
Epoch:   400  |  train loss: 0.3028233469
Epoch:   500  |  train loss: 0.2979826391
Epoch:   600  |  train loss: 0.2936838686
Epoch:   700  |  train loss: 0.2891298234
Epoch:   800  |  train loss: 0.2812120795
Epoch:   900  |  train loss: 0.2786658347
Epoch:  1000  |  train loss: 0.2735065877
Processing class: 54
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.3241735637
Epoch:   200  |  train loss: 0.3064169347
Epoch:   300  |  train loss: 0.2893368602
Epoch:   400  |  train loss: 0.2807170033
Epoch:   500  |  train loss: 0.2729218185
Epoch:   600  |  train loss: 0.2673079967
Epoch:   700  |  train loss: 0.2612887263
Epoch:   800  |  train loss: 0.2572399467
Epoch:   900  |  train loss: 0.2536096662
Epoch:  1000  |  train loss: 0.2478917658
Processing class: 55
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.4016999781
Epoch:   200  |  train loss: 0.3944190979
Epoch:   300  |  train loss: 0.3845564604
Epoch:   400  |  train loss: 0.3767677367
Epoch:   500  |  train loss: 0.3690840065
Epoch:   600  |  train loss: 0.3586538553
Epoch:   700  |  train loss: 0.3505177140
Epoch:   800  |  train loss: 0.3439556539
Epoch:   900  |  train loss: 0.3334758043
Epoch:  1000  |  train loss: 0.3278939128
Processing class: 56
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.3638056934
Epoch:   200  |  train loss: 0.3359363198
Epoch:   300  |  train loss: 0.3068874896
Epoch:   400  |  train loss: 0.2916808963
Epoch:   500  |  train loss: 0.2783484757
Epoch:   600  |  train loss: 0.2669709086
Epoch:   700  |  train loss: 0.2586469352
Epoch:   800  |  train loss: 0.2501795799
Epoch:   900  |  train loss: 0.2454391688
Epoch:  1000  |  train loss: 0.2408533931
Processing class: 57
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.4001804054
Epoch:   200  |  train loss: 0.3932715476
Epoch:   300  |  train loss: 0.3829291821
Epoch:   400  |  train loss: 0.3722203314
Epoch:   500  |  train loss: 0.3625529885
Epoch:   600  |  train loss: 0.3530609548
Epoch:   700  |  train loss: 0.3480988920
Epoch:   800  |  train loss: 0.3402794957
Epoch:   900  |  train loss: 0.3337079227
Epoch:  1000  |  train loss: 0.3237696290
Processing class: 58
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.3750108182
Epoch:   200  |  train loss: 0.3335544884
Epoch:   300  |  train loss: 0.3148313761
Epoch:   400  |  train loss: 0.2974128008
Epoch:   500  |  train loss: 0.2872718453
Epoch:   600  |  train loss: 0.2804760396
Epoch:   700  |  train loss: 0.2719627619
Epoch:   800  |  train loss: 0.2669445038
Epoch:   900  |  train loss: 0.2608695000
Epoch:  1000  |  train loss: 0.2550989777
Processing class: 59
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.3934543014
Epoch:   200  |  train loss: 0.3795443594
Epoch:   300  |  train loss: 0.3591672957
Epoch:   400  |  train loss: 0.3429573119
Epoch:   500  |  train loss: 0.3309493005
Epoch:   600  |  train loss: 0.3227686584
Epoch:   700  |  train loss: 0.3158485830
Epoch:   800  |  train loss: 0.3098347187
Epoch:   900  |  train loss: 0.3034600556
Epoch:  1000  |  train loss: 0.2989055932
2024-03-05 04:23:49,311 [trainer.py] => CNN: {'total': 71.75, '00-09': 82.2, '10-19': 72.8, '20-29': 78.5, '30-39': 74.9, '40-49': 68.4, '50-59': 53.7, 'old': 75.36, 'new': 53.7}
2024-03-05 04:23:49,311 [trainer.py] => No NME accuracy
2024-03-05 04:23:49,311 [trainer.py] => FeCAM: {'total': 72.78, '00-09': 82.6, '10-19': 74.0, '20-29': 81.0, '30-39': 76.4, '40-49': 76.8, '50-59': 45.9, 'old': 78.16, 'new': 45.9}
2024-03-05 04:23:49,311 [trainer.py] => CNN top1 curve: [83.44, 71.75]
2024-03-05 04:23:49,311 [trainer.py] => CNN top5 curve: [96.5, 89.6]
2024-03-05 04:23:49,311 [trainer.py] => FeCAM top1 curve: [82.9, 72.78]
2024-03-05 04:23:49,311 [trainer.py] => FeCAM top5 curve: [95.84, 91.18]

2024-03-05 04:23:49,316 [fecam.py] => Learning on 60-70
Processing class: 60
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.3899295986
Epoch:   200  |  train loss: 0.3746835113
Epoch:   300  |  train loss: 0.3563677907
Epoch:   400  |  train loss: 0.3403416574
Epoch:   500  |  train loss: 0.3276598752
Epoch:   600  |  train loss: 0.3152049959
Epoch:   700  |  train loss: 0.3075997949
Epoch:   800  |  train loss: 0.2952033877
Epoch:   900  |  train loss: 0.2887003779
Epoch:  1000  |  train loss: 0.2825245798
Processing class: 61
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.3612825394
Epoch:   200  |  train loss: 0.3316975176
Epoch:   300  |  train loss: 0.3100563109
Epoch:   400  |  train loss: 0.2915449500
Epoch:   500  |  train loss: 0.2815750480
Epoch:   600  |  train loss: 0.2709922194
Epoch:   700  |  train loss: 0.2638501376
Epoch:   800  |  train loss: 0.2574791789
Epoch:   900  |  train loss: 0.2498768598
Epoch:  1000  |  train loss: 0.2460612178
Processing class: 62
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.4032598555
Epoch:   200  |  train loss: 0.3932819247
Epoch:   300  |  train loss: 0.3864939094
Epoch:   400  |  train loss: 0.3791499317
Epoch:   500  |  train loss: 0.3708686471
Epoch:   600  |  train loss: 0.3656556189
Epoch:   700  |  train loss: 0.3591972053
Epoch:   800  |  train loss: 0.3519531965
Epoch:   900  |  train loss: 0.3441666126
Epoch:  1000  |  train loss: 0.3384273887
Processing class: 63
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.3786698103
Epoch:   200  |  train loss: 0.3678864002
Epoch:   300  |  train loss: 0.3412511289
Epoch:   400  |  train loss: 0.3223088026
Epoch:   500  |  train loss: 0.3083570182
Epoch:   600  |  train loss: 0.3000214756
Epoch:   700  |  train loss: 0.2896273017
Epoch:   800  |  train loss: 0.2863529325
Epoch:   900  |  train loss: 0.2778927445
Epoch:  1000  |  train loss: 0.2732376993
Processing class: 64
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.3677854955
Epoch:   200  |  train loss: 0.3230835855
Epoch:   300  |  train loss: 0.3040660977
Epoch:   400  |  train loss: 0.2901498854
Epoch:   500  |  train loss: 0.2751956522
Epoch:   600  |  train loss: 0.2640700161
Epoch:   700  |  train loss: 0.2547834992
Epoch:   800  |  train loss: 0.2494832784
Epoch:   900  |  train loss: 0.2456008971
Epoch:  1000  |  train loss: 0.2387892574
Processing class: 65
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.3627401590
Epoch:   200  |  train loss: 0.3319556653
Epoch:   300  |  train loss: 0.3048417151
Epoch:   400  |  train loss: 0.2898915052
Epoch:   500  |  train loss: 0.2781515777
Epoch:   600  |  train loss: 0.2702275693
Epoch:   700  |  train loss: 0.2620252252
Epoch:   800  |  train loss: 0.2569680423
Epoch:   900  |  train loss: 0.2491892308
Epoch:  1000  |  train loss: 0.2457337081
Processing class: 66
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.3826572597
Epoch:   200  |  train loss: 0.3645338714
Epoch:   300  |  train loss: 0.3467758417
Epoch:   400  |  train loss: 0.3337668777
Epoch:   500  |  train loss: 0.3271841586
Epoch:   600  |  train loss: 0.3196743250
Epoch:   700  |  train loss: 0.3153598666
Epoch:   800  |  train loss: 0.3109805286
Epoch:   900  |  train loss: 0.3059394360
Epoch:  1000  |  train loss: 0.3019424021
Processing class: 67
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.3849980950
Epoch:   200  |  train loss: 0.3697388828
Epoch:   300  |  train loss: 0.3550435543
Epoch:   400  |  train loss: 0.3444222629
Epoch:   500  |  train loss: 0.3354478478
Epoch:   600  |  train loss: 0.3276115596
Epoch:   700  |  train loss: 0.3205289960
Epoch:   800  |  train loss: 0.3151961625
Epoch:   900  |  train loss: 0.3087992668
Epoch:  1000  |  train loss: 0.3035396934
Processing class: 68
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.3555696070
Epoch:   200  |  train loss: 0.3433077157
Epoch:   300  |  train loss: 0.3160179019
Epoch:   400  |  train loss: 0.3041227520
Epoch:   500  |  train loss: 0.2957626641
Epoch:   600  |  train loss: 0.2874159396
Epoch:   700  |  train loss: 0.2815380275
Epoch:   800  |  train loss: 0.2750123501
Epoch:   900  |  train loss: 0.2698165834
Epoch:  1000  |  train loss: 0.2672885478
Processing class: 69
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.2779824495
Epoch:   200  |  train loss: 0.2610501379
Epoch:   300  |  train loss: 0.2444577068
Epoch:   400  |  train loss: 0.2334212095
Epoch:   500  |  train loss: 0.2260365158
Epoch:   600  |  train loss: 0.2204827935
Epoch:   700  |  train loss: 0.2184853405
Epoch:   800  |  train loss: 0.2148943007
Epoch:   900  |  train loss: 0.2151204735
Epoch:  1000  |  train loss: 0.2103716880
2024-03-05 04:30:20,779 [trainer.py] => CNN: {'total': 64.66, '00-09': 75.0, '10-19': 70.1, '20-29': 77.1, '30-39': 70.6, '40-49': 64.0, '50-59': 45.8, '60-69': 50.0, 'old': 67.1, 'new': 50.0}
2024-03-05 04:30:20,780 [trainer.py] => No NME accuracy
2024-03-05 04:30:20,780 [trainer.py] => FeCAM: {'total': 67.27, '00-09': 79.9, '10-19': 70.6, '20-29': 79.1, '30-39': 71.9, '40-49': 74.1, '50-59': 41.2, '60-69': 54.1, 'old': 69.47, 'new': 54.1}
2024-03-05 04:30:20,780 [trainer.py] => CNN top1 curve: [83.44, 71.75, 64.66]
2024-03-05 04:30:20,780 [trainer.py] => CNN top5 curve: [96.5, 89.6, 86.54]
2024-03-05 04:30:20,780 [trainer.py] => FeCAM top1 curve: [82.9, 72.78, 67.27]
2024-03-05 04:30:20,780 [trainer.py] => FeCAM top5 curve: [95.84, 91.18, 87.21]

2024-03-05 04:30:20,784 [fecam.py] => Learning on 70-80
Processing class: 70
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.3711968899
Epoch:   200  |  train loss: 0.3546361506
Epoch:   300  |  train loss: 0.3312371314
Epoch:   400  |  train loss: 0.3188530207
Epoch:   500  |  train loss: 0.3047332227
Epoch:   600  |  train loss: 0.2951159358
Epoch:   700  |  train loss: 0.2858906448
Epoch:   800  |  train loss: 0.2746592104
Epoch:   900  |  train loss: 0.2708941936
Epoch:  1000  |  train loss: 0.2654464066
Processing class: 71
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.3895556986
Epoch:   200  |  train loss: 0.3772787571
Epoch:   300  |  train loss: 0.3532587230
Epoch:   400  |  train loss: 0.3396964192
Epoch:   500  |  train loss: 0.3275926471
Epoch:   600  |  train loss: 0.3169977546
Epoch:   700  |  train loss: 0.3069411278
Epoch:   800  |  train loss: 0.2988366365
Epoch:   900  |  train loss: 0.2919586003
Epoch:  1000  |  train loss: 0.2842187405
Processing class: 72
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.3815368891
Epoch:   200  |  train loss: 0.3672735453
Epoch:   300  |  train loss: 0.3502041698
Epoch:   400  |  train loss: 0.3356479645
Epoch:   500  |  train loss: 0.3269648254
Epoch:   600  |  train loss: 0.3164987266
Epoch:   700  |  train loss: 0.3092286229
Epoch:   800  |  train loss: 0.3011661828
Epoch:   900  |  train loss: 0.2946389139
Epoch:  1000  |  train loss: 0.2900263548
Processing class: 73
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.3816456914
Epoch:   200  |  train loss: 0.3615879178
Epoch:   300  |  train loss: 0.3388437331
Epoch:   400  |  train loss: 0.3262987196
Epoch:   500  |  train loss: 0.3222300291
Epoch:   600  |  train loss: 0.3155828476
Epoch:   700  |  train loss: 0.3078016579
Epoch:   800  |  train loss: 0.3020309746
Epoch:   900  |  train loss: 0.2960407019
Epoch:  1000  |  train loss: 0.2919004023
Processing class: 74
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.4004727125
Epoch:   200  |  train loss: 0.3961046100
Epoch:   300  |  train loss: 0.3808370113
Epoch:   400  |  train loss: 0.3659804761
Epoch:   500  |  train loss: 0.3534998119
Epoch:   600  |  train loss: 0.3423764348
Epoch:   700  |  train loss: 0.3367247880
Epoch:   800  |  train loss: 0.3260968506
Epoch:   900  |  train loss: 0.3193708003
Epoch:  1000  |  train loss: 0.3148417056
Processing class: 75
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.3846807003
Epoch:   200  |  train loss: 0.3691044211
Epoch:   300  |  train loss: 0.3512824535
Epoch:   400  |  train loss: 0.3395181417
Epoch:   500  |  train loss: 0.3302160561
Epoch:   600  |  train loss: 0.3222412407
Epoch:   700  |  train loss: 0.3107709944
Epoch:   800  |  train loss: 0.3066813767
Epoch:   900  |  train loss: 0.3009842277
Epoch:  1000  |  train loss: 0.2966473520
Processing class: 76
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.2832518935
Epoch:   200  |  train loss: 0.2597398549
Epoch:   300  |  train loss: 0.2439617425
Epoch:   400  |  train loss: 0.2363312751
Epoch:   500  |  train loss: 0.2316552550
Epoch:   600  |  train loss: 0.2271525204
Epoch:   700  |  train loss: 0.2249753743
Epoch:   800  |  train loss: 0.2181993991
Epoch:   900  |  train loss: 0.2166250110
Epoch:  1000  |  train loss: 0.2147769421
Processing class: 77
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.3608360350
Epoch:   200  |  train loss: 0.3420886815
Epoch:   300  |  train loss: 0.3205615401
Epoch:   400  |  train loss: 0.3069967449
Epoch:   500  |  train loss: 0.2910851002
Epoch:   600  |  train loss: 0.2783618510
Epoch:   700  |  train loss: 0.2694063604
Epoch:   800  |  train loss: 0.2630553365
Epoch:   900  |  train loss: 0.2562660187
Epoch:  1000  |  train loss: 0.2497234762
Processing class: 78
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.3729314208
Epoch:   200  |  train loss: 0.3442699850
Epoch:   300  |  train loss: 0.3318789780
Epoch:   400  |  train loss: 0.3173038781
Epoch:   500  |  train loss: 0.3027583361
Epoch:   600  |  train loss: 0.2944023252
Epoch:   700  |  train loss: 0.2873519361
Epoch:   800  |  train loss: 0.2806346893
Epoch:   900  |  train loss: 0.2762737393
Epoch:  1000  |  train loss: 0.2714563131
Processing class: 79
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.4023485065
Epoch:   200  |  train loss: 0.3886916757
Epoch:   300  |  train loss: 0.3803427458
Epoch:   400  |  train loss: 0.3676670194
Epoch:   500  |  train loss: 0.3582550883
Epoch:   600  |  train loss: 0.3503706753
Epoch:   700  |  train loss: 0.3417731822
Epoch:   800  |  train loss: 0.3334982276
Epoch:   900  |  train loss: 0.3268888831
Epoch:  1000  |  train loss: 0.3182157338
2024-03-05 04:38:14,484 [trainer.py] => CNN: {'total': 59.18, '00-09': 73.6, '10-19': 68.4, '20-29': 76.9, '30-39': 69.1, '40-49': 60.6, '50-59': 37.2, '60-69': 45.7, '70-79': 41.9, 'old': 61.64, 'new': 41.9}
2024-03-05 04:38:14,485 [trainer.py] => No NME accuracy
2024-03-05 04:38:14,485 [trainer.py] => FeCAM: {'total': 61.72, '00-09': 78.1, '10-19': 69.3, '20-29': 78.0, '30-39': 69.5, '40-49': 72.1, '50-59': 35.7, '60-69': 49.1, '70-79': 42.0, 'old': 64.54, 'new': 42.0}
2024-03-05 04:38:14,485 [trainer.py] => CNN top1 curve: [83.44, 71.75, 64.66, 59.18]
2024-03-05 04:38:14,485 [trainer.py] => CNN top5 curve: [96.5, 89.6, 86.54, 84.09]
2024-03-05 04:38:14,485 [trainer.py] => FeCAM top1 curve: [82.9, 72.78, 67.27, 61.72]
2024-03-05 04:38:14,485 [trainer.py] => FeCAM top5 curve: [95.84, 91.18, 87.21, 84.09]

2024-03-05 04:38:14,490 [fecam.py] => Learning on 80-90
Processing class: 80
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.3202458501
Epoch:   200  |  train loss: 0.2775127232
Epoch:   300  |  train loss: 0.2627648473
Epoch:   400  |  train loss: 0.2537218422
Epoch:   500  |  train loss: 0.2450398117
Epoch:   600  |  train loss: 0.2403076857
Epoch:   700  |  train loss: 0.2336247623
Epoch:   800  |  train loss: 0.2327249080
Epoch:   900  |  train loss: 0.2291185021
Epoch:  1000  |  train loss: 0.2266764194
Processing class: 81
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.3630718291
Epoch:   200  |  train loss: 0.3406057596
Epoch:   300  |  train loss: 0.3231758475
Epoch:   400  |  train loss: 0.3137403309
Epoch:   500  |  train loss: 0.3030368209
Epoch:   600  |  train loss: 0.2964810252
Epoch:   700  |  train loss: 0.2886653662
Epoch:   800  |  train loss: 0.2846565545
Epoch:   900  |  train loss: 0.2778129876
Epoch:  1000  |  train loss: 0.2717204034
Processing class: 82
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.3450206876
Epoch:   200  |  train loss: 0.3203067243
Epoch:   300  |  train loss: 0.3047430396
Epoch:   400  |  train loss: 0.2905989647
Epoch:   500  |  train loss: 0.2792599261
Epoch:   600  |  train loss: 0.2710295677
Epoch:   700  |  train loss: 0.2648624778
Epoch:   800  |  train loss: 0.2599911869
Epoch:   900  |  train loss: 0.2546645969
Epoch:  1000  |  train loss: 0.2497134298
Processing class: 83
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.3350361347
Epoch:   200  |  train loss: 0.2942760646
Epoch:   300  |  train loss: 0.2725082874
Epoch:   400  |  train loss: 0.2574256480
Epoch:   500  |  train loss: 0.2488490224
Epoch:   600  |  train loss: 0.2410638124
Epoch:   700  |  train loss: 0.2380726665
Epoch:   800  |  train loss: 0.2351537377
Epoch:   900  |  train loss: 0.2301171333
Epoch:  1000  |  train loss: 0.2258276075
Processing class: 84
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.3791432977
Epoch:   200  |  train loss: 0.3442660511
Epoch:   300  |  train loss: 0.3270054579
Epoch:   400  |  train loss: 0.3097891688
Epoch:   500  |  train loss: 0.2989859939
Epoch:   600  |  train loss: 0.2890707850
Epoch:   700  |  train loss: 0.2830001056
Epoch:   800  |  train loss: 0.2784548521
Epoch:   900  |  train loss: 0.2705013335
Epoch:  1000  |  train loss: 0.2647702485
Processing class: 85
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.3820916414
Epoch:   200  |  train loss: 0.3516298175
Epoch:   300  |  train loss: 0.3307317078
Epoch:   400  |  train loss: 0.3147933066
Epoch:   500  |  train loss: 0.3054202020
Epoch:   600  |  train loss: 0.2968899488
Epoch:   700  |  train loss: 0.2919691086
Epoch:   800  |  train loss: 0.2842677653
Epoch:   900  |  train loss: 0.2787628829
Epoch:  1000  |  train loss: 0.2747901201
Processing class: 86
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.4023933053
Epoch:   200  |  train loss: 0.3858952999
Epoch:   300  |  train loss: 0.3656642854
Epoch:   400  |  train loss: 0.3486829877
Epoch:   500  |  train loss: 0.3373174846
Epoch:   600  |  train loss: 0.3274768651
Epoch:   700  |  train loss: 0.3186799228
Epoch:   800  |  train loss: 0.3108747065
Epoch:   900  |  train loss: 0.3047493577
Epoch:  1000  |  train loss: 0.3012531817
Processing class: 87
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.3490157127
Epoch:   200  |  train loss: 0.3148951471
Epoch:   300  |  train loss: 0.3013705373
Epoch:   400  |  train loss: 0.2852332950
Epoch:   500  |  train loss: 0.2748531520
Epoch:   600  |  train loss: 0.2706140280
Epoch:   700  |  train loss: 0.2654426128
Epoch:   800  |  train loss: 0.2598764837
Epoch:   900  |  train loss: 0.2539312035
Epoch:  1000  |  train loss: 0.2519211709
Processing class: 88
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.3734119475
Epoch:   200  |  train loss: 0.3544885993
Epoch:   300  |  train loss: 0.3350153089
Epoch:   400  |  train loss: 0.3195917785
Epoch:   500  |  train loss: 0.3042424917
Epoch:   600  |  train loss: 0.2911290526
Epoch:   700  |  train loss: 0.2841930747
Epoch:   800  |  train loss: 0.2769491315
Epoch:   900  |  train loss: 0.2715715289
Epoch:  1000  |  train loss: 0.2652149200
Processing class: 89
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.3679715991
Epoch:   200  |  train loss: 0.3442414105
Epoch:   300  |  train loss: 0.3211875200
Epoch:   400  |  train loss: 0.3097032726
Epoch:   500  |  train loss: 0.2986306667
Epoch:   600  |  train loss: 0.2873268902
Epoch:   700  |  train loss: 0.2779271424
Epoch:   800  |  train loss: 0.2734427571
Epoch:   900  |  train loss: 0.2660201728
Epoch:  1000  |  train loss: 0.2608219504
2024-03-05 04:47:13,584 [trainer.py] => CNN: {'total': 54.08, '00-09': 67.9, '10-19': 63.6, '20-29': 73.2, '30-39': 68.8, '40-49': 57.2, '50-59': 32.6, '60-69': 40.9, '70-79': 38.1, '80-89': 44.4, 'old': 55.29, 'new': 44.4}
2024-03-05 04:47:13,584 [trainer.py] => No NME accuracy
2024-03-05 04:47:13,584 [trainer.py] => FeCAM: {'total': 57.66, '00-09': 76.1, '10-19': 65.8, '20-29': 75.3, '30-39': 67.9, '40-49': 68.5, '50-59': 33.0, '60-69': 45.0, '70-79': 39.0, '80-89': 48.3, 'old': 58.82, 'new': 48.3}
2024-03-05 04:47:13,584 [trainer.py] => CNN top1 curve: [83.44, 71.75, 64.66, 59.18, 54.08]
2024-03-05 04:47:13,584 [trainer.py] => CNN top5 curve: [96.5, 89.6, 86.54, 84.09, 81.74]
2024-03-05 04:47:13,584 [trainer.py] => FeCAM top1 curve: [82.9, 72.78, 67.27, 61.72, 57.66]
2024-03-05 04:47:13,584 [trainer.py] => FeCAM top5 curve: [95.84, 91.18, 87.21, 84.09, 81.63]

2024-03-05 04:47:13,592 [fecam.py] => Learning on 90-100
Processing class: 90
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.3810814440
Epoch:   200  |  train loss: 0.3657489955
Epoch:   300  |  train loss: 0.3399324536
Epoch:   400  |  train loss: 0.3206989586
Epoch:   500  |  train loss: 0.3053232133
Epoch:   600  |  train loss: 0.2927850902
Epoch:   700  |  train loss: 0.2809138477
Epoch:   800  |  train loss: 0.2738185346
Epoch:   900  |  train loss: 0.2689334750
Epoch:  1000  |  train loss: 0.2645376384
Processing class: 91
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.2860689700
Epoch:   200  |  train loss: 0.2706983447
Epoch:   300  |  train loss: 0.2573894471
Epoch:   400  |  train loss: 0.2517969519
Epoch:   500  |  train loss: 0.2428115636
Epoch:   600  |  train loss: 0.2368549913
Epoch:   700  |  train loss: 0.2281731337
Epoch:   800  |  train loss: 0.2215720475
Epoch:   900  |  train loss: 0.2166179299
Epoch:  1000  |  train loss: 0.2134287804
Processing class: 92
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.3631195962
Epoch:   200  |  train loss: 0.3241432130
Epoch:   300  |  train loss: 0.3100937784
Epoch:   400  |  train loss: 0.2991764903
Epoch:   500  |  train loss: 0.2857476473
Epoch:   600  |  train loss: 0.2718596280
Epoch:   700  |  train loss: 0.2621830523
Epoch:   800  |  train loss: 0.2530186206
Epoch:   900  |  train loss: 0.2478007823
Epoch:  1000  |  train loss: 0.2427876204
Processing class: 93
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.3734973073
Epoch:   200  |  train loss: 0.3397260666
Epoch:   300  |  train loss: 0.3199436545
Epoch:   400  |  train loss: 0.3076664031
Epoch:   500  |  train loss: 0.2943599522
Epoch:   600  |  train loss: 0.2822260380
Epoch:   700  |  train loss: 0.2761282265
Epoch:   800  |  train loss: 0.2683849096
Epoch:   900  |  train loss: 0.2627234906
Epoch:  1000  |  train loss: 0.2556460679
Processing class: 94
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.3402675331
Epoch:   200  |  train loss: 0.3188545108
Epoch:   300  |  train loss: 0.2913909078
Epoch:   400  |  train loss: 0.2767378569
Epoch:   500  |  train loss: 0.2637140810
Epoch:   600  |  train loss: 0.2570310235
Epoch:   700  |  train loss: 0.2513475984
Epoch:   800  |  train loss: 0.2414163709
Epoch:   900  |  train loss: 0.2380354702
Epoch:  1000  |  train loss: 0.2323963463
Processing class: 95
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.3882881463
Epoch:   200  |  train loss: 0.3784036875
Epoch:   300  |  train loss: 0.3631616652
Epoch:   400  |  train loss: 0.3458888352
Epoch:   500  |  train loss: 0.3329210818
Epoch:   600  |  train loss: 0.3224280775
Epoch:   700  |  train loss: 0.3135218680
Epoch:   800  |  train loss: 0.3048030734
Epoch:   900  |  train loss: 0.2985181570
Epoch:  1000  |  train loss: 0.2932219863
Processing class: 96
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.3935051143
Epoch:   200  |  train loss: 0.3747692287
Epoch:   300  |  train loss: 0.3541305721
Epoch:   400  |  train loss: 0.3427786708
Epoch:   500  |  train loss: 0.3295463383
Epoch:   600  |  train loss: 0.3198337734
Epoch:   700  |  train loss: 0.3111674011
Epoch:   800  |  train loss: 0.3019623637
Epoch:   900  |  train loss: 0.2952635407
Epoch:  1000  |  train loss: 0.2882600009
Processing class: 97
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.3732370615
Epoch:   200  |  train loss: 0.3401469648
Epoch:   300  |  train loss: 0.3172204137
Epoch:   400  |  train loss: 0.3048777640
Epoch:   500  |  train loss: 0.2947224915
Epoch:   600  |  train loss: 0.2862109065
Epoch:   700  |  train loss: 0.2790257514
Epoch:   800  |  train loss: 0.2722239673
Epoch:   900  |  train loss: 0.2660979748
Epoch:  1000  |  train loss: 0.2602206677
Processing class: 98
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.3414762318
Epoch:   200  |  train loss: 0.3182233453
Epoch:   300  |  train loss: 0.2942062676
Epoch:   400  |  train loss: 0.2795723915
Epoch:   500  |  train loss: 0.2673752248
Epoch:   600  |  train loss: 0.2583520889
Epoch:   700  |  train loss: 0.2501335800
Epoch:   800  |  train loss: 0.2458736092
Epoch:   900  |  train loss: 0.2414469928
Epoch:  1000  |  train loss: 0.2374801666
Processing class: 99
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.3817813635
Epoch:   200  |  train loss: 0.3598088980
Epoch:   300  |  train loss: 0.3340912461
Epoch:   400  |  train loss: 0.3173572779
Epoch:   500  |  train loss: 0.3020339549
Epoch:   600  |  train loss: 0.2914824903
Epoch:   700  |  train loss: 0.2837261498
Epoch:   800  |  train loss: 0.2784522891
Epoch:   900  |  train loss: 0.2714068949
Epoch:  1000  |  train loss: 0.2661351144
2024-03-05 04:57:47,374 [trainer.py] => CNN: {'total': 50.36, '00-09': 57.7, '10-19': 63.6, '20-29': 71.4, '30-39': 68.5, '40-49': 56.8, '50-59': 29.6, '60-69': 38.9, '70-79': 36.6, '80-89': 42.7, '90-99': 37.8, 'old': 51.76, 'new': 37.8}
2024-03-05 04:57:47,375 [trainer.py] => No NME accuracy
2024-03-05 04:57:47,375 [trainer.py] => FeCAM: {'total': 54.74, '00-09': 73.3, '10-19': 65.1, '20-29': 74.1, '30-39': 67.2, '40-49': 67.1, '50-59': 30.8, '60-69': 43.3, '70-79': 37.2, '80-89': 46.9, '90-99': 42.4, 'old': 56.11, 'new': 42.4}
2024-03-05 04:57:47,377 [trainer.py] => CNN top1 curve: [83.44, 71.75, 64.66, 59.18, 54.08, 50.36]
2024-03-05 04:57:47,377 [trainer.py] => CNN top5 curve: [96.5, 89.6, 86.54, 84.09, 81.74, 79.63]
2024-03-05 04:57:47,377 [trainer.py] => FeCAM top1 curve: [82.9, 72.78, 67.27, 61.72, 57.66, 54.74]
2024-03-05 04:57:47,377 [trainer.py] => FeCAM top5 curve: [95.84, 91.18, 87.21, 84.09, 81.63, 79.21]

=========================================
2024-03-05 04:57:56,170 [trainer.py] => config: ./exps/FeCAM_cifar100.json
2024-03-05 04:57:56,170 [trainer.py] => prefix: train
2024-03-05 04:57:56,170 [trainer.py] => dataset: cifar100
2024-03-05 04:57:56,170 [trainer.py] => memory_size: 0
2024-03-05 04:57:56,170 [trainer.py] => shuffle: True
2024-03-05 04:57:56,170 [trainer.py] => init_cls: 50
2024-03-05 04:57:56,170 [trainer.py] => increment: 10
2024-03-05 04:57:56,170 [trainer.py] => model_name: fecam
2024-03-05 04:57:56,170 [trainer.py] => convnet_type: resnet18
2024-03-05 04:57:56,170 [trainer.py] => device: [device(type='cuda', index=0)]
2024-03-05 04:57:56,170 [trainer.py] => seed: 1993
2024-03-05 04:57:56,170 [trainer.py] => init_epochs: 200
2024-03-05 04:57:56,170 [trainer.py] => init_lr: 0.1
2024-03-05 04:57:56,170 [trainer.py] => init_weight_decay: 0.0005
2024-03-05 04:57:56,170 [trainer.py] => batch_size: 128
2024-03-05 04:57:56,170 [trainer.py] => num_workers: 8
2024-03-05 04:57:56,170 [trainer.py] => T: 5
2024-03-05 04:57:56,170 [trainer.py] => beta: 0.5
2024-03-05 04:57:56,170 [trainer.py] => alpha1: 1
2024-03-05 04:57:56,171 [trainer.py] => alpha2: 1
2024-03-05 04:57:56,171 [trainer.py] => ncm: False
2024-03-05 04:57:56,171 [trainer.py] => tukey: False
2024-03-05 04:57:56,171 [trainer.py] => diagonal: False
2024-03-05 04:57:56,171 [trainer.py] => per_class: True
2024-03-05 04:57:56,171 [trainer.py] => full_cov: True
2024-03-05 04:57:56,171 [trainer.py] => shrink: True
2024-03-05 04:57:56,171 [trainer.py] => norm_cov: False
2024-03-05 04:57:56,171 [trainer.py] => vecnorm: False
2024-03-05 04:57:56,171 [trainer.py] => ae_type: wae
2024-03-05 04:57:56,171 [trainer.py] => epochs: 1000
2024-03-05 04:57:56,171 [trainer.py] => ae_latent_dim: 32
2024-03-05 04:57:56,171 [trainer.py] => wae_sigma: 10
2024-03-05 04:57:56,171 [trainer.py] => wae_C: 0.5
Files already downloaded and verified
Files already downloaded and verified
2024-03-05 04:57:57,819 [data_manager.py] => [68, 56, 78, 8, 23, 84, 90, 65, 74, 76, 40, 89, 3, 92, 55, 9, 26, 80, 43, 38, 58, 70, 77, 1, 85, 19, 17, 50, 28, 53, 13, 81, 45, 82, 6, 59, 83, 16, 15, 44, 91, 41, 72, 60, 79, 52, 20, 10, 31, 54, 37, 95, 14, 71, 96, 98, 97, 2, 64, 66, 42, 22, 35, 86, 24, 34, 87, 21, 99, 0, 88, 27, 18, 94, 11, 12, 47, 25, 30, 46, 62, 69, 36, 61, 7, 63, 75, 5, 32, 4, 51, 48, 73, 93, 39, 67, 29, 49, 57, 33]
2024-03-05 04:57:58,097 [fecam.py] => Learning on 0-50
/home/z1165703/miniconda3/lib/python3.11/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
/home/z1165703/miniconda3/lib/python3.11/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
Processing class: 0
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.2664368451
Epoch:   200  |  train loss: 0.2542789906
Epoch:   300  |  train loss: 0.2552567601
Epoch:   400  |  train loss: 0.2510034174
Epoch:   500  |  train loss: 0.2473591864
Epoch:   600  |  train loss: 0.2467014641
Epoch:   700  |  train loss: 0.2454738855
Epoch:   800  |  train loss: 0.2449626744
Epoch:   900  |  train loss: 0.2422632217
Epoch:  1000  |  train loss: 0.2400859535
Processing class: 1
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.2830913484
Epoch:   200  |  train loss: 0.2802556992
Epoch:   300  |  train loss: 0.2766336083
Epoch:   400  |  train loss: 0.2729497492
Epoch:   500  |  train loss: 0.2651181370
Epoch:   600  |  train loss: 0.2665493429
Epoch:   700  |  train loss: 0.2621631086
Epoch:   800  |  train loss: 0.2608499587
Epoch:   900  |  train loss: 0.2609309435
Epoch:  1000  |  train loss: 0.2559000134
Processing class: 2
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.2924103916
Epoch:   200  |  train loss: 0.2834516585
Epoch:   300  |  train loss: 0.2773572922
Epoch:   400  |  train loss: 0.2686100185
Epoch:   500  |  train loss: 0.2577570677
Epoch:   600  |  train loss: 0.2546221018
Epoch:   700  |  train loss: 0.2489127040
Epoch:   800  |  train loss: 0.2475296825
Epoch:   900  |  train loss: 0.2416199923
Epoch:  1000  |  train loss: 0.2371062011
Processing class: 3
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.2670610011
Epoch:   200  |  train loss: 0.2693294704
Epoch:   300  |  train loss: 0.2655349642
Epoch:   400  |  train loss: 0.2575874329
Epoch:   500  |  train loss: 0.2515739769
Epoch:   600  |  train loss: 0.2516506970
Epoch:   700  |  train loss: 0.2476278603
Epoch:   800  |  train loss: 0.2431117505
Epoch:   900  |  train loss: 0.2397942841
Epoch:  1000  |  train loss: 0.2363432229
Processing class: 4
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.2823145747
Epoch:   200  |  train loss: 0.2780260205
Epoch:   300  |  train loss: 0.2708160639
Epoch:   400  |  train loss: 0.2639848113
Epoch:   500  |  train loss: 0.2599071294
Epoch:   600  |  train loss: 0.2528056294
Epoch:   700  |  train loss: 0.2522439212
Epoch:   800  |  train loss: 0.2503158659
Epoch:   900  |  train loss: 0.2481843889
Epoch:  1000  |  train loss: 0.2471418768
Processing class: 5
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.2952634394
Epoch:   200  |  train loss: 0.2815203667
Epoch:   300  |  train loss: 0.2753433406
Epoch:   400  |  train loss: 0.2668770373
Epoch:   500  |  train loss: 0.2633025289
Epoch:   600  |  train loss: 0.2627573133
Epoch:   700  |  train loss: 0.2575762033
Epoch:   800  |  train loss: 0.2513870269
Epoch:   900  |  train loss: 0.2468005806
Epoch:  1000  |  train loss: 0.2454851955
Processing class: 6
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.2812412679
Epoch:   200  |  train loss: 0.2820529103
Epoch:   300  |  train loss: 0.2758724391
Epoch:   400  |  train loss: 0.2676579773
Epoch:   500  |  train loss: 0.2657573640
Epoch:   600  |  train loss: 0.2624324322
Epoch:   700  |  train loss: 0.2591116846
Epoch:   800  |  train loss: 0.2564520895
Epoch:   900  |  train loss: 0.2502184987
Epoch:  1000  |  train loss: 0.2490271568
Processing class: 7
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.2922255814
Epoch:   200  |  train loss: 0.2839659512
Epoch:   300  |  train loss: 0.2795784831
Epoch:   400  |  train loss: 0.2714540541
Epoch:   500  |  train loss: 0.2654626131
Epoch:   600  |  train loss: 0.2607510269
Epoch:   700  |  train loss: 0.2582122415
Epoch:   800  |  train loss: 0.2573938310
Epoch:   900  |  train loss: 0.2511428654
Epoch:  1000  |  train loss: 0.2495705128
Processing class: 8
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.2866611123
Epoch:   200  |  train loss: 0.2824264348
Epoch:   300  |  train loss: 0.2698346078
Epoch:   400  |  train loss: 0.2672642529
Epoch:   500  |  train loss: 0.2615396440
Epoch:   600  |  train loss: 0.2566579759
Epoch:   700  |  train loss: 0.2555111974
Epoch:   800  |  train loss: 0.2499575019
Epoch:   900  |  train loss: 0.2484300047
Epoch:  1000  |  train loss: 0.2435223341
Processing class: 9
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.2774014473
Epoch:   200  |  train loss: 0.2782927334
Epoch:   300  |  train loss: 0.2775887787
Epoch:   400  |  train loss: 0.2690938890
Epoch:   500  |  train loss: 0.2688536704
Epoch:   600  |  train loss: 0.2649334669
Epoch:   700  |  train loss: 0.2646132886
Epoch:   800  |  train loss: 0.2635939300
Epoch:   900  |  train loss: 0.2618407845
Epoch:  1000  |  train loss: 0.2575871468
Processing class: 10
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.2898396194
Epoch:   200  |  train loss: 0.2800116479
Epoch:   300  |  train loss: 0.2764005125
Epoch:   400  |  train loss: 0.2744854093
Epoch:   500  |  train loss: 0.2645554721
Epoch:   600  |  train loss: 0.2616851270
Epoch:   700  |  train loss: 0.2591535568
Epoch:   800  |  train loss: 0.2548924953
Epoch:   900  |  train loss: 0.2534325004
Epoch:  1000  |  train loss: 0.2509802580
Processing class: 11
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.2956235588
Epoch:   200  |  train loss: 0.2837757945
Epoch:   300  |  train loss: 0.2769078076
Epoch:   400  |  train loss: 0.2722112834
Epoch:   500  |  train loss: 0.2652555883
Epoch:   600  |  train loss: 0.2634601414
Epoch:   700  |  train loss: 0.2627712429
Epoch:   800  |  train loss: 0.2592842489
Epoch:   900  |  train loss: 0.2559423417
Epoch:  1000  |  train loss: 0.2539489508
Processing class: 12
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.2848575294
Epoch:   200  |  train loss: 0.2843546748
Epoch:   300  |  train loss: 0.2767912865
Epoch:   400  |  train loss: 0.2696139097
Epoch:   500  |  train loss: 0.2646013439
Epoch:   600  |  train loss: 0.2577512205
Epoch:   700  |  train loss: 0.2545842975
Epoch:   800  |  train loss: 0.2529710650
Epoch:   900  |  train loss: 0.2516643941
Epoch:  1000  |  train loss: 0.2477275461
Processing class: 13
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.2806004643
Epoch:   200  |  train loss: 0.2735891879
Epoch:   300  |  train loss: 0.2671991825
Epoch:   400  |  train loss: 0.2597028852
Epoch:   500  |  train loss: 0.2558098704
Epoch:   600  |  train loss: 0.2514408171
Epoch:   700  |  train loss: 0.2490308076
Epoch:   800  |  train loss: 0.2482858539
Epoch:   900  |  train loss: 0.2471827477
Epoch:  1000  |  train loss: 0.2453702450
Processing class: 14
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.3012490451
Epoch:   200  |  train loss: 0.3017328620
Epoch:   300  |  train loss: 0.2951908052
Epoch:   400  |  train loss: 0.2894554019
Epoch:   500  |  train loss: 0.2837673426
Epoch:   600  |  train loss: 0.2792101085
Epoch:   700  |  train loss: 0.2781771541
Epoch:   800  |  train loss: 0.2784076989
Epoch:   900  |  train loss: 0.2755241692
Epoch:  1000  |  train loss: 0.2716829419
Processing class: 15
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.2777387559
Epoch:   200  |  train loss: 0.2746671379
Epoch:   300  |  train loss: 0.2652126253
Epoch:   400  |  train loss: 0.2609566510
Epoch:   500  |  train loss: 0.2582424998
Epoch:   600  |  train loss: 0.2533768237
Epoch:   700  |  train loss: 0.2537843853
Epoch:   800  |  train loss: 0.2494041741
Epoch:   900  |  train loss: 0.2507303923
Epoch:  1000  |  train loss: 0.2480089188
Processing class: 16
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.2834662080
Epoch:   200  |  train loss: 0.2747603953
Epoch:   300  |  train loss: 0.2711546123
Epoch:   400  |  train loss: 0.2657132268
Epoch:   500  |  train loss: 0.2658694208
Epoch:   600  |  train loss: 0.2617478132
Epoch:   700  |  train loss: 0.2577336371
Epoch:   800  |  train loss: 0.2578619301
Epoch:   900  |  train loss: 0.2523878753
Epoch:  1000  |  train loss: 0.2540092319
Processing class: 17
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.2787351787
Epoch:   200  |  train loss: 0.2794989407
Epoch:   300  |  train loss: 0.2800538838
Epoch:   400  |  train loss: 0.2743134618
Epoch:   500  |  train loss: 0.2620772302
Epoch:   600  |  train loss: 0.2598093033
Epoch:   700  |  train loss: 0.2561609924
Epoch:   800  |  train loss: 0.2533331841
Epoch:   900  |  train loss: 0.2512067318
Epoch:  1000  |  train loss: 0.2492161334
Processing class: 18
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.2798559070
Epoch:   200  |  train loss: 0.2796002626
Epoch:   300  |  train loss: 0.2644917130
Epoch:   400  |  train loss: 0.2579774141
Epoch:   500  |  train loss: 0.2520058125
Epoch:   600  |  train loss: 0.2445033073
Epoch:   700  |  train loss: 0.2450059921
Epoch:   800  |  train loss: 0.2406851172
Epoch:   900  |  train loss: 0.2385606408
Epoch:  1000  |  train loss: 0.2375543028
Processing class: 19
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.2706358433
Epoch:   200  |  train loss: 0.2683362544
Epoch:   300  |  train loss: 0.2617784321
Epoch:   400  |  train loss: 0.2593818963
Epoch:   500  |  train loss: 0.2513767898
Epoch:   600  |  train loss: 0.2492811292
Epoch:   700  |  train loss: 0.2473646641
Epoch:   800  |  train loss: 0.2431582451
Epoch:   900  |  train loss: 0.2437241703
Epoch:  1000  |  train loss: 0.2419541538
Processing class: 20
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.2816199481
Epoch:   200  |  train loss: 0.2741424859
Epoch:   300  |  train loss: 0.2728567243
Epoch:   400  |  train loss: 0.2661358714
Epoch:   500  |  train loss: 0.2667886674
Epoch:   600  |  train loss: 0.2614927649
Epoch:   700  |  train loss: 0.2581041515
Epoch:   800  |  train loss: 0.2567903548
Epoch:   900  |  train loss: 0.2549405903
Epoch:  1000  |  train loss: 0.2553967714
Processing class: 21
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.2841849387
Epoch:   200  |  train loss: 0.2771388412
Epoch:   300  |  train loss: 0.2623025298
Epoch:   400  |  train loss: 0.2546035737
Epoch:   500  |  train loss: 0.2499704748
Epoch:   600  |  train loss: 0.2476292133
Epoch:   700  |  train loss: 0.2446912289
Epoch:   800  |  train loss: 0.2406104475
Epoch:   900  |  train loss: 0.2394102246
Epoch:  1000  |  train loss: 0.2349922299
Processing class: 22
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.2918741822
Epoch:   200  |  train loss: 0.2906071007
Epoch:   300  |  train loss: 0.2890760601
Epoch:   400  |  train loss: 0.2809322715
Epoch:   500  |  train loss: 0.2759680390
Epoch:   600  |  train loss: 0.2695226252
Epoch:   700  |  train loss: 0.2659230590
Epoch:   800  |  train loss: 0.2607755899
Epoch:   900  |  train loss: 0.2570930094
Epoch:  1000  |  train loss: 0.2585131645
Processing class: 23
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.2786311686
Epoch:   200  |  train loss: 0.2792969346
Epoch:   300  |  train loss: 0.2753071427
Epoch:   400  |  train loss: 0.2670122802
Epoch:   500  |  train loss: 0.2628256798
Epoch:   600  |  train loss: 0.2551381558
Epoch:   700  |  train loss: 0.2518234968
Epoch:   800  |  train loss: 0.2468511343
Epoch:   900  |  train loss: 0.2452057242
Epoch:  1000  |  train loss: 0.2400280833
Processing class: 24
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.2829627752
Epoch:   200  |  train loss: 0.2818055809
Epoch:   300  |  train loss: 0.2803004146
Epoch:   400  |  train loss: 0.2743536949
Epoch:   500  |  train loss: 0.2689237595
Epoch:   600  |  train loss: 0.2668902695
Epoch:   700  |  train loss: 0.2601378679
Epoch:   800  |  train loss: 0.2589055181
Epoch:   900  |  train loss: 0.2579253852
Epoch:  1000  |  train loss: 0.2538567573
Processing class: 25
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.2863426566
Epoch:   200  |  train loss: 0.2808423102
Epoch:   300  |  train loss: 0.2773701370
Epoch:   400  |  train loss: 0.2662098289
Epoch:   500  |  train loss: 0.2619248211
Epoch:   600  |  train loss: 0.2566658765
Epoch:   700  |  train loss: 0.2563690841
Epoch:   800  |  train loss: 0.2520051599
Epoch:   900  |  train loss: 0.2479070902
Epoch:  1000  |  train loss: 0.2471237063
Processing class: 26
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.2782395959
Epoch:   200  |  train loss: 0.2799407840
Epoch:   300  |  train loss: 0.2802552402
Epoch:   400  |  train loss: 0.2772506714
Epoch:   500  |  train loss: 0.2740203083
Epoch:   600  |  train loss: 0.2632593989
Epoch:   700  |  train loss: 0.2614618659
Epoch:   800  |  train loss: 0.2603582948
Epoch:   900  |  train loss: 0.2563042104
Epoch:  1000  |  train loss: 0.2561845481
Processing class: 27
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.2927555978
Epoch:   200  |  train loss: 0.2813513458
Epoch:   300  |  train loss: 0.2793712795
Epoch:   400  |  train loss: 0.2715415359
Epoch:   500  |  train loss: 0.2657985151
Epoch:   600  |  train loss: 0.2633668900
Epoch:   700  |  train loss: 0.2616456687
Epoch:   800  |  train loss: 0.2588862598
Epoch:   900  |  train loss: 0.2563980103
Epoch:  1000  |  train loss: 0.2496534705
Processing class: 28
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.2867711782
Epoch:   200  |  train loss: 0.2869343460
Epoch:   300  |  train loss: 0.2797006786
Epoch:   400  |  train loss: 0.2738310814
Epoch:   500  |  train loss: 0.2693204403
Epoch:   600  |  train loss: 0.2668988645
Epoch:   700  |  train loss: 0.2659334540
Epoch:   800  |  train loss: 0.2645447671
Epoch:   900  |  train loss: 0.2633359492
Epoch:  1000  |  train loss: 0.2597381890
Processing class: 29
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.2854064465
Epoch:   200  |  train loss: 0.2768796206
Epoch:   300  |  train loss: 0.2685142577
Epoch:   400  |  train loss: 0.2590700209
Epoch:   500  |  train loss: 0.2565032363
Epoch:   600  |  train loss: 0.2535135210
Epoch:   700  |  train loss: 0.2487572759
Epoch:   800  |  train loss: 0.2428940713
Epoch:   900  |  train loss: 0.2403917432
Epoch:  1000  |  train loss: 0.2373674721
Processing class: 30
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.2775758386
Epoch:   200  |  train loss: 0.2781135976
Epoch:   300  |  train loss: 0.2698013067
Epoch:   400  |  train loss: 0.2691293836
Epoch:   500  |  train loss: 0.2669691503
Epoch:   600  |  train loss: 0.2632015526
Epoch:   700  |  train loss: 0.2581573308
Epoch:   800  |  train loss: 0.2575684965
Epoch:   900  |  train loss: 0.2566061735
Epoch:  1000  |  train loss: 0.2527531683
Processing class: 31
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.2881287098
Epoch:   200  |  train loss: 0.2819098949
Epoch:   300  |  train loss: 0.2743078470
Epoch:   400  |  train loss: 0.2748110294
Epoch:   500  |  train loss: 0.2710726857
Epoch:   600  |  train loss: 0.2700430393
Epoch:   700  |  train loss: 0.2662980855
Epoch:   800  |  train loss: 0.2646692038
Epoch:   900  |  train loss: 0.2634344041
Epoch:  1000  |  train loss: 0.2587735236
Processing class: 32
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.2967402041
Epoch:   200  |  train loss: 0.2961568892
Epoch:   300  |  train loss: 0.2894592226
Epoch:   400  |  train loss: 0.2802665353
Epoch:   500  |  train loss: 0.2771668553
Epoch:   600  |  train loss: 0.2687326014
Epoch:   700  |  train loss: 0.2669316351
Epoch:   800  |  train loss: 0.2646202624
Epoch:   900  |  train loss: 0.2609512329
Epoch:  1000  |  train loss: 0.2574051678
Processing class: 33
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.2842699409
Epoch:   200  |  train loss: 0.2871197581
Epoch:   300  |  train loss: 0.2721136034
Epoch:   400  |  train loss: 0.2646932364
Epoch:   500  |  train loss: 0.2597193539
Epoch:   600  |  train loss: 0.2579002678
Epoch:   700  |  train loss: 0.2528386712
Epoch:   800  |  train loss: 0.2506511271
Epoch:   900  |  train loss: 0.2472501427
Epoch:  1000  |  train loss: 0.2433526754
Processing class: 34
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.2909661651
Epoch:   200  |  train loss: 0.2872038603
Epoch:   300  |  train loss: 0.2784507155
Epoch:   400  |  train loss: 0.2724422157
Epoch:   500  |  train loss: 0.2660257757
Epoch:   600  |  train loss: 0.2630751669
Epoch:   700  |  train loss: 0.2568772197
Epoch:   800  |  train loss: 0.2557850093
Epoch:   900  |  train loss: 0.2535019249
Epoch:  1000  |  train loss: 0.2511877775
Processing class: 35
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.3052092075
Epoch:   200  |  train loss: 0.2934048533
Epoch:   300  |  train loss: 0.2892801523
Epoch:   400  |  train loss: 0.2859370649
Epoch:   500  |  train loss: 0.2790537119
Epoch:   600  |  train loss: 0.2787044823
Epoch:   700  |  train loss: 0.2755445659
Epoch:   800  |  train loss: 0.2722172558
Epoch:   900  |  train loss: 0.2700982273
Epoch:  1000  |  train loss: 0.2664026529
Processing class: 36
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.2696563244
Epoch:   200  |  train loss: 0.2648248315
Epoch:   300  |  train loss: 0.2597455561
Epoch:   400  |  train loss: 0.2568977177
Epoch:   500  |  train loss: 0.2545626342
Epoch:   600  |  train loss: 0.2482267827
Epoch:   700  |  train loss: 0.2425064445
Epoch:   800  |  train loss: 0.2420423955
Epoch:   900  |  train loss: 0.2360835642
Epoch:  1000  |  train loss: 0.2348337352
Processing class: 37
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.2978487313
Epoch:   200  |  train loss: 0.2931113005
Epoch:   300  |  train loss: 0.2841934085
Epoch:   400  |  train loss: 0.2785692155
Epoch:   500  |  train loss: 0.2747796059
Epoch:   600  |  train loss: 0.2690220773
Epoch:   700  |  train loss: 0.2655892074
Epoch:   800  |  train loss: 0.2640079260
Epoch:   900  |  train loss: 0.2586941063
Epoch:  1000  |  train loss: 0.2573699415
Processing class: 38
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.2763800442
Epoch:   200  |  train loss: 0.2756366014
Epoch:   300  |  train loss: 0.2733490944
Epoch:   400  |  train loss: 0.2720177710
Epoch:   500  |  train loss: 0.2661566556
Epoch:   600  |  train loss: 0.2599415243
Epoch:   700  |  train loss: 0.2549133450
Epoch:   800  |  train loss: 0.2550189465
Epoch:   900  |  train loss: 0.2501426548
Epoch:  1000  |  train loss: 0.2501241088
Processing class: 39
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.2853615046
Epoch:   200  |  train loss: 0.2804137886
Epoch:   300  |  train loss: 0.2765347242
Epoch:   400  |  train loss: 0.2721307099
Epoch:   500  |  train loss: 0.2654027581
Epoch:   600  |  train loss: 0.2626908958
Epoch:   700  |  train loss: 0.2595628947
Epoch:   800  |  train loss: 0.2572984457
Epoch:   900  |  train loss: 0.2539938331
Epoch:  1000  |  train loss: 0.2513331205
Processing class: 40
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.2816089332
Epoch:   200  |  train loss: 0.2697450757
Epoch:   300  |  train loss: 0.2704398215
Epoch:   400  |  train loss: 0.2672610343
Epoch:   500  |  train loss: 0.2646974385
Epoch:   600  |  train loss: 0.2567321926
Epoch:   700  |  train loss: 0.2558543801
Epoch:   800  |  train loss: 0.2531466544
Epoch:   900  |  train loss: 0.2500380278
Epoch:  1000  |  train loss: 0.2512215823
Processing class: 41
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.2705105066
Epoch:   200  |  train loss: 0.2716502726
Epoch:   300  |  train loss: 0.2620877743
Epoch:   400  |  train loss: 0.2533644617
Epoch:   500  |  train loss: 0.2491166949
Epoch:   600  |  train loss: 0.2462735325
Epoch:   700  |  train loss: 0.2452485710
Epoch:   800  |  train loss: 0.2420012623
Epoch:   900  |  train loss: 0.2393065870
Epoch:  1000  |  train loss: 0.2373408079
Processing class: 42
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.2898943126
Epoch:   200  |  train loss: 0.2847458124
Epoch:   300  |  train loss: 0.2783859789
Epoch:   400  |  train loss: 0.2713587403
Epoch:   500  |  train loss: 0.2663090885
Epoch:   600  |  train loss: 0.2609288931
Epoch:   700  |  train loss: 0.2577919334
Epoch:   800  |  train loss: 0.2538951159
Epoch:   900  |  train loss: 0.2488830805
Epoch:  1000  |  train loss: 0.2455811143
Processing class: 43
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.2954070866
Epoch:   200  |  train loss: 0.2882381618
Epoch:   300  |  train loss: 0.2801602364
Epoch:   400  |  train loss: 0.2802958250
Epoch:   500  |  train loss: 0.2784065247
Epoch:   600  |  train loss: 0.2765039444
Epoch:   700  |  train loss: 0.2768079996
Epoch:   800  |  train loss: 0.2745574057
Epoch:   900  |  train loss: 0.2699031651
Epoch:  1000  |  train loss: 0.2705282986
Processing class: 44
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.2952330351
Epoch:   200  |  train loss: 0.2872605622
Epoch:   300  |  train loss: 0.2823314488
Epoch:   400  |  train loss: 0.2756915331
Epoch:   500  |  train loss: 0.2732785583
Epoch:   600  |  train loss: 0.2696361125
Epoch:   700  |  train loss: 0.2688409805
Epoch:   800  |  train loss: 0.2652002752
Epoch:   900  |  train loss: 0.2609636366
Epoch:  1000  |  train loss: 0.2601389319
Processing class: 45
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.2829564214
Epoch:   200  |  train loss: 0.2797955453
Epoch:   300  |  train loss: 0.2699174225
Epoch:   400  |  train loss: 0.2733789265
Epoch:   500  |  train loss: 0.2715001166
Epoch:   600  |  train loss: 0.2712779343
Epoch:   700  |  train loss: 0.2686386526
Epoch:   800  |  train loss: 0.2662468910
Epoch:   900  |  train loss: 0.2659866929
Epoch:  1000  |  train loss: 0.2658759058
Processing class: 46
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.2755542040
Epoch:   200  |  train loss: 0.2714948356
Epoch:   300  |  train loss: 0.2646850109
Epoch:   400  |  train loss: 0.2648120403
Epoch:   500  |  train loss: 0.2631640017
Epoch:   600  |  train loss: 0.2621858656
Epoch:   700  |  train loss: 0.2605824709
Epoch:   800  |  train loss: 0.2605459630
Epoch:   900  |  train loss: 0.2565372616
Epoch:  1000  |  train loss: 0.2562535077
Processing class: 47
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.2920558512
Epoch:   200  |  train loss: 0.2871987939
Epoch:   300  |  train loss: 0.2741607547
Epoch:   400  |  train loss: 0.2701006413
Epoch:   500  |  train loss: 0.2651995420
Epoch:   600  |  train loss: 0.2595428884
Epoch:   700  |  train loss: 0.2556246549
Epoch:   800  |  train loss: 0.2506521344
Epoch:   900  |  train loss: 0.2483153969
Epoch:  1000  |  train loss: 0.2449740440
Processing class: 48
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.2901650250
Epoch:   200  |  train loss: 0.2891481102
Epoch:   300  |  train loss: 0.2741632879
Epoch:   400  |  train loss: 0.2663185000
Epoch:   500  |  train loss: 0.2687451780
Epoch:   600  |  train loss: 0.2642526329
Epoch:   700  |  train loss: 0.2598907560
Epoch:   800  |  train loss: 0.2583352864
Epoch:   900  |  train loss: 0.2601025879
Epoch:  1000  |  train loss: 0.2584103495
Processing class: 49
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.2932008445
Epoch:   200  |  train loss: 0.2877430677
Epoch:   300  |  train loss: 0.2826756597
Epoch:   400  |  train loss: 0.2776776612
Epoch:   500  |  train loss: 0.2723454297
Epoch:   600  |  train loss: 0.2693751216
Epoch:   700  |  train loss: 0.2652442098
Epoch:   800  |  train loss: 0.2618910044
Epoch:   900  |  train loss: 0.2590643644
Epoch:  1000  |  train loss: 0.2569997728
/home/z1165703/FeCAM/models/base.py:87: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  vec = torch.tensor(vec).cuda()
2024-03-05 05:16:10,393 [trainer.py] => CNN: {'total': 83.44, '00-09': 87.7, '10-19': 79.6, '20-29': 84.9, '30-39': 81.0, '40-49': 84.0, 'old': 0, 'new': 83.44}
2024-03-05 05:16:10,425 [trainer.py] => No NME accuracy
2024-03-05 05:16:10,425 [trainer.py] => FeCAM: {'total': 82.9, '00-09': 86.2, '10-19': 79.7, '20-29': 85.0, '30-39': 80.6, '40-49': 83.0, 'old': 0, 'new': 82.9}
2024-03-05 05:16:10,425 [trainer.py] => CNN top1 curve: [83.44]
2024-03-05 05:16:10,425 [trainer.py] => CNN top5 curve: [96.5]
2024-03-05 05:16:10,425 [trainer.py] => FeCAM top1 curve: [82.9]
2024-03-05 05:16:10,425 [trainer.py] => FeCAM top5 curve: [95.84]

2024-03-05 05:16:10,435 [fecam.py] => Learning on 50-60
Processing class: 50
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.3137078166
Epoch:   200  |  train loss: 0.3038240314
Epoch:   300  |  train loss: 0.2973420680
Epoch:   400  |  train loss: 0.2925679028
Epoch:   500  |  train loss: 0.2857734203
Epoch:   600  |  train loss: 0.2784443140
Epoch:   700  |  train loss: 0.2757242143
Epoch:   800  |  train loss: 0.2727143884
Epoch:   900  |  train loss: 0.2711134911
Epoch:  1000  |  train loss: 0.2686204493
Processing class: 51
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.3163421631
Epoch:   200  |  train loss: 0.3030404449
Epoch:   300  |  train loss: 0.2991177320
Epoch:   400  |  train loss: 0.2941705346
Epoch:   500  |  train loss: 0.2861824393
Epoch:   600  |  train loss: 0.2800350785
Epoch:   700  |  train loss: 0.2730396807
Epoch:   800  |  train loss: 0.2700143933
Epoch:   900  |  train loss: 0.2695814312
Epoch:  1000  |  train loss: 0.2640587091
Processing class: 52
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.3267219663
Epoch:   200  |  train loss: 0.3212903261
Epoch:   300  |  train loss: 0.3156981826
Epoch:   400  |  train loss: 0.3106981754
Epoch:   500  |  train loss: 0.3045440972
Epoch:   600  |  train loss: 0.2974950969
Epoch:   700  |  train loss: 0.2954347372
Epoch:   800  |  train loss: 0.2913051426
Epoch:   900  |  train loss: 0.2875116050
Epoch:  1000  |  train loss: 0.2818581104
Processing class: 53
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.3017278075
Epoch:   200  |  train loss: 0.2906087995
Epoch:   300  |  train loss: 0.2831002176
Epoch:   400  |  train loss: 0.2776054144
Epoch:   500  |  train loss: 0.2757195413
Epoch:   600  |  train loss: 0.2746116161
Epoch:   700  |  train loss: 0.2733483255
Epoch:   800  |  train loss: 0.2657125592
Epoch:   900  |  train loss: 0.2657962441
Epoch:  1000  |  train loss: 0.2614506721
Processing class: 54
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.2837056875
Epoch:   200  |  train loss: 0.2744515002
Epoch:   300  |  train loss: 0.2660826087
Epoch:   400  |  train loss: 0.2613482714
Epoch:   500  |  train loss: 0.2574919432
Epoch:   600  |  train loss: 0.2557971120
Epoch:   700  |  train loss: 0.2526196420
Epoch:   800  |  train loss: 0.2514179438
Epoch:   900  |  train loss: 0.2501090199
Epoch:  1000  |  train loss: 0.2458854645
Processing class: 55
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.3288427174
Epoch:   200  |  train loss: 0.3258070767
Epoch:   300  |  train loss: 0.3202155769
Epoch:   400  |  train loss: 0.3187204719
Epoch:   500  |  train loss: 0.3177066743
Epoch:   600  |  train loss: 0.3130025268
Epoch:   700  |  train loss: 0.3097602606
Epoch:   800  |  train loss: 0.3083392799
Epoch:   900  |  train loss: 0.3002556324
Epoch:  1000  |  train loss: 0.2986974180
Processing class: 56
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.3095777929
Epoch:   200  |  train loss: 0.2962374568
Epoch:   300  |  train loss: 0.2782137692
Epoch:   400  |  train loss: 0.2719947815
Epoch:   500  |  train loss: 0.2644990772
Epoch:   600  |  train loss: 0.2577084064
Epoch:   700  |  train loss: 0.2530450135
Epoch:   800  |  train loss: 0.2469186187
Epoch:   900  |  train loss: 0.2450808197
Epoch:  1000  |  train loss: 0.2428515673
Processing class: 57
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.3268645346
Epoch:   200  |  train loss: 0.3261200547
Epoch:   300  |  train loss: 0.3206265569
Epoch:   400  |  train loss: 0.3157056093
Epoch:   500  |  train loss: 0.3125001073
Epoch:   600  |  train loss: 0.3059954703
Epoch:   700  |  train loss: 0.3059389770
Epoch:   800  |  train loss: 0.3017082512
Epoch:   900  |  train loss: 0.2996339023
Epoch:  1000  |  train loss: 0.2921444297
Processing class: 58
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.3155707121
Epoch:   200  |  train loss: 0.2936994672
Epoch:   300  |  train loss: 0.2849173129
Epoch:   400  |  train loss: 0.2769174218
Epoch:   500  |  train loss: 0.2704334617
Epoch:   600  |  train loss: 0.2678020239
Epoch:   700  |  train loss: 0.2620532513
Epoch:   800  |  train loss: 0.2602147162
Epoch:   900  |  train loss: 0.2564987361
Epoch:  1000  |  train loss: 0.2528012514
Processing class: 59
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.3269404471
Epoch:   200  |  train loss: 0.3231448293
Epoch:   300  |  train loss: 0.3116259456
Epoch:   400  |  train loss: 0.3040545642
Epoch:   500  |  train loss: 0.2990843654
Epoch:   600  |  train loss: 0.2948981762
Epoch:   700  |  train loss: 0.2919200003
Epoch:   800  |  train loss: 0.2887967169
Epoch:   900  |  train loss: 0.2848438919
Epoch:  1000  |  train loss: 0.2820026696
2024-03-05 05:22:02,043 [trainer.py] => CNN: {'total': 71.75, '00-09': 82.2, '10-19': 72.8, '20-29': 78.5, '30-39': 74.9, '40-49': 68.4, '50-59': 53.7, 'old': 75.36, 'new': 53.7}
2024-03-05 05:22:02,045 [trainer.py] => No NME accuracy
2024-03-05 05:22:02,045 [trainer.py] => FeCAM: {'total': 72.78, '00-09': 82.6, '10-19': 74.0, '20-29': 81.0, '30-39': 76.4, '40-49': 76.8, '50-59': 45.9, 'old': 78.16, 'new': 45.9}
2024-03-05 05:22:02,045 [trainer.py] => CNN top1 curve: [83.44, 71.75]
2024-03-05 05:22:02,045 [trainer.py] => CNN top5 curve: [96.5, 89.6]
2024-03-05 05:22:02,045 [trainer.py] => FeCAM top1 curve: [82.9, 72.78]
2024-03-05 05:22:02,045 [trainer.py] => FeCAM top5 curve: [95.84, 91.18]

2024-03-05 05:22:02,056 [fecam.py] => Learning on 60-70
Processing class: 60
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.3259421051
Epoch:   200  |  train loss: 0.3187276363
Epoch:   300  |  train loss: 0.3082910001
Epoch:   400  |  train loss: 0.2995708227
Epoch:   500  |  train loss: 0.2946658194
Epoch:   600  |  train loss: 0.2878004193
Epoch:   700  |  train loss: 0.2874299109
Epoch:   800  |  train loss: 0.2777974069
Epoch:   900  |  train loss: 0.2742686868
Epoch:  1000  |  train loss: 0.2716119170
Processing class: 61
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.3081242263
Epoch:   200  |  train loss: 0.2914610088
Epoch:   300  |  train loss: 0.2814166844
Epoch:   400  |  train loss: 0.2709458768
Epoch:   500  |  train loss: 0.2673268318
Epoch:   600  |  train loss: 0.2593365490
Epoch:   700  |  train loss: 0.2566523790
Epoch:   800  |  train loss: 0.2539773077
Epoch:   900  |  train loss: 0.2471741438
Epoch:  1000  |  train loss: 0.2457569242
Processing class: 62
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.3312181830
Epoch:   200  |  train loss: 0.3255309820
Epoch:   300  |  train loss: 0.3241118848
Epoch:   400  |  train loss: 0.3213594079
Epoch:   500  |  train loss: 0.3161087930
Epoch:   600  |  train loss: 0.3157512009
Epoch:   700  |  train loss: 0.3134661198
Epoch:   800  |  train loss: 0.3100827098
Epoch:   900  |  train loss: 0.3052594423
Epoch:  1000  |  train loss: 0.3030322373
Processing class: 63
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.3162046790
Epoch:   200  |  train loss: 0.3119722784
Epoch:   300  |  train loss: 0.3019432366
Epoch:   400  |  train loss: 0.2919660091
Epoch:   500  |  train loss: 0.2848597109
Epoch:   600  |  train loss: 0.2822163880
Epoch:   700  |  train loss: 0.2738331139
Epoch:   800  |  train loss: 0.2750464797
Epoch:   900  |  train loss: 0.2680066288
Epoch:  1000  |  train loss: 0.2659128308
Processing class: 64
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.3152028203
Epoch:   200  |  train loss: 0.2900569141
Epoch:   300  |  train loss: 0.2825596154
Epoch:   400  |  train loss: 0.2747521222
Epoch:   500  |  train loss: 0.2657061040
Epoch:   600  |  train loss: 0.2588282883
Epoch:   700  |  train loss: 0.2515196621
Epoch:   800  |  train loss: 0.2491459548
Epoch:   900  |  train loss: 0.2482196271
Epoch:  1000  |  train loss: 0.2431739360
Processing class: 65
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.3097156346
Epoch:   200  |  train loss: 0.2929641068
Epoch:   300  |  train loss: 0.2814821184
Epoch:   400  |  train loss: 0.2740146458
Epoch:   500  |  train loss: 0.2664005876
Epoch:   600  |  train loss: 0.2640275121
Epoch:   700  |  train loss: 0.2591637671
Epoch:   800  |  train loss: 0.2581329793
Epoch:   900  |  train loss: 0.2519990474
Epoch:  1000  |  train loss: 0.2519193858
Processing class: 66
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.3220358849
Epoch:   200  |  train loss: 0.3144083679
Epoch:   300  |  train loss: 0.3066000938
Epoch:   400  |  train loss: 0.2995655298
Epoch:   500  |  train loss: 0.2977514327
Epoch:   600  |  train loss: 0.2936948776
Epoch:   700  |  train loss: 0.2929635763
Epoch:   800  |  train loss: 0.2907114327
Epoch:   900  |  train loss: 0.2875462711
Epoch:  1000  |  train loss: 0.2851923883
Processing class: 67
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.3194939613
Epoch:   200  |  train loss: 0.3138485253
Epoch:   300  |  train loss: 0.3064568162
Epoch:   400  |  train loss: 0.3014519274
Epoch:   500  |  train loss: 0.2966580391
Epoch:   600  |  train loss: 0.2926190495
Epoch:   700  |  train loss: 0.2894001901
Epoch:   800  |  train loss: 0.2875850081
Epoch:   900  |  train loss: 0.2837724030
Epoch:  1000  |  train loss: 0.2807024121
Processing class: 68
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.3040652156
Epoch:   200  |  train loss: 0.2980761707
Epoch:   300  |  train loss: 0.2824006140
Epoch:   400  |  train loss: 0.2762394071
Epoch:   500  |  train loss: 0.2717946529
Epoch:   600  |  train loss: 0.2665716588
Epoch:   700  |  train loss: 0.2639918208
Epoch:   800  |  train loss: 0.2592866719
Epoch:   900  |  train loss: 0.2565226853
Epoch:  1000  |  train loss: 0.2571384102
Processing class: 69
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.2593995929
Epoch:   200  |  train loss: 0.2506616592
Epoch:   300  |  train loss: 0.2427152634
Epoch:   400  |  train loss: 0.2367002398
Epoch:   500  |  train loss: 0.2319599718
Epoch:   600  |  train loss: 0.2267520219
Epoch:   700  |  train loss: 0.2266693950
Epoch:   800  |  train loss: 0.2241931170
Epoch:   900  |  train loss: 0.2266068786
Epoch:  1000  |  train loss: 0.2212336153
2024-03-05 05:28:48,829 [trainer.py] => CNN: {'total': 64.66, '00-09': 75.0, '10-19': 70.1, '20-29': 77.1, '30-39': 70.6, '40-49': 64.0, '50-59': 45.8, '60-69': 50.0, 'old': 67.1, 'new': 50.0}
2024-03-05 05:28:49,103 [trainer.py] => No NME accuracy
2024-03-05 05:28:49,103 [trainer.py] => FeCAM: {'total': 67.27, '00-09': 79.9, '10-19': 70.6, '20-29': 79.1, '30-39': 71.9, '40-49': 74.1, '50-59': 41.2, '60-69': 54.1, 'old': 69.47, 'new': 54.1}
2024-03-05 05:28:49,103 [trainer.py] => CNN top1 curve: [83.44, 71.75, 64.66]
2024-03-05 05:28:49,103 [trainer.py] => CNN top5 curve: [96.5, 89.6, 86.54]
2024-03-05 05:28:49,103 [trainer.py] => FeCAM top1 curve: [82.9, 72.78, 67.27]
2024-03-05 05:28:49,103 [trainer.py] => FeCAM top5 curve: [95.84, 91.18, 87.21]

2024-03-05 05:28:49,107 [fecam.py] => Learning on 70-80
Processing class: 70
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.3132074535
Epoch:   200  |  train loss: 0.3041479647
Epoch:   300  |  train loss: 0.2928013265
Epoch:   400  |  train loss: 0.2872042239
Epoch:   500  |  train loss: 0.2780752361
Epoch:   600  |  train loss: 0.2744965672
Epoch:   700  |  train loss: 0.2709250033
Epoch:   800  |  train loss: 0.2619958818
Epoch:   900  |  train loss: 0.2612745106
Epoch:  1000  |  train loss: 0.2573303819
Processing class: 71
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.3236144602
Epoch:   200  |  train loss: 0.3188875318
Epoch:   300  |  train loss: 0.3055949330
Epoch:   400  |  train loss: 0.3001432717
Epoch:   500  |  train loss: 0.2947677135
Epoch:   600  |  train loss: 0.2892941594
Epoch:   700  |  train loss: 0.2840515077
Epoch:   800  |  train loss: 0.2800095916
Epoch:   900  |  train loss: 0.2776509523
Epoch:  1000  |  train loss: 0.2727496922
Processing class: 72
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.3178185225
Epoch:   200  |  train loss: 0.3118301451
Epoch:   300  |  train loss: 0.3038686693
Epoch:   400  |  train loss: 0.2961015701
Epoch:   500  |  train loss: 0.2924277008
Epoch:   600  |  train loss: 0.2856593490
Epoch:   700  |  train loss: 0.2824705303
Epoch:   800  |  train loss: 0.2774908125
Epoch:   900  |  train loss: 0.2742889762
Epoch:  1000  |  train loss: 0.2730049253
Processing class: 73
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.3206183136
Epoch:   200  |  train loss: 0.3104390562
Epoch:   300  |  train loss: 0.3013355076
Epoch:   400  |  train loss: 0.2935591698
Epoch:   500  |  train loss: 0.2941348255
Epoch:   600  |  train loss: 0.2904569566
Epoch:   700  |  train loss: 0.2847572982
Epoch:   800  |  train loss: 0.2817688942
Epoch:   900  |  train loss: 0.2784283578
Epoch:  1000  |  train loss: 0.2768404365
Processing class: 74
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.3304507732
Epoch:   200  |  train loss: 0.3298546433
Epoch:   300  |  train loss: 0.3207519770
Epoch:   400  |  train loss: 0.3126025856
Epoch:   500  |  train loss: 0.3076110542
Epoch:   600  |  train loss: 0.3012230635
Epoch:   700  |  train loss: 0.3010122836
Epoch:   800  |  train loss: 0.2929086626
Epoch:   900  |  train loss: 0.2896566331
Epoch:  1000  |  train loss: 0.2890245259
Processing class: 75
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.3206116676
Epoch:   200  |  train loss: 0.3128219545
Epoch:   300  |  train loss: 0.3062070727
Epoch:   400  |  train loss: 0.2996914029
Epoch:   500  |  train loss: 0.2963500977
Epoch:   600  |  train loss: 0.2935473025
Epoch:   700  |  train loss: 0.2833676040
Epoch:   800  |  train loss: 0.2836729467
Epoch:   900  |  train loss: 0.2802140713
Epoch:  1000  |  train loss: 0.2792824149
Processing class: 76
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.2584787667
Epoch:   200  |  train loss: 0.2449550152
Epoch:   300  |  train loss: 0.2379741251
Epoch:   400  |  train loss: 0.2347590625
Epoch:   500  |  train loss: 0.2325015932
Epoch:   600  |  train loss: 0.2308819979
Epoch:   700  |  train loss: 0.2300460547
Epoch:   800  |  train loss: 0.2240123272
Epoch:   900  |  train loss: 0.2248757541
Epoch:  1000  |  train loss: 0.2246632069
Processing class: 77
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.3079864085
Epoch:   200  |  train loss: 0.2987397671
Epoch:   300  |  train loss: 0.2857830286
Epoch:   400  |  train loss: 0.2791656494
Epoch:   500  |  train loss: 0.2703095078
Epoch:   600  |  train loss: 0.2629072249
Epoch:   700  |  train loss: 0.2581034422
Epoch:   800  |  train loss: 0.2558989733
Epoch:   900  |  train loss: 0.2509961814
Epoch:  1000  |  train loss: 0.2468270600
Processing class: 78
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.3161520898
Epoch:   200  |  train loss: 0.3033104420
Epoch:   300  |  train loss: 0.2980777919
Epoch:   400  |  train loss: 0.2900528371
Epoch:   500  |  train loss: 0.2814795434
Epoch:   600  |  train loss: 0.2781596243
Epoch:   700  |  train loss: 0.2742389917
Epoch:   800  |  train loss: 0.2706910372
Epoch:   900  |  train loss: 0.2681926012
Epoch:  1000  |  train loss: 0.2650597632
Processing class: 79
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.3298147500
Epoch:   200  |  train loss: 0.3225754201
Epoch:   300  |  train loss: 0.3209434986
Epoch:   400  |  train loss: 0.3147325993
Epoch:   500  |  train loss: 0.3103590250
Epoch:   600  |  train loss: 0.3082492173
Epoch:   700  |  train loss: 0.3039863050
Epoch:   800  |  train loss: 0.3004552603
Epoch:   900  |  train loss: 0.2984405935
Epoch:  1000  |  train loss: 0.2923900902
2024-03-05 05:36:32,977 [trainer.py] => CNN: {'total': 59.18, '00-09': 73.6, '10-19': 68.4, '20-29': 76.9, '30-39': 69.1, '40-49': 60.6, '50-59': 37.2, '60-69': 45.7, '70-79': 41.9, 'old': 61.64, 'new': 41.9}
2024-03-05 05:36:33,220 [trainer.py] => No NME accuracy
2024-03-05 05:36:33,220 [trainer.py] => FeCAM: {'total': 61.72, '00-09': 78.1, '10-19': 69.3, '20-29': 78.0, '30-39': 69.5, '40-49': 72.1, '50-59': 35.7, '60-69': 49.1, '70-79': 42.0, 'old': 64.54, 'new': 42.0}
2024-03-05 05:36:33,220 [trainer.py] => CNN top1 curve: [83.44, 71.75, 64.66, 59.18]
2024-03-05 05:36:33,220 [trainer.py] => CNN top5 curve: [96.5, 89.6, 86.54, 84.09]
2024-03-05 05:36:33,220 [trainer.py] => FeCAM top1 curve: [82.9, 72.78, 67.27, 61.72]
2024-03-05 05:36:33,220 [trainer.py] => FeCAM top5 curve: [95.84, 91.18, 87.21, 84.09]

2024-03-05 05:36:33,230 [fecam.py] => Learning on 80-90
Processing class: 80
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.2847409666
Epoch:   200  |  train loss: 0.2631589204
Epoch:   300  |  train loss: 0.2552186996
Epoch:   400  |  train loss: 0.2504285187
Epoch:   500  |  train loss: 0.2450321138
Epoch:   600  |  train loss: 0.2421125263
Epoch:   700  |  train loss: 0.2364159793
Epoch:   800  |  train loss: 0.2380023330
Epoch:   900  |  train loss: 0.2352696091
Epoch:  1000  |  train loss: 0.2347980201
Processing class: 81
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.3094912946
Epoch:   200  |  train loss: 0.3012197077
Epoch:   300  |  train loss: 0.2902801931
Epoch:   400  |  train loss: 0.2874110520
Epoch:   500  |  train loss: 0.2809115171
Epoch:   600  |  train loss: 0.2788830698
Epoch:   700  |  train loss: 0.2739616513
Epoch:   800  |  train loss: 0.2728237092
Epoch:   900  |  train loss: 0.2687383235
Epoch:  1000  |  train loss: 0.2654168665
Processing class: 82
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.2986064613
Epoch:   200  |  train loss: 0.2873861969
Epoch:   300  |  train loss: 0.2791413367
Epoch:   400  |  train loss: 0.2709560752
Epoch:   500  |  train loss: 0.2648573220
Epoch:   600  |  train loss: 0.2596552968
Epoch:   700  |  train loss: 0.2559616774
Epoch:   800  |  train loss: 0.2537353277
Epoch:   900  |  train loss: 0.2504706532
Epoch:  1000  |  train loss: 0.2475239515
Processing class: 83
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.2949320793
Epoch:   200  |  train loss: 0.2749579012
Epoch:   300  |  train loss: 0.2614796579
Epoch:   400  |  train loss: 0.2531441599
Epoch:   500  |  train loss: 0.2494819760
Epoch:   600  |  train loss: 0.2434022427
Epoch:   700  |  train loss: 0.2430561990
Epoch:   800  |  train loss: 0.2431703210
Epoch:   900  |  train loss: 0.2392365813
Epoch:  1000  |  train loss: 0.2354781777
Processing class: 84
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.3191938698
Epoch:   200  |  train loss: 0.3016802967
Epoch:   300  |  train loss: 0.2942712247
Epoch:   400  |  train loss: 0.2850970984
Epoch:   500  |  train loss: 0.2801514030
Epoch:   600  |  train loss: 0.2732628942
Epoch:   700  |  train loss: 0.2711955845
Epoch:   800  |  train loss: 0.2698529482
Epoch:   900  |  train loss: 0.2633881480
Epoch:  1000  |  train loss: 0.2598030210
Processing class: 85
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.3198261976
Epoch:   200  |  train loss: 0.3047525465
Epoch:   300  |  train loss: 0.2957703114
Epoch:   400  |  train loss: 0.2864026487
Epoch:   500  |  train loss: 0.2821342826
Epoch:   600  |  train loss: 0.2785222054
Epoch:   700  |  train loss: 0.2776588380
Epoch:   800  |  train loss: 0.2716929436
Epoch:   900  |  train loss: 0.2687928379
Epoch:  1000  |  train loss: 0.2677915037
Processing class: 86
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.3311714470
Epoch:   200  |  train loss: 0.3221928358
Epoch:   300  |  train loss: 0.3155420005
Epoch:   400  |  train loss: 0.3061002970
Epoch:   500  |  train loss: 0.3005432248
Epoch:   600  |  train loss: 0.2961047769
Epoch:   700  |  train loss: 0.2916035533
Epoch:   800  |  train loss: 0.2871116638
Epoch:   900  |  train loss: 0.2837583244
Epoch:  1000  |  train loss: 0.2837600172
Processing class: 87
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.3014800191
Epoch:   200  |  train loss: 0.2843384206
Epoch:   300  |  train loss: 0.2766157389
Epoch:   400  |  train loss: 0.2681933165
Epoch:   500  |  train loss: 0.2618347645
Epoch:   600  |  train loss: 0.2602979034
Epoch:   700  |  train loss: 0.2575815648
Epoch:   800  |  train loss: 0.2541840017
Epoch:   900  |  train loss: 0.2494725317
Epoch:  1000  |  train loss: 0.2503504395
Processing class: 88
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.3141800344
Epoch:   200  |  train loss: 0.3043499708
Epoch:   300  |  train loss: 0.2940038383
Epoch:   400  |  train loss: 0.2862855911
Epoch:   500  |  train loss: 0.2776631773
Epoch:   600  |  train loss: 0.2693276286
Epoch:   700  |  train loss: 0.2666297793
Epoch:   800  |  train loss: 0.2622447908
Epoch:   900  |  train loss: 0.2594238579
Epoch:  1000  |  train loss: 0.2548888892
Processing class: 89
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.3128691554
Epoch:   200  |  train loss: 0.3022097588
Epoch:   300  |  train loss: 0.2907197297
Epoch:   400  |  train loss: 0.2849786758
Epoch:   500  |  train loss: 0.2804193199
Epoch:   600  |  train loss: 0.2739130497
Epoch:   700  |  train loss: 0.2673931241
Epoch:   800  |  train loss: 0.2673564970
Epoch:   900  |  train loss: 0.2623089910
Epoch:  1000  |  train loss: 0.2598971963
2024-03-05 05:45:47,409 [trainer.py] => CNN: {'total': 54.08, '00-09': 67.9, '10-19': 63.6, '20-29': 73.2, '30-39': 68.8, '40-49': 57.2, '50-59': 32.6, '60-69': 40.9, '70-79': 38.1, '80-89': 44.4, 'old': 55.29, 'new': 44.4}
2024-03-05 05:45:48,262 [trainer.py] => No NME accuracy
2024-03-05 05:45:48,262 [trainer.py] => FeCAM: {'total': 57.66, '00-09': 76.1, '10-19': 65.8, '20-29': 75.3, '30-39': 67.9, '40-49': 68.5, '50-59': 33.0, '60-69': 45.0, '70-79': 39.0, '80-89': 48.3, 'old': 58.82, 'new': 48.3}
2024-03-05 05:45:48,263 [trainer.py] => CNN top1 curve: [83.44, 71.75, 64.66, 59.18, 54.08]
2024-03-05 05:45:48,263 [trainer.py] => CNN top5 curve: [96.5, 89.6, 86.54, 84.09, 81.74]
2024-03-05 05:45:48,264 [trainer.py] => FeCAM top1 curve: [82.9, 72.78, 67.27, 61.72, 57.66]
2024-03-05 05:45:48,264 [trainer.py] => FeCAM top5 curve: [95.84, 91.18, 87.21, 84.09, 81.63]

2024-03-05 05:45:48,272 [fecam.py] => Learning on 90-100
Processing class: 90
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.3195065737
Epoch:   200  |  train loss: 0.3123649955
Epoch:   300  |  train loss: 0.2974777222
Epoch:   400  |  train loss: 0.2865943789
Epoch:   500  |  train loss: 0.2776570201
Epoch:   600  |  train loss: 0.2712952018
Epoch:   700  |  train loss: 0.2639328063
Epoch:   800  |  train loss: 0.2601289630
Epoch:   900  |  train loss: 0.2580382288
Epoch:  1000  |  train loss: 0.2568570733
Processing class: 91
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.2630851090
Epoch:   200  |  train loss: 0.2534082890
Epoch:   300  |  train loss: 0.2471499950
Epoch:   400  |  train loss: 0.2451961905
Epoch:   500  |  train loss: 0.2405486256
Epoch:   600  |  train loss: 0.2375649989
Epoch:   700  |  train loss: 0.2322823793
Epoch:   800  |  train loss: 0.2286563843
Epoch:   900  |  train loss: 0.2261347711
Epoch:  1000  |  train loss: 0.2243277103
Processing class: 92
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.3120268703
Epoch:   200  |  train loss: 0.2898995876
Epoch:   300  |  train loss: 0.2802748144
Epoch:   400  |  train loss: 0.2752798140
Epoch:   500  |  train loss: 0.2683930874
Epoch:   600  |  train loss: 0.2612217546
Epoch:   700  |  train loss: 0.2554586142
Epoch:   800  |  train loss: 0.2477306753
Epoch:   900  |  train loss: 0.2449616253
Epoch:  1000  |  train loss: 0.2420790702
Processing class: 93
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.3151350737
Epoch:   200  |  train loss: 0.2995271683
Epoch:   300  |  train loss: 0.2922022223
Epoch:   400  |  train loss: 0.2861754060
Epoch:   500  |  train loss: 0.2782235146
Epoch:   600  |  train loss: 0.2699731886
Epoch:   700  |  train loss: 0.2688241720
Epoch:   800  |  train loss: 0.2637994051
Epoch:   900  |  train loss: 0.2606040120
Epoch:  1000  |  train loss: 0.2558774471
Processing class: 94
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.2897447109
Epoch:   200  |  train loss: 0.2782944262
Epoch:   300  |  train loss: 0.2610009789
Epoch:   400  |  train loss: 0.2532177836
Epoch:   500  |  train loss: 0.2430868864
Epoch:   600  |  train loss: 0.2404148072
Epoch:   700  |  train loss: 0.2382464021
Epoch:   800  |  train loss: 0.2292798042
Epoch:   900  |  train loss: 0.2291715562
Epoch:  1000  |  train loss: 0.2251600534
Processing class: 95
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.3220733762
Epoch:   200  |  train loss: 0.3196785510
Epoch:   300  |  train loss: 0.3115273535
Epoch:   400  |  train loss: 0.3025810301
Epoch:   500  |  train loss: 0.2982258141
Epoch:   600  |  train loss: 0.2924787879
Epoch:   700  |  train loss: 0.2885251522
Epoch:   800  |  train loss: 0.2830879450
Epoch:   900  |  train loss: 0.2809476912
Epoch:  1000  |  train loss: 0.2791317821
Processing class: 96
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.3268875897
Epoch:   200  |  train loss: 0.3154161990
Epoch:   300  |  train loss: 0.3071351945
Epoch:   400  |  train loss: 0.3039931715
Epoch:   500  |  train loss: 0.2962054431
Epoch:   600  |  train loss: 0.2908963025
Epoch:   700  |  train loss: 0.2863488257
Epoch:   800  |  train loss: 0.2801985502
Epoch:   900  |  train loss: 0.2786692798
Epoch:  1000  |  train loss: 0.2744215310
Processing class: 97
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.3166639745
Epoch:   200  |  train loss: 0.3000676095
Epoch:   300  |  train loss: 0.2882708728
Epoch:   400  |  train loss: 0.2819862127
Epoch:   500  |  train loss: 0.2777293801
Epoch:   600  |  train loss: 0.2736417532
Epoch:   700  |  train loss: 0.2704803765
Epoch:   800  |  train loss: 0.2663138688
Epoch:   900  |  train loss: 0.2632028103
Epoch:  1000  |  train loss: 0.2596324205
Processing class: 98
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.2968945146
Epoch:   200  |  train loss: 0.2839720011
Epoch:   300  |  train loss: 0.2709564745
Epoch:   400  |  train loss: 0.2631087422
Epoch:   500  |  train loss: 0.2553717583
Epoch:   600  |  train loss: 0.2507665634
Epoch:   700  |  train loss: 0.2442488909
Epoch:   800  |  train loss: 0.2427206874
Epoch:   900  |  train loss: 0.2401110530
Epoch:  1000  |  train loss: 0.2368922740
Processing class: 99
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.3221994758
Epoch:   200  |  train loss: 0.3097774982
Epoch:   300  |  train loss: 0.2992180943
Epoch:   400  |  train loss: 0.2923351169
Epoch:   500  |  train loss: 0.2832107484
Epoch:   600  |  train loss: 0.2766773880
Epoch:   700  |  train loss: 0.2718888223
Epoch:   800  |  train loss: 0.2702274919
Epoch:   900  |  train loss: 0.2650287390
Epoch:  1000  |  train loss: 0.2622854829
2024-03-05 05:56:21,124 [trainer.py] => CNN: {'total': 50.36, '00-09': 57.7, '10-19': 63.6, '20-29': 71.4, '30-39': 68.5, '40-49': 56.8, '50-59': 29.6, '60-69': 38.9, '70-79': 36.6, '80-89': 42.7, '90-99': 37.8, 'old': 51.76, 'new': 37.8}
2024-03-05 05:56:21,125 [trainer.py] => No NME accuracy
2024-03-05 05:56:21,125 [trainer.py] => FeCAM: {'total': 54.74, '00-09': 73.3, '10-19': 65.1, '20-29': 74.1, '30-39': 67.2, '40-49': 67.1, '50-59': 30.8, '60-69': 43.3, '70-79': 37.2, '80-89': 46.9, '90-99': 42.4, 'old': 56.11, 'new': 42.4}
2024-03-05 05:56:21,125 [trainer.py] => CNN top1 curve: [83.44, 71.75, 64.66, 59.18, 54.08, 50.36]
2024-03-05 05:56:21,125 [trainer.py] => CNN top5 curve: [96.5, 89.6, 86.54, 84.09, 81.74, 79.63]
2024-03-05 05:56:21,125 [trainer.py] => FeCAM top1 curve: [82.9, 72.78, 67.27, 61.72, 57.66, 54.74]
2024-03-05 05:56:21,125 [trainer.py] => FeCAM top5 curve: [95.84, 91.18, 87.21, 84.09, 81.63, 79.21]

=========================================
2024-03-05 05:56:29,533 [trainer.py] => config: ./exps/FeCAM_cifar100.json
2024-03-05 05:56:29,533 [trainer.py] => prefix: train
2024-03-05 05:56:29,533 [trainer.py] => dataset: cifar100
2024-03-05 05:56:29,533 [trainer.py] => memory_size: 0
2024-03-05 05:56:29,533 [trainer.py] => shuffle: True
2024-03-05 05:56:29,533 [trainer.py] => init_cls: 50
2024-03-05 05:56:29,533 [trainer.py] => increment: 10
2024-03-05 05:56:29,533 [trainer.py] => model_name: fecam
2024-03-05 05:56:29,533 [trainer.py] => convnet_type: resnet18
2024-03-05 05:56:29,533 [trainer.py] => device: [device(type='cuda', index=0)]
2024-03-05 05:56:29,533 [trainer.py] => seed: 1993
2024-03-05 05:56:29,533 [trainer.py] => init_epochs: 200
2024-03-05 05:56:29,533 [trainer.py] => init_lr: 0.1
2024-03-05 05:56:29,533 [trainer.py] => init_weight_decay: 0.0005
2024-03-05 05:56:29,533 [trainer.py] => batch_size: 128
2024-03-05 05:56:29,533 [trainer.py] => num_workers: 8
2024-03-05 05:56:29,533 [trainer.py] => T: 5
2024-03-05 05:56:29,533 [trainer.py] => beta: 0.5
2024-03-05 05:56:29,533 [trainer.py] => alpha1: 1
2024-03-05 05:56:29,533 [trainer.py] => alpha2: 1
2024-03-05 05:56:29,533 [trainer.py] => ncm: False
2024-03-05 05:56:29,533 [trainer.py] => tukey: False
2024-03-05 05:56:29,533 [trainer.py] => diagonal: False
2024-03-05 05:56:29,533 [trainer.py] => per_class: True
2024-03-05 05:56:29,533 [trainer.py] => full_cov: True
2024-03-05 05:56:29,533 [trainer.py] => shrink: True
2024-03-05 05:56:29,533 [trainer.py] => norm_cov: False
2024-03-05 05:56:29,533 [trainer.py] => vecnorm: False
2024-03-05 05:56:29,534 [trainer.py] => ae_type: wae
2024-03-05 05:56:29,534 [trainer.py] => epochs: 1000
2024-03-05 05:56:29,534 [trainer.py] => ae_latent_dim: 32
2024-03-05 05:56:29,534 [trainer.py] => wae_sigma: 20
2024-03-05 05:56:29,534 [trainer.py] => wae_C: 0.5
Files already downloaded and verified
Files already downloaded and verified
2024-03-05 05:56:31,183 [data_manager.py] => [68, 56, 78, 8, 23, 84, 90, 65, 74, 76, 40, 89, 3, 92, 55, 9, 26, 80, 43, 38, 58, 70, 77, 1, 85, 19, 17, 50, 28, 53, 13, 81, 45, 82, 6, 59, 83, 16, 15, 44, 91, 41, 72, 60, 79, 52, 20, 10, 31, 54, 37, 95, 14, 71, 96, 98, 97, 2, 64, 66, 42, 22, 35, 86, 24, 34, 87, 21, 99, 0, 88, 27, 18, 94, 11, 12, 47, 25, 30, 46, 62, 69, 36, 61, 7, 63, 75, 5, 32, 4, 51, 48, 73, 93, 39, 67, 29, 49, 57, 33]
2024-03-05 05:56:31,645 [fecam.py] => Learning on 0-50
/home/z1165703/miniconda3/lib/python3.11/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
/home/z1165703/miniconda3/lib/python3.11/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
Processing class: 0
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.2170336485
Epoch:   200  |  train loss: 0.2138577312
Epoch:   300  |  train loss: 0.2163585156
Epoch:   400  |  train loss: 0.2156437486
Epoch:   500  |  train loss: 0.2132406324
Epoch:   600  |  train loss: 0.2131522357
Epoch:   700  |  train loss: 0.2134409547
Epoch:   800  |  train loss: 0.2135242999
Epoch:   900  |  train loss: 0.2115602404
Epoch:  1000  |  train loss: 0.2106275409
Processing class: 1
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.2218742609
Epoch:   200  |  train loss: 0.2201113939
Epoch:   300  |  train loss: 0.2234290093
Epoch:   400  |  train loss: 0.2233543903
Epoch:   500  |  train loss: 0.2178881109
Epoch:   600  |  train loss: 0.2218575239
Epoch:   700  |  train loss: 0.2187392950
Epoch:   800  |  train loss: 0.2199648470
Epoch:   900  |  train loss: 0.2225726247
Epoch:  1000  |  train loss: 0.2181674659
Processing class: 2
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.2246381849
Epoch:   200  |  train loss: 0.2214580536
Epoch:   300  |  train loss: 0.2226013720
Epoch:   400  |  train loss: 0.2187936187
Epoch:   500  |  train loss: 0.2102363557
Epoch:   600  |  train loss: 0.2127781302
Epoch:   700  |  train loss: 0.2100505114
Epoch:   800  |  train loss: 0.2121313691
Epoch:   900  |  train loss: 0.2081707418
Epoch:  1000  |  train loss: 0.2055957913
Processing class: 3
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.2220623195
Epoch:   200  |  train loss: 0.2256166130
Epoch:   300  |  train loss: 0.2255846649
Epoch:   400  |  train loss: 0.2233161449
Epoch:   500  |  train loss: 0.2198853672
Epoch:   600  |  train loss: 0.2225852966
Epoch:   700  |  train loss: 0.2228264421
Epoch:   800  |  train loss: 0.2191285580
Epoch:   900  |  train loss: 0.2192254543
Epoch:  1000  |  train loss: 0.2182539761
Processing class: 4
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.2208388895
Epoch:   200  |  train loss: 0.2179798096
Epoch:   300  |  train loss: 0.2192889243
Epoch:   400  |  train loss: 0.2168522656
Epoch:   500  |  train loss: 0.2168399006
Epoch:   600  |  train loss: 0.2134306788
Epoch:   700  |  train loss: 0.2148640543
Epoch:   800  |  train loss: 0.2145780921
Epoch:   900  |  train loss: 0.2144905031
Epoch:  1000  |  train loss: 0.2162328809
Processing class: 5
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.2263257146
Epoch:   200  |  train loss: 0.2208596140
Epoch:   300  |  train loss: 0.2197325259
Epoch:   400  |  train loss: 0.2169062853
Epoch:   500  |  train loss: 0.2153438359
Epoch:   600  |  train loss: 0.2191681981
Epoch:   700  |  train loss: 0.2180333495
Epoch:   800  |  train loss: 0.2137872487
Epoch:   900  |  train loss: 0.2115095884
Epoch:  1000  |  train loss: 0.2127677798
Processing class: 6
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.2215697944
Epoch:   200  |  train loss: 0.2232396156
Epoch:   300  |  train loss: 0.2248998612
Epoch:   400  |  train loss: 0.2215683639
Epoch:   500  |  train loss: 0.2237354189
Epoch:   600  |  train loss: 0.2212829828
Epoch:   700  |  train loss: 0.2190841764
Epoch:   800  |  train loss: 0.2190736979
Epoch:   900  |  train loss: 0.2152419835
Epoch:  1000  |  train loss: 0.2160358071
Processing class: 7
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.2232441604
Epoch:   200  |  train loss: 0.2211767912
Epoch:   300  |  train loss: 0.2208139449
Epoch:   400  |  train loss: 0.2188769341
Epoch:   500  |  train loss: 0.2180976301
Epoch:   600  |  train loss: 0.2154234827
Epoch:   700  |  train loss: 0.2160472900
Epoch:   800  |  train loss: 0.2183728307
Epoch:   900  |  train loss: 0.2136197388
Epoch:  1000  |  train loss: 0.2149826556
Processing class: 8
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.2220854372
Epoch:   200  |  train loss: 0.2227151752
Epoch:   300  |  train loss: 0.2175108284
Epoch:   400  |  train loss: 0.2201201767
Epoch:   500  |  train loss: 0.2165460229
Epoch:   600  |  train loss: 0.2155709594
Epoch:   700  |  train loss: 0.2176593810
Epoch:   800  |  train loss: 0.2133734941
Epoch:   900  |  train loss: 0.2143235594
Epoch:  1000  |  train loss: 0.2111050069
Processing class: 9
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.2166911751
Epoch:   200  |  train loss: 0.2189910471
Epoch:   300  |  train loss: 0.2218532801
Epoch:   400  |  train loss: 0.2161921054
Epoch:   500  |  train loss: 0.2189897954
Epoch:   600  |  train loss: 0.2157158405
Epoch:   700  |  train loss: 0.2167140037
Epoch:   800  |  train loss: 0.2171618283
Epoch:   900  |  train loss: 0.2165992588
Epoch:  1000  |  train loss: 0.2131710380
Processing class: 10
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.2196607947
Epoch:   200  |  train loss: 0.2159807891
Epoch:   300  |  train loss: 0.2157447845
Epoch:   400  |  train loss: 0.2185762972
Epoch:   500  |  train loss: 0.2110867947
Epoch:   600  |  train loss: 0.2114911199
Epoch:   700  |  train loss: 0.2118085861
Epoch:   800  |  train loss: 0.2093017787
Epoch:   900  |  train loss: 0.2097739249
Epoch:  1000  |  train loss: 0.2087073863
Processing class: 11
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.2265730143
Epoch:   200  |  train loss: 0.2236967325
Epoch:   300  |  train loss: 0.2198170274
Epoch:   400  |  train loss: 0.2197999120
Epoch:   500  |  train loss: 0.2172930747
Epoch:   600  |  train loss: 0.2186109573
Epoch:   700  |  train loss: 0.2213974535
Epoch:   800  |  train loss: 0.2196769834
Epoch:   900  |  train loss: 0.2185043603
Epoch:  1000  |  train loss: 0.2192650110
Processing class: 12
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.2184782654
Epoch:   200  |  train loss: 0.2232859910
Epoch:   300  |  train loss: 0.2207716048
Epoch:   400  |  train loss: 0.2204436868
Epoch:   500  |  train loss: 0.2191188574
Epoch:   600  |  train loss: 0.2152622610
Epoch:   700  |  train loss: 0.2148830622
Epoch:   800  |  train loss: 0.2150817275
Epoch:   900  |  train loss: 0.2162173450
Epoch:  1000  |  train loss: 0.2135567099
Processing class: 13
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.2186749488
Epoch:   200  |  train loss: 0.2199083745
Epoch:   300  |  train loss: 0.2203049272
Epoch:   400  |  train loss: 0.2182218850
Epoch:   500  |  train loss: 0.2188648969
Epoch:   600  |  train loss: 0.2154276729
Epoch:   700  |  train loss: 0.2157537103
Epoch:   800  |  train loss: 0.2169066727
Epoch:   900  |  train loss: 0.2173440129
Epoch:  1000  |  train loss: 0.2174410671
Processing class: 14
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.2276745141
Epoch:   200  |  train loss: 0.2293777436
Epoch:   300  |  train loss: 0.2282379419
Epoch:   400  |  train loss: 0.2288140386
Epoch:   500  |  train loss: 0.2260239303
Epoch:   600  |  train loss: 0.2243381977
Epoch:   700  |  train loss: 0.2256409556
Epoch:   800  |  train loss: 0.2288286626
Epoch:   900  |  train loss: 0.2267302036
Epoch:  1000  |  train loss: 0.2233208001
Processing class: 15
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.2191878408
Epoch:   200  |  train loss: 0.2196701050
Epoch:   300  |  train loss: 0.2191954434
Epoch:   400  |  train loss: 0.2187561989
Epoch:   500  |  train loss: 0.2195635200
Epoch:   600  |  train loss: 0.2159275264
Epoch:   700  |  train loss: 0.2186714858
Epoch:   800  |  train loss: 0.2147176862
Epoch:   900  |  train loss: 0.2178026766
Epoch:  1000  |  train loss: 0.2158363581
Processing class: 16
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.2218647391
Epoch:   200  |  train loss: 0.2249082386
Epoch:   300  |  train loss: 0.2261532903
Epoch:   400  |  train loss: 0.2227535635
Epoch:   500  |  train loss: 0.2249933243
Epoch:   600  |  train loss: 0.2218482554
Epoch:   700  |  train loss: 0.2196585357
Epoch:   800  |  train loss: 0.2227978915
Epoch:   900  |  train loss: 0.2178523242
Epoch:  1000  |  train loss: 0.2213211983
Processing class: 17
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.2155524164
Epoch:   200  |  train loss: 0.2177440017
Epoch:   300  |  train loss: 0.2207414865
Epoch:   400  |  train loss: 0.2193445116
Epoch:   500  |  train loss: 0.2116022348
Epoch:   600  |  train loss: 0.2127721757
Epoch:   700  |  train loss: 0.2108290762
Epoch:   800  |  train loss: 0.2099764973
Epoch:   900  |  train loss: 0.2099150360
Epoch:  1000  |  train loss: 0.2090492785
Processing class: 18
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.2186285377
Epoch:   200  |  train loss: 0.2192760646
Epoch:   300  |  train loss: 0.2164222747
Epoch:   400  |  train loss: 0.2138643056
Epoch:   500  |  train loss: 0.2144557536
Epoch:   600  |  train loss: 0.2104553163
Epoch:   700  |  train loss: 0.2147072077
Epoch:   800  |  train loss: 0.2127459109
Epoch:   900  |  train loss: 0.2126375675
Epoch:  1000  |  train loss: 0.2145058721
Processing class: 19
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.2162764221
Epoch:   200  |  train loss: 0.2205563247
Epoch:   300  |  train loss: 0.2183296025
Epoch:   400  |  train loss: 0.2177871913
Epoch:   500  |  train loss: 0.2131809741
Epoch:   600  |  train loss: 0.2129226416
Epoch:   700  |  train loss: 0.2133117080
Epoch:   800  |  train loss: 0.2109509736
Epoch:   900  |  train loss: 0.2154400140
Epoch:  1000  |  train loss: 0.2156668395
Processing class: 20
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.2209216654
Epoch:   200  |  train loss: 0.2200927466
Epoch:   300  |  train loss: 0.2213834435
Epoch:   400  |  train loss: 0.2201312035
Epoch:   500  |  train loss: 0.2229219407
Epoch:   600  |  train loss: 0.2194925666
Epoch:   700  |  train loss: 0.2188011885
Epoch:   800  |  train loss: 0.2183927208
Epoch:   900  |  train loss: 0.2187017053
Epoch:  1000  |  train loss: 0.2206275672
Processing class: 21
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.2222294927
Epoch:   200  |  train loss: 0.2221476853
Epoch:   300  |  train loss: 0.2179299623
Epoch:   400  |  train loss: 0.2158284903
Epoch:   500  |  train loss: 0.2151151985
Epoch:   600  |  train loss: 0.2156269968
Epoch:   700  |  train loss: 0.2141517609
Epoch:   800  |  train loss: 0.2118227214
Epoch:   900  |  train loss: 0.2134695053
Epoch:  1000  |  train loss: 0.2110134125
Processing class: 22
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.2235584080
Epoch:   200  |  train loss: 0.2237528592
Epoch:   300  |  train loss: 0.2258165449
Epoch:   400  |  train loss: 0.2258464754
Epoch:   500  |  train loss: 0.2244161129
Epoch:   600  |  train loss: 0.2208417594
Epoch:   700  |  train loss: 0.2206211776
Epoch:   800  |  train loss: 0.2178563863
Epoch:   900  |  train loss: 0.2155670017
Epoch:  1000  |  train loss: 0.2197347820
Processing class: 23
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.2218221515
Epoch:   200  |  train loss: 0.2243526250
Epoch:   300  |  train loss: 0.2255986899
Epoch:   400  |  train loss: 0.2201611370
Epoch:   500  |  train loss: 0.2184498221
Epoch:   600  |  train loss: 0.2142655581
Epoch:   700  |  train loss: 0.2143996984
Epoch:   800  |  train loss: 0.2115013063
Epoch:   900  |  train loss: 0.2124199510
Epoch:  1000  |  train loss: 0.2089702189
Processing class: 24
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.2222042114
Epoch:   200  |  train loss: 0.2216607630
Epoch:   300  |  train loss: 0.2189229071
Epoch:   400  |  train loss: 0.2196161240
Epoch:   500  |  train loss: 0.2164970338
Epoch:   600  |  train loss: 0.2173205107
Epoch:   700  |  train loss: 0.2116557866
Epoch:   800  |  train loss: 0.2123172164
Epoch:   900  |  train loss: 0.2139088303
Epoch:  1000  |  train loss: 0.2115320891
Processing class: 25
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.2214559585
Epoch:   200  |  train loss: 0.2215307534
Epoch:   300  |  train loss: 0.2229136437
Epoch:   400  |  train loss: 0.2170994461
Epoch:   500  |  train loss: 0.2182442069
Epoch:   600  |  train loss: 0.2150147647
Epoch:   700  |  train loss: 0.2175478280
Epoch:   800  |  train loss: 0.2148976535
Epoch:   900  |  train loss: 0.2124748349
Epoch:  1000  |  train loss: 0.2131400168
Processing class: 26
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.2228489935
Epoch:   200  |  train loss: 0.2264102131
Epoch:   300  |  train loss: 0.2248804659
Epoch:   400  |  train loss: 0.2263246477
Epoch:   500  |  train loss: 0.2275271386
Epoch:   600  |  train loss: 0.2202267021
Epoch:   700  |  train loss: 0.2207798988
Epoch:   800  |  train loss: 0.2216160476
Epoch:   900  |  train loss: 0.2195072651
Epoch:  1000  |  train loss: 0.2215041876
Processing class: 27
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.2245445490
Epoch:   200  |  train loss: 0.2194467962
Epoch:   300  |  train loss: 0.2199468434
Epoch:   400  |  train loss: 0.2180394590
Epoch:   500  |  train loss: 0.2175437242
Epoch:   600  |  train loss: 0.2168353528
Epoch:   700  |  train loss: 0.2169023246
Epoch:   800  |  train loss: 0.2158191413
Epoch:   900  |  train loss: 0.2157588094
Epoch:  1000  |  train loss: 0.2107647032
Processing class: 28
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.2249732852
Epoch:   200  |  train loss: 0.2275526941
Epoch:   300  |  train loss: 0.2275953531
Epoch:   400  |  train loss: 0.2268018007
Epoch:   500  |  train loss: 0.2260652691
Epoch:   600  |  train loss: 0.2251460195
Epoch:   700  |  train loss: 0.2254505903
Epoch:   800  |  train loss: 0.2258022249
Epoch:   900  |  train loss: 0.2258666605
Epoch:  1000  |  train loss: 0.2227101594
Processing class: 29
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.2221559882
Epoch:   200  |  train loss: 0.2207761973
Epoch:   300  |  train loss: 0.2176870942
Epoch:   400  |  train loss: 0.2132640302
Epoch:   500  |  train loss: 0.2145522505
Epoch:   600  |  train loss: 0.2143755466
Epoch:   700  |  train loss: 0.2125376791
Epoch:   800  |  train loss: 0.2088639796
Epoch:   900  |  train loss: 0.2083091617
Epoch:  1000  |  train loss: 0.2066114277
Processing class: 30
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.2226946563
Epoch:   200  |  train loss: 0.2254913300
Epoch:   300  |  train loss: 0.2266052008
Epoch:   400  |  train loss: 0.2295912623
Epoch:   500  |  train loss: 0.2273467541
Epoch:   600  |  train loss: 0.2255325377
Epoch:   700  |  train loss: 0.2231146097
Epoch:   800  |  train loss: 0.2251729518
Epoch:   900  |  train loss: 0.2262985706
Epoch:  1000  |  train loss: 0.2231086940
Processing class: 31
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.2255036205
Epoch:   200  |  train loss: 0.2266435653
Epoch:   300  |  train loss: 0.2241450250
Epoch:   400  |  train loss: 0.2272792935
Epoch:   500  |  train loss: 0.2270664334
Epoch:   600  |  train loss: 0.2288502812
Epoch:   700  |  train loss: 0.2269169897
Epoch:   800  |  train loss: 0.2265514374
Epoch:   900  |  train loss: 0.2279799640
Epoch:  1000  |  train loss: 0.2240388453
Processing class: 32
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.2261987805
Epoch:   200  |  train loss: 0.2286218822
Epoch:   300  |  train loss: 0.2258301169
Epoch:   400  |  train loss: 0.2204380453
Epoch:   500  |  train loss: 0.2237823695
Epoch:   600  |  train loss: 0.2194592327
Epoch:   700  |  train loss: 0.2220969707
Epoch:   800  |  train loss: 0.2223195851
Epoch:   900  |  train loss: 0.2202851921
Epoch:  1000  |  train loss: 0.2180323064
Processing class: 33
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.2176137388
Epoch:   200  |  train loss: 0.2251819134
Epoch:   300  |  train loss: 0.2156269550
Epoch:   400  |  train loss: 0.2135664403
Epoch:   500  |  train loss: 0.2121514678
Epoch:   600  |  train loss: 0.2149721771
Epoch:   700  |  train loss: 0.2113068789
Epoch:   800  |  train loss: 0.2112817407
Epoch:   900  |  train loss: 0.2097019196
Epoch:  1000  |  train loss: 0.2073757291
Processing class: 34
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.2249121338
Epoch:   200  |  train loss: 0.2229117095
Epoch:   300  |  train loss: 0.2221979499
Epoch:   400  |  train loss: 0.2188170880
Epoch:   500  |  train loss: 0.2163738936
Epoch:   600  |  train loss: 0.2174814463
Epoch:   700  |  train loss: 0.2132984966
Epoch:   800  |  train loss: 0.2151708841
Epoch:   900  |  train loss: 0.2150706530
Epoch:  1000  |  train loss: 0.2148020178
Processing class: 35
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.2271944582
Epoch:   200  |  train loss: 0.2261228919
Epoch:   300  |  train loss: 0.2268461704
Epoch:   400  |  train loss: 0.2265167326
Epoch:   500  |  train loss: 0.2224570900
Epoch:   600  |  train loss: 0.2238861769
Epoch:   700  |  train loss: 0.2234240949
Epoch:   800  |  train loss: 0.2220224708
Epoch:   900  |  train loss: 0.2217405617
Epoch:  1000  |  train loss: 0.2194261849
Processing class: 36
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.2173370808
Epoch:   200  |  train loss: 0.2162802637
Epoch:   300  |  train loss: 0.2162012339
Epoch:   400  |  train loss: 0.2152033120
Epoch:   500  |  train loss: 0.2171911687
Epoch:   600  |  train loss: 0.2134699583
Epoch:   700  |  train loss: 0.2098784953
Epoch:   800  |  train loss: 0.2128663331
Epoch:   900  |  train loss: 0.2078994364
Epoch:  1000  |  train loss: 0.2092866480
Processing class: 37
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.2282928169
Epoch:   200  |  train loss: 0.2261895627
Epoch:   300  |  train loss: 0.2229292452
Epoch:   400  |  train loss: 0.2225198120
Epoch:   500  |  train loss: 0.2224741220
Epoch:   600  |  train loss: 0.2188028544
Epoch:   700  |  train loss: 0.2178596675
Epoch:   800  |  train loss: 0.2181683809
Epoch:   900  |  train loss: 0.2147963345
Epoch:  1000  |  train loss: 0.2153346866
Processing class: 38
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.2189381510
Epoch:   200  |  train loss: 0.2197277099
Epoch:   300  |  train loss: 0.2212331861
Epoch:   400  |  train loss: 0.2219500184
Epoch:   500  |  train loss: 0.2204797417
Epoch:   600  |  train loss: 0.2174931467
Epoch:   700  |  train loss: 0.2160100996
Epoch:   800  |  train loss: 0.2181915760
Epoch:   900  |  train loss: 0.2137398690
Epoch:  1000  |  train loss: 0.2155114472
Processing class: 39
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.2227634400
Epoch:   200  |  train loss: 0.2202405214
Epoch:   300  |  train loss: 0.2206152529
Epoch:   400  |  train loss: 0.2203778684
Epoch:   500  |  train loss: 0.2172594905
Epoch:   600  |  train loss: 0.2170988321
Epoch:   700  |  train loss: 0.2168924212
Epoch:   800  |  train loss: 0.2175060332
Epoch:   900  |  train loss: 0.2169464707
Epoch:  1000  |  train loss: 0.2165018380
Processing class: 40
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.2219540209
Epoch:   200  |  train loss: 0.2161084086
Epoch:   300  |  train loss: 0.2206730247
Epoch:   400  |  train loss: 0.2206223994
Epoch:   500  |  train loss: 0.2208983064
Epoch:   600  |  train loss: 0.2166418701
Epoch:   700  |  train loss: 0.2174889147
Epoch:   800  |  train loss: 0.2163601160
Epoch:   900  |  train loss: 0.2141242415
Epoch:  1000  |  train loss: 0.2183338583
Processing class: 41
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.2170401067
Epoch:   200  |  train loss: 0.2179186612
Epoch:   300  |  train loss: 0.2166397542
Epoch:   400  |  train loss: 0.2124960572
Epoch:   500  |  train loss: 0.2109014273
Epoch:   600  |  train loss: 0.2100401700
Epoch:   700  |  train loss: 0.2114788622
Epoch:   800  |  train loss: 0.2092630774
Epoch:   900  |  train loss: 0.2077134222
Epoch:  1000  |  train loss: 0.2070012748
Processing class: 42
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.2221375465
Epoch:   200  |  train loss: 0.2234364241
Epoch:   300  |  train loss: 0.2231754899
Epoch:   400  |  train loss: 0.2213345587
Epoch:   500  |  train loss: 0.2206650555
Epoch:   600  |  train loss: 0.2195534170
Epoch:   700  |  train loss: 0.2195511252
Epoch:   800  |  train loss: 0.2187559962
Epoch:   900  |  train loss: 0.2157185644
Epoch:  1000  |  train loss: 0.2142870247
Processing class: 43
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.2251668811
Epoch:   200  |  train loss: 0.2224871248
Epoch:   300  |  train loss: 0.2243673742
Epoch:   400  |  train loss: 0.2286453724
Epoch:   500  |  train loss: 0.2290308297
Epoch:   600  |  train loss: 0.2273636818
Epoch:   700  |  train loss: 0.2290972948
Epoch:   800  |  train loss: 0.2275453776
Epoch:   900  |  train loss: 0.2235341340
Epoch:  1000  |  train loss: 0.2265731424
Processing class: 44
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.2238217235
Epoch:   200  |  train loss: 0.2246317774
Epoch:   300  |  train loss: 0.2218589872
Epoch:   400  |  train loss: 0.2202430815
Epoch:   500  |  train loss: 0.2207752287
Epoch:   600  |  train loss: 0.2185220450
Epoch:   700  |  train loss: 0.2204588234
Epoch:   800  |  train loss: 0.2185053200
Epoch:   900  |  train loss: 0.2160711020
Epoch:  1000  |  train loss: 0.2181141853
Processing class: 45
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.2225142837
Epoch:   200  |  train loss: 0.2257726759
Epoch:   300  |  train loss: 0.2216003925
Epoch:   400  |  train loss: 0.2255383968
Epoch:   500  |  train loss: 0.2243711799
Epoch:   600  |  train loss: 0.2276720911
Epoch:   700  |  train loss: 0.2274924308
Epoch:   800  |  train loss: 0.2257334173
Epoch:   900  |  train loss: 0.2260526091
Epoch:  1000  |  train loss: 0.2277374834
Processing class: 46
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.2211141169
Epoch:   200  |  train loss: 0.2199152708
Epoch:   300  |  train loss: 0.2190071970
Epoch:   400  |  train loss: 0.2201158583
Epoch:   500  |  train loss: 0.2197971553
Epoch:   600  |  train loss: 0.2196172416
Epoch:   700  |  train loss: 0.2194195956
Epoch:   800  |  train loss: 0.2220133901
Epoch:   900  |  train loss: 0.2189379960
Epoch:  1000  |  train loss: 0.2201276064
Processing class: 47
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.2228720129
Epoch:   200  |  train loss: 0.2238909900
Epoch:   300  |  train loss: 0.2181305081
Epoch:   400  |  train loss: 0.2193589061
Epoch:   500  |  train loss: 0.2176663011
Epoch:   600  |  train loss: 0.2155579686
Epoch:   700  |  train loss: 0.2148541987
Epoch:   800  |  train loss: 0.2116263211
Epoch:   900  |  train loss: 0.2115181744
Epoch:  1000  |  train loss: 0.2093975753
Processing class: 48
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.2237421811
Epoch:   200  |  train loss: 0.2264321834
Epoch:   300  |  train loss: 0.2208974570
Epoch:   400  |  train loss: 0.2157841027
Epoch:   500  |  train loss: 0.2200859636
Epoch:   600  |  train loss: 0.2181367874
Epoch:   700  |  train loss: 0.2148467928
Epoch:   800  |  train loss: 0.2143036783
Epoch:   900  |  train loss: 0.2183678836
Epoch:  1000  |  train loss: 0.2185437739
Processing class: 49
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.2242712647
Epoch:   200  |  train loss: 0.2234041125
Epoch:   300  |  train loss: 0.2259939432
Epoch:   400  |  train loss: 0.2249909401
Epoch:   500  |  train loss: 0.2241264373
Epoch:   600  |  train loss: 0.2232465386
Epoch:   700  |  train loss: 0.2217334479
Epoch:   800  |  train loss: 0.2197320223
Epoch:   900  |  train loss: 0.2182798922
Epoch:  1000  |  train loss: 0.2181067824
/home/z1165703/FeCAM/models/base.py:87: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  vec = torch.tensor(vec).cuda()
2024-03-05 06:14:36,993 [trainer.py] => CNN: {'total': 83.44, '00-09': 87.7, '10-19': 79.6, '20-29': 84.9, '30-39': 81.0, '40-49': 84.0, 'old': 0, 'new': 83.44}
2024-03-05 06:14:36,994 [trainer.py] => No NME accuracy
2024-03-05 06:14:36,994 [trainer.py] => FeCAM: {'total': 82.9, '00-09': 86.2, '10-19': 79.7, '20-29': 85.0, '30-39': 80.6, '40-49': 83.0, 'old': 0, 'new': 82.9}
2024-03-05 06:14:36,994 [trainer.py] => CNN top1 curve: [83.44]
2024-03-05 06:14:36,994 [trainer.py] => CNN top5 curve: [96.5]
2024-03-05 06:14:36,994 [trainer.py] => FeCAM top1 curve: [82.9]
2024-03-05 06:14:36,994 [trainer.py] => FeCAM top5 curve: [95.84]

2024-03-05 06:14:37,006 [fecam.py] => Learning on 50-60
Processing class: 50
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.2291726291
Epoch:   200  |  train loss: 0.2270380557
Epoch:   300  |  train loss: 0.2254076332
Epoch:   400  |  train loss: 0.2262926608
Epoch:   500  |  train loss: 0.2253135204
Epoch:   600  |  train loss: 0.2213739634
Epoch:   700  |  train loss: 0.2228758365
Epoch:   800  |  train loss: 0.2229896337
Epoch:   900  |  train loss: 0.2239473462
Epoch:  1000  |  train loss: 0.2235696942
Processing class: 51
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.2311239004
Epoch:   200  |  train loss: 0.2245128185
Epoch:   300  |  train loss: 0.2273923099
Epoch:   400  |  train loss: 0.2265798092
Epoch:   500  |  train loss: 0.2245027304
Epoch:   600  |  train loss: 0.2237933040
Epoch:   700  |  train loss: 0.2194155455
Epoch:   800  |  train loss: 0.2188865155
Epoch:   900  |  train loss: 0.2216538519
Epoch:  1000  |  train loss: 0.2183605850
Processing class: 52
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.2350194424
Epoch:   200  |  train loss: 0.2324450135
Epoch:   300  |  train loss: 0.2325013310
Epoch:   400  |  train loss: 0.2322871685
Epoch:   500  |  train loss: 0.2300141394
Epoch:   600  |  train loss: 0.2262076735
Epoch:   700  |  train loss: 0.2280192256
Epoch:   800  |  train loss: 0.2269046545
Epoch:   900  |  train loss: 0.2262785167
Epoch:  1000  |  train loss: 0.2227612853
Processing class: 53
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.2250342846
Epoch:   200  |  train loss: 0.2225300670
Epoch:   300  |  train loss: 0.2176107973
Epoch:   400  |  train loss: 0.2168360204
Epoch:   500  |  train loss: 0.2174663186
Epoch:   600  |  train loss: 0.2191661447
Epoch:   700  |  train loss: 0.2209251016
Epoch:   800  |  train loss: 0.2139476359
Epoch:   900  |  train loss: 0.2162050724
Epoch:  1000  |  train loss: 0.2130312085
Processing class: 54
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.2119838327
Epoch:   200  |  train loss: 0.2097504467
Epoch:   300  |  train loss: 0.2090475321
Epoch:   400  |  train loss: 0.2080771536
Epoch:   500  |  train loss: 0.2078372389
Epoch:   600  |  train loss: 0.2098094285
Epoch:   700  |  train loss: 0.2094029009
Epoch:   800  |  train loss: 0.2110030442
Epoch:   900  |  train loss: 0.2119518280
Epoch:  1000  |  train loss: 0.2096384585
Processing class: 55
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.2341622919
Epoch:   200  |  train loss: 0.2336976796
Epoch:   300  |  train loss: 0.2307137936
Epoch:   400  |  train loss: 0.2331454158
Epoch:   500  |  train loss: 0.2365492821
Epoch:   600  |  train loss: 0.2356400996
Epoch:   700  |  train loss: 0.2358505398
Epoch:   800  |  train loss: 0.2381986946
Epoch:   900  |  train loss: 0.2322411060
Epoch:  1000  |  train loss: 0.2337880075
Processing class: 56
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.2267891526
Epoch:   200  |  train loss: 0.2238540351
Epoch:   300  |  train loss: 0.2154376537
Epoch:   400  |  train loss: 0.2168006212
Epoch:   500  |  train loss: 0.2152020127
Epoch:   600  |  train loss: 0.2133408129
Epoch:   700  |  train loss: 0.2127829045
Epoch:   800  |  train loss: 0.2092727631
Epoch:   900  |  train loss: 0.2106188178
Epoch:  1000  |  train loss: 0.2109939218
Processing class: 57
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.2320520133
Epoch:   200  |  train loss: 0.2346377462
Epoch:   300  |  train loss: 0.2320745200
Epoch:   400  |  train loss: 0.2307683170
Epoch:   500  |  train loss: 0.2318217039
Epoch:   600  |  train loss: 0.2274536461
Epoch:   700  |  train loss: 0.2309512645
Epoch:   800  |  train loss: 0.2295185566
Epoch:   900  |  train loss: 0.2308437616
Epoch:  1000  |  train loss: 0.2255145162
Processing class: 58
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.2291490167
Epoch:   200  |  train loss: 0.2213899076
Epoch:   300  |  train loss: 0.2209598631
Epoch:   400  |  train loss: 0.2211636513
Epoch:   500  |  train loss: 0.2181655705
Epoch:   600  |  train loss: 0.2193368137
Epoch:   700  |  train loss: 0.2166212797
Epoch:   800  |  train loss: 0.2176775604
Epoch:   900  |  train loss: 0.2163439393
Epoch:  1000  |  train loss: 0.2150198519
Processing class: 59
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.2357968152
Epoch:   200  |  train loss: 0.2382525653
Epoch:   300  |  train loss: 0.2332070649
Epoch:   400  |  train loss: 0.2322663695
Epoch:   500  |  train loss: 0.2325756252
Epoch:   600  |  train loss: 0.2316118598
Epoch:   700  |  train loss: 0.2319045842
Epoch:   800  |  train loss: 0.2313676268
Epoch:   900  |  train loss: 0.2295978904
Epoch:  1000  |  train loss: 0.2280801535
2024-03-05 06:20:20,656 [trainer.py] => CNN: {'total': 71.75, '00-09': 82.2, '10-19': 72.8, '20-29': 78.5, '30-39': 74.9, '40-49': 68.4, '50-59': 53.7, 'old': 75.36, 'new': 53.7}
2024-03-05 06:20:22,732 [trainer.py] => No NME accuracy
2024-03-05 06:20:22,732 [trainer.py] => FeCAM: {'total': 72.78, '00-09': 82.6, '10-19': 74.0, '20-29': 81.0, '30-39': 76.4, '40-49': 76.8, '50-59': 45.9, 'old': 78.16, 'new': 45.9}
2024-03-05 06:20:22,734 [trainer.py] => CNN top1 curve: [83.44, 71.75]
2024-03-05 06:20:22,734 [trainer.py] => CNN top5 curve: [96.5, 89.6]
2024-03-05 06:20:22,734 [trainer.py] => FeCAM top1 curve: [82.9, 72.78]
2024-03-05 06:20:22,734 [trainer.py] => FeCAM top5 curve: [95.84, 91.18]

2024-03-05 06:20:22,738 [fecam.py] => Learning on 60-70
Processing class: 60
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.2366056323
Epoch:   200  |  train loss: 0.2342733949
Epoch:   300  |  train loss: 0.2295178503
Epoch:   400  |  train loss: 0.2262545615
Epoch:   500  |  train loss: 0.2277066946
Epoch:   600  |  train loss: 0.2256003052
Epoch:   700  |  train loss: 0.2318221599
Epoch:   800  |  train loss: 0.2250333905
Epoch:   900  |  train loss: 0.2243488878
Epoch:  1000  |  train loss: 0.2251058549
Processing class: 61
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.2262501121
Epoch:   200  |  train loss: 0.2189911544
Epoch:   300  |  train loss: 0.2186506152
Epoch:   400  |  train loss: 0.2155343860
Epoch:   500  |  train loss: 0.2178400636
Epoch:   600  |  train loss: 0.2126188338
Epoch:   700  |  train loss: 0.2145898253
Epoch:   800  |  train loss: 0.2155287981
Epoch:   900  |  train loss: 0.2101739258
Epoch:  1000  |  train loss: 0.2109907836
Processing class: 62
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.2369771868
Epoch:   200  |  train loss: 0.2337499380
Epoch:   300  |  train loss: 0.2356228948
Epoch:   400  |  train loss: 0.2358691752
Epoch:   500  |  train loss: 0.2326755762
Epoch:   600  |  train loss: 0.2358329684
Epoch:   700  |  train loss: 0.2362683266
Epoch:   800  |  train loss: 0.2358655185
Epoch:   900  |  train loss: 0.2334442317
Epoch:  1000  |  train loss: 0.2337566793
Processing class: 63
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.2276962668
Epoch:   200  |  train loss: 0.2274304897
Epoch:   300  |  train loss: 0.2289769888
Epoch:   400  |  train loss: 0.2261009932
Epoch:   500  |  train loss: 0.2250866503
Epoch:   600  |  train loss: 0.2272495508
Epoch:   700  |  train loss: 0.2210294783
Epoch:   800  |  train loss: 0.2262911648
Epoch:   900  |  train loss: 0.2208131254
Epoch:  1000  |  train loss: 0.2209893078
Processing class: 64
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.2333191574
Epoch:   200  |  train loss: 0.2230252773
Epoch:   300  |  train loss: 0.2254820228
Epoch:   400  |  train loss: 0.2232304275
Epoch:   500  |  train loss: 0.2202668905
Epoch:   600  |  train loss: 0.2177272290
Epoch:   700  |  train loss: 0.2128507674
Epoch:   800  |  train loss: 0.2133779824
Epoch:   900  |  train loss: 0.2154059917
Epoch:  1000  |  train loss: 0.2126432747
Processing class: 65
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.2283102989
Epoch:   200  |  train loss: 0.2223818332
Epoch:   300  |  train loss: 0.2235626847
Epoch:   400  |  train loss: 0.2229621351
Epoch:   500  |  train loss: 0.2195118397
Epoch:   600  |  train loss: 0.2225722462
Epoch:   700  |  train loss: 0.2209760845
Epoch:   800  |  train loss: 0.2241386503
Epoch:   900  |  train loss: 0.2197809428
Epoch:  1000  |  train loss: 0.2233104438
Processing class: 66
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.2347384870
Epoch:   200  |  train loss: 0.2343744874
Epoch:   300  |  train loss: 0.2341102272
Epoch:   400  |  train loss: 0.2318913162
Epoch:   500  |  train loss: 0.2338799268
Epoch:   600  |  train loss: 0.2328477830
Epoch:   700  |  train loss: 0.2352939397
Epoch:   800  |  train loss: 0.2348911762
Epoch:   900  |  train loss: 0.2336120337
Epoch:  1000  |  train loss: 0.2325927973
Processing class: 67
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.2293188304
Epoch:   200  |  train loss: 0.2294300586
Epoch:   300  |  train loss: 0.2271788061
Epoch:   400  |  train loss: 0.2262628227
Epoch:   500  |  train loss: 0.2248063028
Epoch:   600  |  train loss: 0.2235497773
Epoch:   700  |  train loss: 0.2232444972
Epoch:   800  |  train loss: 0.2243410379
Epoch:   900  |  train loss: 0.2227180779
Epoch:  1000  |  train loss: 0.2215585142
Processing class: 68
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.2238106430
Epoch:   200  |  train loss: 0.2220578879
Epoch:   300  |  train loss: 0.2160715401
Epoch:   400  |  train loss: 0.2150248021
Epoch:   500  |  train loss: 0.2138916790
Epoch:   600  |  train loss: 0.2116378814
Epoch:   700  |  train loss: 0.2120261580
Epoch:   800  |  train loss: 0.2091729641
Epoch:   900  |  train loss: 0.2089502871
Epoch:  1000  |  train loss: 0.2123650372
Processing class: 69
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.2060252577
Epoch:   200  |  train loss: 0.2051065296
Epoch:   300  |  train loss: 0.2063597322
Epoch:   400  |  train loss: 0.2059459120
Epoch:   500  |  train loss: 0.2043784887
Epoch:   600  |  train loss: 0.1997771710
Epoch:   700  |  train loss: 0.2020529628
Epoch:   800  |  train loss: 0.2009328395
Epoch:   900  |  train loss: 0.2054400921
Epoch:  1000  |  train loss: 0.1998972774
2024-03-05 06:26:58,866 [trainer.py] => CNN: {'total': 64.66, '00-09': 75.0, '10-19': 70.1, '20-29': 77.1, '30-39': 70.6, '40-49': 64.0, '50-59': 45.8, '60-69': 50.0, 'old': 67.1, 'new': 50.0}
2024-03-05 06:26:58,867 [trainer.py] => No NME accuracy
2024-03-05 06:26:58,867 [trainer.py] => FeCAM: {'total': 67.27, '00-09': 79.9, '10-19': 70.6, '20-29': 79.1, '30-39': 71.9, '40-49': 74.1, '50-59': 41.2, '60-69': 54.1, 'old': 69.47, 'new': 54.1}
2024-03-05 06:26:58,867 [trainer.py] => CNN top1 curve: [83.44, 71.75, 64.66]
2024-03-05 06:26:58,867 [trainer.py] => CNN top5 curve: [96.5, 89.6, 86.54]
2024-03-05 06:26:58,867 [trainer.py] => FeCAM top1 curve: [82.9, 72.78, 67.27]
2024-03-05 06:26:58,867 [trainer.py] => FeCAM top5 curve: [95.84, 91.18, 87.21]

2024-03-05 06:26:58,872 [fecam.py] => Learning on 70-80
Processing class: 70
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.2278309494
Epoch:   200  |  train loss: 0.2239018530
Epoch:   300  |  train loss: 0.2216363996
Epoch:   400  |  train loss: 0.2213149756
Epoch:   500  |  train loss: 0.2169950455
Epoch:   600  |  train loss: 0.2183449775
Epoch:   700  |  train loss: 0.2199753344
Epoch:   800  |  train loss: 0.2137248635
Epoch:   900  |  train loss: 0.2157859623
Epoch:  1000  |  train loss: 0.2135264844
Processing class: 71
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.2330762327
Epoch:   200  |  train loss: 0.2329014391
Epoch:   300  |  train loss: 0.2267911136
Epoch:   400  |  train loss: 0.2272442907
Epoch:   500  |  train loss: 0.2270610869
Epoch:   600  |  train loss: 0.2256656140
Epoch:   700  |  train loss: 0.2243849576
Epoch:   800  |  train loss: 0.2237750739
Epoch:   900  |  train loss: 0.2254912138
Epoch:  1000  |  train loss: 0.2233015895
Processing class: 72
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.2290479660
Epoch:   200  |  train loss: 0.2283863723
Epoch:   300  |  train loss: 0.2266535103
Epoch:   400  |  train loss: 0.2241607070
Epoch:   500  |  train loss: 0.2245904446
Epoch:   600  |  train loss: 0.2207766622
Epoch:   700  |  train loss: 0.2208739817
Epoch:   800  |  train loss: 0.2185134619
Epoch:   900  |  train loss: 0.2182012796
Epoch:  1000  |  train loss: 0.2199339479
Processing class: 73
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.2330607265
Epoch:   200  |  train loss: 0.2294127584
Epoch:   300  |  train loss: 0.2299307108
Epoch:   400  |  train loss: 0.2260343850
Epoch:   500  |  train loss: 0.2297911704
Epoch:   600  |  train loss: 0.2285139620
Epoch:   700  |  train loss: 0.2248542190
Epoch:   800  |  train loss: 0.2240423471
Epoch:   900  |  train loss: 0.2232174635
Epoch:  1000  |  train loss: 0.2239934772
Processing class: 74
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.2374703854
Epoch:   200  |  train loss: 0.2390740544
Epoch:   300  |  train loss: 0.2337176561
Epoch:   400  |  train loss: 0.2301041365
Epoch:   500  |  train loss: 0.2302557379
Epoch:   600  |  train loss: 0.2273958385
Epoch:   700  |  train loss: 0.2310488999
Epoch:   800  |  train loss: 0.2251743644
Epoch:   900  |  train loss: 0.2246363193
Epoch:  1000  |  train loss: 0.2272474527
Processing class: 75
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.2309165478
Epoch:   200  |  train loss: 0.2281420290
Epoch:   300  |  train loss: 0.2292377979
Epoch:   400  |  train loss: 0.2268283933
Epoch:   500  |  train loss: 0.2279922396
Epoch:   600  |  train loss: 0.2292830139
Epoch:   700  |  train loss: 0.2205169737
Epoch:   800  |  train loss: 0.2242216468
Epoch:   900  |  train loss: 0.2228079796
Epoch:  1000  |  train loss: 0.2248675019
Processing class: 76
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.2025073856
Epoch:   200  |  train loss: 0.1990611911
Epoch:   300  |  train loss: 0.2003083974
Epoch:   400  |  train loss: 0.2015755087
Epoch:   500  |  train loss: 0.2017923981
Epoch:   600  |  train loss: 0.2031187475
Epoch:   700  |  train loss: 0.2034133196
Epoch:   800  |  train loss: 0.1985406160
Epoch:   900  |  train loss: 0.2019386530
Epoch:  1000  |  train loss: 0.2033023179
Processing class: 77
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.2267988235
Epoch:   200  |  train loss: 0.2245221704
Epoch:   300  |  train loss: 0.2186415672
Epoch:   400  |  train loss: 0.2178545177
Epoch:   500  |  train loss: 0.2155474365
Epoch:   600  |  train loss: 0.2131055355
Epoch:   700  |  train loss: 0.2122427404
Epoch:   800  |  train loss: 0.2139289051
Epoch:   900  |  train loss: 0.2110765696
Epoch:  1000  |  train loss: 0.2093611985
Processing class: 78
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.2314793885
Epoch:   200  |  train loss: 0.2298377365
Epoch:   300  |  train loss: 0.2303961515
Epoch:   400  |  train loss: 0.2278058320
Epoch:   500  |  train loss: 0.2245566905
Epoch:   600  |  train loss: 0.2255578160
Epoch:   700  |  train loss: 0.2247696191
Epoch:   800  |  train loss: 0.2242633760
Epoch:   900  |  train loss: 0.2236660570
Epoch:  1000  |  train loss: 0.2220821917
Processing class: 79
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.2352883339
Epoch:   200  |  train loss: 0.2316192836
Epoch:   300  |  train loss: 0.2339433670
Epoch:   400  |  train loss: 0.2321973681
Epoch:   500  |  train loss: 0.2310747594
Epoch:   600  |  train loss: 0.2332183272
Epoch:   700  |  train loss: 0.2319799751
Epoch:   800  |  train loss: 0.2321703166
Epoch:   900  |  train loss: 0.2337112725
Epoch:  1000  |  train loss: 0.2299924195
2024-03-05 06:34:48,344 [trainer.py] => CNN: {'total': 59.18, '00-09': 73.6, '10-19': 68.4, '20-29': 76.9, '30-39': 69.1, '40-49': 60.6, '50-59': 37.2, '60-69': 45.7, '70-79': 41.9, 'old': 61.64, 'new': 41.9}
2024-03-05 06:34:48,345 [trainer.py] => No NME accuracy
2024-03-05 06:34:48,345 [trainer.py] => FeCAM: {'total': 61.72, '00-09': 78.1, '10-19': 69.3, '20-29': 78.0, '30-39': 69.5, '40-49': 72.1, '50-59': 35.7, '60-69': 49.1, '70-79': 42.0, 'old': 64.54, 'new': 42.0}
2024-03-05 06:34:48,345 [trainer.py] => CNN top1 curve: [83.44, 71.75, 64.66, 59.18]
2024-03-05 06:34:48,345 [trainer.py] => CNN top5 curve: [96.5, 89.6, 86.54, 84.09]
2024-03-05 06:34:48,345 [trainer.py] => FeCAM top1 curve: [82.9, 72.78, 67.27, 61.72]
2024-03-05 06:34:48,345 [trainer.py] => FeCAM top5 curve: [95.84, 91.18, 87.21, 84.09]

2024-03-05 06:34:48,349 [fecam.py] => Learning on 80-90
Processing class: 80
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.2169408858
Epoch:   200  |  train loss: 0.2143549323
Epoch:   300  |  train loss: 0.2133528620
Epoch:   400  |  train loss: 0.2128009200
Epoch:   500  |  train loss: 0.2108551860
Epoch:   600  |  train loss: 0.2099180669
Epoch:   700  |  train loss: 0.2057015926
Epoch:   800  |  train loss: 0.2097806603
Epoch:   900  |  train loss: 0.2080339074
Epoch:  1000  |  train loss: 0.2097338676
Processing class: 81
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.2270217180
Epoch:   200  |  train loss: 0.2285420775
Epoch:   300  |  train loss: 0.2230075419
Epoch:   400  |  train loss: 0.2256257653
Epoch:   500  |  train loss: 0.2227943391
Epoch:   600  |  train loss: 0.2246725798
Epoch:   700  |  train loss: 0.2226949275
Epoch:   800  |  train loss: 0.2240277618
Epoch:   900  |  train loss: 0.2228437811
Epoch:  1000  |  train loss: 0.2224748701
Processing class: 82
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.2219684631
Epoch:   200  |  train loss: 0.2208223015
Epoch:   300  |  train loss: 0.2187288702
Epoch:   400  |  train loss: 0.2158876002
Epoch:   500  |  train loss: 0.2147062391
Epoch:   600  |  train loss: 0.2124500006
Epoch:   700  |  train loss: 0.2113033056
Epoch:   800  |  train loss: 0.2113638461
Epoch:   900  |  train loss: 0.2105170935
Epoch:  1000  |  train loss: 0.2096503317
Processing class: 83
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.2226292908
Epoch:   200  |  train loss: 0.2197448820
Epoch:   300  |  train loss: 0.2146323800
Epoch:   400  |  train loss: 0.2131235242
Epoch:   500  |  train loss: 0.2146738827
Epoch:   600  |  train loss: 0.2108166873
Epoch:   700  |  train loss: 0.2133701414
Epoch:   800  |  train loss: 0.2165538073
Epoch:   900  |  train loss: 0.2139955819
Epoch:  1000  |  train loss: 0.2114669591
Processing class: 84
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.2324649900
Epoch:   200  |  train loss: 0.2270399749
Epoch:   300  |  train loss: 0.2273718029
Epoch:   400  |  train loss: 0.2248152852
Epoch:   500  |  train loss: 0.2251850516
Epoch:   600  |  train loss: 0.2210617840
Epoch:   700  |  train loss: 0.2226636708
Epoch:   800  |  train loss: 0.2242104411
Epoch:   900  |  train loss: 0.2194658309
Epoch:  1000  |  train loss: 0.2179189712
Processing class: 85
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.2315205574
Epoch:   200  |  train loss: 0.2267046541
Epoch:   300  |  train loss: 0.2266538739
Epoch:   400  |  train loss: 0.2227862895
Epoch:   500  |  train loss: 0.2230621219
Epoch:   600  |  train loss: 0.2238780797
Epoch:   700  |  train loss: 0.2265388280
Epoch:   800  |  train loss: 0.2222880453
Epoch:   900  |  train loss: 0.2217634171
Epoch:  1000  |  train loss: 0.2236666828
Processing class: 86
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.2375436544
Epoch:   200  |  train loss: 0.2328156829
Epoch:   300  |  train loss: 0.2345531523
Epoch:   400  |  train loss: 0.2306707561
Epoch:   500  |  train loss: 0.2292524189
Epoch:   600  |  train loss: 0.2291508675
Epoch:   700  |  train loss: 0.2280620873
Epoch:   800  |  train loss: 0.2263988346
Epoch:   900  |  train loss: 0.2253990978
Epoch:  1000  |  train loss: 0.2285091221
Processing class: 87
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.2232240230
Epoch:   200  |  train loss: 0.2201913834
Epoch:   300  |  train loss: 0.2178916842
Epoch:   400  |  train loss: 0.2167528838
Epoch:   500  |  train loss: 0.2144492030
Epoch:   600  |  train loss: 0.2154042482
Epoch:   700  |  train loss: 0.2152346104
Epoch:   800  |  train loss: 0.2141472161
Epoch:   900  |  train loss: 0.2110797316
Epoch:  1000  |  train loss: 0.2148667872
Processing class: 88
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.2285640627
Epoch:   200  |  train loss: 0.2248420626
Epoch:   300  |  train loss: 0.2217604101
Epoch:   400  |  train loss: 0.2202339977
Epoch:   500  |  train loss: 0.2174197376
Epoch:   600  |  train loss: 0.2137472123
Epoch:   700  |  train loss: 0.2149173021
Epoch:   800  |  train loss: 0.2135474086
Epoch:   900  |  train loss: 0.2132436305
Epoch:  1000  |  train loss: 0.2105161458
Processing class: 89
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.2296535075
Epoch:   200  |  train loss: 0.2287930757
Epoch:   300  |  train loss: 0.2265197337
Epoch:   400  |  train loss: 0.2254668295
Epoch:   500  |  train loss: 0.2263958007
Epoch:   600  |  train loss: 0.2244868845
Epoch:   700  |  train loss: 0.2205106020
Epoch:   800  |  train loss: 0.2247475564
Epoch:   900  |  train loss: 0.2220732808
Epoch:  1000  |  train loss: 0.2224948764
2024-03-05 06:43:56,408 [trainer.py] => CNN: {'total': 54.08, '00-09': 67.9, '10-19': 63.6, '20-29': 73.2, '30-39': 68.8, '40-49': 57.2, '50-59': 32.6, '60-69': 40.9, '70-79': 38.1, '80-89': 44.4, 'old': 55.29, 'new': 44.4}
2024-03-05 06:43:56,408 [trainer.py] => No NME accuracy
2024-03-05 06:43:56,408 [trainer.py] => FeCAM: {'total': 57.66, '00-09': 76.1, '10-19': 65.8, '20-29': 75.3, '30-39': 67.9, '40-49': 68.5, '50-59': 33.0, '60-69': 45.0, '70-79': 39.0, '80-89': 48.3, 'old': 58.82, 'new': 48.3}
2024-03-05 06:43:56,409 [trainer.py] => CNN top1 curve: [83.44, 71.75, 64.66, 59.18, 54.08]
2024-03-05 06:43:56,409 [trainer.py] => CNN top5 curve: [96.5, 89.6, 86.54, 84.09, 81.74]
2024-03-05 06:43:56,409 [trainer.py] => FeCAM top1 curve: [82.9, 72.78, 67.27, 61.72, 57.66]
2024-03-05 06:43:56,409 [trainer.py] => FeCAM top5 curve: [95.84, 91.18, 87.21, 84.09, 81.63]

2024-03-05 06:43:56,413 [fecam.py] => Learning on 90-100
Processing class: 90
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.2319092929
Epoch:   200  |  train loss: 0.2303860575
Epoch:   300  |  train loss: 0.2236767828
Epoch:   400  |  train loss: 0.2196329474
Epoch:   500  |  train loss: 0.2161062777
Epoch:   600  |  train loss: 0.2152190685
Epoch:   700  |  train loss: 0.2122553170
Epoch:   800  |  train loss: 0.2116705567
Epoch:   900  |  train loss: 0.2122428745
Epoch:  1000  |  train loss: 0.2141813666
Processing class: 91
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.2070266813
Epoch:   200  |  train loss: 0.2032888502
Epoch:   300  |  train loss: 0.2035592437
Epoch:   400  |  train loss: 0.2050212085
Epoch:   500  |  train loss: 0.2051517397
Epoch:   600  |  train loss: 0.2051943898
Epoch:   700  |  train loss: 0.2038710266
Epoch:   800  |  train loss: 0.2036902010
Epoch:   900  |  train loss: 0.2041528881
Epoch:  1000  |  train loss: 0.2039554894
Processing class: 92
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.2315017015
Epoch:   200  |  train loss: 0.2222765505
Epoch:   300  |  train loss: 0.2165718943
Epoch:   400  |  train loss: 0.2167576671
Epoch:   500  |  train loss: 0.2158150434
Epoch:   600  |  train loss: 0.2154659659
Epoch:   700  |  train loss: 0.2136166275
Epoch:   800  |  train loss: 0.2079741776
Epoch:   900  |  train loss: 0.2078189075
Epoch:  1000  |  train loss: 0.2073033363
Processing class: 93
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.2294162601
Epoch:   200  |  train loss: 0.2266221553
Epoch:   300  |  train loss: 0.2295653045
Epoch:   400  |  train loss: 0.2287176877
Epoch:   500  |  train loss: 0.2258698434
Epoch:   600  |  train loss: 0.2215556949
Epoch:   700  |  train loss: 0.2252985090
Epoch:   800  |  train loss: 0.2232576072
Epoch:   900  |  train loss: 0.2226131499
Epoch:  1000  |  train loss: 0.2205360055
Processing class: 94
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.2126176506
Epoch:   200  |  train loss: 0.2087958485
Epoch:   300  |  train loss: 0.2003239363
Epoch:   400  |  train loss: 0.1989926934
Epoch:   500  |  train loss: 0.1918274045
Epoch:   600  |  train loss: 0.1929058105
Epoch:   700  |  train loss: 0.1941314161
Epoch:   800  |  train loss: 0.1865569144
Epoch:   900  |  train loss: 0.1895377964
Epoch:  1000  |  train loss: 0.1872964084
Processing class: 95
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.2311379790
Epoch:   200  |  train loss: 0.2333054125
Epoch:   300  |  train loss: 0.2296667427
Epoch:   400  |  train loss: 0.2264238745
Epoch:   500  |  train loss: 0.2286023021
Epoch:   600  |  train loss: 0.2265918434
Epoch:   700  |  train loss: 0.2267060935
Epoch:   800  |  train loss: 0.2240368873
Epoch:   900  |  train loss: 0.2255682319
Epoch:  1000  |  train loss: 0.2269441545
Processing class: 96
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.2357405871
Epoch:   200  |  train loss: 0.2289825261
Epoch:   300  |  train loss: 0.2292399466
Epoch:   400  |  train loss: 0.2323194116
Epoch:   500  |  train loss: 0.2287935644
Epoch:   600  |  train loss: 0.2269810975
Epoch:   700  |  train loss: 0.2260819703
Epoch:   800  |  train loss: 0.2227173209
Epoch:   900  |  train loss: 0.2259741247
Epoch:  1000  |  train loss: 0.2245214969
Processing class: 97
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.2318736196
Epoch:   200  |  train loss: 0.2272852600
Epoch:   300  |  train loss: 0.2244269997
Epoch:   400  |  train loss: 0.2234466434
Epoch:   500  |  train loss: 0.2245167583
Epoch:   600  |  train loss: 0.2247118473
Epoch:   700  |  train loss: 0.2253028035
Epoch:   800  |  train loss: 0.2242212057
Epoch:   900  |  train loss: 0.2241181999
Epoch:  1000  |  train loss: 0.2227870286
Processing class: 98
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.2218633085
Epoch:   200  |  train loss: 0.2174755067
Epoch:   300  |  train loss: 0.2140213579
Epoch:   400  |  train loss: 0.2126591235
Epoch:   500  |  train loss: 0.2096381336
Epoch:   600  |  train loss: 0.2095467061
Epoch:   700  |  train loss: 0.2050630510
Epoch:   800  |  train loss: 0.2065012515
Epoch:   900  |  train loss: 0.2057564139
Epoch:  1000  |  train loss: 0.2036041290
Processing class: 99
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.2355954587
Epoch:   200  |  train loss: 0.2296861917
Epoch:   300  |  train loss: 0.2305778384
Epoch:   400  |  train loss: 0.2316118836
Epoch:   500  |  train loss: 0.2281490713
Epoch:   600  |  train loss: 0.2255822420
Epoch:   700  |  train loss: 0.2235209852
Epoch:   800  |  train loss: 0.2253839195
Epoch:   900  |  train loss: 0.2224621892
Epoch:  1000  |  train loss: 0.2221487284
2024-03-05 06:54:32,058 [trainer.py] => CNN: {'total': 50.36, '00-09': 57.7, '10-19': 63.6, '20-29': 71.4, '30-39': 68.5, '40-49': 56.8, '50-59': 29.6, '60-69': 38.9, '70-79': 36.6, '80-89': 42.7, '90-99': 37.8, 'old': 51.76, 'new': 37.8}
2024-03-05 06:54:32,060 [trainer.py] => No NME accuracy
2024-03-05 06:54:32,060 [trainer.py] => FeCAM: {'total': 54.74, '00-09': 73.3, '10-19': 65.1, '20-29': 74.1, '30-39': 67.2, '40-49': 67.1, '50-59': 30.8, '60-69': 43.3, '70-79': 37.2, '80-89': 46.9, '90-99': 42.4, 'old': 56.11, 'new': 42.4}
2024-03-05 06:54:32,060 [trainer.py] => CNN top1 curve: [83.44, 71.75, 64.66, 59.18, 54.08, 50.36]
2024-03-05 06:54:32,060 [trainer.py] => CNN top5 curve: [96.5, 89.6, 86.54, 84.09, 81.74, 79.63]
2024-03-05 06:54:32,060 [trainer.py] => FeCAM top1 curve: [82.9, 72.78, 67.27, 61.72, 57.66, 54.74]
2024-03-05 06:54:32,060 [trainer.py] => FeCAM top5 curve: [95.84, 91.18, 87.21, 84.09, 81.63, 79.21]

=========================================
2024-03-05 06:54:47,753 [trainer.py] => config: ./exps/FeCAM_cifar100.json
2024-03-05 06:54:47,754 [trainer.py] => prefix: train
2024-03-05 06:54:47,754 [trainer.py] => dataset: cifar100
2024-03-05 06:54:47,754 [trainer.py] => memory_size: 0
2024-03-05 06:54:47,754 [trainer.py] => shuffle: True
2024-03-05 06:54:47,754 [trainer.py] => init_cls: 50
2024-03-05 06:54:47,754 [trainer.py] => increment: 10
2024-03-05 06:54:47,754 [trainer.py] => model_name: fecam
2024-03-05 06:54:47,754 [trainer.py] => convnet_type: resnet18
2024-03-05 06:54:47,754 [trainer.py] => device: [device(type='cuda', index=0)]
2024-03-05 06:54:47,754 [trainer.py] => seed: 1993
2024-03-05 06:54:47,754 [trainer.py] => init_epochs: 200
2024-03-05 06:54:47,754 [trainer.py] => init_lr: 0.1
2024-03-05 06:54:47,754 [trainer.py] => init_weight_decay: 0.0005
2024-03-05 06:54:47,754 [trainer.py] => batch_size: 128
2024-03-05 06:54:47,754 [trainer.py] => num_workers: 8
2024-03-05 06:54:47,754 [trainer.py] => T: 5
2024-03-05 06:54:47,754 [trainer.py] => beta: 0.5
2024-03-05 06:54:47,754 [trainer.py] => alpha1: 1
2024-03-05 06:54:47,754 [trainer.py] => alpha2: 1
2024-03-05 06:54:47,754 [trainer.py] => ncm: False
2024-03-05 06:54:47,754 [trainer.py] => tukey: False
2024-03-05 06:54:47,754 [trainer.py] => diagonal: False
2024-03-05 06:54:47,754 [trainer.py] => per_class: True
2024-03-05 06:54:47,754 [trainer.py] => full_cov: True
2024-03-05 06:54:47,754 [trainer.py] => shrink: True
2024-03-05 06:54:47,754 [trainer.py] => norm_cov: False
2024-03-05 06:54:47,754 [trainer.py] => vecnorm: False
2024-03-05 06:54:47,754 [trainer.py] => ae_type: wae
2024-03-05 06:54:47,754 [trainer.py] => epochs: 1000
2024-03-05 06:54:47,754 [trainer.py] => ae_latent_dim: 32
2024-03-05 06:54:47,754 [trainer.py] => wae_sigma: 30
2024-03-05 06:54:47,755 [trainer.py] => wae_C: 0.5
Files already downloaded and verified
Files already downloaded and verified
2024-03-05 06:54:49,438 [data_manager.py] => [68, 56, 78, 8, 23, 84, 90, 65, 74, 76, 40, 89, 3, 92, 55, 9, 26, 80, 43, 38, 58, 70, 77, 1, 85, 19, 17, 50, 28, 53, 13, 81, 45, 82, 6, 59, 83, 16, 15, 44, 91, 41, 72, 60, 79, 52, 20, 10, 31, 54, 37, 95, 14, 71, 96, 98, 97, 2, 64, 66, 42, 22, 35, 86, 24, 34, 87, 21, 99, 0, 88, 27, 18, 94, 11, 12, 47, 25, 30, 46, 62, 69, 36, 61, 7, 63, 75, 5, 32, 4, 51, 48, 73, 93, 39, 67, 29, 49, 57, 33]
2024-03-05 06:54:49,730 [fecam.py] => Learning on 0-50
/home/z1165703/miniconda3/lib/python3.11/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
/home/z1165703/miniconda3/lib/python3.11/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
Processing class: 0
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.1776723981
Epoch:   200  |  train loss: 0.1786884159
Epoch:   300  |  train loss: 0.1817200571
Epoch:   400  |  train loss: 0.1825138241
Epoch:   500  |  train loss: 0.1807786971
Epoch:   600  |  train loss: 0.1807889163
Epoch:   700  |  train loss: 0.1818233788
Epoch:   800  |  train loss: 0.1821848094
Epoch:   900  |  train loss: 0.1806226939
Epoch:  1000  |  train loss: 0.1802649111
Processing class: 1
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.1771783710
Epoch:   200  |  train loss: 0.1758149564
Epoch:   300  |  train loss: 0.1819314569
Epoch:   400  |  train loss: 0.1831444353
Epoch:   500  |  train loss: 0.1790768325
Epoch:   600  |  train loss: 0.1838343322
Epoch:   700  |  train loss: 0.1814667821
Epoch:   800  |  train loss: 0.1837611109
Epoch:   900  |  train loss: 0.1873355627
Epoch:  1000  |  train loss: 0.1834804595
Processing class: 2
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.1769315213
Epoch:   200  |  train loss: 0.1760418475
Epoch:   300  |  train loss: 0.1799959421
Epoch:   400  |  train loss: 0.1785168052
Epoch:   500  |  train loss: 0.1714278787
Epoch:   600  |  train loss: 0.1762447923
Epoch:   700  |  train loss: 0.1750335217
Epoch:   800  |  train loss: 0.1786917508
Epoch:   900  |  train loss: 0.1759361029
Epoch:  1000  |  train loss: 0.1743493319
Processing class: 3
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.1847167909
Epoch:   200  |  train loss: 0.1888358325
Epoch:   300  |  train loss: 0.1904234916
Epoch:   400  |  train loss: 0.1906422377
Epoch:   500  |  train loss: 0.1885006130
Epoch:   600  |  train loss: 0.1925003648
Epoch:   700  |  train loss: 0.1947144181
Epoch:   800  |  train loss: 0.1915500790
Epoch:   900  |  train loss: 0.1933175027
Epoch:  1000  |  train loss: 0.1936770797
Processing class: 4
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.1759365886
Epoch:   200  |  train loss: 0.1738128215
Epoch:   300  |  train loss: 0.1785227984
Epoch:   400  |  train loss: 0.1782209814
Epoch:   500  |  train loss: 0.1799775332
Epoch:   600  |  train loss: 0.1785028219
Epoch:   700  |  train loss: 0.1806969970
Epoch:   800  |  train loss: 0.1811570287
Epoch:   900  |  train loss: 0.1820351541
Epoch:  1000  |  train loss: 0.1850176126
Processing class: 5
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.1779431283
Epoch:   200  |  train loss: 0.1759233922
Epoch:   300  |  train loss: 0.1771052986
Epoch:   400  |  train loss: 0.1766327918
Epoch:   500  |  train loss: 0.1759537071
Epoch:   600  |  train loss: 0.1815189958
Epoch:   700  |  train loss: 0.1821866810
Epoch:   800  |  train loss: 0.1790749282
Epoch:   900  |  train loss: 0.1780697078
Epoch:  1000  |  train loss: 0.1804851532
Processing class: 6
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.1775157869
Epoch:   200  |  train loss: 0.1795261234
Epoch:   300  |  train loss: 0.1843951970
Epoch:   400  |  train loss: 0.1831552207
Epoch:   500  |  train loss: 0.1871772081
Epoch:   600  |  train loss: 0.1852315366
Epoch:   700  |  train loss: 0.1836089700
Epoch:   800  |  train loss: 0.1846804708
Epoch:   900  |  train loss: 0.1823219091
Epoch:  1000  |  train loss: 0.1839343667
Processing class: 7
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.1750487447
Epoch:   200  |  train loss: 0.1754947186
Epoch:   300  |  train loss: 0.1767741710
Epoch:   400  |  train loss: 0.1773899615
Epoch:   500  |  train loss: 0.1790264636
Epoch:   600  |  train loss: 0.1772524297
Epoch:   700  |  train loss: 0.1792501837
Epoch:   800  |  train loss: 0.1829536587
Epoch:   900  |  train loss: 0.1791393697
Epoch:  1000  |  train loss: 0.1819459945
Processing class: 8
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.1759084672
Epoch:   200  |  train loss: 0.1784024298
Epoch:   300  |  train loss: 0.1762915760
Epoch:   400  |  train loss: 0.1810108036
Epoch:   500  |  train loss: 0.1784192622
Epoch:   600  |  train loss: 0.1793002158
Epoch:   700  |  train loss: 0.1826832026
Epoch:   800  |  train loss: 0.1792458355
Epoch:   900  |  train loss: 0.1812032312
Epoch:  1000  |  train loss: 0.1789345264
Processing class: 9
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.1722534984
Epoch:   200  |  train loss: 0.1750621378
Epoch:   300  |  train loss: 0.1790799797
Epoch:   400  |  train loss: 0.1747774690
Epoch:   500  |  train loss: 0.1786952913
Epoch:   600  |  train loss: 0.1759138554
Epoch:   700  |  train loss: 0.1773602515
Epoch:   800  |  train loss: 0.1783978403
Epoch:   900  |  train loss: 0.1783672780
Epoch:  1000  |  train loss: 0.1755469471
Processing class: 10
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.1710485697
Epoch:   200  |  train loss: 0.1700263172
Epoch:   300  |  train loss: 0.1712212771
Epoch:   400  |  train loss: 0.1756487101
Epoch:   500  |  train loss: 0.1695820957
Epoch:   600  |  train loss: 0.1714285016
Epoch:   700  |  train loss: 0.1729149938
Epoch:   800  |  train loss: 0.1714003593
Epoch:   900  |  train loss: 0.1726085573
Epoch:  1000  |  train loss: 0.1721764147
Processing class: 11
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.1782211304
Epoch:   200  |  train loss: 0.1789783120
Epoch:   300  |  train loss: 0.1765691251
Epoch:   400  |  train loss: 0.1783679247
Epoch:   500  |  train loss: 0.1779916316
Epoch:   600  |  train loss: 0.1806956619
Epoch:   700  |  train loss: 0.1848628461
Epoch:   800  |  train loss: 0.1838506281
Epoch:   900  |  train loss: 0.1837805718
Epoch:  1000  |  train loss: 0.1857691437
Processing class: 12
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.1715434402
Epoch:   200  |  train loss: 0.1781353891
Epoch:   300  |  train loss: 0.1778987229
Epoch:   400  |  train loss: 0.1806204945
Epoch:   500  |  train loss: 0.1807876766
Epoch:   600  |  train loss: 0.1783848733
Epoch:   700  |  train loss: 0.1793450028
Epoch:   800  |  train loss: 0.1803195536
Epoch:   900  |  train loss: 0.1824140251
Epoch:  1000  |  train loss: 0.1805033177
Processing class: 13
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.1738569885
Epoch:   200  |  train loss: 0.1784016371
Epoch:   300  |  train loss: 0.1816467077
Epoch:   400  |  train loss: 0.1820794106
Epoch:   500  |  train loss: 0.1849435717
Epoch:   600  |  train loss: 0.1819802105
Epoch:   700  |  train loss: 0.1836830139
Epoch:   800  |  train loss: 0.1855792880
Epoch:   900  |  train loss: 0.1866515607
Epoch:  1000  |  train loss: 0.1877234429
Processing class: 14
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.1775132358
Epoch:   200  |  train loss: 0.1795419425
Epoch:   300  |  train loss: 0.1804231375
Epoch:   400  |  train loss: 0.1834659487
Epoch:   500  |  train loss: 0.1821611226
Epoch:   600  |  train loss: 0.1817941725
Epoch:   700  |  train loss: 0.1837775618
Epoch:   800  |  train loss: 0.1881549656
Epoch:   900  |  train loss: 0.1864361227
Epoch:  1000  |  train loss: 0.1832791507
Processing class: 15
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.1754735440
Epoch:   200  |  train loss: 0.1774072707
Epoch:   300  |  train loss: 0.1809897602
Epoch:   400  |  train loss: 0.1824354172
Epoch:   500  |  train loss: 0.1845662951
Epoch:   600  |  train loss: 0.1818279475
Epoch:   700  |  train loss: 0.1855491430
Epoch:   800  |  train loss: 0.1818676829
Epoch:   900  |  train loss: 0.1855760187
Epoch:  1000  |  train loss: 0.1838819206
Processing class: 16
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.1770749599
Epoch:   200  |  train loss: 0.1848547697
Epoch:   300  |  train loss: 0.1881840348
Epoch:   400  |  train loss: 0.1857829124
Epoch:   500  |  train loss: 0.1889402092
Epoch:   600  |  train loss: 0.1861046791
Epoch:   700  |  train loss: 0.1848619282
Epoch:   800  |  train loss: 0.1893144578
Epoch:   900  |  train loss: 0.1847766161
Epoch:  1000  |  train loss: 0.1889140099
Processing class: 17
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.1702253014
Epoch:   200  |  train loss: 0.1727789670
Epoch:   300  |  train loss: 0.1765754461
Epoch:   400  |  train loss: 0.1768471330
Epoch:   500  |  train loss: 0.1715658665
Epoch:   600  |  train loss: 0.1741488665
Epoch:   700  |  train loss: 0.1730754584
Epoch:   800  |  train loss: 0.1731534690
Epoch:   900  |  train loss: 0.1740486085
Epoch:  1000  |  train loss: 0.1735461771
Processing class: 18
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.1739724874
Epoch:   200  |  train loss: 0.1750002265
Epoch:   300  |  train loss: 0.1771560222
Epoch:   400  |  train loss: 0.1763022274
Epoch:   500  |  train loss: 0.1798203230
Epoch:   600  |  train loss: 0.1776829720
Epoch:   700  |  train loss: 0.1834880114
Epoch:   800  |  train loss: 0.1828086793
Epoch:   900  |  train loss: 0.1837446332
Epoch:  1000  |  train loss: 0.1869264930
Processing class: 19
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.1747827142
Epoch:   200  |  train loss: 0.1817267656
Epoch:   300  |  train loss: 0.1813152015
Epoch:   400  |  train loss: 0.1818166524
Epoch:   500  |  train loss: 0.1788860351
Epoch:   600  |  train loss: 0.1793329746
Epoch:   700  |  train loss: 0.1808232784
Epoch:   800  |  train loss: 0.1793871492
Epoch:   900  |  train loss: 0.1856730670
Epoch:  1000  |  train loss: 0.1868021876
Processing class: 20
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.1763569832
Epoch:   200  |  train loss: 0.1781973630
Epoch:   300  |  train loss: 0.1805311829
Epoch:   400  |  train loss: 0.1817440838
Epoch:   500  |  train loss: 0.1852959603
Epoch:   600  |  train loss: 0.1827862561
Epoch:   700  |  train loss: 0.1832833767
Epoch:   800  |  train loss: 0.1832499593
Epoch:   900  |  train loss: 0.1845518321
Epoch:  1000  |  train loss: 0.1870661050
Processing class: 21
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.1771026731
Epoch:   200  |  train loss: 0.1796025395
Epoch:   300  |  train loss: 0.1801539868
Epoch:   400  |  train loss: 0.1806983709
Epoch:   500  |  train loss: 0.1817402214
Epoch:   600  |  train loss: 0.1836046576
Epoch:   700  |  train loss: 0.1827579468
Epoch:   800  |  train loss: 0.1814828068
Epoch:   900  |  train loss: 0.1844065368
Epoch:  1000  |  train loss: 0.1831108749
Processing class: 22
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.1755885720
Epoch:   200  |  train loss: 0.1765674204
Epoch:   300  |  train loss: 0.1796679169
Epoch:   400  |  train loss: 0.1828313947
Epoch:   500  |  train loss: 0.1832280278
Epoch:   600  |  train loss: 0.1809701741
Epoch:   700  |  train loss: 0.1822428375
Epoch:   800  |  train loss: 0.1807522446
Epoch:   900  |  train loss: 0.1790628761
Epoch:  1000  |  train loss: 0.1841664016
Processing class: 23
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.1789525002
Epoch:   200  |  train loss: 0.1821894377
Epoch:   300  |  train loss: 0.1851967365
Epoch:   400  |  train loss: 0.1811704427
Epoch:   500  |  train loss: 0.1807061225
Epoch:   600  |  train loss: 0.1782417446
Epoch:   700  |  train loss: 0.1800361663
Epoch:   800  |  train loss: 0.1780721843
Epoch:   900  |  train loss: 0.1801126242
Epoch:  1000  |  train loss: 0.1776995838
Processing class: 24
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.1773666054
Epoch:   200  |  train loss: 0.1772765100
Epoch:   300  |  train loss: 0.1741513491
Epoch:   400  |  train loss: 0.1772058278
Epoch:   500  |  train loss: 0.1752269328
Epoch:   600  |  train loss: 0.1771540016
Epoch:   700  |  train loss: 0.1722762614
Epoch:   800  |  train loss: 0.1736852348
Epoch:   900  |  train loss: 0.1762589782
Epoch:  1000  |  train loss: 0.1748633474
Processing class: 25
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.1751853913
Epoch:   200  |  train loss: 0.1773595035
Epoch:   300  |  train loss: 0.1807166010
Epoch:   400  |  train loss: 0.1774196565
Epoch:   500  |  train loss: 0.1810624421
Epoch:   600  |  train loss: 0.1787702531
Epoch:   700  |  train loss: 0.1824534535
Epoch:   800  |  train loss: 0.1806634337
Epoch:   900  |  train loss: 0.1792509019
Epoch:  1000  |  train loss: 0.1804910183
Processing class: 26
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.1805862844
Epoch:   200  |  train loss: 0.1849109560
Epoch:   300  |  train loss: 0.1825117081
Epoch:   400  |  train loss: 0.1856678307
Epoch:   500  |  train loss: 0.1886785597
Epoch:   600  |  train loss: 0.1831808835
Epoch:   700  |  train loss: 0.1848118812
Epoch:   800  |  train loss: 0.1863135815
Epoch:   900  |  train loss: 0.1852436125
Epoch:  1000  |  train loss: 0.1880579472
Processing class: 27
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.1764778614
Epoch:   200  |  train loss: 0.1742646515
Epoch:   300  |  train loss: 0.1756122887
Epoch:   400  |  train loss: 0.1761542916
Epoch:   500  |  train loss: 0.1778805465
Epoch:   600  |  train loss: 0.1778323740
Epoch:   700  |  train loss: 0.1788680047
Epoch:   800  |  train loss: 0.1785349935
Epoch:   900  |  train loss: 0.1795117885
Epoch:  1000  |  train loss: 0.1756513923
Processing class: 28
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.1797118247
Epoch:   200  |  train loss: 0.1832261086
Epoch:   300  |  train loss: 0.1864265859
Epoch:   400  |  train loss: 0.1878155887
Epoch:   500  |  train loss: 0.1885872215
Epoch:   600  |  train loss: 0.1885308385
Epoch:   700  |  train loss: 0.1892734081
Epoch:   800  |  train loss: 0.1903153360
Epoch:   900  |  train loss: 0.1909139365
Epoch:  1000  |  train loss: 0.1879609674
Processing class: 29
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.1763014197
Epoch:   200  |  train loss: 0.1780907065
Epoch:   300  |  train loss: 0.1772774756
Epoch:   400  |  train loss: 0.1751611114
Epoch:   500  |  train loss: 0.1779534250
Epoch:   600  |  train loss: 0.1791364819
Epoch:   700  |  train loss: 0.1786949962
Epoch:   800  |  train loss: 0.1761941254
Epoch:   900  |  train loss: 0.1765756845
Epoch:  1000  |  train loss: 0.1755929500
Processing class: 30
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.1804124147
Epoch:   200  |  train loss: 0.1842176586
Epoch:   300  |  train loss: 0.1891188085
Epoch:   400  |  train loss: 0.1936818719
Epoch:   500  |  train loss: 0.1914067805
Epoch:   600  |  train loss: 0.1907687187
Epoch:   700  |  train loss: 0.1896438718
Epoch:   800  |  train loss: 0.1927428275
Epoch:   900  |  train loss: 0.1946909547
Epoch:  1000  |  train loss: 0.1920591325
Processing class: 31
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.1800312012
Epoch:   200  |  train loss: 0.1840038270
Epoch:   300  |  train loss: 0.1838961184
Epoch:   400  |  train loss: 0.1880706072
Epoch:   500  |  train loss: 0.1892463565
Epoch:   600  |  train loss: 0.1923991382
Epoch:   700  |  train loss: 0.1913026959
Epoch:   800  |  train loss: 0.1914300442
Epoch:   900  |  train loss: 0.1939457178
Epoch:  1000  |  train loss: 0.1905768394
Processing class: 32
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.1772672653
Epoch:   200  |  train loss: 0.1807640731
Epoch:   300  |  train loss: 0.1796599895
Epoch:   400  |  train loss: 0.1760476589
Epoch:   500  |  train loss: 0.1817079931
Epoch:   600  |  train loss: 0.1794872731
Epoch:   700  |  train loss: 0.1838065892
Epoch:   800  |  train loss: 0.1852425098
Epoch:   900  |  train loss: 0.1840056688
Epoch:  1000  |  train loss: 0.1825108707
Processing class: 33
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.1708326697
Epoch:   200  |  train loss: 0.1793601274
Epoch:   300  |  train loss: 0.1725406289
Epoch:   400  |  train loss: 0.1728359342
Epoch:   500  |  train loss: 0.1731005251
Epoch:   600  |  train loss: 0.1776363045
Epoch:   700  |  train loss: 0.1748150617
Epoch:   800  |  train loss: 0.1757472038
Epoch:   900  |  train loss: 0.1750751376
Epoch:  1000  |  train loss: 0.1735142410
Processing class: 34
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.1778705865
Epoch:   200  |  train loss: 0.1767497957
Epoch:   300  |  train loss: 0.1790401399
Epoch:   400  |  train loss: 0.1770060152
Epoch:   500  |  train loss: 0.1763488859
Epoch:   600  |  train loss: 0.1789699614
Epoch:   700  |  train loss: 0.1760250777
Epoch:   800  |  train loss: 0.1791818738
Epoch:   900  |  train loss: 0.1800436378
Epoch:  1000  |  train loss: 0.1805822641
Processing class: 35
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.1753913105
Epoch:   200  |  train loss: 0.1782787442
Epoch:   300  |  train loss: 0.1807864964
Epoch:   400  |  train loss: 0.1817721933
Epoch:   500  |  train loss: 0.1790630013
Epoch:   600  |  train loss: 0.1809995264
Epoch:   700  |  train loss: 0.1818362355
Epoch:   800  |  train loss: 0.1812492341
Epoch:   900  |  train loss: 0.1816488802
Epoch:  1000  |  train loss: 0.1800342143
Processing class: 36
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.1768400699
Epoch:   200  |  train loss: 0.1775641084
Epoch:   300  |  train loss: 0.1796607614
Epoch:   400  |  train loss: 0.1795507997
Epoch:   500  |  train loss: 0.1834519118
Epoch:   600  |  train loss: 0.1811193287
Epoch:   700  |  train loss: 0.1786776870
Epoch:   800  |  train loss: 0.1832257420
Epoch:   900  |  train loss: 0.1789802939
Epoch:  1000  |  train loss: 0.1815969527
Processing class: 37
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.1796073735
Epoch:   200  |  train loss: 0.1786768407
Epoch:   300  |  train loss: 0.1777284443
Epoch:   400  |  train loss: 0.1793185204
Epoch:   500  |  train loss: 0.1808163881
Epoch:   600  |  train loss: 0.1782416970
Epoch:   700  |  train loss: 0.1784605980
Epoch:   800  |  train loss: 0.1794915229
Epoch:   900  |  train loss: 0.1772039354
Epoch:  1000  |  train loss: 0.1784038872
Processing class: 38
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.1758129507
Epoch:   200  |  train loss: 0.1772769660
Epoch:   300  |  train loss: 0.1804242939
Epoch:   400  |  train loss: 0.1816427231
Epoch:   500  |  train loss: 0.1822557956
Epoch:   600  |  train loss: 0.1808670223
Epoch:   700  |  train loss: 0.1809841692
Epoch:   800  |  train loss: 0.1839573205
Epoch:   900  |  train loss: 0.1800223917
Epoch:  1000  |  train loss: 0.1825322986
Processing class: 39
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.1775038153
Epoch:   200  |  train loss: 0.1761131585
Epoch:   300  |  train loss: 0.1782143325
Epoch:   400  |  train loss: 0.1797106028
Epoch:   500  |  train loss: 0.1781784475
Epoch:   600  |  train loss: 0.1790793419
Epoch:   700  |  train loss: 0.1802329600
Epoch:   800  |  train loss: 0.1821033418
Epoch:   900  |  train loss: 0.1829074979
Epoch:  1000  |  train loss: 0.1834963620
Processing class: 40
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.1778232425
Epoch:   200  |  train loss: 0.1748681992
Epoch:   300  |  train loss: 0.1808993816
Epoch:   400  |  train loss: 0.1820079774
Epoch:   500  |  train loss: 0.1835195601
Epoch:   600  |  train loss: 0.1812388331
Epoch:   700  |  train loss: 0.1827253699
Epoch:   800  |  train loss: 0.1823731065
Epoch:   900  |  train loss: 0.1806245238
Epoch:  1000  |  train loss: 0.1860019088
Processing class: 41
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.1756916136
Epoch:   200  |  train loss: 0.1763866931
Epoch:   300  |  train loss: 0.1785810739
Epoch:   400  |  train loss: 0.1768173605
Epoch:   500  |  train loss: 0.1766013682
Epoch:   600  |  train loss: 0.1766420603
Epoch:   700  |  train loss: 0.1790788203
Epoch:   800  |  train loss: 0.1773995727
Epoch:   900  |  train loss: 0.1765332669
Epoch:  1000  |  train loss: 0.1765097439
Processing class: 42
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.1745665669
Epoch:   200  |  train loss: 0.1782767862
Epoch:   300  |  train loss: 0.1805117905
Epoch:   400  |  train loss: 0.1810165703
Epoch:   500  |  train loss: 0.1822170794
Epoch:   600  |  train loss: 0.1829080403
Epoch:   700  |  train loss: 0.1843556851
Epoch:   800  |  train loss: 0.1851762652
Epoch:   900  |  train loss: 0.1832331866
Epoch:  1000  |  train loss: 0.1827226579
Processing class: 43
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.1764169335
Epoch:   200  |  train loss: 0.1756642610
Epoch:   300  |  train loss: 0.1816695035
Epoch:   400  |  train loss: 0.1874141455
Epoch:   500  |  train loss: 0.1887356997
Epoch:   600  |  train loss: 0.1872195005
Epoch:   700  |  train loss: 0.1895446539
Epoch:   800  |  train loss: 0.1883441299
Epoch:   900  |  train loss: 0.1848592252
Epoch:  1000  |  train loss: 0.1888388723
Processing class: 44
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.1747661829
Epoch:   200  |  train loss: 0.1788800925
Epoch:   300  |  train loss: 0.1770597905
Epoch:   400  |  train loss: 0.1775971442
Epoch:   500  |  train loss: 0.1793034077
Epoch:   600  |  train loss: 0.1777352363
Epoch:   700  |  train loss: 0.1806909859
Epoch:   800  |  train loss: 0.1795479506
Epoch:   900  |  train loss: 0.1780084103
Epoch:  1000  |  train loss: 0.1811980784
Processing class: 45
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.1779956698
Epoch:   200  |  train loss: 0.1836240143
Epoch:   300  |  train loss: 0.1824866921
Epoch:   400  |  train loss: 0.1863180280
Epoch:   500  |  train loss: 0.1855397135
Epoch:   600  |  train loss: 0.1902172357
Epoch:   700  |  train loss: 0.1911247462
Epoch:   800  |  train loss: 0.1897386849
Epoch:   900  |  train loss: 0.1902661175
Epoch:  1000  |  train loss: 0.1927199751
Processing class: 46
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.1791496754
Epoch:   200  |  train loss: 0.1794376969
Epoch:   300  |  train loss: 0.1812098533
Epoch:   400  |  train loss: 0.1826750398
Epoch:   500  |  train loss: 0.1829614073
Epoch:   600  |  train loss: 0.1830302775
Epoch:   700  |  train loss: 0.1834427088
Epoch:   800  |  train loss: 0.1871631145
Epoch:   900  |  train loss: 0.1846950442
Epoch:  1000  |  train loss: 0.1863914013
Processing class: 47
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.1746025741
Epoch:   200  |  train loss: 0.1778129220
Epoch:   300  |  train loss: 0.1753346533
Epoch:   400  |  train loss: 0.1786645025
Epoch:   500  |  train loss: 0.1784109443
Epoch:   600  |  train loss: 0.1779700875
Epoch:   700  |  train loss: 0.1788133323
Epoch:   800  |  train loss: 0.1765765846
Epoch:   900  |  train loss: 0.1775072753
Epoch:  1000  |  train loss: 0.1761742681
Processing class: 48
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.1765958607
Epoch:   200  |  train loss: 0.1807612926
Epoch:   300  |  train loss: 0.1791852087
Epoch:   400  |  train loss: 0.1755440354
Epoch:   500  |  train loss: 0.1802932948
Epoch:   600  |  train loss: 0.1793759227
Epoch:   700  |  train loss: 0.1769294351
Epoch:   800  |  train loss: 0.1767793536
Epoch:   900  |  train loss: 0.1815044016
Epoch:  1000  |  train loss: 0.1825759351
Processing class: 49
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.1761309713
Epoch:   200  |  train loss: 0.1770318538
Epoch:   300  |  train loss: 0.1825606942
Epoch:   400  |  train loss: 0.1832348287
Epoch:   500  |  train loss: 0.1843098551
Epoch:   600  |  train loss: 0.1842795670
Epoch:   700  |  train loss: 0.1839736193
Epoch:   800  |  train loss: 0.1827655375
Epoch:   900  |  train loss: 0.1819727421
Epoch:  1000  |  train loss: 0.1825516939
/home/z1165703/FeCAM/models/base.py:87: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  vec = torch.tensor(vec).cuda()
2024-03-05 07:12:33,343 [trainer.py] => CNN: {'total': 83.44, '00-09': 87.7, '10-19': 79.6, '20-29': 84.9, '30-39': 81.0, '40-49': 84.0, 'old': 0, 'new': 83.44}
2024-03-05 07:12:33,447 [trainer.py] => No NME accuracy
2024-03-05 07:12:33,447 [trainer.py] => FeCAM: {'total': 82.9, '00-09': 86.2, '10-19': 79.7, '20-29': 85.0, '30-39': 80.6, '40-49': 83.0, 'old': 0, 'new': 82.9}
2024-03-05 07:12:33,447 [trainer.py] => CNN top1 curve: [83.44]
2024-03-05 07:12:33,447 [trainer.py] => CNN top5 curve: [96.5]
2024-03-05 07:12:33,447 [trainer.py] => FeCAM top1 curve: [82.9]
2024-03-05 07:12:33,447 [trainer.py] => FeCAM top5 curve: [95.84]

2024-03-05 07:12:33,460 [fecam.py] => Learning on 50-60
Processing class: 50
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.1750593662
Epoch:   200  |  train loss: 0.1757112384
Epoch:   300  |  train loss: 0.1760836333
Epoch:   400  |  train loss: 0.1789751559
Epoch:   500  |  train loss: 0.1804553449
Epoch:   600  |  train loss: 0.1781003654
Epoch:   700  |  train loss: 0.1813661844
Epoch:   800  |  train loss: 0.1828316242
Epoch:   900  |  train loss: 0.1847995281
Epoch:  1000  |  train loss: 0.1854494601
Processing class: 51
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.1766567826
Epoch:   200  |  train loss: 0.1727162451
Epoch:   300  |  train loss: 0.1777754307
Epoch:   400  |  train loss: 0.1786251307
Epoch:   500  |  train loss: 0.1790050805
Epoch:   600  |  train loss: 0.1806098104
Epoch:   700  |  train loss: 0.1775089324
Epoch:   800  |  train loss: 0.1781088203
Epoch:   900  |  train loss: 0.1821000844
Epoch:  1000  |  train loss: 0.1801387906
Processing class: 52
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.1781396896
Epoch:   200  |  train loss: 0.1766732693
Epoch:   300  |  train loss: 0.1786750048
Epoch:   400  |  train loss: 0.1800926685
Epoch:   500  |  train loss: 0.1793482304
Epoch:   600  |  train loss: 0.1770310402
Epoch:   700  |  train loss: 0.1801116645
Epoch:   800  |  train loss: 0.1801690102
Epoch:   900  |  train loss: 0.1808301181
Epoch:  1000  |  train loss: 0.1782883883
Processing class: 53
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.1736822069
Epoch:   200  |  train loss: 0.1745002627
Epoch:   300  |  train loss: 0.1709739476
Epoch:   400  |  train loss: 0.1720674783
Epoch:   500  |  train loss: 0.1736861289
Epoch:   600  |  train loss: 0.1763840228
Epoch:   700  |  train loss: 0.1793890506
Epoch:   800  |  train loss: 0.1731634021
Epoch:   900  |  train loss: 0.1761116385
Epoch:  1000  |  train loss: 0.1737910748
Processing class: 54
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.1634830087
Epoch:   200  |  train loss: 0.1640327066
Epoch:   300  |  train loss: 0.1665212512
Epoch:   400  |  train loss: 0.1673384815
Epoch:   500  |  train loss: 0.1686441392
Epoch:   600  |  train loss: 0.1721164644
Epoch:   700  |  train loss: 0.1729589283
Epoch:   800  |  train loss: 0.1757761091
Epoch:   900  |  train loss: 0.1777074575
Epoch:  1000  |  train loss: 0.1765222073
Processing class: 55
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.1764231950
Epoch:   200  |  train loss: 0.1768517286
Epoch:   300  |  train loss: 0.1749498159
Epoch:   400  |  train loss: 0.1785685241
Epoch:   500  |  train loss: 0.1833445072
Epoch:   600  |  train loss: 0.1837595791
Epoch:   700  |  train loss: 0.1852209479
Epoch:   800  |  train loss: 0.1887954146
Epoch:   900  |  train loss: 0.1841163576
Epoch:  1000  |  train loss: 0.1867258847
Processing class: 56
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.1733578682
Epoch:   200  |  train loss: 0.1742676497
Epoch:   300  |  train loss: 0.1702750653
Epoch:   400  |  train loss: 0.1743840128
Epoch:   500  |  train loss: 0.1755503267
Epoch:   600  |  train loss: 0.1761596620
Epoch:   700  |  train loss: 0.1776129216
Epoch:   800  |  train loss: 0.1754619628
Epoch:   900  |  train loss: 0.1782790482
Epoch:  1000  |  train loss: 0.1798781991
Processing class: 57
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.1745046347
Epoch:   200  |  train loss: 0.1778159231
Epoch:   300  |  train loss: 0.1763856918
Epoch:   400  |  train loss: 0.1763389051
Epoch:   500  |  train loss: 0.1787382185
Epoch:   600  |  train loss: 0.1754042298
Epoch:   700  |  train loss: 0.1799795598
Epoch:   800  |  train loss: 0.1797413647
Epoch:   900  |  train loss: 0.1821879476
Epoch:  1000  |  train loss: 0.1780847937
Processing class: 58
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.1742904156
Epoch:   200  |  train loss: 0.1720433891
Epoch:   300  |  train loss: 0.1749626845
Epoch:   400  |  train loss: 0.1785900563
Epoch:   500  |  train loss: 0.1772834361
Epoch:   600  |  train loss: 0.1799547076
Epoch:   700  |  train loss: 0.1788590193
Epoch:   800  |  train loss: 0.1810523719
Epoch:   900  |  train loss: 0.1808018088
Epoch:  1000  |  train loss: 0.1807301998
Processing class: 59
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.1789610326
Epoch:   200  |  train loss: 0.1833085448
Epoch:   300  |  train loss: 0.1810951471
Epoch:   400  |  train loss: 0.1828220993
Epoch:   500  |  train loss: 0.1850183725
Epoch:   600  |  train loss: 0.1853227615
Epoch:   700  |  train loss: 0.1868985415
Epoch:   800  |  train loss: 0.1874874502
Epoch:   900  |  train loss: 0.1867672980
Epoch:  1000  |  train loss: 0.1857201576
2024-03-05 07:18:12,625 [trainer.py] => CNN: {'total': 71.75, '00-09': 82.2, '10-19': 72.8, '20-29': 78.5, '30-39': 74.9, '40-49': 68.4, '50-59': 53.7, 'old': 75.36, 'new': 53.7}
2024-03-05 07:18:13,883 [trainer.py] => No NME accuracy
2024-03-05 07:18:13,883 [trainer.py] => FeCAM: {'total': 72.78, '00-09': 82.6, '10-19': 74.0, '20-29': 81.0, '30-39': 76.4, '40-49': 76.8, '50-59': 45.9, 'old': 78.16, 'new': 45.9}
2024-03-05 07:18:13,883 [trainer.py] => CNN top1 curve: [83.44, 71.75]
2024-03-05 07:18:13,883 [trainer.py] => CNN top5 curve: [96.5, 89.6]
2024-03-05 07:18:13,883 [trainer.py] => FeCAM top1 curve: [82.9, 72.78]
2024-03-05 07:18:13,883 [trainer.py] => FeCAM top5 curve: [95.84, 91.18]

2024-03-05 07:18:13,896 [fecam.py] => Learning on 60-70
Processing class: 60
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.1804689527
Epoch:   200  |  train loss: 0.1797488242
Epoch:   300  |  train loss: 0.1773118943
Epoch:   400  |  train loss: 0.1762457460
Epoch:   500  |  train loss: 0.1801726043
Epoch:   600  |  train loss: 0.1800192207
Epoch:   700  |  train loss: 0.1888163358
Epoch:   800  |  train loss: 0.1836171508
Epoch:   900  |  train loss: 0.1842362612
Epoch:  1000  |  train loss: 0.1864643902
Processing class: 61
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.1731488347
Epoch:   200  |  train loss: 0.1696612239
Epoch:   300  |  train loss: 0.1732216120
Epoch:   400  |  train loss: 0.1733326644
Epoch:   500  |  train loss: 0.1780015767
Epoch:   600  |  train loss: 0.1743732095
Epoch:   700  |  train loss: 0.1784656435
Epoch:   800  |  train loss: 0.1809466898
Epoch:   900  |  train loss: 0.1766479284
Epoch:  1000  |  train loss: 0.1783545703
Processing class: 62
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.1791397363
Epoch:   200  |  train loss: 0.1769307971
Epoch:   300  |  train loss: 0.1797880113
Epoch:   400  |  train loss: 0.1810024917
Epoch:   500  |  train loss: 0.1787970841
Epoch:   600  |  train loss: 0.1830836982
Epoch:   700  |  train loss: 0.1843617707
Epoch:   800  |  train loss: 0.1851710171
Epoch:   900  |  train loss: 0.1838568091
Epoch:  1000  |  train loss: 0.1850048900
Processing class: 63
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.1722586781
Epoch:   200  |  train loss: 0.1732742399
Epoch:   300  |  train loss: 0.1787732124
Epoch:   400  |  train loss: 0.1788158238
Epoch:   500  |  train loss: 0.1804596514
Epoch:   600  |  train loss: 0.1843046844
Epoch:   700  |  train loss: 0.1793630838
Epoch:   800  |  train loss: 0.1861255020
Epoch:   900  |  train loss: 0.1815798283
Epoch:  1000  |  train loss: 0.1826207727
Processing class: 64
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.1798104405
Epoch:   200  |  train loss: 0.1756864160
Epoch:   300  |  train loss: 0.1820589364
Epoch:   400  |  train loss: 0.1821722984
Epoch:   500  |  train loss: 0.1822179735
Epoch:   600  |  train loss: 0.1816846579
Epoch:   700  |  train loss: 0.1782369792
Epoch:   800  |  train loss: 0.1799486399
Epoch:   900  |  train loss: 0.1832323223
Epoch:  1000  |  train loss: 0.1818224818
Processing class: 65
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.1753551126
Epoch:   200  |  train loss: 0.1739953011
Epoch:   300  |  train loss: 0.1801610202
Epoch:   400  |  train loss: 0.1825020015
Epoch:   500  |  train loss: 0.1811343640
Epoch:   600  |  train loss: 0.1865662843
Epoch:   700  |  train loss: 0.1864808857
Epoch:   800  |  train loss: 0.1914941579
Epoch:   900  |  train loss: 0.1881301552
Epoch:  1000  |  train loss: 0.1932841629
Processing class: 66
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.1793236136
Epoch:   200  |  train loss: 0.1815448701
Epoch:   300  |  train loss: 0.1841648906
Epoch:   400  |  train loss: 0.1840350389
Epoch:   500  |  train loss: 0.1873828560
Epoch:   600  |  train loss: 0.1876734644
Epoch:   700  |  train loss: 0.1913753062
Epoch:   800  |  train loss: 0.1917448699
Epoch:   900  |  train loss: 0.1914800555
Epoch:  1000  |  train loss: 0.1909405708
Processing class: 67
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.1733957857
Epoch:   200  |  train loss: 0.1751701504
Epoch:   300  |  train loss: 0.1748669088
Epoch:   400  |  train loss: 0.1755333155
Epoch:   500  |  train loss: 0.1755134225
Epoch:   600  |  train loss: 0.1752695680
Epoch:   700  |  train loss: 0.1760170728
Epoch:   800  |  train loss: 0.1782397062
Epoch:   900  |  train loss: 0.1775729656
Epoch:  1000  |  train loss: 0.1772532314
Processing class: 68
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.1714225888
Epoch:   200  |  train loss: 0.1712297589
Epoch:   300  |  train loss: 0.1694906712
Epoch:   400  |  train loss: 0.1705741853
Epoch:   500  |  train loss: 0.1707979143
Epoch:   600  |  train loss: 0.1699903041
Epoch:   700  |  train loss: 0.1715467483
Epoch:   800  |  train loss: 0.1697614610
Epoch:   900  |  train loss: 0.1707749248
Epoch:  1000  |  train loss: 0.1751882136
Processing class: 69
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.1650265366
Epoch:   200  |  train loss: 0.1675006479
Epoch:   300  |  train loss: 0.1730120242
Epoch:   400  |  train loss: 0.1752983660
Epoch:   500  |  train loss: 0.1753892809
Epoch:   600  |  train loss: 0.1712606400
Epoch:   700  |  train loss: 0.1747498512
Epoch:   800  |  train loss: 0.1743464738
Epoch:   900  |  train loss: 0.1795764804
Epoch:  1000  |  train loss: 0.1743194282
2024-03-05 07:24:52,397 [trainer.py] => CNN: {'total': 64.66, '00-09': 75.0, '10-19': 70.1, '20-29': 77.1, '30-39': 70.6, '40-49': 64.0, '50-59': 45.8, '60-69': 50.0, 'old': 67.1, 'new': 50.0}
2024-03-05 07:24:52,397 [trainer.py] => No NME accuracy
2024-03-05 07:24:52,397 [trainer.py] => FeCAM: {'total': 67.27, '00-09': 79.9, '10-19': 70.6, '20-29': 79.1, '30-39': 71.9, '40-49': 74.1, '50-59': 41.2, '60-69': 54.1, 'old': 69.47, 'new': 54.1}
2024-03-05 07:24:52,398 [trainer.py] => CNN top1 curve: [83.44, 71.75, 64.66]
2024-03-05 07:24:52,398 [trainer.py] => CNN top5 curve: [96.5, 89.6, 86.54]
2024-03-05 07:24:52,398 [trainer.py] => FeCAM top1 curve: [82.9, 72.78, 67.27]
2024-03-05 07:24:52,398 [trainer.py] => FeCAM top5 curve: [95.84, 91.18, 87.21]

2024-03-05 07:24:52,402 [fecam.py] => Learning on 70-80
Processing class: 70
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.1734476000
Epoch:   200  |  train loss: 0.1715512007
Epoch:   300  |  train loss: 0.1727986306
Epoch:   400  |  train loss: 0.1743998080
Epoch:   500  |  train loss: 0.1725095958
Epoch:   600  |  train loss: 0.1755726546
Epoch:   700  |  train loss: 0.1793230951
Epoch:   800  |  train loss: 0.1747012317
Epoch:   900  |  train loss: 0.1778164089
Epoch:  1000  |  train loss: 0.1764736921
Processing class: 71
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.1766652048
Epoch:   200  |  train loss: 0.1779513389
Epoch:   300  |  train loss: 0.1747469425
Epoch:   400  |  train loss: 0.1773658603
Epoch:   500  |  train loss: 0.1791842341
Epoch:   600  |  train loss: 0.1794424146
Epoch:   700  |  train loss: 0.1797463655
Epoch:   800  |  train loss: 0.1805301517
Epoch:   900  |  train loss: 0.1838401198
Epoch:  1000  |  train loss: 0.1829071105
Processing class: 72
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.1736213505
Epoch:   200  |  train loss: 0.1747548342
Epoch:   300  |  train loss: 0.1752245665
Epoch:   400  |  train loss: 0.1748741090
Epoch:   500  |  train loss: 0.1769790471
Epoch:   600  |  train loss: 0.1744990796
Epoch:   700  |  train loss: 0.1758017778
Epoch:   800  |  train loss: 0.1746274203
Epoch:   900  |  train loss: 0.1755312085
Epoch:  1000  |  train loss: 0.1784825295
Processing class: 73
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.1776146740
Epoch:   200  |  train loss: 0.1764468074
Epoch:   300  |  train loss: 0.1803891361
Epoch:   400  |  train loss: 0.1782740891
Epoch:   500  |  train loss: 0.1827931285
Epoch:   600  |  train loss: 0.1824920416
Epoch:   700  |  train loss: 0.1800106168
Epoch:   800  |  train loss: 0.1799794346
Epoch:   900  |  train loss: 0.1802896500
Epoch:  1000  |  train loss: 0.1820722133
Processing class: 74
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.1800652295
Epoch:   200  |  train loss: 0.1821894854
Epoch:   300  |  train loss: 0.1784627557
Epoch:   400  |  train loss: 0.1766893625
Epoch:   500  |  train loss: 0.1785780817
Epoch:   600  |  train loss: 0.1771961033
Epoch:   700  |  train loss: 0.1819190860
Epoch:   800  |  train loss: 0.1773694903
Epoch:   900  |  train loss: 0.1778499335
Epoch:  1000  |  train loss: 0.1816090345
Processing class: 75
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.1748611510
Epoch:   200  |  train loss: 0.1739216268
Epoch:   300  |  train loss: 0.1776212543
Epoch:   400  |  train loss: 0.1769805402
Epoch:   500  |  train loss: 0.1797469825
Epoch:   600  |  train loss: 0.1825169355
Epoch:   700  |  train loss: 0.1749262869
Epoch:   800  |  train loss: 0.1796263039
Epoch:   900  |  train loss: 0.1791549981
Epoch:  1000  |  train loss: 0.1823425680
Processing class: 76
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.1612237662
Epoch:   200  |  train loss: 0.1626913130
Epoch:   300  |  train loss: 0.1675186545
Epoch:   400  |  train loss: 0.1709098011
Epoch:   500  |  train loss: 0.1722693712
Epoch:   600  |  train loss: 0.1749727905
Epoch:   700  |  train loss: 0.1756877422
Epoch:   800  |  train loss: 0.1716760218
Epoch:   900  |  train loss: 0.1761267900
Epoch:  1000  |  train loss: 0.1781314135
Processing class: 77
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.1740948766
Epoch:   200  |  train loss: 0.1743925065
Epoch:   300  |  train loss: 0.1717245281
Epoch:   400  |  train loss: 0.1732556880
Epoch:   500  |  train loss: 0.1739084423
Epoch:   600  |  train loss: 0.1737942338
Epoch:   700  |  train loss: 0.1746908188
Epoch:   800  |  train loss: 0.1779318660
Epoch:   900  |  train loss: 0.1761774898
Epoch:  1000  |  train loss: 0.1756685227
Processing class: 78
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.1771388054
Epoch:   200  |  train loss: 0.1795884162
Epoch:   300  |  train loss: 0.1824428737
Epoch:   400  |  train loss: 0.1821016997
Epoch:   500  |  train loss: 0.1812102020
Epoch:   600  |  train loss: 0.1838534892
Epoch:   700  |  train loss: 0.1845739692
Epoch:   800  |  train loss: 0.1854591727
Epoch:   900  |  train loss: 0.1857954770
Epoch:  1000  |  train loss: 0.1848958850
Processing class: 79
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.1775187612
Epoch:   200  |  train loss: 0.1751297623
Epoch:   300  |  train loss: 0.1785623252
Epoch:   400  |  train loss: 0.1786294639
Epoch:   500  |  train loss: 0.1785901934
Epoch:   600  |  train loss: 0.1822344750
Epoch:   700  |  train loss: 0.1820786983
Epoch:   800  |  train loss: 0.1836867064
Epoch:   900  |  train loss: 0.1864732385
Epoch:  1000  |  train loss: 0.1839206100
2024-03-05 07:32:33,099 [trainer.py] => CNN: {'total': 59.18, '00-09': 73.6, '10-19': 68.4, '20-29': 76.9, '30-39': 69.1, '40-49': 60.6, '50-59': 37.2, '60-69': 45.7, '70-79': 41.9, 'old': 61.64, 'new': 41.9}
2024-03-05 07:32:33,100 [trainer.py] => No NME accuracy
2024-03-05 07:32:33,100 [trainer.py] => FeCAM: {'total': 61.72, '00-09': 78.1, '10-19': 69.3, '20-29': 78.0, '30-39': 69.5, '40-49': 72.1, '50-59': 35.7, '60-69': 49.1, '70-79': 42.0, 'old': 64.54, 'new': 42.0}
2024-03-05 07:32:33,100 [trainer.py] => CNN top1 curve: [83.44, 71.75, 64.66, 59.18]
2024-03-05 07:32:33,100 [trainer.py] => CNN top5 curve: [96.5, 89.6, 86.54, 84.09]
2024-03-05 07:32:33,100 [trainer.py] => FeCAM top1 curve: [82.9, 72.78, 67.27, 61.72]
2024-03-05 07:32:33,100 [trainer.py] => FeCAM top5 curve: [95.84, 91.18, 87.21, 84.09]

2024-03-05 07:32:33,108 [fecam.py] => Learning on 80-90
Processing class: 80
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.1696553171
Epoch:   200  |  train loss: 0.1751705647
Epoch:   300  |  train loss: 0.1774744421
Epoch:   400  |  train loss: 0.1788894117
Epoch:   500  |  train loss: 0.1786282599
Epoch:   600  |  train loss: 0.1786794871
Epoch:   700  |  train loss: 0.1754853100
Epoch:   800  |  train loss: 0.1805473834
Epoch:   900  |  train loss: 0.1793695807
Epoch:  1000  |  train loss: 0.1820767999
Processing class: 81
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.1735867441
Epoch:   200  |  train loss: 0.1783651173
Epoch:   300  |  train loss: 0.1753577471
Epoch:   400  |  train loss: 0.1801079750
Epoch:   500  |  train loss: 0.1789462477
Epoch:   600  |  train loss: 0.1822864592
Epoch:   700  |  train loss: 0.1817619026
Epoch:   800  |  train loss: 0.1839907914
Epoch:   900  |  train loss: 0.1842382759
Epoch:  1000  |  train loss: 0.1852775425
Processing class: 82
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.1710777730
Epoch:   200  |  train loss: 0.1736703604
Epoch:   300  |  train loss: 0.1741336465
Epoch:   400  |  train loss: 0.1736343503
Epoch:   500  |  train loss: 0.1746907979
Epoch:   600  |  train loss: 0.1737955838
Epoch:   700  |  train loss: 0.1739242256
Epoch:   800  |  train loss: 0.1747840881
Epoch:   900  |  train loss: 0.1752145678
Epoch:  1000  |  train loss: 0.1753195643
Processing class: 83
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.1731701910
Epoch:   200  |  train loss: 0.1771155864
Epoch:   300  |  train loss: 0.1761325419
Epoch:   400  |  train loss: 0.1777332962
Epoch:   500  |  train loss: 0.1816646814
Epoch:   600  |  train loss: 0.1791552722
Epoch:   700  |  train loss: 0.1830557913
Epoch:   800  |  train loss: 0.1874993205
Epoch:   900  |  train loss: 0.1857040286
Epoch:  1000  |  train loss: 0.1841856569
Processing class: 84
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.1774432629
Epoch:   200  |  train loss: 0.1765666008
Epoch:   300  |  train loss: 0.1798947006
Epoch:   400  |  train loss: 0.1800408065
Epoch:   500  |  train loss: 0.1826651961
Epoch:   600  |  train loss: 0.1798395932
Epoch:   700  |  train loss: 0.1829188704
Epoch:   800  |  train loss: 0.1856127679
Epoch:   900  |  train loss: 0.1819162458
Epoch:  1000  |  train loss: 0.1812575787
Processing class: 85
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.1759348392
Epoch:   200  |  train loss: 0.1749488503
Epoch:   300  |  train loss: 0.1781678319
Epoch:   400  |  train loss: 0.1766900718
Epoch:   500  |  train loss: 0.1788784266
Epoch:   600  |  train loss: 0.1815209687
Epoch:   700  |  train loss: 0.1854935795
Epoch:   800  |  train loss: 0.1821860164
Epoch:   900  |  train loss: 0.1826352805
Epoch:  1000  |  train loss: 0.1857961148
Processing class: 86
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.1799580038
Epoch:   200  |  train loss: 0.1768806338
Epoch:   300  |  train loss: 0.1811755121
Epoch:   400  |  train loss: 0.1796360552
Epoch:   500  |  train loss: 0.1796780080
Epoch:   600  |  train loss: 0.1813380778
Epoch:   700  |  train loss: 0.1815543294
Epoch:   800  |  train loss: 0.1811395615
Epoch:   900  |  train loss: 0.1811104268
Epoch:  1000  |  train loss: 0.1853835046
Processing class: 87
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.1714858979
Epoch:   200  |  train loss: 0.1742561638
Epoch:   300  |  train loss: 0.1744009823
Epoch:   400  |  train loss: 0.1764370412
Epoch:   500  |  train loss: 0.1760287225
Epoch:   600  |  train loss: 0.1779928327
Epoch:   700  |  train loss: 0.1790499598
Epoch:   800  |  train loss: 0.1790671110
Epoch:   900  |  train loss: 0.1770362169
Epoch:  1000  |  train loss: 0.1820246041
Processing class: 88
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.1741952509
Epoch:   200  |  train loss: 0.1727798671
Epoch:   300  |  train loss: 0.1726634026
Epoch:   400  |  train loss: 0.1735506445
Epoch:   500  |  train loss: 0.1732024491
Epoch:   600  |  train loss: 0.1718479723
Epoch:   700  |  train loss: 0.1746052086
Epoch:   800  |  train loss: 0.1747826695
Epoch:   900  |  train loss: 0.1756109446
Epoch:  1000  |  train loss: 0.1737870425
Processing class: 89
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.1759847581
Epoch:   200  |  train loss: 0.1789211243
Epoch:   300  |  train loss: 0.1804246187
Epoch:   400  |  train loss: 0.1812803388
Epoch:   500  |  train loss: 0.1843309313
Epoch:   600  |  train loss: 0.1846088797
Epoch:   700  |  train loss: 0.1817481697
Epoch:   800  |  train loss: 0.1877417445
Epoch:   900  |  train loss: 0.1862104833
Epoch:  1000  |  train loss: 0.1879213691
2024-03-05 07:41:34,734 [trainer.py] => CNN: {'total': 54.08, '00-09': 67.9, '10-19': 63.6, '20-29': 73.2, '30-39': 68.8, '40-49': 57.2, '50-59': 32.6, '60-69': 40.9, '70-79': 38.1, '80-89': 44.4, 'old': 55.29, 'new': 44.4}
2024-03-05 07:41:40,749 [trainer.py] => No NME accuracy
2024-03-05 07:41:40,749 [trainer.py] => FeCAM: {'total': 57.66, '00-09': 76.1, '10-19': 65.8, '20-29': 75.3, '30-39': 67.9, '40-49': 68.5, '50-59': 33.0, '60-69': 45.0, '70-79': 39.0, '80-89': 48.3, 'old': 58.82, 'new': 48.3}
2024-03-05 07:41:40,749 [trainer.py] => CNN top1 curve: [83.44, 71.75, 64.66, 59.18, 54.08]
2024-03-05 07:41:40,749 [trainer.py] => CNN top5 curve: [96.5, 89.6, 86.54, 84.09, 81.74]
2024-03-05 07:41:40,749 [trainer.py] => FeCAM top1 curve: [82.9, 72.78, 67.27, 61.72, 57.66]
2024-03-05 07:41:40,749 [trainer.py] => FeCAM top5 curve: [95.84, 91.18, 87.21, 84.09, 81.63]

2024-03-05 07:41:40,763 [fecam.py] => Learning on 90-100
Processing class: 90
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.1766982615
Epoch:   200  |  train loss: 0.1771483988
Epoch:   300  |  train loss: 0.1737314284
Epoch:   400  |  train loss: 0.1726388276
Epoch:   500  |  train loss: 0.1714090586
Epoch:   600  |  train loss: 0.1728448987
Epoch:   700  |  train loss: 0.1719786882
Epoch:   800  |  train loss: 0.1728580087
Epoch:   900  |  train loss: 0.1745293170
Epoch:  1000  |  train loss: 0.1777604908
Processing class: 91
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.1651079565
Epoch:   200  |  train loss: 0.1643700868
Epoch:   300  |  train loss: 0.1674539089
Epoch:   400  |  train loss: 0.1702989876
Epoch:   500  |  train loss: 0.1728085250
Epoch:   600  |  train loss: 0.1742540240
Epoch:   700  |  train loss: 0.1749874145
Epoch:   800  |  train loss: 0.1765712440
Epoch:   900  |  train loss: 0.1786095917
Epoch:  1000  |  train loss: 0.1791849434
Processing class: 92
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.1785576791
Epoch:   200  |  train loss: 0.1747611105
Epoch:   300  |  train loss: 0.1709006876
Epoch:   400  |  train loss: 0.1731145144
Epoch:   500  |  train loss: 0.1747000933
Epoch:   600  |  train loss: 0.1775378138
Epoch:   700  |  train loss: 0.1774835765
Epoch:   800  |  train loss: 0.1732490301
Epoch:   900  |  train loss: 0.1743072331
Epoch:  1000  |  train loss: 0.1749553055
Processing class: 93
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.1749380261
Epoch:   200  |  train loss: 0.1768577546
Epoch:   300  |  train loss: 0.1837459862
Epoch:   400  |  train loss: 0.1848896295
Epoch:   500  |  train loss: 0.1843535990
Epoch:   600  |  train loss: 0.1820476383
Epoch:   700  |  train loss: 0.1878391236
Epoch:   800  |  train loss: 0.1873134851
Epoch:   900  |  train loss: 0.1879049540
Epoch:  1000  |  train loss: 0.1871454895
Processing class: 94
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.1627036899
Epoch:   200  |  train loss: 0.1619145244
Epoch:   300  |  train loss: 0.1575398654
Epoch:   400  |  train loss: 0.1591342449
Epoch:   500  |  train loss: 0.1536714792
Epoch:   600  |  train loss: 0.1563412756
Epoch:   700  |  train loss: 0.1589880168
Epoch:   800  |  train loss: 0.1525129586
Epoch:   900  |  train loss: 0.1567102104
Epoch:  1000  |  train loss: 0.1553662479
Processing class: 95
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.1746973217
Epoch:   200  |  train loss: 0.1781551540
Epoch:   300  |  train loss: 0.1762170345
Epoch:   400  |  train loss: 0.1751218587
Epoch:   500  |  train loss: 0.1796653807
Epoch:   600  |  train loss: 0.1791732788
Epoch:   700  |  train loss: 0.1808922946
Epoch:   800  |  train loss: 0.1794464797
Epoch:   900  |  train loss: 0.1823839039
Epoch:  1000  |  train loss: 0.1851057053
Processing class: 96
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.1789573163
Epoch:   200  |  train loss: 0.1743045270
Epoch:   300  |  train loss: 0.1775042593
Epoch:   400  |  train loss: 0.1827433735
Epoch:   500  |  train loss: 0.1810416013
Epoch:   600  |  train loss: 0.1805780232
Epoch:   700  |  train loss: 0.1812846094
Epoch:   800  |  train loss: 0.1792411506
Epoch:   900  |  train loss: 0.1844164848
Epoch:  1000  |  train loss: 0.1842771262
Processing class: 97
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.1773435652
Epoch:   200  |  train loss: 0.1774242282
Epoch:   300  |  train loss: 0.1782229453
Epoch:   400  |  train loss: 0.1794134855
Epoch:   500  |  train loss: 0.1827379137
Epoch:   600  |  train loss: 0.1848105073
Epoch:   700  |  train loss: 0.1869642019
Epoch:   800  |  train loss: 0.1875421584
Epoch:   900  |  train loss: 0.1887637228
Epoch:  1000  |  train loss: 0.1883964956
Processing class: 98
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.1715662330
Epoch:   200  |  train loss: 0.1708162695
Epoch:   300  |  train loss: 0.1714677721
Epoch:   400  |  train loss: 0.1729922265
Epoch:   500  |  train loss: 0.1723512918
Epoch:   600  |  train loss: 0.1742816716
Epoch:   700  |  train loss: 0.1710051715
Epoch:   800  |  train loss: 0.1738567293
Epoch:   900  |  train loss: 0.1739946663
Epoch:  1000  |  train loss: 0.1725023776
Processing class: 99
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.1802980423
Epoch:   200  |  train loss: 0.1770423800
Epoch:   300  |  train loss: 0.1822741449
Epoch:   400  |  train loss: 0.1861972749
Epoch:   500  |  train loss: 0.1853907615
Epoch:   600  |  train loss: 0.1847577989
Epoch:   700  |  train loss: 0.1838675767
Epoch:   800  |  train loss: 0.1871999979
Epoch:   900  |  train loss: 0.1856537879
Epoch:  1000  |  train loss: 0.1863501370
2024-03-05 07:52:07,393 [trainer.py] => CNN: {'total': 50.36, '00-09': 57.7, '10-19': 63.6, '20-29': 71.4, '30-39': 68.5, '40-49': 56.8, '50-59': 29.6, '60-69': 38.9, '70-79': 36.6, '80-89': 42.7, '90-99': 37.8, 'old': 51.76, 'new': 37.8}
2024-03-05 07:52:08,521 [trainer.py] => No NME accuracy
2024-03-05 07:52:08,521 [trainer.py] => FeCAM: {'total': 54.74, '00-09': 73.3, '10-19': 65.1, '20-29': 74.1, '30-39': 67.2, '40-49': 67.1, '50-59': 30.8, '60-69': 43.3, '70-79': 37.2, '80-89': 46.9, '90-99': 42.4, 'old': 56.11, 'new': 42.4}
2024-03-05 07:52:08,523 [trainer.py] => CNN top1 curve: [83.44, 71.75, 64.66, 59.18, 54.08, 50.36]
2024-03-05 07:52:08,523 [trainer.py] => CNN top5 curve: [96.5, 89.6, 86.54, 84.09, 81.74, 79.63]
2024-03-05 07:52:08,524 [trainer.py] => FeCAM top1 curve: [82.9, 72.78, 67.27, 61.72, 57.66, 54.74]
2024-03-05 07:52:08,524 [trainer.py] => FeCAM top5 curve: [95.84, 91.18, 87.21, 84.09, 81.63, 79.21]

=========================================
2024-03-05 07:52:18,259 [trainer.py] => config: ./exps/FeCAM_cifar100.json
2024-03-05 07:52:18,259 [trainer.py] => prefix: train
2024-03-05 07:52:18,259 [trainer.py] => dataset: cifar100
2024-03-05 07:52:18,259 [trainer.py] => memory_size: 0
2024-03-05 07:52:18,259 [trainer.py] => shuffle: True
2024-03-05 07:52:18,259 [trainer.py] => init_cls: 50
2024-03-05 07:52:18,259 [trainer.py] => increment: 10
2024-03-05 07:52:18,259 [trainer.py] => model_name: fecam
2024-03-05 07:52:18,259 [trainer.py] => convnet_type: resnet18
2024-03-05 07:52:18,259 [trainer.py] => device: [device(type='cuda', index=0)]
2024-03-05 07:52:18,259 [trainer.py] => seed: 1993
2024-03-05 07:52:18,259 [trainer.py] => init_epochs: 200
2024-03-05 07:52:18,259 [trainer.py] => init_lr: 0.1
2024-03-05 07:52:18,259 [trainer.py] => init_weight_decay: 0.0005
2024-03-05 07:52:18,259 [trainer.py] => batch_size: 128
2024-03-05 07:52:18,259 [trainer.py] => num_workers: 8
2024-03-05 07:52:18,259 [trainer.py] => T: 5
2024-03-05 07:52:18,259 [trainer.py] => beta: 0.5
2024-03-05 07:52:18,259 [trainer.py] => alpha1: 1
2024-03-05 07:52:18,259 [trainer.py] => alpha2: 1
2024-03-05 07:52:18,259 [trainer.py] => ncm: False
2024-03-05 07:52:18,259 [trainer.py] => tukey: False
2024-03-05 07:52:18,259 [trainer.py] => diagonal: False
2024-03-05 07:52:18,259 [trainer.py] => per_class: True
2024-03-05 07:52:18,259 [trainer.py] => full_cov: True
2024-03-05 07:52:18,259 [trainer.py] => shrink: True
2024-03-05 07:52:18,259 [trainer.py] => norm_cov: False
2024-03-05 07:52:18,259 [trainer.py] => vecnorm: False
2024-03-05 07:52:18,259 [trainer.py] => ae_type: wae
2024-03-05 07:52:18,259 [trainer.py] => epochs: 1000
2024-03-05 07:52:18,259 [trainer.py] => ae_latent_dim: 32
2024-03-05 07:52:18,259 [trainer.py] => wae_sigma: 40
2024-03-05 07:52:18,259 [trainer.py] => wae_C: 0.5
Files already downloaded and verified
Files already downloaded and verified
2024-03-05 07:52:19,917 [data_manager.py] => [68, 56, 78, 8, 23, 84, 90, 65, 74, 76, 40, 89, 3, 92, 55, 9, 26, 80, 43, 38, 58, 70, 77, 1, 85, 19, 17, 50, 28, 53, 13, 81, 45, 82, 6, 59, 83, 16, 15, 44, 91, 41, 72, 60, 79, 52, 20, 10, 31, 54, 37, 95, 14, 71, 96, 98, 97, 2, 64, 66, 42, 22, 35, 86, 24, 34, 87, 21, 99, 0, 88, 27, 18, 94, 11, 12, 47, 25, 30, 46, 62, 69, 36, 61, 7, 63, 75, 5, 32, 4, 51, 48, 73, 93, 39, 67, 29, 49, 57, 33]
2024-03-05 07:52:20,206 [fecam.py] => Learning on 0-50
/home/z1165703/miniconda3/lib/python3.11/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
/home/z1165703/miniconda3/lib/python3.11/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
Processing class: 0
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.1484621465
Epoch:   200  |  train loss: 0.1516851753
Epoch:   300  |  train loss: 0.1548950940
Epoch:   400  |  train loss: 0.1564309627
Epoch:   500  |  train loss: 0.1551346749
Epoch:   600  |  train loss: 0.1551185071
Epoch:   700  |  train loss: 0.1565553308
Epoch:   800  |  train loss: 0.1570715070
Epoch:   900  |  train loss: 0.1557671815
Epoch:  1000  |  train loss: 0.1557002276
Processing class: 1
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.1452415168
Epoch:   200  |  train loss: 0.1440688908
Epoch:   300  |  train loss: 0.1514533430
Epoch:   400  |  train loss: 0.1531619996
Epoch:   500  |  train loss: 0.1500679612
Epoch:   600  |  train loss: 0.1549893856
Epoch:   700  |  train loss: 0.1531448662
Epoch:   800  |  train loss: 0.1559250504
Epoch:   900  |  train loss: 0.1598374784
Epoch:  1000  |  train loss: 0.1564754128
Processing class: 2
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.1434324056
Epoch:   200  |  train loss: 0.1436315656
Epoch:   300  |  train loss: 0.1488278985
Epoch:   400  |  train loss: 0.1486172080
Epoch:   500  |  train loss: 0.1426294237
Epoch:   600  |  train loss: 0.1483772665
Epoch:   700  |  train loss: 0.1480286062
Epoch:   800  |  train loss: 0.1524376333
Epoch:   900  |  train loss: 0.1504859447
Epoch:  1000  |  train loss: 0.1494513035
Processing class: 3
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.1564090908
Epoch:   200  |  train loss: 0.1607461780
Epoch:   300  |  train loss: 0.1631411761
Epoch:   400  |  train loss: 0.1646377265
Epoch:   500  |  train loss: 0.1632446468
Epoch:   600  |  train loss: 0.1679312885
Epoch:   700  |  train loss: 0.1711480051
Epoch:   800  |  train loss: 0.1683834583
Epoch:   900  |  train loss: 0.1710218310
Epoch:  1000  |  train loss: 0.1721651614
Processing class: 4
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.1438660830
Epoch:   200  |  train loss: 0.1422124833
Epoch:   300  |  train loss: 0.1484475464
Epoch:   400  |  train loss: 0.1493318260
Epoch:   500  |  train loss: 0.1519420058
Epoch:   600  |  train loss: 0.1516174853
Epoch:   700  |  train loss: 0.1540992409
Epoch:   800  |  train loss: 0.1549349844
Epoch:   900  |  train loss: 0.1563109636
Epoch:  1000  |  train loss: 0.1598679751
Processing class: 5
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.1439947546
Epoch:   200  |  train loss: 0.1437149078
Epoch:   300  |  train loss: 0.1461092681
Epoch:   400  |  train loss: 0.1468173265
Epoch:   500  |  train loss: 0.1465995282
Epoch:   600  |  train loss: 0.1528743148
Epoch:   700  |  train loss: 0.1544447243
Epoch:   800  |  train loss: 0.1520805568
Epoch:   900  |  train loss: 0.1518405586
Epoch:  1000  |  train loss: 0.1547938824
Processing class: 6
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.1458758533
Epoch:   200  |  train loss: 0.1480176002
Epoch:   300  |  train loss: 0.1543505818
Epoch:   400  |  train loss: 0.1541752756
Epoch:   500  |  train loss: 0.1591129154
Epoch:   600  |  train loss: 0.1575243175
Epoch:   700  |  train loss: 0.1562549859
Epoch:   800  |  train loss: 0.1578002304
Epoch:   900  |  train loss: 0.1564207166
Epoch:  1000  |  train loss: 0.1583922207
Processing class: 7
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.1413300514
Epoch:   200  |  train loss: 0.1430080652
Epoch:   300  |  train loss: 0.1450516313
Epoch:   400  |  train loss: 0.1468915343
Epoch:   500  |  train loss: 0.1497921318
Epoch:   600  |  train loss: 0.1485020161
Epoch:   700  |  train loss: 0.1511492223
Epoch:   800  |  train loss: 0.1554752737
Epoch:   900  |  train loss: 0.1523227274
Epoch:  1000  |  train loss: 0.1558868319
Processing class: 8
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.1432571441
Epoch:   200  |  train loss: 0.1465606570
Epoch:   300  |  train loss: 0.1460301131
Epoch:   400  |  train loss: 0.1516801089
Epoch:   500  |  train loss: 0.1496309042
Epoch:   600  |  train loss: 0.1515135914
Epoch:   700  |  train loss: 0.1554203004
Epoch:   800  |  train loss: 0.1526114881
Epoch:   900  |  train loss: 0.1549824566
Epoch:  1000  |  train loss: 0.1533168197
Processing class: 9
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.1405597836
Epoch:   200  |  train loss: 0.1435287058
Epoch:   300  |  train loss: 0.1479042470
Epoch:   400  |  train loss: 0.1444297284
Epoch:   500  |  train loss: 0.1487612247
Epoch:   600  |  train loss: 0.1463853985
Epoch:   700  |  train loss: 0.1479799896
Epoch:   800  |  train loss: 0.1492756128
Epoch:   900  |  train loss: 0.1495188028
Epoch:  1000  |  train loss: 0.1471748739
Processing class: 10
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.1372346461
Epoch:   200  |  train loss: 0.1375784665
Epoch:   300  |  train loss: 0.1394919068
Epoch:   400  |  train loss: 0.1444524676
Epoch:   500  |  train loss: 0.1393671781
Epoch:   600  |  train loss: 0.1419164598
Epoch:   700  |  train loss: 0.1439247608
Epoch:   800  |  train loss: 0.1430476934
Epoch:   900  |  train loss: 0.1445478827
Epoch:  1000  |  train loss: 0.1444441140
Processing class: 11
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.1443110108
Epoch:   200  |  train loss: 0.1468231022
Epoch:   300  |  train loss: 0.1452982455
Epoch:   400  |  train loss: 0.1478901684
Epoch:   500  |  train loss: 0.1486886948
Epoch:   600  |  train loss: 0.1520657152
Epoch:   700  |  train loss: 0.1567877650
Epoch:   800  |  train loss: 0.1561074615
Epoch:   900  |  train loss: 0.1566616029
Epoch:  1000  |  train loss: 0.1592401773
Processing class: 12
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.1385988444
Epoch:   200  |  train loss: 0.1457614779
Epoch:   300  |  train loss: 0.1467622161
Epoch:   400  |  train loss: 0.1510098726
Epoch:   500  |  train loss: 0.1518545091
Epoch:   600  |  train loss: 0.1502902180
Epoch:   700  |  train loss: 0.1519743532
Epoch:   800  |  train loss: 0.1533152223
Epoch:   900  |  train loss: 0.1557942271
Epoch:  1000  |  train loss: 0.1543751091
Processing class: 13
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.1420137912
Epoch:   200  |  train loss: 0.1480350792
Epoch:   300  |  train loss: 0.1526180655
Epoch:   400  |  train loss: 0.1543625593
Epoch:   500  |  train loss: 0.1584103733
Epoch:   600  |  train loss: 0.1557608902
Epoch:   700  |  train loss: 0.1582139909
Epoch:   800  |  train loss: 0.1604024231
Epoch:   900  |  train loss: 0.1617574722
Epoch:  1000  |  train loss: 0.1633689195
Processing class: 14
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.1427629590
Epoch:   200  |  train loss: 0.1448345333
Epoch:   300  |  train loss: 0.1466318816
Epoch:   400  |  train loss: 0.1507863253
Epoch:   500  |  train loss: 0.1503732443
Epoch:   600  |  train loss: 0.1507292688
Epoch:   700  |  train loss: 0.1528688222
Epoch:   800  |  train loss: 0.1577339470
Epoch:   900  |  train loss: 0.1562658429
Epoch:  1000  |  train loss: 0.1533075213
Processing class: 15
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.1439764529
Epoch:   200  |  train loss: 0.1466081500
Epoch:   300  |  train loss: 0.1522090852
Epoch:   400  |  train loss: 0.1546938896
Epoch:   500  |  train loss: 0.1573407531
Epoch:   600  |  train loss: 0.1552724987
Epoch:   700  |  train loss: 0.1594039172
Epoch:   800  |  train loss: 0.1559689760
Epoch:   900  |  train loss: 0.1598518342
Epoch:  1000  |  train loss: 0.1582881868
Processing class: 16
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.1451309413
Epoch:   200  |  train loss: 0.1549942851
Epoch:   300  |  train loss: 0.1593127549
Epoch:   400  |  train loss: 0.1575216204
Epoch:   500  |  train loss: 0.1610918880
Epoch:   600  |  train loss: 0.1584095001
Epoch:   700  |  train loss: 0.1577438891
Epoch:   800  |  train loss: 0.1627887517
Epoch:   900  |  train loss: 0.1586061418
Epoch:  1000  |  train loss: 0.1629364938
Processing class: 17
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.1381734818
Epoch:   200  |  train loss: 0.1407520205
Epoch:   300  |  train loss: 0.1447931588
Epoch:   400  |  train loss: 0.1458185166
Epoch:   500  |  train loss: 0.1421324730
Epoch:   600  |  train loss: 0.1453359574
Epoch:   700  |  train loss: 0.1447838932
Epoch:   800  |  train loss: 0.1453605205
Epoch:   900  |  train loss: 0.1467348456
Epoch:  1000  |  train loss: 0.1463567972
Processing class: 18
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.1421302557
Epoch:   200  |  train loss: 0.1433326483
Epoch:   300  |  train loss: 0.1478857189
Epoch:   400  |  train loss: 0.1478991181
Epoch:   500  |  train loss: 0.1528503895
Epoch:   600  |  train loss: 0.1518195271
Epoch:   700  |  train loss: 0.1582683742
Epoch:   800  |  train loss: 0.1583456099
Epoch:   900  |  train loss: 0.1598706663
Epoch:  1000  |  train loss: 0.1636614889
Processing class: 19
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.1445502788
Epoch:   200  |  train loss: 0.1526380241
Epoch:   300  |  train loss: 0.1531465173
Epoch:   400  |  train loss: 0.1542675048
Epoch:   500  |  train loss: 0.1523274869
Epoch:   600  |  train loss: 0.1530826300
Epoch:   700  |  train loss: 0.1551388562
Epoch:   800  |  train loss: 0.1542529613
Epoch:   900  |  train loss: 0.1613717824
Epoch:  1000  |  train loss: 0.1629359454
Processing class: 20
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.1444673717
Epoch:   200  |  train loss: 0.1475612730
Epoch:   300  |  train loss: 0.1503367275
Epoch:   400  |  train loss: 0.1528458744
Epoch:   500  |  train loss: 0.1566478103
Epoch:   600  |  train loss: 0.1546930909
Epoch:   700  |  train loss: 0.1557886720
Epoch:   800  |  train loss: 0.1559321851
Epoch:   900  |  train loss: 0.1577255100
Epoch:  1000  |  train loss: 0.1604737908
Processing class: 21
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.1449117482
Epoch:   200  |  train loss: 0.1485006750
Epoch:   300  |  train loss: 0.1515451759
Epoch:   400  |  train loss: 0.1535060406
Epoch:   500  |  train loss: 0.1554261178
Epoch:   600  |  train loss: 0.1579798222
Epoch:   700  |  train loss: 0.1574445099
Epoch:   800  |  train loss: 0.1568363756
Epoch:   900  |  train loss: 0.1603599072
Epoch:  1000  |  train loss: 0.1598004520
Processing class: 22
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.1419580579
Epoch:   200  |  train loss: 0.1434022933
Epoch:   300  |  train loss: 0.1467605442
Epoch:   400  |  train loss: 0.1512813985
Epoch:   500  |  train loss: 0.1527499020
Epoch:   600  |  train loss: 0.1512264311
Epoch:   700  |  train loss: 0.1532656401
Epoch:   800  |  train loss: 0.1525398403
Epoch:   900  |  train loss: 0.1511749834
Epoch:  1000  |  train loss: 0.1565324903
Processing class: 23
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.1478657454
Epoch:   200  |  train loss: 0.1513580829
Epoch:   300  |  train loss: 0.1549922734
Epoch:   400  |  train loss: 0.1518408269
Epoch:   500  |  train loss: 0.1521022409
Epoch:   600  |  train loss: 0.1506259829
Epoch:   700  |  train loss: 0.1532806605
Epoch:   800  |  train loss: 0.1518261015
Epoch:   900  |  train loss: 0.1543800116
Epoch:  1000  |  train loss: 0.1526475430
Processing class: 24
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.1451942235
Epoch:   200  |  train loss: 0.1454213202
Epoch:   300  |  train loss: 0.1422487617
Epoch:   400  |  train loss: 0.1462405950
Epoch:   500  |  train loss: 0.1449436188
Epoch:   600  |  train loss: 0.1473366171
Epoch:   700  |  train loss: 0.1430991024
Epoch:   800  |  train loss: 0.1448347747
Epoch:   900  |  train loss: 0.1477904886
Epoch:  1000  |  train loss: 0.1470282406
Processing class: 25
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.1425173491
Epoch:   200  |  train loss: 0.1455924034
Epoch:   300  |  train loss: 0.1498157650
Epoch:   400  |  train loss: 0.1479392350
Epoch:   500  |  train loss: 0.1528433502
Epoch:   600  |  train loss: 0.1510739118
Epoch:   700  |  train loss: 0.1552287340
Epoch:   800  |  train loss: 0.1539420336
Epoch:   900  |  train loss: 0.1532014608
Epoch:  1000  |  train loss: 0.1546678483
Processing class: 26
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.1497719169
Epoch:   200  |  train loss: 0.1543810993
Epoch:   300  |  train loss: 0.1515563101
Epoch:   400  |  train loss: 0.1554470181
Epoch:   500  |  train loss: 0.1592889518
Epoch:   600  |  train loss: 0.1549456209
Epoch:   700  |  train loss: 0.1571134776
Epoch:   800  |  train loss: 0.1588420331
Epoch:   900  |  train loss: 0.1583904088
Epoch:  1000  |  train loss: 0.1615336239
Processing class: 27
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.1427208304
Epoch:   200  |  train loss: 0.1421140850
Epoch:   300  |  train loss: 0.1437811911
Epoch:   400  |  train loss: 0.1454945147
Epoch:   500  |  train loss: 0.1482970446
Epoch:   600  |  train loss: 0.1485513419
Epoch:   700  |  train loss: 0.1501464993
Epoch:   800  |  train loss: 0.1502011210
Epoch:   900  |  train loss: 0.1516844422
Epoch:  1000  |  train loss: 0.1486363858
Processing class: 28
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.1473417461
Epoch:   200  |  train loss: 0.1512018621
Epoch:   300  |  train loss: 0.1559634864
Epoch:   400  |  train loss: 0.1584331453
Epoch:   500  |  train loss: 0.1598881692
Epoch:   600  |  train loss: 0.1603816092
Epoch:   700  |  train loss: 0.1612803012
Epoch:   800  |  train loss: 0.1626281202
Epoch:   900  |  train loss: 0.1634749204
Epoch:  1000  |  train loss: 0.1606744379
Processing class: 29
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.1436896026
Epoch:   200  |  train loss: 0.1471145004
Epoch:   300  |  train loss: 0.1474744499
Epoch:   400  |  train loss: 0.1466058552
Epoch:   500  |  train loss: 0.1500094384
Epoch:   600  |  train loss: 0.1519068748
Epoch:   700  |  train loss: 0.1522368163
Epoch:   800  |  train loss: 0.1504536390
Epoch:   900  |  train loss: 0.1513300836
Epoch:  1000  |  train loss: 0.1507713407
Processing class: 30
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.1494715184
Epoch:   200  |  train loss: 0.1537384003
Epoch:   300  |  train loss: 0.1603696048
Epoch:   400  |  train loss: 0.1656389534
Epoch:   500  |  train loss: 0.1633935332
Epoch:   600  |  train loss: 0.1635431886
Epoch:   700  |  train loss: 0.1631477058
Epoch:   800  |  train loss: 0.1666599423
Epoch:   900  |  train loss: 0.1689444959
Epoch:  1000  |  train loss: 0.1667498976
Processing class: 31
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.1475942522
Epoch:   200  |  train loss: 0.1527795941
Epoch:   300  |  train loss: 0.1539943933
Epoch:   400  |  train loss: 0.1585684955
Epoch:   500  |  train loss: 0.1603635013
Epoch:   600  |  train loss: 0.1642362833
Epoch:   700  |  train loss: 0.1635879189
Epoch:   800  |  train loss: 0.1639292955
Epoch:   900  |  train loss: 0.1669218302
Epoch:  1000  |  train loss: 0.1640191138
Processing class: 32
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.1431134582
Epoch:   200  |  train loss: 0.1470005691
Epoch:   300  |  train loss: 0.1468192607
Epoch:   400  |  train loss: 0.1442623973
Epoch:   500  |  train loss: 0.1507615000
Epoch:   600  |  train loss: 0.1497949272
Epoch:   700  |  train loss: 0.1547514707
Epoch:   800  |  train loss: 0.1568256617
Epoch:   900  |  train loss: 0.1560310155
Epoch:  1000  |  train loss: 0.1550449014
Processing class: 33
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.1381170869
Epoch:   200  |  train loss: 0.1465108395
Epoch:   300  |  train loss: 0.1414108217
Epoch:   400  |  train loss: 0.1429076135
Epoch:   500  |  train loss: 0.1440555871
Epoch:   600  |  train loss: 0.1492286831
Epoch:   700  |  train loss: 0.1469921380
Epoch:   800  |  train loss: 0.1483925641
Epoch:   900  |  train loss: 0.1482323647
Epoch:  1000  |  train loss: 0.1471088111
Processing class: 34
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.1446175188
Epoch:   200  |  train loss: 0.1440313816
Epoch:   300  |  train loss: 0.1476309121
Epoch:   400  |  train loss: 0.1464093715
Epoch:   500  |  train loss: 0.1467030019
Epoch:   600  |  train loss: 0.1499256968
Epoch:   700  |  train loss: 0.1478324145
Epoch:   800  |  train loss: 0.1515761733
Epoch:   900  |  train loss: 0.1529116690
Epoch:  1000  |  train loss: 0.1537897140
Processing class: 35
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.1399558842
Epoch:   200  |  train loss: 0.1445682347
Epoch:   300  |  train loss: 0.1478215218
Epoch:   400  |  train loss: 0.1494712621
Epoch:   500  |  train loss: 0.1475727230
Epoch:   600  |  train loss: 0.1496031374
Epoch:   700  |  train loss: 0.1511459053
Epoch:   800  |  train loss: 0.1509759992
Epoch:   900  |  train loss: 0.1516497791
Epoch:  1000  |  train loss: 0.1504619241
Processing class: 36
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.1470712513
Epoch:   200  |  train loss: 0.1487819135
Epoch:   300  |  train loss: 0.1519438088
Epoch:   400  |  train loss: 0.1523319751
Epoch:   500  |  train loss: 0.1571394354
Epoch:   600  |  train loss: 0.1556855559
Epoch:   700  |  train loss: 0.1539516062
Epoch:   800  |  train loss: 0.1592292696
Epoch:   900  |  train loss: 0.1555472225
Epoch:  1000  |  train loss: 0.1587590873
Processing class: 37
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.1454845130
Epoch:   200  |  train loss: 0.1451978475
Epoch:   300  |  train loss: 0.1454009116
Epoch:   400  |  train loss: 0.1478815764
Epoch:   500  |  train loss: 0.1501009166
Epoch:   600  |  train loss: 0.1482122213
Epoch:   700  |  train loss: 0.1490436137
Epoch:   800  |  train loss: 0.1503655463
Epoch:   900  |  train loss: 0.1487744868
Epoch:  1000  |  train loss: 0.1502071589
Processing class: 38
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.1446517795
Epoch:   200  |  train loss: 0.1464457422
Epoch:   300  |  train loss: 0.1503774315
Epoch:   400  |  train loss: 0.1516515672
Epoch:   500  |  train loss: 0.1534029216
Epoch:   600  |  train loss: 0.1529262185
Epoch:   700  |  train loss: 0.1538589060
Epoch:   800  |  train loss: 0.1571194232
Epoch:   900  |  train loss: 0.1536692113
Epoch:  1000  |  train loss: 0.1564798772
Processing class: 39
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.1452741951
Epoch:   200  |  train loss: 0.1445372403
Epoch:   300  |  train loss: 0.1474396020
Epoch:   400  |  train loss: 0.1497607112
Epoch:   500  |  train loss: 0.1490778804
Epoch:   600  |  train loss: 0.1504763424
Epoch:   700  |  train loss: 0.1523350775
Epoch:   800  |  train loss: 0.1547910750
Epoch:   900  |  train loss: 0.1563441098
Epoch:  1000  |  train loss: 0.1574773490
Processing class: 40
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.1460947186
Epoch:   200  |  train loss: 0.1447820097
Epoch:   300  |  train loss: 0.1513597727
Epoch:   400  |  train loss: 0.1529451817
Epoch:   500  |  train loss: 0.1550545156
Epoch:   600  |  train loss: 0.1539778620
Epoch:   700  |  train loss: 0.1556963921
Epoch:   800  |  train loss: 0.1557981431
Epoch:   900  |  train loss: 0.1543680936
Epoch:  1000  |  train loss: 0.1601747990
Processing class: 41
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.1454381913
Epoch:   200  |  train loss: 0.1459957570
Epoch:   300  |  train loss: 0.1498666495
Epoch:   400  |  train loss: 0.1494929880
Epoch:   500  |  train loss: 0.1500726223
Epoch:   600  |  train loss: 0.1505592644
Epoch:   700  |  train loss: 0.1534187227
Epoch:   800  |  train loss: 0.1520686239
Epoch:   900  |  train loss: 0.1516470522
Epoch:  1000  |  train loss: 0.1520314395
Processing class: 42
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.1412240684
Epoch:   200  |  train loss: 0.1459686726
Epoch:   300  |  train loss: 0.1493711293
Epoch:   400  |  train loss: 0.1511099637
Epoch:   500  |  train loss: 0.1531999588
Epoch:   600  |  train loss: 0.1547513962
Epoch:   700  |  train loss: 0.1569329947
Epoch:   800  |  train loss: 0.1586716473
Epoch:   900  |  train loss: 0.1574015766
Epoch:  1000  |  train loss: 0.1574063838
Processing class: 43
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.1424068630
Epoch:   200  |  train loss: 0.1426887661
Epoch:   300  |  train loss: 0.1505968571
Epoch:   400  |  train loss: 0.1567924440
Epoch:   500  |  train loss: 0.1585487902
Epoch:   600  |  train loss: 0.1571598828
Epoch:   700  |  train loss: 0.1597463727
Epoch:   800  |  train loss: 0.1587554544
Epoch:   900  |  train loss: 0.1557154864
Epoch:  1000  |  train loss: 0.1600573391
Processing class: 44
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.1407066226
Epoch:   200  |  train loss: 0.1462420911
Epoch:   300  |  train loss: 0.1449509770
Epoch:   400  |  train loss: 0.1465792567
Epoch:   500  |  train loss: 0.1487889647
Epoch:   600  |  train loss: 0.1476585478
Epoch:   700  |  train loss: 0.1509953916
Epoch:   800  |  train loss: 0.1503170639
Epoch:   900  |  train loss: 0.1493067771
Epoch:  1000  |  train loss: 0.1529593170
Processing class: 45
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.1460534394
Epoch:   200  |  train loss: 0.1526317805
Epoch:   300  |  train loss: 0.1533027917
Epoch:   400  |  train loss: 0.1568563461
Epoch:   500  |  train loss: 0.1563373893
Epoch:   600  |  train loss: 0.1615576357
Epoch:   700  |  train loss: 0.1629973322
Epoch:   800  |  train loss: 0.1618794501
Epoch:   900  |  train loss: 0.1624816328
Epoch:  1000  |  train loss: 0.1652723521
Processing class: 46
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.1484178483
Epoch:   200  |  train loss: 0.1495629668
Epoch:   300  |  train loss: 0.1527250737
Epoch:   400  |  train loss: 0.1543164253
Epoch:   500  |  train loss: 0.1549367338
Epoch:   600  |  train loss: 0.1550861716
Epoch:   700  |  train loss: 0.1558150917
Epoch:   800  |  train loss: 0.1600226998
Epoch:   900  |  train loss: 0.1579984516
Epoch:  1000  |  train loss: 0.1598621845
Processing class: 47
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.1408669233
Epoch:   200  |  train loss: 0.1450125456
Epoch:   300  |  train loss: 0.1443281740
Epoch:   400  |  train loss: 0.1485627145
Epoch:   500  |  train loss: 0.1490482002
Epoch:   600  |  train loss: 0.1495008230
Epoch:   700  |  train loss: 0.1511850178
Epoch:   800  |  train loss: 0.1495715201
Epoch:   900  |  train loss: 0.1510293782
Epoch:  1000  |  train loss: 0.1502223223
Processing class: 48
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.1433711946
Epoch:   200  |  train loss: 0.1481364220
Epoch:   300  |  train loss: 0.1485770553
Epoch:   400  |  train loss: 0.1458891571
Epoch:   500  |  train loss: 0.1506165892
Epoch:   600  |  train loss: 0.1502113879
Epoch:   700  |  train loss: 0.1484199554
Epoch:   800  |  train loss: 0.1484343827
Epoch:   900  |  train loss: 0.1532465070
Epoch:  1000  |  train loss: 0.1547885180
Processing class: 49
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.1424454480
Epoch:   200  |  train loss: 0.1441577584
Epoch:   300  |  train loss: 0.1509298384
Epoch:   400  |  train loss: 0.1524052501
Epoch:   500  |  train loss: 0.1544537634
Epoch:   600  |  train loss: 0.1548388302
Epoch:   700  |  train loss: 0.1551723570
Epoch:   800  |  train loss: 0.1544803083
Epoch:   900  |  train loss: 0.1540734231
Epoch:  1000  |  train loss: 0.1549769998
/home/z1165703/FeCAM/models/base.py:87: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  vec = torch.tensor(vec).cuda()
2024-03-05 08:10:10,089 [trainer.py] => CNN: {'total': 83.44, '00-09': 87.7, '10-19': 79.6, '20-29': 84.9, '30-39': 81.0, '40-49': 84.0, 'old': 0, 'new': 83.44}
2024-03-05 08:10:10,091 [trainer.py] => No NME accuracy
2024-03-05 08:10:10,091 [trainer.py] => FeCAM: {'total': 82.9, '00-09': 86.2, '10-19': 79.7, '20-29': 85.0, '30-39': 80.6, '40-49': 83.0, 'old': 0, 'new': 82.9}
2024-03-05 08:10:10,091 [trainer.py] => CNN top1 curve: [83.44]
2024-03-05 08:10:10,091 [trainer.py] => CNN top5 curve: [96.5]
2024-03-05 08:10:10,091 [trainer.py] => FeCAM top1 curve: [82.9]
2024-03-05 08:10:10,091 [trainer.py] => FeCAM top5 curve: [95.84]

2024-03-05 08:10:10,101 [fecam.py] => Learning on 50-60
Processing class: 50
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.1385945320
Epoch:   200  |  train loss: 0.1404816866
Epoch:   300  |  train loss: 0.1418554753
Epoch:   400  |  train loss: 0.1455363601
Epoch:   500  |  train loss: 0.1482257485
Epoch:   600  |  train loss: 0.1467793524
Epoch:   700  |  train loss: 0.1508437961
Epoch:   800  |  train loss: 0.1529729217
Epoch:   900  |  train loss: 0.1553600073
Epoch:  1000  |  train loss: 0.1565661818
Processing class: 51
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.1399489760
Epoch:   200  |  train loss: 0.1374487370
Epoch:   300  |  train loss: 0.1432113707
Epoch:   400  |  train loss: 0.1448484659
Epoch:   500  |  train loss: 0.1464534402
Epoch:   600  |  train loss: 0.1491789043
Epoch:   700  |  train loss: 0.1468420625
Epoch:   800  |  train loss: 0.1480155975
Epoch:   900  |  train loss: 0.1524577707
Epoch:  1000  |  train loss: 0.1513743699
Processing class: 52
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.1402577668
Epoch:   200  |  train loss: 0.1393762112
Epoch:   300  |  train loss: 0.1421968311
Epoch:   400  |  train loss: 0.1442664683
Epoch:   500  |  train loss: 0.1442826927
Epoch:   600  |  train loss: 0.1428450882
Epoch:   700  |  train loss: 0.1463449121
Epoch:   800  |  train loss: 0.1469271421
Epoch:   900  |  train loss: 0.1481926292
Epoch:  1000  |  train loss: 0.1462166488
Processing class: 53
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.1384539664
Epoch:   200  |  train loss: 0.1407933950
Epoch:   300  |  train loss: 0.1381905526
Epoch:   400  |  train loss: 0.1401392490
Epoch:   500  |  train loss: 0.1421917140
Epoch:   600  |  train loss: 0.1452287406
Epoch:   700  |  train loss: 0.1487805277
Epoch:   800  |  train loss: 0.1432684958
Epoch:   900  |  train loss: 0.1463907361
Epoch:  1000  |  train loss: 0.1447083175
Processing class: 54
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.1301677436
Epoch:   200  |  train loss: 0.1319982141
Epoch:   300  |  train loss: 0.1360011816
Epoch:   400  |  train loss: 0.1377742022
Epoch:   500  |  train loss: 0.1398270220
Epoch:   600  |  train loss: 0.1439451218
Epoch:   700  |  train loss: 0.1454188824
Epoch:   800  |  train loss: 0.1487964302
Epoch:   900  |  train loss: 0.1511910021
Epoch:  1000  |  train loss: 0.1507199764
Processing class: 55
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.1382744104
Epoch:   200  |  train loss: 0.1390881807
Epoch:   300  |  train loss: 0.1378062755
Epoch:   400  |  train loss: 0.1417943299
Epoch:   500  |  train loss: 0.1470004261
Epoch:   600  |  train loss: 0.1479815274
Epoch:   700  |  train loss: 0.1499967188
Epoch:   800  |  train loss: 0.1539626211
Epoch:   900  |  train loss: 0.1502104878
Epoch:  1000  |  train loss: 0.1532000721
Processing class: 56
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.1372640967
Epoch:   200  |  train loss: 0.1399009764
Epoch:   300  |  train loss: 0.1383740276
Epoch:   400  |  train loss: 0.1435275167
Epoch:   500  |  train loss: 0.1461407274
Epoch:   600  |  train loss: 0.1481308997
Epoch:   700  |  train loss: 0.1506416231
Epoch:   800  |  train loss: 0.1492964953
Epoch:   900  |  train loss: 0.1528248191
Epoch:  1000  |  train loss: 0.1550366044
Processing class: 57
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.1366040260
Epoch:   200  |  train loss: 0.1399474114
Epoch:   300  |  train loss: 0.1391243786
Epoch:   400  |  train loss: 0.1396523476
Epoch:   500  |  train loss: 0.1425185621
Epoch:   600  |  train loss: 0.1398565918
Epoch:   700  |  train loss: 0.1446979612
Epoch:   800  |  train loss: 0.1450966120
Epoch:   900  |  train loss: 0.1479061455
Epoch:  1000  |  train loss: 0.1446561605
Processing class: 58
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.1374691159
Epoch:   200  |  train loss: 0.1379199386
Epoch:   300  |  train loss: 0.1423783273
Epoch:   400  |  train loss: 0.1476292759
Epoch:   500  |  train loss: 0.1472879350
Epoch:   600  |  train loss: 0.1505880892
Epoch:   700  |  train loss: 0.1504615128
Epoch:   800  |  train loss: 0.1531376332
Epoch:   900  |  train loss: 0.1534403443
Epoch:  1000  |  train loss: 0.1540970147
Processing class: 59
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.1410004079
Epoch:   200  |  train loss: 0.1459611028
Epoch:   300  |  train loss: 0.1453257442
Epoch:   400  |  train loss: 0.1483467013
Epoch:   500  |  train loss: 0.1513232350
Epoch:   600  |  train loss: 0.1522115290
Epoch:   700  |  train loss: 0.1543559074
Epoch:   800  |  train loss: 0.1555108756
Epoch:   900  |  train loss: 0.1554036140
Epoch:  1000  |  train loss: 0.1545623541
2024-03-05 08:15:56,182 [trainer.py] => CNN: {'total': 71.75, '00-09': 82.2, '10-19': 72.8, '20-29': 78.5, '30-39': 74.9, '40-49': 68.4, '50-59': 53.7, 'old': 75.36, 'new': 53.7}
2024-03-05 08:15:56,182 [trainer.py] => No NME accuracy
2024-03-05 08:15:56,182 [trainer.py] => FeCAM: {'total': 72.78, '00-09': 82.6, '10-19': 74.0, '20-29': 81.0, '30-39': 76.4, '40-49': 76.8, '50-59': 45.9, 'old': 78.16, 'new': 45.9}
2024-03-05 08:15:56,182 [trainer.py] => CNN top1 curve: [83.44, 71.75]
2024-03-05 08:15:56,182 [trainer.py] => CNN top5 curve: [96.5, 89.6]
2024-03-05 08:15:56,182 [trainer.py] => FeCAM top1 curve: [82.9, 72.78]
2024-03-05 08:15:56,182 [trainer.py] => FeCAM top5 curve: [95.84, 91.18]

2024-03-05 08:15:56,187 [fecam.py] => Learning on 60-70
Processing class: 60
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.1428252578
Epoch:   200  |  train loss: 0.1427993745
Epoch:   300  |  train loss: 0.1415839463
Epoch:   400  |  train loss: 0.1416332215
Epoch:   500  |  train loss: 0.1466414511
Epoch:   600  |  train loss: 0.1474231392
Epoch:   700  |  train loss: 0.1572309703
Epoch:   800  |  train loss: 0.1530810952
Epoch:   900  |  train loss: 0.1543845206
Epoch:  1000  |  train loss: 0.1572942168
Processing class: 61
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.1371849895
Epoch:   200  |  train loss: 0.1356285274
Epoch:   300  |  train loss: 0.1409859955
Epoch:   400  |  train loss: 0.1427314728
Epoch:   500  |  train loss: 0.1483832181
Epoch:   600  |  train loss: 0.1458189309
Epoch:   700  |  train loss: 0.1509335369
Epoch:   800  |  train loss: 0.1541285515
Epoch:   900  |  train loss: 0.1506018251
Epoch:  1000  |  train loss: 0.1526875526
Processing class: 62
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.1407539040
Epoch:   200  |  train loss: 0.1391411662
Epoch:   300  |  train loss: 0.1423043132
Epoch:   400  |  train loss: 0.1438683033
Epoch:   500  |  train loss: 0.1423066974
Epoch:   600  |  train loss: 0.1469269902
Epoch:   700  |  train loss: 0.1484996349
Epoch:   800  |  train loss: 0.1499019116
Epoch:   900  |  train loss: 0.1492258310
Epoch:  1000  |  train loss: 0.1506728828
Processing class: 63
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.1353190333
Epoch:   200  |  train loss: 0.1368382782
Epoch:   300  |  train loss: 0.1438845336
Epoch:   400  |  train loss: 0.1453917384
Epoch:   500  |  train loss: 0.1483790070
Epoch:   600  |  train loss: 0.1527962327
Epoch:   700  |  train loss: 0.1487417042
Epoch:   800  |  train loss: 0.1560306162
Epoch:   900  |  train loss: 0.1521402299
Epoch:  1000  |  train loss: 0.1535396785
Processing class: 64
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.1433616757
Epoch:   200  |  train loss: 0.1424518198
Epoch:   300  |  train loss: 0.1505106270
Epoch:   400  |  train loss: 0.1517801464
Epoch:   500  |  train loss: 0.1535212278
Epoch:   600  |  train loss: 0.1540353447
Epoch:   700  |  train loss: 0.1515254319
Epoch:   800  |  train loss: 0.1537366629
Epoch:   900  |  train loss: 0.1575749606
Epoch:  1000  |  train loss: 0.1570334166
Processing class: 65
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.1393685818
Epoch:   200  |  train loss: 0.1403833002
Epoch:   300  |  train loss: 0.1487961084
Epoch:   400  |  train loss: 0.1525570571
Epoch:   500  |  train loss: 0.1523902863
Epoch:   600  |  train loss: 0.1589333266
Epoch:   700  |  train loss: 0.1596550167
Epoch:   800  |  train loss: 0.1655096263
Epoch:   900  |  train loss: 0.1628161460
Epoch:  1000  |  train loss: 0.1687151104
Processing class: 66
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.1420383930
Epoch:   200  |  train loss: 0.1453589141
Epoch:   300  |  train loss: 0.1492759496
Epoch:   400  |  train loss: 0.1502520502
Epoch:   500  |  train loss: 0.1541148871
Epoch:   600  |  train loss: 0.1550782055
Epoch:   700  |  train loss: 0.1592952400
Epoch:   800  |  train loss: 0.1600381970
Epoch:   900  |  train loss: 0.1604014665
Epoch:  1000  |  train loss: 0.1600558043
Processing class: 67
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.1362522393
Epoch:   200  |  train loss: 0.1385561913
Epoch:   300  |  train loss: 0.1391771853
Epoch:   400  |  train loss: 0.1405957192
Epoch:   500  |  train loss: 0.1413276732
Epoch:   600  |  train loss: 0.1415350497
Epoch:   700  |  train loss: 0.1427253157
Epoch:   800  |  train loss: 0.1454289466
Epoch:   900  |  train loss: 0.1452686131
Epoch:  1000  |  train loss: 0.1453855008
Processing class: 68
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.1358465791
Epoch:   200  |  train loss: 0.1363614053
Epoch:   300  |  train loss: 0.1368713737
Epoch:   400  |  train loss: 0.1389567882
Epoch:   500  |  train loss: 0.1398328066
Epoch:   600  |  train loss: 0.1398418337
Epoch:   700  |  train loss: 0.1418929130
Epoch:   800  |  train loss: 0.1408246338
Epoch:   900  |  train loss: 0.1424935341
Epoch:  1000  |  train loss: 0.1472336233
Processing class: 69
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.1353299230
Epoch:   200  |  train loss: 0.1394381970
Epoch:   300  |  train loss: 0.1470658183
Epoch:   400  |  train loss: 0.1507595092
Epoch:   500  |  train loss: 0.1517850310
Epoch:   600  |  train loss: 0.1480461627
Epoch:   700  |  train loss: 0.1521684289
Epoch:   800  |  train loss: 0.1521817714
Epoch:   900  |  train loss: 0.1575841904
Epoch:  1000  |  train loss: 0.1527255774
2024-03-05 08:22:26,550 [trainer.py] => CNN: {'total': 64.66, '00-09': 75.0, '10-19': 70.1, '20-29': 77.1, '30-39': 70.6, '40-49': 64.0, '50-59': 45.8, '60-69': 50.0, 'old': 67.1, 'new': 50.0}
2024-03-05 08:22:26,551 [trainer.py] => No NME accuracy
2024-03-05 08:22:26,551 [trainer.py] => FeCAM: {'total': 67.27, '00-09': 79.9, '10-19': 70.6, '20-29': 79.1, '30-39': 71.9, '40-49': 74.1, '50-59': 41.2, '60-69': 54.1, 'old': 69.47, 'new': 54.1}
2024-03-05 08:22:26,551 [trainer.py] => CNN top1 curve: [83.44, 71.75, 64.66]
2024-03-05 08:22:26,551 [trainer.py] => CNN top5 curve: [96.5, 89.6, 86.54]
2024-03-05 08:22:26,551 [trainer.py] => FeCAM top1 curve: [82.9, 72.78, 67.27]
2024-03-05 08:22:26,551 [trainer.py] => FeCAM top5 curve: [95.84, 91.18, 87.21]

2024-03-05 08:22:26,556 [fecam.py] => Learning on 70-80
Processing class: 70
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.1369109660
Epoch:   200  |  train loss: 0.1360607713
Epoch:   300  |  train loss: 0.1389506012
Epoch:   400  |  train loss: 0.1413455695
Epoch:   500  |  train loss: 0.1408815235
Epoch:   600  |  train loss: 0.1445583075
Epoch:   700  |  train loss: 0.1492383957
Epoch:   800  |  train loss: 0.1457328677
Epoch:   900  |  train loss: 0.1492503166
Epoch:  1000  |  train loss: 0.1484728962
Processing class: 71
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.1390448987
Epoch:   200  |  train loss: 0.1408899158
Epoch:   300  |  train loss: 0.1392505348
Epoch:   400  |  train loss: 0.1427996248
Epoch:   500  |  train loss: 0.1455248415
Epoch:   600  |  train loss: 0.1465958506
Epoch:   700  |  train loss: 0.1476449609
Epoch:   800  |  train loss: 0.1491032690
Epoch:   900  |  train loss: 0.1530710876
Epoch:  1000  |  train loss: 0.1528283000
Processing class: 72
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.1366712272
Epoch:   200  |  train loss: 0.1385413826
Epoch:   300  |  train loss: 0.1399676621
Epoch:   400  |  train loss: 0.1406817734
Epoch:   500  |  train loss: 0.1435663760
Epoch:   600  |  train loss: 0.1418543547
Epoch:   700  |  train loss: 0.1436374784
Epoch:   800  |  train loss: 0.1431226701
Epoch:   900  |  train loss: 0.1446219742
Epoch:  1000  |  train loss: 0.1480945736
Processing class: 73
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.1403755814
Epoch:   200  |  train loss: 0.1404268384
Epoch:   300  |  train loss: 0.1458110094
Epoch:   400  |  train loss: 0.1447158217
Epoch:   500  |  train loss: 0.1492707789
Epoch:   600  |  train loss: 0.1494482517
Epoch:   700  |  train loss: 0.1477670312
Epoch:   800  |  train loss: 0.1480625480
Epoch:   900  |  train loss: 0.1489530861
Epoch:  1000  |  train loss: 0.1512059242
Processing class: 74
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.1418516845
Epoch:   200  |  train loss: 0.1440354854
Epoch:   300  |  train loss: 0.1413037837
Epoch:   400  |  train loss: 0.1405082762
Epoch:   500  |  train loss: 0.1430767149
Epoch:   600  |  train loss: 0.1424855649
Epoch:   700  |  train loss: 0.1474002481
Epoch:   800  |  train loss: 0.1438083261
Epoch:   900  |  train loss: 0.1447435349
Epoch:  1000  |  train loss: 0.1488944054
Processing class: 75
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.1374962151
Epoch:   200  |  train loss: 0.1374309003
Epoch:   300  |  train loss: 0.1421336800
Epoch:   400  |  train loss: 0.1424346298
Epoch:   500  |  train loss: 0.1458369106
Epoch:   600  |  train loss: 0.1491785496
Epoch:   700  |  train loss: 0.1426061511
Epoch:   800  |  train loss: 0.1475068808
Epoch:   900  |  train loss: 0.1475483537
Epoch:  1000  |  train loss: 0.1511706442
Processing class: 76
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.1316630095
Epoch:   200  |  train loss: 0.1358058035
Epoch:   300  |  train loss: 0.1423652977
Epoch:   400  |  train loss: 0.1468635231
Epoch:   500  |  train loss: 0.1487942308
Epoch:   600  |  train loss: 0.1521986425
Epoch:   700  |  train loss: 0.1530856311
Epoch:   800  |  train loss: 0.1497329831
Epoch:   900  |  train loss: 0.1546028972
Epoch:  1000  |  train loss: 0.1568793476
Processing class: 77
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.1383564085
Epoch:   200  |  train loss: 0.1397947460
Epoch:   300  |  train loss: 0.1388984740
Epoch:   400  |  train loss: 0.1414862812
Epoch:   500  |  train loss: 0.1436662197
Epoch:   600  |  train loss: 0.1448330760
Epoch:   700  |  train loss: 0.1466231823
Epoch:   800  |  train loss: 0.1505093664
Epoch:   900  |  train loss: 0.1494316280
Epoch:  1000  |  train loss: 0.1496015221
Processing class: 78
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.1404529989
Epoch:   200  |  train loss: 0.1446869820
Epoch:   300  |  train loss: 0.1486046255
Epoch:   400  |  train loss: 0.1493711740
Epoch:   500  |  train loss: 0.1497118115
Epoch:   600  |  train loss: 0.1530354738
Epoch:   700  |  train loss: 0.1545697898
Epoch:   800  |  train loss: 0.1561813116
Epoch:   900  |  train loss: 0.1570320398
Epoch:  1000  |  train loss: 0.1565000832
Processing class: 79
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.1392995894
Epoch:   200  |  train loss: 0.1375540107
Epoch:   300  |  train loss: 0.1412995756
Epoch:   400  |  train loss: 0.1422971427
Epoch:   500  |  train loss: 0.1427104264
Epoch:   600  |  train loss: 0.1469307333
Epoch:   700  |  train loss: 0.1472747296
Epoch:   800  |  train loss: 0.1494957596
Epoch:   900  |  train loss: 0.1527577519
Epoch:  1000  |  train loss: 0.1509228647
2024-03-05 08:29:58,711 [trainer.py] => CNN: {'total': 59.18, '00-09': 73.6, '10-19': 68.4, '20-29': 76.9, '30-39': 69.1, '40-49': 60.6, '50-59': 37.2, '60-69': 45.7, '70-79': 41.9, 'old': 61.64, 'new': 41.9}
2024-03-05 08:29:58,712 [trainer.py] => No NME accuracy
2024-03-05 08:29:58,712 [trainer.py] => FeCAM: {'total': 61.72, '00-09': 78.1, '10-19': 69.3, '20-29': 78.0, '30-39': 69.5, '40-49': 72.1, '50-59': 35.7, '60-69': 49.1, '70-79': 42.0, 'old': 64.54, 'new': 42.0}
2024-03-05 08:29:58,712 [trainer.py] => CNN top1 curve: [83.44, 71.75, 64.66, 59.18]
2024-03-05 08:29:58,712 [trainer.py] => CNN top5 curve: [96.5, 89.6, 86.54, 84.09]
2024-03-05 08:29:58,712 [trainer.py] => FeCAM top1 curve: [82.9, 72.78, 67.27, 61.72]
2024-03-05 08:29:58,713 [trainer.py] => FeCAM top5 curve: [95.84, 91.18, 87.21, 84.09]

2024-03-05 08:29:58,723 [fecam.py] => Learning on 80-90
Processing class: 80
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.1366337836
Epoch:   200  |  train loss: 0.1460630476
Epoch:   300  |  train loss: 0.1501278788
Epoch:   400  |  train loss: 0.1525564551
Epoch:   500  |  train loss: 0.1532253861
Epoch:   600  |  train loss: 0.1538271099
Epoch:   700  |  train loss: 0.1513714820
Epoch:   800  |  train loss: 0.1567864507
Epoch:   900  |  train loss: 0.1559843540
Epoch:  1000  |  train loss: 0.1591756284
Processing class: 81
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.1374174535
Epoch:   200  |  train loss: 0.1434076905
Epoch:   300  |  train loss: 0.1418721616
Epoch:   400  |  train loss: 0.1475268722
Epoch:   500  |  train loss: 0.1472793311
Epoch:   600  |  train loss: 0.1511858165
Epoch:   700  |  train loss: 0.1514794707
Epoch:   800  |  train loss: 0.1540436059
Epoch:   900  |  train loss: 0.1550843865
Epoch:  1000  |  train loss: 0.1568548411
Processing class: 82
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.1362825185
Epoch:   200  |  train loss: 0.1405033678
Epoch:   300  |  train loss: 0.1422227323
Epoch:   400  |  train loss: 0.1429133594
Epoch:   500  |  train loss: 0.1451465815
Epoch:   600  |  train loss: 0.1449801117
Epoch:   700  |  train loss: 0.1458345473
Epoch:   800  |  train loss: 0.1469726741
Epoch:   900  |  train loss: 0.1481363446
Epoch:  1000  |  train loss: 0.1487506449
Processing class: 83
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.1389120758
Epoch:   200  |  train loss: 0.1460243791
Epoch:   300  |  train loss: 0.1473711371
Epoch:   400  |  train loss: 0.1505468488
Epoch:   500  |  train loss: 0.1556361258
Epoch:   600  |  train loss: 0.1540176034
Epoch:   700  |  train loss: 0.1585622221
Epoch:   800  |  train loss: 0.1635174692
Epoch:   900  |  train loss: 0.1621994674
Epoch:  1000  |  train loss: 0.1614453882
Processing class: 84
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.1404660434
Epoch:   200  |  train loss: 0.1417409003
Epoch:   300  |  train loss: 0.1464203924
Epoch:   400  |  train loss: 0.1478862464
Epoch:   500  |  train loss: 0.1515851706
Epoch:   600  |  train loss: 0.1494980931
Epoch:   700  |  train loss: 0.1532135844
Epoch:   800  |  train loss: 0.1563977838
Epoch:   900  |  train loss: 0.1534298569
Epoch:  1000  |  train loss: 0.1532139331
Processing class: 85
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.1387303233
Epoch:   200  |  train loss: 0.1395761162
Epoch:   300  |  train loss: 0.1441950321
Epoch:   400  |  train loss: 0.1439770460
Epoch:   500  |  train loss: 0.1470681548
Epoch:   600  |  train loss: 0.1505425751
Epoch:   700  |  train loss: 0.1550114602
Epoch:   800  |  train loss: 0.1523405641
Epoch:   900  |  train loss: 0.1532308787
Epoch:  1000  |  train loss: 0.1569759220
Processing class: 86
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.1416808248
Epoch:   200  |  train loss: 0.1395214438
Epoch:   300  |  train loss: 0.1446972430
Epoch:   400  |  train loss: 0.1444169402
Epoch:   500  |  train loss: 0.1450806051
Epoch:   600  |  train loss: 0.1475888610
Epoch:   700  |  train loss: 0.1483888805
Epoch:   800  |  train loss: 0.1486434430
Epoch:   900  |  train loss: 0.1490949541
Epoch:  1000  |  train loss: 0.1537859917
Processing class: 87
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.1362129003
Epoch:   200  |  train loss: 0.1417719185
Epoch:   300  |  train loss: 0.1431959599
Epoch:   400  |  train loss: 0.1467896551
Epoch:   500  |  train loss: 0.1473894536
Epoch:   600  |  train loss: 0.1497883081
Epoch:   700  |  train loss: 0.1515001804
Epoch:   800  |  train loss: 0.1521004796
Epoch:   900  |  train loss: 0.1507718652
Epoch:  1000  |  train loss: 0.1562441230
Processing class: 88
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.1376548499
Epoch:   200  |  train loss: 0.1373699278
Epoch:   300  |  train loss: 0.1387008548
Epoch:   400  |  train loss: 0.1406989485
Epoch:   500  |  train loss: 0.1415673733
Epoch:   600  |  train loss: 0.1415493339
Epoch:   700  |  train loss: 0.1450179696
Epoch:   800  |  train loss: 0.1460827053
Epoch:   900  |  train loss: 0.1474779874
Epoch:  1000  |  train loss: 0.1461924523
Processing class: 89
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.1396638751
Epoch:   200  |  train loss: 0.1443606824
Epoch:   300  |  train loss: 0.1477054656
Epoch:   400  |  train loss: 0.1494763374
Epoch:   500  |  train loss: 0.1534395188
Epoch:   600  |  train loss: 0.1549234182
Epoch:   700  |  train loss: 0.1526730835
Epoch:   800  |  train loss: 0.1594038427
Epoch:   900  |  train loss: 0.1585231304
Epoch:  1000  |  train loss: 0.1608782947
2024-03-05 08:38:43,765 [trainer.py] => CNN: {'total': 54.08, '00-09': 67.9, '10-19': 63.6, '20-29': 73.2, '30-39': 68.8, '40-49': 57.2, '50-59': 32.6, '60-69': 40.9, '70-79': 38.1, '80-89': 44.4, 'old': 55.29, 'new': 44.4}
2024-03-05 08:38:43,766 [trainer.py] => No NME accuracy
2024-03-05 08:38:43,766 [trainer.py] => FeCAM: {'total': 57.66, '00-09': 76.1, '10-19': 65.8, '20-29': 75.3, '30-39': 67.9, '40-49': 68.5, '50-59': 33.0, '60-69': 45.0, '70-79': 39.0, '80-89': 48.3, 'old': 58.82, 'new': 48.3}
2024-03-05 08:38:43,766 [trainer.py] => CNN top1 curve: [83.44, 71.75, 64.66, 59.18, 54.08]
2024-03-05 08:38:43,766 [trainer.py] => CNN top5 curve: [96.5, 89.6, 86.54, 84.09, 81.74]
2024-03-05 08:38:43,766 [trainer.py] => FeCAM top1 curve: [82.9, 72.78, 67.27, 61.72, 57.66]
2024-03-05 08:38:43,766 [trainer.py] => FeCAM top5 curve: [95.84, 91.18, 87.21, 84.09, 81.63]

2024-03-05 08:38:43,771 [fecam.py] => Learning on 90-100
Processing class: 90
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.1397029638
Epoch:   200  |  train loss: 0.1409822434
Epoch:   300  |  train loss: 0.1392537415
Epoch:   400  |  train loss: 0.1396972120
Epoch:   500  |  train loss: 0.1396368861
Epoch:   600  |  train loss: 0.1421992362
Epoch:   700  |  train loss: 0.1424981833
Epoch:   800  |  train loss: 0.1441278487
Epoch:   900  |  train loss: 0.1462818772
Epoch:  1000  |  train loss: 0.1500729233
Processing class: 91
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.1349649519
Epoch:   200  |  train loss: 0.1359251767
Epoch:   300  |  train loss: 0.1403546870
Epoch:   400  |  train loss: 0.1437906563
Epoch:   500  |  train loss: 0.1475909203
Epoch:   600  |  train loss: 0.1497505963
Epoch:   700  |  train loss: 0.1516453713
Epoch:   800  |  train loss: 0.1542048931
Epoch:   900  |  train loss: 0.1571227789
Epoch:  1000  |  train loss: 0.1580950141
Processing class: 92
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.1423780531
Epoch:   200  |  train loss: 0.1414547205
Epoch:   300  |  train loss: 0.1386607379
Epoch:   400  |  train loss: 0.1417403162
Epoch:   500  |  train loss: 0.1445429623
Epoch:   600  |  train loss: 0.1490301341
Epoch:   700  |  train loss: 0.1499060214
Epoch:   800  |  train loss: 0.1466753483
Epoch:   900  |  train loss: 0.1483399868
Epoch:  1000  |  train loss: 0.1496179193
Processing class: 93
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.1383692294
Epoch:   200  |  train loss: 0.1424007982
Epoch:   300  |  train loss: 0.1509726644
Epoch:   400  |  train loss: 0.1529953271
Epoch:   500  |  train loss: 0.1536597401
Epoch:   600  |  train loss: 0.1525542051
Epoch:   700  |  train loss: 0.1592137486
Epoch:   800  |  train loss: 0.1595537066
Epoch:   900  |  train loss: 0.1608237624
Epoch:  1000  |  train loss: 0.1607818067
Processing class: 94
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.1289323166
Epoch:   200  |  train loss: 0.1296105340
Epoch:   300  |  train loss: 0.1275245517
Epoch:   400  |  train loss: 0.1306246758
Epoch:   500  |  train loss: 0.1263080657
Epoch:   600  |  train loss: 0.1297204822
Epoch:   700  |  train loss: 0.1329939485
Epoch:   800  |  train loss: 0.1274109930
Epoch:   900  |  train loss: 0.1320897609
Epoch:  1000  |  train loss: 0.1312574014
Processing class: 95
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.1371861935
Epoch:   200  |  train loss: 0.1409828424
Epoch:   300  |  train loss: 0.1399069339
Epoch:   400  |  train loss: 0.1398555964
Epoch:   500  |  train loss: 0.1453494072
Epoch:   600  |  train loss: 0.1456164241
Epoch:   700  |  train loss: 0.1480632007
Epoch:   800  |  train loss: 0.1472943634
Epoch:   900  |  train loss: 0.1507915407
Epoch:  1000  |  train loss: 0.1541422784
Processing class: 96
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.1410490781
Epoch:   200  |  train loss: 0.1376884937
Epoch:   300  |  train loss: 0.1420710146
Epoch:   400  |  train loss: 0.1480952770
Epoch:   500  |  train loss: 0.1473801464
Epoch:   600  |  train loss: 0.1475359142
Epoch:   700  |  train loss: 0.1490707606
Epoch:   800  |  train loss: 0.1477866769
Epoch:   900  |  train loss: 0.1537556589
Epoch:  1000  |  train loss: 0.1543186277
Processing class: 97
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.1404936552
Epoch:   200  |  train loss: 0.1428232491
Epoch:   300  |  train loss: 0.1454169005
Epoch:   400  |  train loss: 0.1476181746
Epoch:   500  |  train loss: 0.1520148784
Epoch:   600  |  train loss: 0.1549922645
Epoch:   700  |  train loss: 0.1578625023
Epoch:   800  |  train loss: 0.1594079614
Epoch:   900  |  train loss: 0.1612740904
Epoch:  1000  |  train loss: 0.1613766789
Processing class: 98
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.1370057732
Epoch:   200  |  train loss: 0.1381098121
Epoch:   300  |  train loss: 0.1408125550
Epoch:   400  |  train loss: 0.1437823027
Epoch:   500  |  train loss: 0.1444973737
Epoch:   600  |  train loss: 0.1474080056
Epoch:   700  |  train loss: 0.1449393213
Epoch:   800  |  train loss: 0.1485014021
Epoch:   900  |  train loss: 0.1491152704
Epoch:  1000  |  train loss: 0.1480590910
Processing class: 99
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.1429569542
Epoch:   200  |  train loss: 0.1411385804
Epoch:   300  |  train loss: 0.1482949138
Epoch:   400  |  train loss: 0.1533662677
Epoch:   500  |  train loss: 0.1540430218
Epoch:   600  |  train loss: 0.1545050681
Epoch:   700  |  train loss: 0.1541977435
Epoch:   800  |  train loss: 0.1581688643
Epoch:   900  |  train loss: 0.1575332165
Epoch:  1000  |  train loss: 0.1586909533
2024-03-05 08:48:45,878 [trainer.py] => CNN: {'total': 50.36, '00-09': 57.7, '10-19': 63.6, '20-29': 71.4, '30-39': 68.5, '40-49': 56.8, '50-59': 29.6, '60-69': 38.9, '70-79': 36.6, '80-89': 42.7, '90-99': 37.8, 'old': 51.76, 'new': 37.8}
2024-03-05 08:48:45,880 [trainer.py] => No NME accuracy
2024-03-05 08:48:45,880 [trainer.py] => FeCAM: {'total': 54.74, '00-09': 73.3, '10-19': 65.1, '20-29': 74.1, '30-39': 67.2, '40-49': 67.1, '50-59': 30.8, '60-69': 43.3, '70-79': 37.2, '80-89': 46.9, '90-99': 42.4, 'old': 56.11, 'new': 42.4}
2024-03-05 08:48:45,881 [trainer.py] => CNN top1 curve: [83.44, 71.75, 64.66, 59.18, 54.08, 50.36]
2024-03-05 08:48:45,881 [trainer.py] => CNN top5 curve: [96.5, 89.6, 86.54, 84.09, 81.74, 79.63]
2024-03-05 08:48:45,881 [trainer.py] => FeCAM top1 curve: [82.9, 72.78, 67.27, 61.72, 57.66, 54.74]
2024-03-05 08:48:45,881 [trainer.py] => FeCAM top5 curve: [95.84, 91.18, 87.21, 84.09, 81.63, 79.21]

=========================================
2024-03-05 08:48:57,338 [trainer.py] => config: ./exps/FeCAM_cifar100.json
2024-03-05 08:48:57,338 [trainer.py] => prefix: train
2024-03-05 08:48:57,338 [trainer.py] => dataset: cifar100
2024-03-05 08:48:57,338 [trainer.py] => memory_size: 0
2024-03-05 08:48:57,338 [trainer.py] => shuffle: True
2024-03-05 08:48:57,338 [trainer.py] => init_cls: 50
2024-03-05 08:48:57,338 [trainer.py] => increment: 10
2024-03-05 08:48:57,338 [trainer.py] => model_name: fecam
2024-03-05 08:48:57,338 [trainer.py] => convnet_type: resnet18
2024-03-05 08:48:57,338 [trainer.py] => device: [device(type='cuda', index=0)]
2024-03-05 08:48:57,338 [trainer.py] => seed: 1993
2024-03-05 08:48:57,338 [trainer.py] => init_epochs: 200
2024-03-05 08:48:57,338 [trainer.py] => init_lr: 0.1
2024-03-05 08:48:57,338 [trainer.py] => init_weight_decay: 0.0005
2024-03-05 08:48:57,338 [trainer.py] => batch_size: 128
2024-03-05 08:48:57,338 [trainer.py] => num_workers: 8
2024-03-05 08:48:57,338 [trainer.py] => T: 5
2024-03-05 08:48:57,339 [trainer.py] => beta: 0.5
2024-03-05 08:48:57,339 [trainer.py] => alpha1: 1
2024-03-05 08:48:57,339 [trainer.py] => alpha2: 1
2024-03-05 08:48:57,339 [trainer.py] => ncm: False
2024-03-05 08:48:57,339 [trainer.py] => tukey: False
2024-03-05 08:48:57,339 [trainer.py] => diagonal: False
2024-03-05 08:48:57,339 [trainer.py] => per_class: True
2024-03-05 08:48:57,339 [trainer.py] => full_cov: True
2024-03-05 08:48:57,339 [trainer.py] => shrink: True
2024-03-05 08:48:57,339 [trainer.py] => norm_cov: False
2024-03-05 08:48:57,339 [trainer.py] => vecnorm: False
2024-03-05 08:48:57,339 [trainer.py] => ae_type: wae
2024-03-05 08:48:57,339 [trainer.py] => epochs: 1000
2024-03-05 08:48:57,339 [trainer.py] => ae_latent_dim: 32
2024-03-05 08:48:57,339 [trainer.py] => wae_sigma: 50
2024-03-05 08:48:57,339 [trainer.py] => wae_C: 0.5
Files already downloaded and verified
Files already downloaded and verified
2024-03-05 08:48:59,000 [data_manager.py] => [68, 56, 78, 8, 23, 84, 90, 65, 74, 76, 40, 89, 3, 92, 55, 9, 26, 80, 43, 38, 58, 70, 77, 1, 85, 19, 17, 50, 28, 53, 13, 81, 45, 82, 6, 59, 83, 16, 15, 44, 91, 41, 72, 60, 79, 52, 20, 10, 31, 54, 37, 95, 14, 71, 96, 98, 97, 2, 64, 66, 42, 22, 35, 86, 24, 34, 87, 21, 99, 0, 88, 27, 18, 94, 11, 12, 47, 25, 30, 46, 62, 69, 36, 61, 7, 63, 75, 5, 32, 4, 51, 48, 73, 93, 39, 67, 29, 49, 57, 33]
2024-03-05 08:48:59,267 [fecam.py] => Learning on 0-50
/home/z1165703/miniconda3/lib/python3.11/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
/home/z1165703/miniconda3/lib/python3.11/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
Processing class: 0
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.1264714509
Epoch:   200  |  train loss: 0.1309168190
Epoch:   300  |  train loss: 0.1341495410
Epoch:   400  |  train loss: 0.1360677510
Epoch:   500  |  train loss: 0.1350839823
Epoch:   600  |  train loss: 0.1350087106
Epoch:   700  |  train loss: 0.1366667509
Epoch:   800  |  train loss: 0.1372760952
Epoch:   900  |  train loss: 0.1361525863
Epoch:  1000  |  train loss: 0.1362409681
Processing class: 1
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.1217733920
Epoch:   200  |  train loss: 0.1207127109
Epoch:   300  |  train loss: 0.1286539584
Epoch:   400  |  train loss: 0.1305320948
Epoch:   500  |  train loss: 0.1281541094
Epoch:   600  |  train loss: 0.1329904795
Epoch:   700  |  train loss: 0.1315318167
Epoch:   800  |  train loss: 0.1345232159
Epoch:   900  |  train loss: 0.1384930432
Epoch:  1000  |  train loss: 0.1355601311
Processing class: 2
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.1191103026
Epoch:   200  |  train loss: 0.1198677182
Epoch:   300  |  train loss: 0.1256089464
Epoch:   400  |  train loss: 0.1261419520
Epoch:   500  |  train loss: 0.1210185185
Epoch:   600  |  train loss: 0.1270939380
Epoch:   700  |  train loss: 0.1272690132
Epoch:   800  |  train loss: 0.1320221901
Epoch:   900  |  train loss: 0.1306341231
Epoch:  1000  |  train loss: 0.1299290344
Processing class: 3
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.1347672284
Epoch:   200  |  train loss: 0.1391479641
Epoch:   300  |  train loss: 0.1419585615
Epoch:   400  |  train loss: 0.1441595912
Epoch:   500  |  train loss: 0.1432397127
Epoch:   600  |  train loss: 0.1482825339
Epoch:   700  |  train loss: 0.1520222992
Epoch:   800  |  train loss: 0.1495731324
Epoch:   900  |  train loss: 0.1526723444
Epoch:  1000  |  train loss: 0.1542989731
Processing class: 4
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.1203202918
Epoch:   200  |  train loss: 0.1189991802
Epoch:   300  |  train loss: 0.1259109601
Epoch:   400  |  train loss: 0.1274965927
Epoch:   500  |  train loss: 0.1305261537
Epoch:   600  |  train loss: 0.1309308469
Epoch:   700  |  train loss: 0.1334902331
Epoch:   800  |  train loss: 0.1345238984
Epoch:   900  |  train loss: 0.1361627400
Epoch:  1000  |  train loss: 0.1399706692
Processing class: 5
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.1193595812
Epoch:   200  |  train loss: 0.1200375855
Epoch:   300  |  train loss: 0.1231054991
Epoch:   400  |  train loss: 0.1244544029
Epoch:   500  |  train loss: 0.1245012954
Epoch:   600  |  train loss: 0.1310161710
Epoch:   700  |  train loss: 0.1330583632
Epoch:   800  |  train loss: 0.1312200993
Epoch:   900  |  train loss: 0.1314651757
Epoch:  1000  |  train loss: 0.1346549809
Processing class: 6
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.1225505039
Epoch:   200  |  train loss: 0.1247260138
Epoch:   300  |  train loss: 0.1317185536
Epoch:   400  |  train loss: 0.1321316659
Epoch:   500  |  train loss: 0.1375171393
Epoch:   600  |  train loss: 0.1361979306
Epoch:   700  |  train loss: 0.1351665109
Epoch:   800  |  train loss: 0.1369109005
Epoch:   900  |  train loss: 0.1362095743
Epoch:  1000  |  train loss: 0.1383308351
Processing class: 7
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.1169156641
Epoch:   200  |  train loss: 0.1192427292
Epoch:   300  |  train loss: 0.1216500953
Epoch:   400  |  train loss: 0.1241105944
Epoch:   500  |  train loss: 0.1277041525
Epoch:   600  |  train loss: 0.1267060608
Epoch:   700  |  train loss: 0.1296581224
Epoch:   800  |  train loss: 0.1342417866
Epoch:   900  |  train loss: 0.1315814912
Epoch:  1000  |  train loss: 0.1355459422
Processing class: 8
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.1194416195
Epoch:   200  |  train loss: 0.1230897427
Epoch:   300  |  train loss: 0.1234466776
Epoch:   400  |  train loss: 0.1294858649
Epoch:   500  |  train loss: 0.1277718782
Epoch:   600  |  train loss: 0.1302238494
Epoch:   700  |  train loss: 0.1343032986
Epoch:   800  |  train loss: 0.1319795787
Epoch:   900  |  train loss: 0.1344955415
Epoch:  1000  |  train loss: 0.1332413614
Processing class: 9
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.1173348948
Epoch:   200  |  train loss: 0.1203103930
Epoch:   300  |  train loss: 0.1247202352
Epoch:   400  |  train loss: 0.1218126118
Epoch:   500  |  train loss: 0.1262497678
Epoch:   600  |  train loss: 0.1242078781
Epoch:   700  |  train loss: 0.1258269951
Epoch:   800  |  train loss: 0.1272291780
Epoch:   900  |  train loss: 0.1276217848
Epoch:  1000  |  train loss: 0.1256548077
Processing class: 10
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.1128692910
Epoch:   200  |  train loss: 0.1139714107
Epoch:   300  |  train loss: 0.1162643462
Epoch:   400  |  train loss: 0.1213302493
Epoch:   500  |  train loss: 0.1169702068
Epoch:   600  |  train loss: 0.1198664099
Epoch:   700  |  train loss: 0.1220998302
Epoch:   800  |  train loss: 0.1216594249
Epoch:   900  |  train loss: 0.1232602030
Epoch:  1000  |  train loss: 0.1233399123
Processing class: 11
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.1197040111
Epoch:   200  |  train loss: 0.1231201932
Epoch:   300  |  train loss: 0.1221772552
Epoch:   400  |  train loss: 0.1251112401
Epoch:   500  |  train loss: 0.1266028374
Epoch:   600  |  train loss: 0.1303044647
Epoch:   700  |  train loss: 0.1352061510
Epoch:   800  |  train loss: 0.1346967638
Epoch:   900  |  train loss: 0.1356230229
Epoch:  1000  |  train loss: 0.1384844452
Processing class: 12
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.1147144869
Epoch:   200  |  train loss: 0.1219466701
Epoch:   300  |  train loss: 0.1236747250
Epoch:   400  |  train loss: 0.1287118807
Epoch:   500  |  train loss: 0.1298721969
Epoch:   600  |  train loss: 0.1288383037
Epoch:   700  |  train loss: 0.1309311837
Epoch:   800  |  train loss: 0.1324474216
Epoch:   900  |  train loss: 0.1350561321
Epoch:  1000  |  train loss: 0.1339807183
Processing class: 13
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.1187201187
Epoch:   200  |  train loss: 0.1253872350
Epoch:   300  |  train loss: 0.1306078598
Epoch:   400  |  train loss: 0.1330702543
Epoch:   500  |  train loss: 0.1377623945
Epoch:   600  |  train loss: 0.1353505671
Epoch:   700  |  train loss: 0.1382216811
Epoch:   800  |  train loss: 0.1405017853
Epoch:   900  |  train loss: 0.1419796199
Epoch:  1000  |  train loss: 0.1438967794
Processing class: 14
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.1177424550
Epoch:   200  |  train loss: 0.1197553873
Epoch:   300  |  train loss: 0.1219930515
Epoch:   400  |  train loss: 0.1266550779
Epoch:   500  |  train loss: 0.1268143654
Epoch:   600  |  train loss: 0.1275982320
Epoch:   700  |  train loss: 0.1297023207
Epoch:   800  |  train loss: 0.1347287834
Epoch:   900  |  train loss: 0.1334489107
Epoch:  1000  |  train loss: 0.1306609750
Processing class: 15
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.1207319856
Epoch:   200  |  train loss: 0.1237130031
Epoch:   300  |  train loss: 0.1303421915
Epoch:   400  |  train loss: 0.1334227800
Epoch:   500  |  train loss: 0.1362410784
Epoch:   600  |  train loss: 0.1346792787
Epoch:   700  |  train loss: 0.1389518589
Epoch:   800  |  train loss: 0.1357451439
Epoch:   900  |  train loss: 0.1396073729
Epoch:  1000  |  train loss: 0.1381243110
Processing class: 16
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.1216879576
Epoch:   200  |  train loss: 0.1324305058
Epoch:   300  |  train loss: 0.1372174084
Epoch:   400  |  train loss: 0.1358376354
Epoch:   500  |  train loss: 0.1395714402
Epoch:   600  |  train loss: 0.1369968116
Epoch:   700  |  train loss: 0.1367088318
Epoch:   800  |  train loss: 0.1419948250
Epoch:   900  |  train loss: 0.1381267607
Epoch:  1000  |  train loss: 0.1424403578
Processing class: 17
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.1148156166
Epoch:   200  |  train loss: 0.1172997370
Epoch:   300  |  train loss: 0.1213584438
Epoch:   400  |  train loss: 0.1227379188
Epoch:   500  |  train loss: 0.1201613456
Epoch:   600  |  train loss: 0.1236212701
Epoch:   700  |  train loss: 0.1234067202
Epoch:   800  |  train loss: 0.1242676988
Epoch:   900  |  train loss: 0.1258845806
Epoch:  1000  |  train loss: 0.1255452260
Processing class: 18
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.1187849388
Epoch:   200  |  train loss: 0.1200642243
Epoch:   300  |  train loss: 0.1258259192
Epoch:   400  |  train loss: 0.1263169080
Epoch:   500  |  train loss: 0.1319822013
Epoch:   600  |  train loss: 0.1316526040
Epoch:   700  |  train loss: 0.1383192360
Epoch:   800  |  train loss: 0.1388651550
Epoch:   900  |  train loss: 0.1407338560
Epoch:  1000  |  train loss: 0.1447885841
Processing class: 19
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.1220619962
Epoch:   200  |  train loss: 0.1305936754
Epoch:   300  |  train loss: 0.1316091716
Epoch:   400  |  train loss: 0.1331071109
Epoch:   500  |  train loss: 0.1318063468
Epoch:   600  |  train loss: 0.1326988429
Epoch:   700  |  train loss: 0.1350480437
Epoch:   800  |  train loss: 0.1345218867
Epoch:   900  |  train loss: 0.1419887513
Epoch:  1000  |  train loss: 0.1437618822
Processing class: 20
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.1210262358
Epoch:   200  |  train loss: 0.1247359857
Epoch:   300  |  train loss: 0.1276776507
Epoch:   400  |  train loss: 0.1309088022
Epoch:   500  |  train loss: 0.1347405165
Epoch:   600  |  train loss: 0.1331558108
Epoch:   700  |  train loss: 0.1345701277
Epoch:   800  |  train loss: 0.1348006457
Epoch:   900  |  train loss: 0.1368385047
Epoch:  1000  |  train loss: 0.1396562368
Processing class: 21
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.1212928355
Epoch:   200  |  train loss: 0.1253347799
Epoch:   300  |  train loss: 0.1297668204
Epoch:   400  |  train loss: 0.1325297639
Epoch:   500  |  train loss: 0.1349085420
Epoch:   600  |  train loss: 0.1378166258
Epoch:   700  |  train loss: 0.1374501005
Epoch:   800  |  train loss: 0.1372812837
Epoch:   900  |  train loss: 0.1410762846
Epoch:  1000  |  train loss: 0.1410066426
Processing class: 22
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.1175740123
Epoch:   200  |  train loss: 0.1193022996
Epoch:   300  |  train loss: 0.1226293147
Epoch:   400  |  train loss: 0.1277321532
Epoch:   500  |  train loss: 0.1298614442
Epoch:   600  |  train loss: 0.1287927687
Epoch:   700  |  train loss: 0.1312493652
Epoch:   800  |  train loss: 0.1310106188
Epoch:   900  |  train loss: 0.1298480123
Epoch:  1000  |  train loss: 0.1351743042
Processing class: 23
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.1248011634
Epoch:   200  |  train loss: 0.1283474118
Epoch:   300  |  train loss: 0.1321518213
Epoch:   400  |  train loss: 0.1295994490
Epoch:   500  |  train loss: 0.1303129464
Epoch:   600  |  train loss: 0.1294568703
Epoch:   700  |  train loss: 0.1325639337
Epoch:   800  |  train loss: 0.1314230084
Epoch:   900  |  train loss: 0.1341988027
Epoch:  1000  |  train loss: 0.1329386234
Processing class: 24
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.1215091616
Epoch:   200  |  train loss: 0.1219570190
Epoch:   300  |  train loss: 0.1188724145
Epoch:   400  |  train loss: 0.1232070029
Epoch:   500  |  train loss: 0.1223567948
Epoch:   600  |  train loss: 0.1249339387
Epoch:   700  |  train loss: 0.1212240785
Epoch:   800  |  train loss: 0.1230979383
Epoch:   900  |  train loss: 0.1261700571
Epoch:  1000  |  train loss: 0.1258391440
Processing class: 25
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.1187143803
Epoch:   200  |  train loss: 0.1221760660
Epoch:   300  |  train loss: 0.1267587110
Epoch:   400  |  train loss: 0.1257608578
Epoch:   500  |  train loss: 0.1313095227
Epoch:   600  |  train loss: 0.1298684210
Epoch:   700  |  train loss: 0.1341796339
Epoch:   800  |  train loss: 0.1332152262
Epoch:   900  |  train loss: 0.1329420865
Epoch:  1000  |  train loss: 0.1344795644
Processing class: 26
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.1268232703
Epoch:   200  |  train loss: 0.1314893991
Epoch:   300  |  train loss: 0.1284646481
Epoch:   400  |  train loss: 0.1326524019
Epoch:   500  |  train loss: 0.1368749112
Epoch:   600  |  train loss: 0.1333410233
Epoch:   700  |  train loss: 0.1357779771
Epoch:   800  |  train loss: 0.1375570178
Epoch:   900  |  train loss: 0.1374932602
Epoch:  1000  |  train loss: 0.1407483935
Processing class: 27
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.1182192087
Epoch:   200  |  train loss: 0.1185890496
Epoch:   300  |  train loss: 0.1203625992
Epoch:   400  |  train loss: 0.1226632610
Epoch:   500  |  train loss: 0.1260092959
Epoch:   600  |  train loss: 0.1264144644
Epoch:   700  |  train loss: 0.1283376247
Epoch:   800  |  train loss: 0.1286072642
Epoch:   900  |  train loss: 0.1303483635
Epoch:  1000  |  train loss: 0.1279068798
Processing class: 28
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.1235481068
Epoch:   200  |  train loss: 0.1274894893
Epoch:   300  |  train loss: 0.1330482185
Epoch:   400  |  train loss: 0.1360800326
Epoch:   500  |  train loss: 0.1378508359
Epoch:   600  |  train loss: 0.1387079716
Epoch:   700  |  train loss: 0.1396473795
Epoch:   800  |  train loss: 0.1411279976
Epoch:   900  |  train loss: 0.1420916110
Epoch:  1000  |  train loss: 0.1394251019
Processing class: 29
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.1198241159
Epoch:   200  |  train loss: 0.1241373032
Epoch:   300  |  train loss: 0.1251570627
Epoch:   400  |  train loss: 0.1250222400
Epoch:   500  |  train loss: 0.1286449537
Epoch:   600  |  train loss: 0.1309276924
Epoch:   700  |  train loss: 0.1317170873
Epoch:   800  |  train loss: 0.1304072574
Epoch:   900  |  train loss: 0.1315572917
Epoch:  1000  |  train loss: 0.1312674135
Processing class: 30
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.1263881892
Epoch:   200  |  train loss: 0.1308437064
Epoch:   300  |  train loss: 0.1382718861
Epoch:   400  |  train loss: 0.1438259661
Epoch:   500  |  train loss: 0.1416439712
Epoch:   600  |  train loss: 0.1423395395
Epoch:   700  |  train loss: 0.1423858762
Epoch:   800  |  train loss: 0.1460293978
Epoch:   900  |  train loss: 0.1484347761
Epoch:  1000  |  train loss: 0.1465808302
Processing class: 31
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.1237764865
Epoch:   200  |  train loss: 0.1294629127
Epoch:   300  |  train loss: 0.1314620495
Epoch:   400  |  train loss: 0.1361427248
Epoch:   500  |  train loss: 0.1382164538
Epoch:   600  |  train loss: 0.1424676597
Epoch:   700  |  train loss: 0.1420800477
Epoch:   800  |  train loss: 0.1425163090
Epoch:   900  |  train loss: 0.1457041025
Epoch:  1000  |  train loss: 0.1431807101
Processing class: 32
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.1184118614
Epoch:   200  |  train loss: 0.1223927960
Epoch:   300  |  train loss: 0.1227702484
Epoch:   400  |  train loss: 0.1209090114
Epoch:   500  |  train loss: 0.1276220948
Epoch:   600  |  train loss: 0.1274616718
Epoch:   700  |  train loss: 0.1326009542
Epoch:   800  |  train loss: 0.1350200593
Epoch:   900  |  train loss: 0.1344897896
Epoch:  1000  |  train loss: 0.1338664234
Processing class: 33
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.1144457579
Epoch:   200  |  train loss: 0.1223552465
Epoch:   300  |  train loss: 0.1184404582
Epoch:   400  |  train loss: 0.1205899179
Epoch:   500  |  train loss: 0.1222260743
Epoch:   600  |  train loss: 0.1275743991
Epoch:   700  |  train loss: 0.1257685333
Epoch:   800  |  train loss: 0.1274039909
Epoch:   900  |  train loss: 0.1275536403
Epoch:  1000  |  train loss: 0.1267047122
Processing class: 34
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.1203597724
Epoch:   200  |  train loss: 0.1201284647
Epoch:   300  |  train loss: 0.1243079975
Epoch:   400  |  train loss: 0.1236139670
Epoch:   500  |  train loss: 0.1244556144
Epoch:   600  |  train loss: 0.1278897390
Epoch:   700  |  train loss: 0.1264102548
Epoch:   800  |  train loss: 0.1304042339
Epoch:   900  |  train loss: 0.1319809735
Epoch:  1000  |  train loss: 0.1329957336
Processing class: 35
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.1146609128
Epoch:   200  |  train loss: 0.1200452879
Epoch:   300  |  train loss: 0.1235999122
Epoch:   400  |  train loss: 0.1256024256
Epoch:   500  |  train loss: 0.1242418393
Epoch:   600  |  train loss: 0.1262139887
Epoch:   700  |  train loss: 0.1281562760
Epoch:   800  |  train loss: 0.1282238320
Epoch:   900  |  train loss: 0.1290006578
Epoch:  1000  |  train loss: 0.1280982003
Processing class: 36
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.1247792110
Epoch:   200  |  train loss: 0.1270690739
Epoch:   300  |  train loss: 0.1307679355
Epoch:   400  |  train loss: 0.1314565763
Epoch:   500  |  train loss: 0.1366820246
Epoch:   600  |  train loss: 0.1358282268
Epoch:   700  |  train loss: 0.1345629305
Epoch:   800  |  train loss: 0.1401581138
Epoch:   900  |  train loss: 0.1369290084
Epoch:  1000  |  train loss: 0.1404197991
Processing class: 37
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.1207333714
Epoch:   200  |  train loss: 0.1208326295
Epoch:   300  |  train loss: 0.1216623247
Epoch:   400  |  train loss: 0.1245506153
Epoch:   500  |  train loss: 0.1271144092
Epoch:   600  |  train loss: 0.1256924704
Epoch:   700  |  train loss: 0.1268639490
Epoch:   800  |  train loss: 0.1282907039
Epoch:   900  |  train loss: 0.1271785676
Epoch:  1000  |  train loss: 0.1286680326
Processing class: 38
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.1216061443
Epoch:   200  |  train loss: 0.1235608011
Epoch:   300  |  train loss: 0.1278623596
Epoch:   400  |  train loss: 0.1290463403
Epoch:   500  |  train loss: 0.1314575523
Epoch:   600  |  train loss: 0.1315414190
Epoch:   700  |  train loss: 0.1329126775
Epoch:   800  |  train loss: 0.1362353384
Epoch:   900  |  train loss: 0.1332075566
Epoch:  1000  |  train loss: 0.1361139059
Processing class: 39
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.1216389164
Epoch:   200  |  train loss: 0.1213162974
Epoch:   300  |  train loss: 0.1245945126
Epoch:   400  |  train loss: 0.1273210898
Epoch:   500  |  train loss: 0.1271404162
Epoch:   600  |  train loss: 0.1287777245
Epoch:   700  |  train loss: 0.1310147107
Epoch:   800  |  train loss: 0.1337363064
Epoch:   900  |  train loss: 0.1357187629
Epoch:  1000  |  train loss: 0.1371556699
Processing class: 40
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.1226905257
Epoch:   200  |  train loss: 0.1223907351
Epoch:   300  |  train loss: 0.1291035563
Epoch:   400  |  train loss: 0.1308753818
Epoch:   500  |  train loss: 0.1332794011
Epoch:   600  |  train loss: 0.1329856664
Epoch:   700  |  train loss: 0.1347606480
Epoch:   800  |  train loss: 0.1351497412
Epoch:   900  |  train loss: 0.1339442119
Epoch:  1000  |  train loss: 0.1398455381
Processing class: 41
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.1228882179
Epoch:   200  |  train loss: 0.1233395308
Epoch:   300  |  train loss: 0.1280551061
Epoch:   400  |  train loss: 0.1285433963
Epoch:   500  |  train loss: 0.1296068907
Epoch:   600  |  train loss: 0.1303210795
Epoch:   700  |  train loss: 0.1333423048
Epoch:   800  |  train loss: 0.1322153002
Epoch:   900  |  train loss: 0.1320944637
Epoch:  1000  |  train loss: 0.1327313364
Processing class: 42
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.1170499370
Epoch:   200  |  train loss: 0.1222307250
Epoch:   300  |  train loss: 0.1261942565
Epoch:   400  |  train loss: 0.1286263645
Epoch:   500  |  train loss: 0.1311482668
Epoch:   600  |  train loss: 0.1331296444
Epoch:   700  |  train loss: 0.1356915325
Epoch:   800  |  train loss: 0.1379723310
Epoch:   900  |  train loss: 0.1371442884
Epoch:  1000  |  train loss: 0.1374581099
Processing class: 43
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.1178178445
Epoch:   200  |  train loss: 0.1187105432
Epoch:   300  |  train loss: 0.1274963021
Epoch:   400  |  train loss: 0.1337120116
Epoch:   500  |  train loss: 0.1356727779
Epoch:   600  |  train loss: 0.1343941987
Epoch:   700  |  train loss: 0.1370858312
Epoch:   800  |  train loss: 0.1362306029
Epoch:   900  |  train loss: 0.1335631698
Epoch:  1000  |  train loss: 0.1380044192
Processing class: 44
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.1161627874
Epoch:   200  |  train loss: 0.1223027065
Epoch:   300  |  train loss: 0.1213415951
Epoch:   400  |  train loss: 0.1235595450
Epoch:   500  |  train loss: 0.1259701610
Epoch:   600  |  train loss: 0.1251495451
Epoch:   700  |  train loss: 0.1285899207
Epoch:   800  |  train loss: 0.1282035500
Epoch:   900  |  train loss: 0.1275348470
Epoch:  1000  |  train loss: 0.1313403428
Processing class: 45
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.1225224927
Epoch:   200  |  train loss: 0.1294357747
Epoch:   300  |  train loss: 0.1312431365
Epoch:   400  |  train loss: 0.1344857216
Epoch:   500  |  train loss: 0.1341531366
Epoch:   600  |  train loss: 0.1395361811
Epoch:   700  |  train loss: 0.1412422508
Epoch:   800  |  train loss: 0.1403287351
Epoch:   900  |  train loss: 0.1409488708
Epoch:  1000  |  train loss: 0.1438697845
Processing class: 46
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.1254779398
Epoch:   200  |  train loss: 0.1271436095
Epoch:   300  |  train loss: 0.1310599774
Epoch:   400  |  train loss: 0.1326771617
Epoch:   500  |  train loss: 0.1334961921
Epoch:   600  |  train loss: 0.1336629212
Epoch:   700  |  train loss: 0.1345648080
Epoch:   800  |  train loss: 0.1389539242
Epoch:   900  |  train loss: 0.1372664779
Epoch:  1000  |  train loss: 0.1391576648
Processing class: 47
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.1164574370
Epoch:   200  |  train loss: 0.1209946737
Epoch:   300  |  train loss: 0.1213804677
Epoch:   400  |  train loss: 0.1259877175
Epoch:   500  |  train loss: 0.1268774867
Epoch:   600  |  train loss: 0.1278423652
Epoch:   700  |  train loss: 0.1300090611
Epoch:   800  |  train loss: 0.1288119495
Epoch:   900  |  train loss: 0.1305459082
Epoch:  1000  |  train loss: 0.1301005945
Processing class: 48
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.1191955298
Epoch:   200  |  train loss: 0.1241611242
Epoch:   300  |  train loss: 0.1257243499
Epoch:   400  |  train loss: 0.1237025321
Epoch:   500  |  train loss: 0.1282410532
Epoch:   600  |  train loss: 0.1281170204
Epoch:   700  |  train loss: 0.1268297702
Epoch:   800  |  train loss: 0.1269098595
Epoch:   900  |  train loss: 0.1315884560
Epoch:  1000  |  train loss: 0.1333845615
Processing class: 49
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.1180442244
Epoch:   200  |  train loss: 0.1201469928
Epoch:   300  |  train loss: 0.1274131253
Epoch:   400  |  train loss: 0.1292873129
Epoch:   500  |  train loss: 0.1318495244
Epoch:   600  |  train loss: 0.1324569523
Epoch:   700  |  train loss: 0.1331512660
Epoch:   800  |  train loss: 0.1328108728
Epoch:   900  |  train loss: 0.1326536357
Epoch:  1000  |  train loss: 0.1336930275
/home/z1165703/FeCAM/models/base.py:87: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  vec = torch.tensor(vec).cuda()
2024-03-05 09:06:44,442 [trainer.py] => CNN: {'total': 83.44, '00-09': 87.7, '10-19': 79.6, '20-29': 84.9, '30-39': 81.0, '40-49': 84.0, 'old': 0, 'new': 83.44}
2024-03-05 09:06:44,443 [trainer.py] => No NME accuracy
2024-03-05 09:06:44,443 [trainer.py] => FeCAM: {'total': 82.9, '00-09': 86.2, '10-19': 79.7, '20-29': 85.0, '30-39': 80.6, '40-49': 83.0, 'old': 0, 'new': 82.9}
2024-03-05 09:06:44,443 [trainer.py] => CNN top1 curve: [83.44]
2024-03-05 09:06:44,443 [trainer.py] => CNN top5 curve: [96.5]
2024-03-05 09:06:44,443 [trainer.py] => FeCAM top1 curve: [82.9]
2024-03-05 09:06:44,443 [trainer.py] => FeCAM top5 curve: [95.84]

2024-03-05 09:06:44,456 [fecam.py] => Learning on 50-60
Processing class: 50
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.1128033429
Epoch:   200  |  train loss: 0.1152757809
Epoch:   300  |  train loss: 0.1171874970
Epoch:   400  |  train loss: 0.1211510271
Epoch:   500  |  train loss: 0.1244651511
Epoch:   600  |  train loss: 0.1235951364
Epoch:   700  |  train loss: 0.1280077919
Epoch:   800  |  train loss: 0.1304746956
Epoch:   900  |  train loss: 0.1330161214
Epoch:  1000  |  train loss: 0.1345359355
Processing class: 51
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.1139725074
Epoch:   200  |  train loss: 0.1123623371
Epoch:   300  |  train loss: 0.1182491928
Epoch:   400  |  train loss: 0.1202784285
Epoch:   500  |  train loss: 0.1225430131
Epoch:   600  |  train loss: 0.1258307904
Epoch:   700  |  train loss: 0.1240004763
Epoch:   800  |  train loss: 0.1254779026
Epoch:   900  |  train loss: 0.1300280064
Epoch:  1000  |  train loss: 0.1295504749
Processing class: 52
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.1136411935
Epoch:   200  |  train loss: 0.1131125480
Epoch:   300  |  train loss: 0.1162839264
Epoch:   400  |  train loss: 0.1186080769
Epoch:   500  |  train loss: 0.1190443441
Epoch:   600  |  train loss: 0.1181832790
Epoch:   700  |  train loss: 0.1217635497
Epoch:   800  |  train loss: 0.1225884557
Epoch:   900  |  train loss: 0.1241487190
Epoch:  1000  |  train loss: 0.1225443542
Processing class: 53
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.1132749483
Epoch:   200  |  train loss: 0.1163512468
Epoch:   300  |  train loss: 0.1144059166
Epoch:   400  |  train loss: 0.1167605802
Epoch:   500  |  train loss: 0.1189999506
Epoch:   600  |  train loss: 0.1221004337
Epoch:   700  |  train loss: 0.1258742750
Epoch:   800  |  train loss: 0.1209918082
Epoch:   900  |  train loss: 0.1240828872
Epoch:  1000  |  train loss: 0.1228824675
Processing class: 54
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.1063705802
Epoch:   200  |  train loss: 0.1088184953
Epoch:   300  |  train loss: 0.1135657340
Epoch:   400  |  train loss: 0.1158731893
Epoch:   500  |  train loss: 0.1183005169
Epoch:   600  |  train loss: 0.1226715416
Epoch:   700  |  train loss: 0.1244774491
Epoch:   800  |  train loss: 0.1281003982
Epoch:   900  |  train loss: 0.1307090104
Epoch:  1000  |  train loss: 0.1307093680
Processing class: 55
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.1116126940
Epoch:   200  |  train loss: 0.1125992477
Epoch:   300  |  train loss: 0.1117185503
Epoch:   400  |  train loss: 0.1157584459
Epoch:   500  |  train loss: 0.1210229814
Epoch:   600  |  train loss: 0.1222555771
Epoch:   700  |  train loss: 0.1245284036
Epoch:   800  |  train loss: 0.1285500988
Epoch:   900  |  train loss: 0.1255021214
Epoch:  1000  |  train loss: 0.1285924613
Processing class: 56
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.1117112085
Epoch:   200  |  train loss: 0.1151714623
Epoch:   300  |  train loss: 0.1151479065
Epoch:   400  |  train loss: 0.1206363872
Epoch:   500  |  train loss: 0.1240413770
Epoch:   600  |  train loss: 0.1268437311
Epoch:   700  |  train loss: 0.1299241096
Epoch:   800  |  train loss: 0.1290953964
Epoch:   900  |  train loss: 0.1329591334
Epoch:  1000  |  train loss: 0.1354797721
Processing class: 57
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.1101780623
Epoch:   200  |  train loss: 0.1133287638
Epoch:   300  |  train loss: 0.1128778428
Epoch:   400  |  train loss: 0.1136990711
Epoch:   500  |  train loss: 0.1166894212
Epoch:   600  |  train loss: 0.1145108014
Epoch:   700  |  train loss: 0.1193008274
Epoch:   800  |  train loss: 0.1200729340
Epoch:   900  |  train loss: 0.1229411572
Epoch:  1000  |  train loss: 0.1203240156
Processing class: 58
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.1114994898
Epoch:   200  |  train loss: 0.1134054855
Epoch:   300  |  train loss: 0.1185895428
Epoch:   400  |  train loss: 0.1246283472
Epoch:   500  |  train loss: 0.1248818204
Epoch:   600  |  train loss: 0.1284246027
Epoch:   700  |  train loss: 0.1289107621
Epoch:   800  |  train loss: 0.1317808360
Epoch:   900  |  train loss: 0.1323827267
Epoch:  1000  |  train loss: 0.1334845006
Processing class: 59
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.1142794177
Epoch:   200  |  train loss: 0.1193618491
Epoch:   300  |  train loss: 0.1196938872
Epoch:   400  |  train loss: 0.1233826444
Epoch:   500  |  train loss: 0.1266779289
Epoch:   600  |  train loss: 0.1278480217
Epoch:   700  |  train loss: 0.1302424788
Epoch:   800  |  train loss: 0.1316965789
Epoch:   900  |  train loss: 0.1319794893
Epoch:  1000  |  train loss: 0.1312414408
2024-03-05 09:12:23,573 [trainer.py] => CNN: {'total': 71.75, '00-09': 82.2, '10-19': 72.8, '20-29': 78.5, '30-39': 74.9, '40-49': 68.4, '50-59': 53.7, 'old': 75.36, 'new': 53.7}
2024-03-05 09:12:23,574 [trainer.py] => No NME accuracy
2024-03-05 09:12:23,574 [trainer.py] => FeCAM: {'total': 72.78, '00-09': 82.6, '10-19': 74.0, '20-29': 81.0, '30-39': 76.4, '40-49': 76.8, '50-59': 45.9, 'old': 78.16, 'new': 45.9}
2024-03-05 09:12:23,574 [trainer.py] => CNN top1 curve: [83.44, 71.75]
2024-03-05 09:12:23,574 [trainer.py] => CNN top5 curve: [96.5, 89.6]
2024-03-05 09:12:23,574 [trainer.py] => FeCAM top1 curve: [82.9, 72.78]
2024-03-05 09:12:23,574 [trainer.py] => FeCAM top5 curve: [95.84, 91.18]

2024-03-05 09:12:23,589 [fecam.py] => Learning on 60-70
Processing class: 60
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.1162453756
Epoch:   200  |  train loss: 0.1165538654
Epoch:   300  |  train loss: 0.1160609379
Epoch:   400  |  train loss: 0.1167386845
Epoch:   500  |  train loss: 0.1222070664
Epoch:   600  |  train loss: 0.1234650269
Epoch:   700  |  train loss: 0.1335767001
Epoch:   800  |  train loss: 0.1301744342
Epoch:   900  |  train loss: 0.1318585664
Epoch:  1000  |  train loss: 0.1350762278
Processing class: 61
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.1116804942
Epoch:   200  |  train loss: 0.1112320483
Epoch:   300  |  train loss: 0.1174384624
Epoch:   400  |  train loss: 0.1200639829
Epoch:   500  |  train loss: 0.1260796025
Epoch:   600  |  train loss: 0.1242722467
Epoch:   700  |  train loss: 0.1298662633
Epoch:   800  |  train loss: 0.1333887756
Epoch:   900  |  train loss: 0.1304392129
Epoch:  1000  |  train loss: 0.1326763541
Processing class: 62
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.1138355821
Epoch:   200  |  train loss: 0.1126175627
Epoch:   300  |  train loss: 0.1158261895
Epoch:   400  |  train loss: 0.1174945608
Epoch:   500  |  train loss: 0.1163934365
Epoch:   600  |  train loss: 0.1210257351
Epoch:   700  |  train loss: 0.1226837799
Epoch:   800  |  train loss: 0.1243942007
Epoch:   900  |  train loss: 0.1241236806
Epoch:  1000  |  train loss: 0.1256583527
Processing class: 63
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.1093976915
Epoch:   200  |  train loss: 0.1111118808
Epoch:   300  |  train loss: 0.1187235579
Epoch:   400  |  train loss: 0.1210288048
Epoch:   500  |  train loss: 0.1247307703
Epoch:   600  |  train loss: 0.1292627454
Epoch:   700  |  train loss: 0.1258645609
Epoch:   800  |  train loss: 0.1332510322
Epoch:   900  |  train loss: 0.1298538268
Epoch:  1000  |  train loss: 0.1313955396
Processing class: 64
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.1173879281
Epoch:   200  |  train loss: 0.1183309287
Epoch:   300  |  train loss: 0.1270807683
Epoch:   400  |  train loss: 0.1289481342
Epoch:   500  |  train loss: 0.1317047477
Epoch:   600  |  train loss: 0.1328005880
Epoch:   700  |  train loss: 0.1309446633
Epoch:   800  |  train loss: 0.1333477139
Epoch:   900  |  train loss: 0.1374094695
Epoch:  1000  |  train loss: 0.1374509364
Processing class: 65
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.1137732372
Epoch:   200  |  train loss: 0.1161347568
Epoch:   300  |  train loss: 0.1255696803
Epoch:   400  |  train loss: 0.1300363839
Epoch:   500  |  train loss: 0.1306115836
Epoch:   600  |  train loss: 0.1376586586
Epoch:   700  |  train loss: 0.1388434649
Epoch:   800  |  train loss: 0.1450488180
Epoch:   900  |  train loss: 0.1428488225
Epoch:  1000  |  train loss: 0.1490571529
Processing class: 66
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.1156693697
Epoch:   200  |  train loss: 0.1194658816
Epoch:   300  |  train loss: 0.1239826128
Epoch:   400  |  train loss: 0.1255933359
Epoch:   500  |  train loss: 0.1296157449
Epoch:   600  |  train loss: 0.1309410408
Epoch:   700  |  train loss: 0.1353357583
Epoch:   800  |  train loss: 0.1362686634
Epoch:   900  |  train loss: 0.1370428830
Epoch:  1000  |  train loss: 0.1367765784
Processing class: 67
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.1102224246
Epoch:   200  |  train loss: 0.1126441956
Epoch:   300  |  train loss: 0.1137459069
Epoch:   400  |  train loss: 0.1155479908
Epoch:   500  |  train loss: 0.1167053401
Epoch:   600  |  train loss: 0.1171317860
Epoch:   700  |  train loss: 0.1185102075
Epoch:   800  |  train loss: 0.1214070961
Epoch:   900  |  train loss: 0.1215407759
Epoch:  1000  |  train loss: 0.1219015449
Processing class: 68
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.1105848819
Epoch:   200  |  train loss: 0.1114409789
Epoch:   300  |  train loss: 0.1132485881
Epoch:   400  |  train loss: 0.1158193514
Epoch:   500  |  train loss: 0.1170302048
Epoch:   600  |  train loss: 0.1175324216
Epoch:   700  |  train loss: 0.1197815225
Epoch:   800  |  train loss: 0.1192221239
Epoch:   900  |  train loss: 0.1212466955
Epoch:  1000  |  train loss: 0.1260276273
Processing class: 69
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.1133946240
Epoch:   200  |  train loss: 0.1183098271
Epoch:   300  |  train loss: 0.1269966140
Epoch:   400  |  train loss: 0.1314455181
Epoch:   500  |  train loss: 0.1330207020
Epoch:   600  |  train loss: 0.1296098694
Epoch:   700  |  train loss: 0.1340510130
Epoch:   800  |  train loss: 0.1343218356
Epoch:   900  |  train loss: 0.1396583855
Epoch:  1000  |  train loss: 0.1352040052
2024-03-05 09:18:55,641 [trainer.py] => CNN: {'total': 64.66, '00-09': 75.0, '10-19': 70.1, '20-29': 77.1, '30-39': 70.6, '40-49': 64.0, '50-59': 45.8, '60-69': 50.0, 'old': 67.1, 'new': 50.0}
2024-03-05 09:18:55,642 [trainer.py] => No NME accuracy
2024-03-05 09:18:55,642 [trainer.py] => FeCAM: {'total': 67.27, '00-09': 79.9, '10-19': 70.6, '20-29': 79.1, '30-39': 71.9, '40-49': 74.1, '50-59': 41.2, '60-69': 54.1, 'old': 69.47, 'new': 54.1}
2024-03-05 09:18:55,642 [trainer.py] => CNN top1 curve: [83.44, 71.75, 64.66]
2024-03-05 09:18:55,642 [trainer.py] => CNN top5 curve: [96.5, 89.6, 86.54]
2024-03-05 09:18:55,642 [trainer.py] => FeCAM top1 curve: [82.9, 72.78, 67.27]
2024-03-05 09:18:55,642 [trainer.py] => FeCAM top5 curve: [95.84, 91.18, 87.21]

2024-03-05 09:18:55,647 [fecam.py] => Learning on 70-80
Processing class: 70
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.1111303002
Epoch:   200  |  train loss: 0.1108842343
Epoch:   300  |  train loss: 0.1145936042
Epoch:   400  |  train loss: 0.1173091069
Epoch:   500  |  train loss: 0.1177400768
Epoch:   600  |  train loss: 0.1215829805
Epoch:   700  |  train loss: 0.1266497746
Epoch:   800  |  train loss: 0.1239511967
Epoch:   900  |  train loss: 0.1275867552
Epoch:  1000  |  train loss: 0.1271816283
Processing class: 71
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.1126006171
Epoch:   200  |  train loss: 0.1146480769
Epoch:   300  |  train loss: 0.1139606431
Epoch:   400  |  train loss: 0.1179081559
Epoch:   500  |  train loss: 0.1210560992
Epoch:   600  |  train loss: 0.1225586057
Epoch:   700  |  train loss: 0.1239784122
Epoch:   800  |  train loss: 0.1257876933
Epoch:   900  |  train loss: 0.1299962938
Epoch:  1000  |  train loss: 0.1301671386
Processing class: 72
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.1107099950
Epoch:   200  |  train loss: 0.1128905609
Epoch:   300  |  train loss: 0.1147600755
Epoch:   400  |  train loss: 0.1160516605
Epoch:   500  |  train loss: 0.1193014100
Epoch:   600  |  train loss: 0.1180837929
Epoch:   700  |  train loss: 0.1200414538
Epoch:   800  |  train loss: 0.1199297994
Epoch:   900  |  train loss: 0.1217359379
Epoch:  1000  |  train loss: 0.1254107609
Processing class: 73
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.1140802547
Epoch:   200  |  train loss: 0.1147972837
Epoch:   300  |  train loss: 0.1207911044
Epoch:   400  |  train loss: 0.1203387618
Epoch:   500  |  train loss: 0.1246832430
Epoch:   600  |  train loss: 0.1251146674
Epoch:   700  |  train loss: 0.1240074277
Epoch:   800  |  train loss: 0.1244452327
Epoch:   900  |  train loss: 0.1256457388
Epoch:  1000  |  train loss: 0.1281152621
Processing class: 74
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.1149951756
Epoch:   200  |  train loss: 0.1170855328
Epoch:   300  |  train loss: 0.1150368854
Epoch:   400  |  train loss: 0.1148225263
Epoch:   500  |  train loss: 0.1176433668
Epoch:   600  |  train loss: 0.1175241068
Epoch:   700  |  train loss: 0.1223084196
Epoch:   800  |  train loss: 0.1194445342
Epoch:   900  |  train loss: 0.1205949381
Epoch:  1000  |  train loss: 0.1248169422
Processing class: 75
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.1112570360
Epoch:   200  |  train loss: 0.1116567567
Epoch:   300  |  train loss: 0.1167125046
Epoch:   400  |  train loss: 0.1175614819
Epoch:   500  |  train loss: 0.1211941451
Epoch:   600  |  train loss: 0.1247219294
Epoch:   700  |  train loss: 0.1190100908
Epoch:   800  |  train loss: 0.1238111168
Epoch:   900  |  train loss: 0.1241538882
Epoch:  1000  |  train loss: 0.1279067367
Processing class: 76
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.1099550664
Epoch:   200  |  train loss: 0.1156230509
Epoch:   300  |  train loss: 0.1230344981
Epoch:   400  |  train loss: 0.1281225726
Epoch:   500  |  train loss: 0.1303409189
Epoch:   600  |  train loss: 0.1341043353
Epoch:   700  |  train loss: 0.1350578070
Epoch:   800  |  train loss: 0.1322184697
Epoch:   900  |  train loss: 0.1372089565
Epoch:  1000  |  train loss: 0.1395880282
Processing class: 77
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.1129810661
Epoch:   200  |  train loss: 0.1149543688
Epoch:   300  |  train loss: 0.1151219696
Epoch:   400  |  train loss: 0.1182063386
Epoch:   500  |  train loss: 0.1212154761
Epoch:   600  |  train loss: 0.1231397778
Epoch:   700  |  train loss: 0.1254079118
Epoch:   800  |  train loss: 0.1295294300
Epoch:   900  |  train loss: 0.1289000273
Epoch:  1000  |  train loss: 0.1294795483
Processing class: 78
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.1144691750
Epoch:   200  |  train loss: 0.1195032105
Epoch:   300  |  train loss: 0.1239294231
Epoch:   400  |  train loss: 0.1252833515
Epoch:   500  |  train loss: 0.1263201490
Epoch:   600  |  train loss: 0.1299083471
Epoch:   700  |  train loss: 0.1319005296
Epoch:   800  |  train loss: 0.1339112878
Epoch:   900  |  train loss: 0.1350595266
Epoch:  1000  |  train loss: 0.1347489655
Processing class: 79
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.1125634745
Epoch:   200  |  train loss: 0.1111925215
Epoch:   300  |  train loss: 0.1149591237
Epoch:   400  |  train loss: 0.1164750159
Epoch:   500  |  train loss: 0.1170918897
Epoch:   600  |  train loss: 0.1214950100
Epoch:   700  |  train loss: 0.1221007541
Epoch:   800  |  train loss: 0.1245805755
Epoch:   900  |  train loss: 0.1279988900
Epoch:  1000  |  train loss: 0.1266473472
2024-03-05 09:26:41,710 [trainer.py] => CNN: {'total': 59.18, '00-09': 73.6, '10-19': 68.4, '20-29': 76.9, '30-39': 69.1, '40-49': 60.6, '50-59': 37.2, '60-69': 45.7, '70-79': 41.9, 'old': 61.64, 'new': 41.9}
2024-03-05 09:26:41,711 [trainer.py] => No NME accuracy
2024-03-05 09:26:41,711 [trainer.py] => FeCAM: {'total': 61.72, '00-09': 78.1, '10-19': 69.3, '20-29': 78.0, '30-39': 69.5, '40-49': 72.1, '50-59': 35.7, '60-69': 49.1, '70-79': 42.0, 'old': 64.54, 'new': 42.0}
2024-03-05 09:26:41,711 [trainer.py] => CNN top1 curve: [83.44, 71.75, 64.66, 59.18]
2024-03-05 09:26:41,711 [trainer.py] => CNN top5 curve: [96.5, 89.6, 86.54, 84.09]
2024-03-05 09:26:41,711 [trainer.py] => FeCAM top1 curve: [82.9, 72.78, 67.27, 61.72]
2024-03-05 09:26:41,711 [trainer.py] => FeCAM top5 curve: [95.84, 91.18, 87.21, 84.09]

2024-03-05 09:26:41,715 [fecam.py] => Learning on 80-90
Processing class: 80
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.1127693579
Epoch:   200  |  train loss: 0.1241531700
Epoch:   300  |  train loss: 0.1291989714
Epoch:   400  |  train loss: 0.1321682990
Epoch:   500  |  train loss: 0.1333837569
Epoch:   600  |  train loss: 0.1343106657
Epoch:   700  |  train loss: 0.1324034169
Epoch:   800  |  train loss: 0.1378813356
Epoch:   900  |  train loss: 0.1373432696
Epoch:  1000  |  train loss: 0.1407560468
Processing class: 81
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.1117782995
Epoch:   200  |  train loss: 0.1181584224
Epoch:   300  |  train loss: 0.1175667316
Epoch:   400  |  train loss: 0.1235762462
Epoch:   500  |  train loss: 0.1238737568
Epoch:   600  |  train loss: 0.1279575765
Epoch:   700  |  train loss: 0.1287420154
Epoch:   800  |  train loss: 0.1314060599
Epoch:   900  |  train loss: 0.1329107434
Epoch:  1000  |  train loss: 0.1350706309
Processing class: 82
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.1114647135
Epoch:   200  |  train loss: 0.1164108351
Epoch:   300  |  train loss: 0.1187897637
Epoch:   400  |  train loss: 0.1201245472
Epoch:   500  |  train loss: 0.1230156288
Epoch:   600  |  train loss: 0.1232705504
Epoch:   700  |  train loss: 0.1245624214
Epoch:   800  |  train loss: 0.1257693946
Epoch:   900  |  train loss: 0.1273716152
Epoch:  1000  |  train loss: 0.1282672226
Processing class: 83
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.1142752886
Epoch:   200  |  train loss: 0.1229142502
Epoch:   300  |  train loss: 0.1256610617
Epoch:   400  |  train loss: 0.1296669126
Epoch:   500  |  train loss: 0.1353137314
Epoch:   600  |  train loss: 0.1343139350
Epoch:   700  |  train loss: 0.1391481608
Epoch:   800  |  train loss: 0.1442684829
Epoch:   900  |  train loss: 0.1432730138
Epoch:  1000  |  train loss: 0.1430884689
Processing class: 84
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.1143413946
Epoch:   200  |  train loss: 0.1167320684
Epoch:   300  |  train loss: 0.1220349252
Epoch:   400  |  train loss: 0.1241952345
Epoch:   500  |  train loss: 0.1284158811
Epoch:   600  |  train loss: 0.1267948270
Epoch:   700  |  train loss: 0.1307653189
Epoch:   800  |  train loss: 0.1341440618
Epoch:   900  |  train loss: 0.1317119047
Epoch:  1000  |  train loss: 0.1317317694
Processing class: 85
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.1125248998
Epoch:   200  |  train loss: 0.1143396035
Epoch:   300  |  train loss: 0.1195649177
Epoch:   400  |  train loss: 0.1200736403
Epoch:   500  |  train loss: 0.1235959187
Epoch:   600  |  train loss: 0.1274441421
Epoch:   700  |  train loss: 0.1320535511
Epoch:   800  |  train loss: 0.1298503265
Epoch:   900  |  train loss: 0.1309409708
Epoch:  1000  |  train loss: 0.1349466294
Processing class: 86
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.1148011699
Epoch:   200  |  train loss: 0.1132293016
Epoch:   300  |  train loss: 0.1186467722
Epoch:   400  |  train loss: 0.1191145509
Epoch:   500  |  train loss: 0.1200586706
Epoch:   600  |  train loss: 0.1229951367
Epoch:   700  |  train loss: 0.1240672886
Epoch:   800  |  train loss: 0.1247135863
Epoch:   900  |  train loss: 0.1254249752
Epoch:  1000  |  train loss: 0.1302035809
Processing class: 87
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.1111101761
Epoch:   200  |  train loss: 0.1180778801
Epoch:   300  |  train loss: 0.1202186778
Epoch:   400  |  train loss: 0.1246057972
Epoch:   500  |  train loss: 0.1257787153
Epoch:   600  |  train loss: 0.1283516645
Epoch:   700  |  train loss: 0.1304260820
Epoch:   800  |  train loss: 0.1313492656
Epoch:   900  |  train loss: 0.1305163771
Epoch:  1000  |  train loss: 0.1361351550
Processing class: 88
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.1118491963
Epoch:   200  |  train loss: 0.1121866822
Epoch:   300  |  train loss: 0.1142756864
Epoch:   400  |  train loss: 0.1168163687
Epoch:   500  |  train loss: 0.1183205798
Epoch:   600  |  train loss: 0.1191278875
Epoch:   700  |  train loss: 0.1229070514
Epoch:   800  |  train loss: 0.1245061830
Epoch:   900  |  train loss: 0.1261953741
Epoch:  1000  |  train loss: 0.1252624571
Processing class: 89
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.1138951436
Epoch:   200  |  train loss: 0.1194495276
Epoch:   300  |  train loss: 0.1237469256
Epoch:   400  |  train loss: 0.1259808302
Epoch:   500  |  train loss: 0.1303253502
Epoch:   600  |  train loss: 0.1325183421
Epoch:   700  |  train loss: 0.1306479990
Epoch:   800  |  train loss: 0.1376399815
Epoch:   900  |  train loss: 0.1371649027
Epoch:  1000  |  train loss: 0.1398479402
2024-03-05 09:35:35,917 [trainer.py] => CNN: {'total': 54.08, '00-09': 67.9, '10-19': 63.6, '20-29': 73.2, '30-39': 68.8, '40-49': 57.2, '50-59': 32.6, '60-69': 40.9, '70-79': 38.1, '80-89': 44.4, 'old': 55.29, 'new': 44.4}
2024-03-05 09:35:35,918 [trainer.py] => No NME accuracy
2024-03-05 09:35:35,918 [trainer.py] => FeCAM: {'total': 57.66, '00-09': 76.1, '10-19': 65.8, '20-29': 75.3, '30-39': 67.9, '40-49': 68.5, '50-59': 33.0, '60-69': 45.0, '70-79': 39.0, '80-89': 48.3, 'old': 58.82, 'new': 48.3}
2024-03-05 09:35:35,920 [trainer.py] => CNN top1 curve: [83.44, 71.75, 64.66, 59.18, 54.08]
2024-03-05 09:35:35,920 [trainer.py] => CNN top5 curve: [96.5, 89.6, 86.54, 84.09, 81.74]
2024-03-05 09:35:35,920 [trainer.py] => FeCAM top1 curve: [82.9, 72.78, 67.27, 61.72, 57.66]
2024-03-05 09:35:35,920 [trainer.py] => FeCAM top5 curve: [95.84, 91.18, 87.21, 84.09, 81.63]

2024-03-05 09:35:35,928 [fecam.py] => Learning on 90-100
Processing class: 90
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.1136125281
Epoch:   200  |  train loss: 0.1152499869
Epoch:   300  |  train loss: 0.1144971669
Epoch:   400  |  train loss: 0.1158092946
Epoch:   500  |  train loss: 0.1163973108
Epoch:   600  |  train loss: 0.1195277750
Epoch:   700  |  train loss: 0.1205252320
Epoch:   800  |  train loss: 0.1225570083
Epoch:   900  |  train loss: 0.1249124393
Epoch:  1000  |  train loss: 0.1289246947
Processing class: 91
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.1127786860
Epoch:   200  |  train loss: 0.1147530600
Epoch:   300  |  train loss: 0.1198346287
Epoch:   400  |  train loss: 0.1235025361
Epoch:   500  |  train loss: 0.1280266553
Epoch:   600  |  train loss: 0.1305590123
Epoch:   700  |  train loss: 0.1331376985
Epoch:   800  |  train loss: 0.1362548709
Epoch:   900  |  train loss: 0.1396735966
Epoch:  1000  |  train loss: 0.1408562899
Processing class: 92
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.1165431470
Epoch:   200  |  train loss: 0.1173061222
Epoch:   300  |  train loss: 0.1151966482
Epoch:   400  |  train loss: 0.1186305717
Epoch:   500  |  train loss: 0.1220410332
Epoch:   600  |  train loss: 0.1274117202
Epoch:   700  |  train loss: 0.1287980601
Epoch:   800  |  train loss: 0.1263064727
Epoch:   900  |  train loss: 0.1282787800
Epoch:  1000  |  train loss: 0.1299142689
Processing class: 93
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.1125700191
Epoch:   200  |  train loss: 0.1176014259
Epoch:   300  |  train loss: 0.1268596664
Epoch:   400  |  train loss: 0.1292810693
Epoch:   500  |  train loss: 0.1306100875
Epoch:   600  |  train loss: 0.1302809879
Epoch:   700  |  train loss: 0.1372534364
Epoch:   800  |  train loss: 0.1381198168
Epoch:   900  |  train loss: 0.1397850633
Epoch:  1000  |  train loss: 0.1401550829
Processing class: 94
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.1049988568
Epoch:   200  |  train loss: 0.1064518198
Epoch:   300  |  train loss: 0.1057618454
Epoch:   400  |  train loss: 0.1096759155
Epoch:   500  |  train loss: 0.1061853930
Epoch:   600  |  train loss: 0.1099436387
Epoch:   700  |  train loss: 0.1134773120
Epoch:   800  |  train loss: 0.1086198628
Epoch:   900  |  train loss: 0.1134494081
Epoch:  1000  |  train loss: 0.1129302442
Processing class: 95
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.1108922556
Epoch:   200  |  train loss: 0.1146750927
Epoch:   300  |  train loss: 0.1140993834
Epoch:   400  |  train loss: 0.1146164283
Epoch:   500  |  train loss: 0.1204550236
Epoch:   600  |  train loss: 0.1211390346
Epoch:   700  |  train loss: 0.1239211738
Epoch:   800  |  train loss: 0.1235708311
Epoch:   900  |  train loss: 0.1272557497
Epoch:  1000  |  train loss: 0.1308978140
Processing class: 96
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.1143700629
Epoch:   200  |  train loss: 0.1118963376
Epoch:   300  |  train loss: 0.1167400911
Epoch:   400  |  train loss: 0.1229766026
Epoch:   500  |  train loss: 0.1228563145
Epoch:   600  |  train loss: 0.1233174026
Epoch:   700  |  train loss: 0.1253076151
Epoch:   800  |  train loss: 0.1245035395
Epoch:   900  |  train loss: 0.1307524741
Epoch:  1000  |  train loss: 0.1317162722
Processing class: 97
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.1143804058
Epoch:   200  |  train loss: 0.1178876787
Epoch:   300  |  train loss: 0.1214270920
Epoch:   400  |  train loss: 0.1241136774
Epoch:   500  |  train loss: 0.1290275961
Epoch:   600  |  train loss: 0.1324458182
Epoch:   700  |  train loss: 0.1356433511
Epoch:   800  |  train loss: 0.1377742290
Epoch:   900  |  train loss: 0.1399618953
Epoch:  1000  |  train loss: 0.1403083920
Processing class: 98
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.1122739479
Epoch:   200  |  train loss: 0.1143987656
Epoch:   300  |  train loss: 0.1181890130
Epoch:   400  |  train loss: 0.1219139531
Epoch:   500  |  train loss: 0.1234498054
Epoch:   600  |  train loss: 0.1268404096
Epoch:   700  |  train loss: 0.1249472618
Epoch:   800  |  train loss: 0.1288607523
Epoch:   900  |  train loss: 0.1297493637
Epoch:  1000  |  train loss: 0.1289954156
Processing class: 99
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.1164799795
Epoch:   200  |  train loss: 0.1155435234
Epoch:   300  |  train loss: 0.1235645533
Epoch:   400  |  train loss: 0.1290483236
Epoch:   500  |  train loss: 0.1306216985
Epoch:   600  |  train loss: 0.1317487761
Epoch:   700  |  train loss: 0.1317547590
Epoch:   800  |  train loss: 0.1359781206
Epoch:   900  |  train loss: 0.1359712481
Epoch:  1000  |  train loss: 0.1373423159
2024-03-05 09:45:42,712 [trainer.py] => CNN: {'total': 50.36, '00-09': 57.7, '10-19': 63.6, '20-29': 71.4, '30-39': 68.5, '40-49': 56.8, '50-59': 29.6, '60-69': 38.9, '70-79': 36.6, '80-89': 42.7, '90-99': 37.8, 'old': 51.76, 'new': 37.8}
2024-03-05 09:45:42,714 [trainer.py] => No NME accuracy
2024-03-05 09:45:42,714 [trainer.py] => FeCAM: {'total': 54.74, '00-09': 73.3, '10-19': 65.1, '20-29': 74.1, '30-39': 67.2, '40-49': 67.1, '50-59': 30.8, '60-69': 43.3, '70-79': 37.2, '80-89': 46.9, '90-99': 42.4, 'old': 56.11, 'new': 42.4}
2024-03-05 09:45:42,714 [trainer.py] => CNN top1 curve: [83.44, 71.75, 64.66, 59.18, 54.08, 50.36]
2024-03-05 09:45:42,714 [trainer.py] => CNN top5 curve: [96.5, 89.6, 86.54, 84.09, 81.74, 79.63]
2024-03-05 09:45:42,714 [trainer.py] => FeCAM top1 curve: [82.9, 72.78, 67.27, 61.72, 57.66, 54.74]
2024-03-05 09:45:42,714 [trainer.py] => FeCAM top5 curve: [95.84, 91.18, 87.21, 84.09, 81.63, 79.21]

=========================================
2024-03-05 09:45:51,686 [trainer.py] => config: ./exps/FeCAM_cifar100.json
2024-03-05 09:45:51,686 [trainer.py] => prefix: train
2024-03-05 09:45:51,686 [trainer.py] => dataset: cifar100
2024-03-05 09:45:51,686 [trainer.py] => memory_size: 0
2024-03-05 09:45:51,686 [trainer.py] => shuffle: True
2024-03-05 09:45:51,686 [trainer.py] => init_cls: 50
2024-03-05 09:45:51,686 [trainer.py] => increment: 10
2024-03-05 09:45:51,686 [trainer.py] => model_name: fecam
2024-03-05 09:45:51,686 [trainer.py] => convnet_type: resnet18
2024-03-05 09:45:51,686 [trainer.py] => device: [device(type='cuda', index=0)]
2024-03-05 09:45:51,686 [trainer.py] => seed: 1993
2024-03-05 09:45:51,687 [trainer.py] => init_epochs: 200
2024-03-05 09:45:51,687 [trainer.py] => init_lr: 0.1
2024-03-05 09:45:51,687 [trainer.py] => init_weight_decay: 0.0005
2024-03-05 09:45:51,687 [trainer.py] => batch_size: 128
2024-03-05 09:45:51,687 [trainer.py] => num_workers: 8
2024-03-05 09:45:51,687 [trainer.py] => T: 5
2024-03-05 09:45:51,687 [trainer.py] => beta: 0.5
2024-03-05 09:45:51,687 [trainer.py] => alpha1: 1
2024-03-05 09:45:51,687 [trainer.py] => alpha2: 1
2024-03-05 09:45:51,687 [trainer.py] => ncm: False
2024-03-05 09:45:51,687 [trainer.py] => tukey: False
2024-03-05 09:45:51,687 [trainer.py] => diagonal: False
2024-03-05 09:45:51,687 [trainer.py] => per_class: True
2024-03-05 09:45:51,687 [trainer.py] => full_cov: True
2024-03-05 09:45:51,687 [trainer.py] => shrink: True
2024-03-05 09:45:51,687 [trainer.py] => norm_cov: False
2024-03-05 09:45:51,687 [trainer.py] => vecnorm: False
2024-03-05 09:45:51,687 [trainer.py] => ae_type: wae
2024-03-05 09:45:51,687 [trainer.py] => epochs: 1000
2024-03-05 09:45:51,687 [trainer.py] => ae_latent_dim: 32
2024-03-05 09:45:51,687 [trainer.py] => wae_sigma: 1
2024-03-05 09:45:51,687 [trainer.py] => wae_C: 1
Files already downloaded and verified
Files already downloaded and verified
2024-03-05 09:45:53,347 [data_manager.py] => [68, 56, 78, 8, 23, 84, 90, 65, 74, 76, 40, 89, 3, 92, 55, 9, 26, 80, 43, 38, 58, 70, 77, 1, 85, 19, 17, 50, 28, 53, 13, 81, 45, 82, 6, 59, 83, 16, 15, 44, 91, 41, 72, 60, 79, 52, 20, 10, 31, 54, 37, 95, 14, 71, 96, 98, 97, 2, 64, 66, 42, 22, 35, 86, 24, 34, 87, 21, 99, 0, 88, 27, 18, 94, 11, 12, 47, 25, 30, 46, 62, 69, 36, 61, 7, 63, 75, 5, 32, 4, 51, 48, 73, 93, 39, 67, 29, 49, 57, 33]
2024-03-05 09:45:53,601 [fecam.py] => Learning on 0-50
/home/z1165703/miniconda3/lib/python3.11/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
/home/z1165703/miniconda3/lib/python3.11/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
Processing class: 0
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.4434878945
Epoch:   200  |  train loss: 0.3704999268
Epoch:   300  |  train loss: 0.3584914625
Epoch:   400  |  train loss: 0.3276735544
Epoch:   500  |  train loss: 0.3140553832
Epoch:   600  |  train loss: 0.3081598461
Epoch:   700  |  train loss: 0.2986107230
Epoch:   800  |  train loss: 0.2951838493
Epoch:   900  |  train loss: 0.2875730872
Epoch:  1000  |  train loss: 0.2807712853
Processing class: 1
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.5246866345
Epoch:   200  |  train loss: 0.5129796326
Epoch:   300  |  train loss: 0.4462374091
Epoch:   400  |  train loss: 0.4146183193
Epoch:   500  |  train loss: 0.3858511031
Epoch:   600  |  train loss: 0.3741986394
Epoch:   700  |  train loss: 0.3593098879
Epoch:   800  |  train loss: 0.3442165256
Epoch:   900  |  train loss: 0.3343266666
Epoch:  1000  |  train loss: 0.3238760591
Processing class: 2
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.5726848602
Epoch:   200  |  train loss: 0.5070764065
Epoch:   300  |  train loss: 0.4388417780
Epoch:   400  |  train loss: 0.3997791350
Epoch:   500  |  train loss: 0.3691041291
Epoch:   600  |  train loss: 0.3403245807
Epoch:   700  |  train loss: 0.3211762667
Epoch:   800  |  train loss: 0.3070094407
Epoch:   900  |  train loss: 0.2927479506
Epoch:  1000  |  train loss: 0.2801022351
Processing class: 3
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.4319666207
Epoch:   200  |  train loss: 0.4322523475
Epoch:   300  |  train loss: 0.3911133349
Epoch:   400  |  train loss: 0.3411432087
Epoch:   500  |  train loss: 0.3180392981
Epoch:   600  |  train loss: 0.3066349983
Epoch:   700  |  train loss: 0.2826218724
Epoch:   800  |  train loss: 0.2719620287
Epoch:   900  |  train loss: 0.2562027872
Epoch:  1000  |  train loss: 0.2430786580
Processing class: 4
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.5247941792
Epoch:   200  |  train loss: 0.5094523072
Epoch:   300  |  train loss: 0.4321887910
Epoch:   400  |  train loss: 0.3966426671
Epoch:   500  |  train loss: 0.3680280983
Epoch:   600  |  train loss: 0.3401025176
Epoch:   700  |  train loss: 0.3285974860
Epoch:   800  |  train loss: 0.3189321220
Epoch:   900  |  train loss: 0.3076422334
Epoch:  1000  |  train loss: 0.2957238138
Processing class: 5
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.5853268981
Epoch:   200  |  train loss: 0.4895821989
Epoch:   300  |  train loss: 0.4509246290
Epoch:   400  |  train loss: 0.3938284218
Epoch:   500  |  train loss: 0.3767919421
Epoch:   600  |  train loss: 0.3553527832
Epoch:   700  |  train loss: 0.3277885199
Epoch:   800  |  train loss: 0.3117850184
Epoch:   900  |  train loss: 0.2981558621
Epoch:  1000  |  train loss: 0.2876760006
Processing class: 6
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.5150260568
Epoch:   200  |  train loss: 0.5118987083
Epoch:   300  |  train loss: 0.4436493695
Epoch:   400  |  train loss: 0.3945584297
Epoch:   500  |  train loss: 0.3721413136
Epoch:   600  |  train loss: 0.3614590228
Epoch:   700  |  train loss: 0.3503899157
Epoch:   800  |  train loss: 0.3332415104
Epoch:   900  |  train loss: 0.3146651208
Epoch:  1000  |  train loss: 0.3046405852
Processing class: 7
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.5809822321
Epoch:   200  |  train loss: 0.5087670445
Epoch:   300  |  train loss: 0.4743030608
Epoch:   400  |  train loss: 0.4191801727
Epoch:   500  |  train loss: 0.3833322525
Epoch:   600  |  train loss: 0.3642477930
Epoch:   700  |  train loss: 0.3452323496
Epoch:   800  |  train loss: 0.3316410661
Epoch:   900  |  train loss: 0.3164496601
Epoch:  1000  |  train loss: 0.3048137844
Processing class: 8
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.5486250997
Epoch:   200  |  train loss: 0.4971416652
Epoch:   300  |  train loss: 0.4159850240
Epoch:   400  |  train loss: 0.3829801559
Epoch:   500  |  train loss: 0.3613615513
Epoch:   600  |  train loss: 0.3350818038
Epoch:   700  |  train loss: 0.3191863775
Epoch:   800  |  train loss: 0.3067206204
Epoch:   900  |  train loss: 0.2953694344
Epoch:  1000  |  train loss: 0.2825979948
Processing class: 9
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.5089144289
Epoch:   200  |  train loss: 0.5030578673
Epoch:   300  |  train loss: 0.4645210028
Epoch:   400  |  train loss: 0.4280286014
Epoch:   500  |  train loss: 0.4084555924
Epoch:   600  |  train loss: 0.3972077608
Epoch:   700  |  train loss: 0.3866134703
Epoch:   800  |  train loss: 0.3760934293
Epoch:   900  |  train loss: 0.3663336873
Epoch:  1000  |  train loss: 0.3552734554
Processing class: 10
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.5829617381
Epoch:   200  |  train loss: 0.5152262747
Epoch:   300  |  train loss: 0.4794611692
Epoch:   400  |  train loss: 0.4428387105
Epoch:   500  |  train loss: 0.4123770654
Epoch:   600  |  train loss: 0.3906721652
Epoch:   700  |  train loss: 0.3711137354
Epoch:   800  |  train loss: 0.3577875376
Epoch:   900  |  train loss: 0.3460662067
Epoch:  1000  |  train loss: 0.3372273088
Processing class: 11
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.5887302756
Epoch:   200  |  train loss: 0.4904381633
Epoch:   300  |  train loss: 0.4592011034
Epoch:   400  |  train loss: 0.4164736688
Epoch:   500  |  train loss: 0.3827860057
Epoch:   600  |  train loss: 0.3651968241
Epoch:   700  |  train loss: 0.3475913882
Epoch:   800  |  train loss: 0.3324079096
Epoch:   900  |  train loss: 0.3191215694
Epoch:  1000  |  train loss: 0.3063116312
Processing class: 12
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.5501145959
Epoch:   200  |  train loss: 0.5043747663
Epoch:   300  |  train loss: 0.4529285610
Epoch:   400  |  train loss: 0.4033087790
Epoch:   500  |  train loss: 0.3744654655
Epoch:   600  |  train loss: 0.3474842012
Epoch:   700  |  train loss: 0.3295794547
Epoch:   800  |  train loss: 0.3191442907
Epoch:   900  |  train loss: 0.3071210206
Epoch:  1000  |  train loss: 0.2973229647
Processing class: 13
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.5258793592
Epoch:   200  |  train loss: 0.4551503599
Epoch:   300  |  train loss: 0.4026619315
Epoch:   400  |  train loss: 0.3631789863
Epoch:   500  |  train loss: 0.3372573614
Epoch:   600  |  train loss: 0.3252285540
Epoch:   700  |  train loss: 0.3108260214
Epoch:   800  |  train loss: 0.3011980832
Epoch:   900  |  train loss: 0.2925356567
Epoch:  1000  |  train loss: 0.2826691389
Processing class: 14
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.6254429221
Epoch:   200  |  train loss: 0.6166346431
Epoch:   300  |  train loss: 0.5471396208
Epoch:   400  |  train loss: 0.4861546218
Epoch:   500  |  train loss: 0.4585872471
Epoch:   600  |  train loss: 0.4319076896
Epoch:   700  |  train loss: 0.4141961634
Epoch:   800  |  train loss: 0.3993078828
Epoch:   900  |  train loss: 0.3887589037
Epoch:  1000  |  train loss: 0.3792419553
Processing class: 15
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.4977850676
Epoch:   200  |  train loss: 0.4601240933
Epoch:   300  |  train loss: 0.4022701025
Epoch:   400  |  train loss: 0.3741155267
Epoch:   500  |  train loss: 0.3510797679
Epoch:   600  |  train loss: 0.3386839867
Epoch:   700  |  train loss: 0.3291782856
Epoch:   800  |  train loss: 0.3167467833
Epoch:   900  |  train loss: 0.3117886662
Epoch:  1000  |  train loss: 0.3023372889
Processing class: 16
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.5309730709
Epoch:   200  |  train loss: 0.4403104782
Epoch:   300  |  train loss: 0.4045727313
Epoch:   400  |  train loss: 0.3793518126
Epoch:   500  |  train loss: 0.3697062254
Epoch:   600  |  train loss: 0.3529235065
Epoch:   700  |  train loss: 0.3358779907
Epoch:   800  |  train loss: 0.3237196028
Epoch:   900  |  train loss: 0.3130691111
Epoch:  1000  |  train loss: 0.3084776759
Processing class: 17
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.5291948736
Epoch:   200  |  train loss: 0.5204558969
Epoch:   300  |  train loss: 0.4971329093
Epoch:   400  |  train loss: 0.4460631490
Epoch:   500  |  train loss: 0.3982511818
Epoch:   600  |  train loss: 0.3762351692
Epoch:   700  |  train loss: 0.3616626740
Epoch:   800  |  train loss: 0.3485732436
Epoch:   900  |  train loss: 0.3366812944
Epoch:  1000  |  train loss: 0.3273336589
Processing class: 18
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.5187312543
Epoch:   200  |  train loss: 0.5094865799
Epoch:   300  |  train loss: 0.3924889743
Epoch:   400  |  train loss: 0.3570668399
Epoch:   500  |  train loss: 0.3192242146
Epoch:   600  |  train loss: 0.2956716597
Epoch:   700  |  train loss: 0.2830571592
Epoch:   800  |  train loss: 0.2692781031
Epoch:   900  |  train loss: 0.2592368633
Epoch:  1000  |  train loss: 0.2506381929
Processing class: 19
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.4760565281
Epoch:   200  |  train loss: 0.4287971735
Epoch:   300  |  train loss: 0.3821215570
Epoch:   400  |  train loss: 0.3713580906
Epoch:   500  |  train loss: 0.3375328183
Epoch:   600  |  train loss: 0.3223176241
Epoch:   700  |  train loss: 0.3099003732
Epoch:   800  |  train loss: 0.2953872919
Epoch:   900  |  train loss: 0.2831442684
Epoch:  1000  |  train loss: 0.2734484017
Processing class: 20
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.5170991540
Epoch:   200  |  train loss: 0.4490396500
Epoch:   300  |  train loss: 0.4326389015
Epoch:   400  |  train loss: 0.3837534368
Epoch:   500  |  train loss: 0.3725020289
Epoch:   600  |  train loss: 0.3518733203
Epoch:   700  |  train loss: 0.3331575274
Epoch:   800  |  train loss: 0.3261853993
Epoch:   900  |  train loss: 0.3159090579
Epoch:  1000  |  train loss: 0.3097003341
Processing class: 21
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.5301054835
Epoch:   200  |  train loss: 0.4554714143
Epoch:   300  |  train loss: 0.3633350670
Epoch:   400  |  train loss: 0.3269067645
Epoch:   500  |  train loss: 0.3044266105
Epoch:   600  |  train loss: 0.2908409655
Epoch:   700  |  train loss: 0.2799429297
Epoch:   800  |  train loss: 0.2700456202
Epoch:   900  |  train loss: 0.2592482567
Epoch:  1000  |  train loss: 0.2484477997
Processing class: 22
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.5734120488
Epoch:   200  |  train loss: 0.5631756306
Epoch:   300  |  train loss: 0.5173506856
Epoch:   400  |  train loss: 0.4425034642
Epoch:   500  |  train loss: 0.4159610033
Epoch:   600  |  train loss: 0.3874793112
Epoch:   700  |  train loss: 0.3632564545
Epoch:   800  |  train loss: 0.3456221223
Epoch:   900  |  train loss: 0.3326801956
Epoch:  1000  |  train loss: 0.3230330229
Processing class: 23
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.4935287237
Epoch:   200  |  train loss: 0.4840418100
Epoch:   300  |  train loss: 0.4285842359
Epoch:   400  |  train loss: 0.3914360702
Epoch:   500  |  train loss: 0.3728172719
Epoch:   600  |  train loss: 0.3434228122
Epoch:   700  |  train loss: 0.3248521090
Epoch:   800  |  train loss: 0.3069260955
Epoch:   900  |  train loss: 0.2941316187
Epoch:  1000  |  train loss: 0.2809630990
Processing class: 24
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.5199594200
Epoch:   200  |  train loss: 0.5172791719
Epoch:   300  |  train loss: 0.5108565867
Epoch:   400  |  train loss: 0.4396654367
Epoch:   500  |  train loss: 0.4148978829
Epoch:   600  |  train loss: 0.3935109854
Epoch:   700  |  train loss: 0.3751568496
Epoch:   800  |  train loss: 0.3617587805
Epoch:   900  |  train loss: 0.3469923437
Epoch:  1000  |  train loss: 0.3344116628
Processing class: 25
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.5482532978
Epoch:   200  |  train loss: 0.4938105822
Epoch:   300  |  train loss: 0.4572049856
Epoch:   400  |  train loss: 0.4026448548
Epoch:   500  |  train loss: 0.3699579775
Epoch:   600  |  train loss: 0.3517983079
Epoch:   700  |  train loss: 0.3374261498
Epoch:   800  |  train loss: 0.3234473884
Epoch:   900  |  train loss: 0.3104865074
Epoch:  1000  |  train loss: 0.3037940204
Processing class: 26
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.4883125722
Epoch:   200  |  train loss: 0.4843917310
Epoch:   300  |  train loss: 0.4908005536
Epoch:   400  |  train loss: 0.4438748598
Epoch:   500  |  train loss: 0.4058090687
Epoch:   600  |  train loss: 0.3650584221
Epoch:   700  |  train loss: 0.3519458890
Epoch:   800  |  train loss: 0.3386279047
Epoch:   900  |  train loss: 0.3236732006
Epoch:  1000  |  train loss: 0.3137785017
Processing class: 27
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.5725247502
Epoch:   200  |  train loss: 0.4996161401
Epoch:   300  |  train loss: 0.4740157127
Epoch:   400  |  train loss: 0.4199245989
Epoch:   500  |  train loss: 0.3797602534
Epoch:   600  |  train loss: 0.3650922537
Epoch:   700  |  train loss: 0.3552877426
Epoch:   800  |  train loss: 0.3419740260
Epoch:   900  |  train loss: 0.3249710441
Epoch:  1000  |  train loss: 0.3085551977
Processing class: 28
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.5280232668
Epoch:   200  |  train loss: 0.5166201890
Epoch:   300  |  train loss: 0.4534735620
Epoch:   400  |  train loss: 0.4104145348
Epoch:   500  |  train loss: 0.3769467175
Epoch:   600  |  train loss: 0.3654556692
Epoch:   700  |  train loss: 0.3543006539
Epoch:   800  |  train loss: 0.3412999451
Epoch:   900  |  train loss: 0.3334775627
Epoch:  1000  |  train loss: 0.3232052386
Processing class: 29
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.5332437992
Epoch:   200  |  train loss: 0.4603364289
Epoch:   300  |  train loss: 0.4118249238
Epoch:   400  |  train loss: 0.3680160403
Epoch:   500  |  train loss: 0.3464027166
Epoch:   600  |  train loss: 0.3316218257
Epoch:   700  |  train loss: 0.3113760471
Epoch:   800  |  train loss: 0.2936215341
Epoch:   900  |  train loss: 0.2831611454
Epoch:  1000  |  train loss: 0.2751118958
Processing class: 30
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.4800880909
Epoch:   200  |  train loss: 0.4688731432
Epoch:   300  |  train loss: 0.3839317381
Epoch:   400  |  train loss: 0.3644143224
Epoch:   500  |  train loss: 0.3588616550
Epoch:   600  |  train loss: 0.3421530187
Epoch:   700  |  train loss: 0.3224479198
Epoch:   800  |  train loss: 0.3096802294
Epoch:   900  |  train loss: 0.2993242562
Epoch:  1000  |  train loss: 0.2927664101
Processing class: 31
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.5407607675
Epoch:   200  |  train loss: 0.4723697186
Epoch:   300  |  train loss: 0.4235471785
Epoch:   400  |  train loss: 0.4128379047
Epoch:   500  |  train loss: 0.3824459672
Epoch:   600  |  train loss: 0.3675243676
Epoch:   700  |  train loss: 0.3530201197
Epoch:   800  |  train loss: 0.3432079911
Epoch:   900  |  train loss: 0.3283341587
Epoch:  1000  |  train loss: 0.3174445391
Processing class: 32
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.6000739217
Epoch:   200  |  train loss: 0.5684056759
Epoch:   300  |  train loss: 0.5175035477
Epoch:   400  |  train loss: 0.4697969735
Epoch:   500  |  train loss: 0.4241877675
Epoch:   600  |  train loss: 0.3912339509
Epoch:   700  |  train loss: 0.3671802044
Epoch:   800  |  train loss: 0.3535441041
Epoch:   900  |  train loss: 0.3407159984
Epoch:  1000  |  train loss: 0.3302647233
Processing class: 33
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.5508871198
Epoch:   200  |  train loss: 0.5018716633
Epoch:   300  |  train loss: 0.4317299306
Epoch:   400  |  train loss: 0.3893085063
Epoch:   500  |  train loss: 0.3669935286
Epoch:   600  |  train loss: 0.3417444944
Epoch:   700  |  train loss: 0.3282787621
Epoch:   800  |  train loss: 0.3164315164
Epoch:   900  |  train loss: 0.3046194434
Epoch:  1000  |  train loss: 0.2921119511
Processing class: 34
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.5646847010
Epoch:   200  |  train loss: 0.5433590770
Epoch:   300  |  train loss: 0.4532540619
Epoch:   400  |  train loss: 0.4253435731
Epoch:   500  |  train loss: 0.3896922231
Epoch:   600  |  train loss: 0.3619213641
Epoch:   700  |  train loss: 0.3437235773
Epoch:   800  |  train loss: 0.3288755059
Epoch:   900  |  train loss: 0.3157652676
Epoch:  1000  |  train loss: 0.3045035243
Processing class: 35
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.6664573073
Epoch:   200  |  train loss: 0.5443877935
Epoch:   300  |  train loss: 0.4960800231
Epoch:   400  |  train loss: 0.4724360108
Epoch:   500  |  train loss: 0.4385595798
Epoch:   600  |  train loss: 0.4262609541
Epoch:   700  |  train loss: 0.4063331962
Epoch:   800  |  train loss: 0.3901356757
Epoch:   900  |  train loss: 0.3761549532
Epoch:  1000  |  train loss: 0.3635339379
Processing class: 36
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.4710916877
Epoch:   200  |  train loss: 0.4315686524
Epoch:   300  |  train loss: 0.3915687859
Epoch:   400  |  train loss: 0.3766071677
Epoch:   500  |  train loss: 0.3513861954
Epoch:   600  |  train loss: 0.3261037707
Epoch:   700  |  train loss: 0.3076440930
Epoch:   800  |  train loss: 0.2940516293
Epoch:   900  |  train loss: 0.2816326916
Epoch:  1000  |  train loss: 0.2714011133
Processing class: 37
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.5902364612
Epoch:   200  |  train loss: 0.5533442259
Epoch:   300  |  train loss: 0.4906368494
Epoch:   400  |  train loss: 0.4421839714
Epoch:   500  |  train loss: 0.4137058437
Epoch:   600  |  train loss: 0.3932371855
Epoch:   700  |  train loss: 0.3766449392
Epoch:   800  |  train loss: 0.3643566012
Epoch:   900  |  train loss: 0.3480169356
Epoch:  1000  |  train loss: 0.3367840827
Processing class: 38
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.4932227790
Epoch:   200  |  train loss: 0.4797036827
Epoch:   300  |  train loss: 0.4482555330
Epoch:   400  |  train loss: 0.4246010005
Epoch:   500  |  train loss: 0.3891849875
Epoch:   600  |  train loss: 0.3632358432
Epoch:   700  |  train loss: 0.3381879210
Epoch:   800  |  train loss: 0.3288444757
Epoch:   900  |  train loss: 0.3199005485
Epoch:  1000  |  train loss: 0.3126625299
Processing class: 39
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.5421078086
Epoch:   200  |  train loss: 0.5098559380
Epoch:   300  |  train loss: 0.4685359776
Epoch:   400  |  train loss: 0.4313235879
Epoch:   500  |  train loss: 0.3939819932
Epoch:   600  |  train loss: 0.3750456154
Epoch:   700  |  train loss: 0.3563438594
Epoch:   800  |  train loss: 0.3400428414
Epoch:   900  |  train loss: 0.3248197913
Epoch:  1000  |  train loss: 0.3119172752
Processing class: 40
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.5165939689
Epoch:   200  |  train loss: 0.4540647388
Epoch:   300  |  train loss: 0.4312675536
Epoch:   400  |  train loss: 0.4023406565
Epoch:   500  |  train loss: 0.3837166965
Epoch:   600  |  train loss: 0.3511988819
Epoch:   700  |  train loss: 0.3402213752
Epoch:   800  |  train loss: 0.3274003029
Epoch:   900  |  train loss: 0.3177174091
Epoch:  1000  |  train loss: 0.3065247595
Processing class: 41
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.4653301597
Epoch:   200  |  train loss: 0.4679597259
Epoch:   300  |  train loss: 0.3927156985
Epoch:   400  |  train loss: 0.3564383864
Epoch:   500  |  train loss: 0.3379148960
Epoch:   600  |  train loss: 0.3257032156
Epoch:   700  |  train loss: 0.3130076408
Epoch:   800  |  train loss: 0.3031188905
Epoch:   900  |  train loss: 0.2940724611
Epoch:  1000  |  train loss: 0.2870062649
Processing class: 42
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.5676770926
Epoch:   200  |  train loss: 0.4995412588
Epoch:   300  |  train loss: 0.4458748281
Epoch:   400  |  train loss: 0.4053103387
Epoch:   500  |  train loss: 0.3742172539
Epoch:   600  |  train loss: 0.3453338444
Epoch:   700  |  train loss: 0.3283056021
Epoch:   800  |  train loss: 0.3120196342
Epoch:   900  |  train loss: 0.2978516817
Epoch:  1000  |  train loss: 0.2859273314
Processing class: 43
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.5917136788
Epoch:   200  |  train loss: 0.5319973111
Epoch:   300  |  train loss: 0.4603676736
Epoch:   400  |  train loss: 0.4400172532
Epoch:   500  |  train loss: 0.4229697347
Epoch:   600  |  train loss: 0.4162210047
Epoch:   700  |  train loss: 0.4085913837
Epoch:   800  |  train loss: 0.3999712110
Epoch:   900  |  train loss: 0.3889207542
Epoch:  1000  |  train loss: 0.3781362891
Processing class: 44
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.6037265301
Epoch:   200  |  train loss: 0.5144745588
Epoch:   300  |  train loss: 0.4871320844
Epoch:   400  |  train loss: 0.4392325878
Epoch:   500  |  train loss: 0.4179173589
Epoch:   600  |  train loss: 0.4007774413
Epoch:   700  |  train loss: 0.3852503181
Epoch:   800  |  train loss: 0.3703952491
Epoch:   900  |  train loss: 0.3540110528
Epoch:  1000  |  train loss: 0.3405385494
Processing class: 45
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.5209209025
Epoch:   200  |  train loss: 0.4605711877
Epoch:   300  |  train loss: 0.4166640759
Epoch:   400  |  train loss: 0.4195592046
Epoch:   500  |  train loss: 0.4117450595
Epoch:   600  |  train loss: 0.3903598070
Epoch:   700  |  train loss: 0.3732341945
Epoch:   800  |  train loss: 0.3646985292
Epoch:   900  |  train loss: 0.3596155822
Epoch:  1000  |  train loss: 0.3526774585
Processing class: 46
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.4794831395
Epoch:   200  |  train loss: 0.4619073808
Epoch:   300  |  train loss: 0.4086875081
Epoch:   400  |  train loss: 0.4020817637
Epoch:   500  |  train loss: 0.3885028839
Epoch:   600  |  train loss: 0.3784376502
Epoch:   700  |  train loss: 0.3630245447
Epoch:   800  |  train loss: 0.3520608485
Epoch:   900  |  train loss: 0.3404147327
Epoch:  1000  |  train loss: 0.3308994293
Processing class: 47
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.5783547163
Epoch:   200  |  train loss: 0.5169676542
Epoch:   300  |  train loss: 0.4388943017
Epoch:   400  |  train loss: 0.4048961878
Epoch:   500  |  train loss: 0.3815761089
Epoch:   600  |  train loss: 0.3565755069
Epoch:   700  |  train loss: 0.3365059197
Epoch:   800  |  train loss: 0.3225945711
Epoch:   900  |  train loss: 0.3117585599
Epoch:  1000  |  train loss: 0.3034527481
Processing class: 48
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.5623076916
Epoch:   200  |  train loss: 0.5388171256
Epoch:   300  |  train loss: 0.4328139603
Epoch:   400  |  train loss: 0.4011560380
Epoch:   500  |  train loss: 0.3932476819
Epoch:   600  |  train loss: 0.3675915837
Epoch:   700  |  train loss: 0.3563391566
Epoch:   800  |  train loss: 0.3480610430
Epoch:   900  |  train loss: 0.3390508175
Epoch:  1000  |  train loss: 0.3290795386
Processing class: 49
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.5822773457
Epoch:   200  |  train loss: 0.5258137345
Epoch:   300  |  train loss: 0.4624041677
Epoch:   400  |  train loss: 0.4310010552
Epoch:   500  |  train loss: 0.3936222315
Epoch:   600  |  train loss: 0.3754140615
Epoch:   700  |  train loss: 0.3544932783
Epoch:   800  |  train loss: 0.3439755738
Epoch:   900  |  train loss: 0.3325902522
Epoch:  1000  |  train loss: 0.3205670536
/home/z1165703/FeCAM/models/base.py:87: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  vec = torch.tensor(vec).cuda()
2024-03-05 10:03:23,776 [trainer.py] => CNN: {'total': 83.44, '00-09': 87.7, '10-19': 79.6, '20-29': 84.9, '30-39': 81.0, '40-49': 84.0, 'old': 0, 'new': 83.44}
2024-03-05 10:03:23,859 [trainer.py] => No NME accuracy
2024-03-05 10:03:23,859 [trainer.py] => FeCAM: {'total': 82.9, '00-09': 86.2, '10-19': 79.7, '20-29': 85.0, '30-39': 80.6, '40-49': 83.0, 'old': 0, 'new': 82.9}
2024-03-05 10:03:23,859 [trainer.py] => CNN top1 curve: [83.44]
2024-03-05 10:03:23,859 [trainer.py] => CNN top5 curve: [96.5]
2024-03-05 10:03:23,859 [trainer.py] => FeCAM top1 curve: [82.9]
2024-03-05 10:03:23,859 [trainer.py] => FeCAM top5 curve: [95.84]

2024-03-05 10:03:23,872 [fecam.py] => Learning on 50-60
Processing class: 50
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.7649674892
Epoch:   200  |  train loss: 0.6504406214
Epoch:   300  |  train loss: 0.5923657417
Epoch:   400  |  train loss: 0.5366772890
Epoch:   500  |  train loss: 0.4855926692
Epoch:   600  |  train loss: 0.4513110280
Epoch:   700  |  train loss: 0.4247882783
Epoch:   800  |  train loss: 0.4035861790
Epoch:   900  |  train loss: 0.3881299973
Epoch:  1000  |  train loss: 0.3750936151
Processing class: 51
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.7732218385
Epoch:   200  |  train loss: 0.6607279301
Epoch:   300  |  train loss: 0.5883879781
Epoch:   400  |  train loss: 0.5472075462
Epoch:   500  |  train loss: 0.4901573539
Epoch:   600  |  train loss: 0.4476743996
Epoch:   700  |  train loss: 0.4206807792
Epoch:   800  |  train loss: 0.4042145252
Epoch:   900  |  train loss: 0.3886082828
Epoch:  1000  |  train loss: 0.3719512522
Processing class: 52
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.8900705695
Epoch:   200  |  train loss: 0.8319995761
Epoch:   300  |  train loss: 0.7330448508
Epoch:   400  |  train loss: 0.6682837844
Epoch:   500  |  train loss: 0.6171296477
Epoch:   600  |  train loss: 0.5721151233
Epoch:   700  |  train loss: 0.5369404554
Epoch:   800  |  train loss: 0.5083173633
Epoch:   900  |  train loss: 0.4807362437
Epoch:  1000  |  train loss: 0.4572791636
Processing class: 53
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.6438652992
Epoch:   200  |  train loss: 0.5526742458
Epoch:   300  |  train loss: 0.5147383928
Epoch:   400  |  train loss: 0.4718271136
Epoch:   500  |  train loss: 0.4529768646
Epoch:   600  |  train loss: 0.4366555154
Epoch:   700  |  train loss: 0.4182051003
Epoch:   800  |  train loss: 0.4026600659
Epoch:   900  |  train loss: 0.3905552864
Epoch:  1000  |  train loss: 0.3792791784
Processing class: 54
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.5918694615
Epoch:   200  |  train loss: 0.5231625140
Epoch:   300  |  train loss: 0.4583757341
Epoch:   400  |  train loss: 0.4280128181
Epoch:   500  |  train loss: 0.4014008999
Epoch:   600  |  train loss: 0.3802765310
Epoch:   700  |  train loss: 0.3617210150
Epoch:   800  |  train loss: 0.3471239984
Epoch:   900  |  train loss: 0.3349501371
Epoch:  1000  |  train loss: 0.3219963193
Processing class: 55
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.9619550705
Epoch:   200  |  train loss: 0.8991708040
Epoch:   300  |  train loss: 0.8325635314
Epoch:   400  |  train loss: 0.7676772118
Epoch:   500  |  train loss: 0.7094771743
Epoch:   600  |  train loss: 0.6529790401
Epoch:   700  |  train loss: 0.6115261555
Epoch:   800  |  train loss: 0.5756634355
Epoch:   900  |  train loss: 0.5440402269
Epoch:  1000  |  train loss: 0.5183551490
Processing class: 56
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.7371234655
Epoch:   200  |  train loss: 0.5990613580
Epoch:   300  |  train loss: 0.4981336951
Epoch:   400  |  train loss: 0.4419036567
Epoch:   500  |  train loss: 0.4018980145
Epoch:   600  |  train loss: 0.3710719168
Epoch:   700  |  train loss: 0.3491479278
Epoch:   800  |  train loss: 0.3297900736
Epoch:   900  |  train loss: 0.3161733150
Epoch:  1000  |  train loss: 0.3039853573
Processing class: 57
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.9599176049
Epoch:   200  |  train loss: 0.8808971763
Epoch:   300  |  train loss: 0.8125529051
Epoch:   400  |  train loss: 0.7461543202
Epoch:   500  |  train loss: 0.6867360711
Epoch:   600  |  train loss: 0.6482225299
Epoch:   700  |  train loss: 0.6157492399
Epoch:   800  |  train loss: 0.5833929777
Epoch:   900  |  train loss: 0.5527836800
Epoch:  1000  |  train loss: 0.5225562334
Processing class: 58
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.7920479774
Epoch:   200  |  train loss: 0.5892084956
Epoch:   300  |  train loss: 0.5169165969
Epoch:   400  |  train loss: 0.4507607996
Epoch:   500  |  train loss: 0.4206647813
Epoch:   600  |  train loss: 0.3974985123
Epoch:   700  |  train loss: 0.3757275462
Epoch:   800  |  train loss: 0.3589629292
Epoch:   900  |  train loss: 0.3429615021
Epoch:  1000  |  train loss: 0.3288477182
Processing class: 59
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.8800176740
Epoch:   200  |  train loss: 0.7696322680
Epoch:   300  |  train loss: 0.6716680288
Epoch:   400  |  train loss: 0.5970879674
Epoch:   500  |  train loss: 0.5444828868
Epoch:   600  |  train loss: 0.5135548890
Epoch:   700  |  train loss: 0.4863308489
Epoch:   800  |  train loss: 0.4661798596
Epoch:   900  |  train loss: 0.4461968362
Epoch:  1000  |  train loss: 0.4322788119
2024-03-05 10:09:02,038 [trainer.py] => CNN: {'total': 71.75, '00-09': 82.2, '10-19': 72.8, '20-29': 78.5, '30-39': 74.9, '40-49': 68.4, '50-59': 53.7, 'old': 75.36, 'new': 53.7}
2024-03-05 10:09:02,038 [trainer.py] => No NME accuracy
2024-03-05 10:09:02,039 [trainer.py] => FeCAM: {'total': 72.78, '00-09': 82.6, '10-19': 74.0, '20-29': 81.0, '30-39': 76.4, '40-49': 76.8, '50-59': 45.9, 'old': 78.16, 'new': 45.9}
2024-03-05 10:09:02,039 [trainer.py] => CNN top1 curve: [83.44, 71.75]
2024-03-05 10:09:02,039 [trainer.py] => CNN top5 curve: [96.5, 89.6]
2024-03-05 10:09:02,039 [trainer.py] => FeCAM top1 curve: [82.9, 72.78]
2024-03-05 10:09:02,039 [trainer.py] => FeCAM top5 curve: [95.84, 91.18]

2024-03-05 10:09:02,045 [fecam.py] => Learning on 60-70
Processing class: 60
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.8568617702
Epoch:   200  |  train loss: 0.7624401569
Epoch:   300  |  train loss: 0.6788882971
Epoch:   400  |  train loss: 0.6078443527
Epoch:   500  |  train loss: 0.5509865880
Epoch:   600  |  train loss: 0.5069328010
Epoch:   700  |  train loss: 0.4713431537
Epoch:   800  |  train loss: 0.4395796537
Epoch:   900  |  train loss: 0.4191369534
Epoch:  1000  |  train loss: 0.3989953995
Processing class: 61
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.7322500587
Epoch:   200  |  train loss: 0.5969519734
Epoch:   300  |  train loss: 0.5084166706
Epoch:   400  |  train loss: 0.4479730606
Epoch:   500  |  train loss: 0.4127596974
Epoch:   600  |  train loss: 0.3857721925
Epoch:   700  |  train loss: 0.3627798855
Epoch:   800  |  train loss: 0.3425616860
Epoch:   900  |  train loss: 0.3288678467
Epoch:  1000  |  train loss: 0.3163745403
Processing class: 62
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.9616975427
Epoch:   200  |  train loss: 0.8862370729
Epoch:   300  |  train loss: 0.8273329854
Epoch:   400  |  train loss: 0.7807663202
Epoch:   500  |  train loss: 0.7394746065
Epoch:   600  |  train loss: 0.7022690654
Epoch:   700  |  train loss: 0.6650186419
Epoch:   800  |  train loss: 0.6311626077
Epoch:   900  |  train loss: 0.6015792370
Epoch:  1000  |  train loss: 0.5747224212
Processing class: 63
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.8148365617
Epoch:   200  |  train loss: 0.7368084908
Epoch:   300  |  train loss: 0.5905948400
Epoch:   400  |  train loss: 0.5163788617
Epoch:   500  |  train loss: 0.4679584086
Epoch:   600  |  train loss: 0.4356755495
Epoch:   700  |  train loss: 0.4118440270
Epoch:   800  |  train loss: 0.3942597806
Epoch:   900  |  train loss: 0.3761540234
Epoch:  1000  |  train loss: 0.3612620771
Processing class: 64
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.7357915282
Epoch:   200  |  train loss: 0.5428633809
Epoch:   300  |  train loss: 0.4688527286
Epoch:   400  |  train loss: 0.4248592257
Epoch:   500  |  train loss: 0.3825901330
Epoch:   600  |  train loss: 0.3533673346
Epoch:   700  |  train loss: 0.3332978845
Epoch:   800  |  train loss: 0.3175636470
Epoch:   900  |  train loss: 0.3043720305
Epoch:  1000  |  train loss: 0.2899802089
Processing class: 65
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.7368420959
Epoch:   200  |  train loss: 0.5942445159
Epoch:   300  |  train loss: 0.4753677607
Epoch:   400  |  train loss: 0.4257841289
Epoch:   500  |  train loss: 0.3932333231
Epoch:   600  |  train loss: 0.3649333119
Epoch:   700  |  train loss: 0.3421029329
Epoch:   800  |  train loss: 0.3251128435
Epoch:   900  |  train loss: 0.3080837369
Epoch:  1000  |  train loss: 0.2957300246
Processing class: 66
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.8141679883
Epoch:   200  |  train loss: 0.7106378675
Epoch:   300  |  train loss: 0.6234973431
Epoch:   400  |  train loss: 0.5700826645
Epoch:   500  |  train loss: 0.5392191052
Epoch:   600  |  train loss: 0.5117953181
Epoch:   700  |  train loss: 0.4920932174
Epoch:   800  |  train loss: 0.4766624391
Epoch:   900  |  train loss: 0.4608605385
Epoch:  1000  |  train loss: 0.4480792344
Processing class: 67
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.8523747683
Epoch:   200  |  train loss: 0.7452936769
Epoch:   300  |  train loss: 0.6705943227
Epoch:   400  |  train loss: 0.6183086991
Epoch:   500  |  train loss: 0.5806311727
Epoch:   600  |  train loss: 0.5480829000
Epoch:   700  |  train loss: 0.5180536509
Epoch:   800  |  train loss: 0.4951540649
Epoch:   900  |  train loss: 0.4739149511
Epoch:  1000  |  train loss: 0.4567599177
Processing class: 68
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.7208012700
Epoch:   200  |  train loss: 0.6489501357
Epoch:   300  |  train loss: 0.5415480494
Epoch:   400  |  train loss: 0.4992480755
Epoch:   500  |  train loss: 0.4685149133
Epoch:   600  |  train loss: 0.4413662255
Epoch:   700  |  train loss: 0.4207184076
Epoch:   800  |  train loss: 0.4021290362
Epoch:   900  |  train loss: 0.3862994909
Epoch:  1000  |  train loss: 0.3734600008
Processing class: 69
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.4232305050
Epoch:   200  |  train loss: 0.3673943043
Epoch:   300  |  train loss: 0.3190551221
Epoch:   400  |  train loss: 0.2903439939
Epoch:   500  |  train loss: 0.2729983568
Epoch:   600  |  train loss: 0.2634418160
Epoch:   700  |  train loss: 0.2568413466
Epoch:   800  |  train loss: 0.2492100000
Epoch:   900  |  train loss: 0.2452718437
Epoch:  1000  |  train loss: 0.2394012779
2024-03-05 10:15:31,540 [trainer.py] => CNN: {'total': 64.66, '00-09': 75.0, '10-19': 70.1, '20-29': 77.1, '30-39': 70.6, '40-49': 64.0, '50-59': 45.8, '60-69': 50.0, 'old': 67.1, 'new': 50.0}
2024-03-05 10:15:31,540 [trainer.py] => No NME accuracy
2024-03-05 10:15:31,540 [trainer.py] => FeCAM: {'total': 67.27, '00-09': 79.9, '10-19': 70.6, '20-29': 79.1, '30-39': 71.9, '40-49': 74.1, '50-59': 41.2, '60-69': 54.1, 'old': 69.47, 'new': 54.1}
2024-03-05 10:15:31,540 [trainer.py] => CNN top1 curve: [83.44, 71.75, 64.66]
2024-03-05 10:15:31,540 [trainer.py] => CNN top5 curve: [96.5, 89.6, 86.54]
2024-03-05 10:15:31,540 [trainer.py] => FeCAM top1 curve: [82.9, 72.78, 67.27]
2024-03-05 10:15:31,540 [trainer.py] => FeCAM top5 curve: [95.84, 91.18, 87.21]

2024-03-05 10:15:31,544 [fecam.py] => Learning on 70-80
Processing class: 70
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.7760486364
Epoch:   200  |  train loss: 0.6879049540
Epoch:   300  |  train loss: 0.5796018481
Epoch:   400  |  train loss: 0.5268123150
Epoch:   500  |  train loss: 0.4825782776
Epoch:   600  |  train loss: 0.4454274416
Epoch:   700  |  train loss: 0.4129469872
Epoch:   800  |  train loss: 0.3869779587
Epoch:   900  |  train loss: 0.3727467775
Epoch:  1000  |  train loss: 0.3599041700
Processing class: 71
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.8718440533
Epoch:   200  |  train loss: 0.7758996725
Epoch:   300  |  train loss: 0.6508991003
Epoch:   400  |  train loss: 0.5834025025
Epoch:   500  |  train loss: 0.5315672040
Epoch:   600  |  train loss: 0.4915657222
Epoch:   700  |  train loss: 0.4563005745
Epoch:   800  |  train loss: 0.4293638051
Epoch:   900  |  train loss: 0.4055698574
Epoch:  1000  |  train loss: 0.3850979805
Processing class: 72
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.8397357106
Epoch:   200  |  train loss: 0.7460792422
Epoch:   300  |  train loss: 0.6533419132
Epoch:   400  |  train loss: 0.5917672515
Epoch:   500  |  train loss: 0.5526168585
Epoch:   600  |  train loss: 0.5158056259
Epoch:   700  |  train loss: 0.4866189241
Epoch:   800  |  train loss: 0.4598823309
Epoch:   900  |  train loss: 0.4367879033
Epoch:  1000  |  train loss: 0.4186762989
Processing class: 73
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.8151405931
Epoch:   200  |  train loss: 0.7002483368
Epoch:   300  |  train loss: 0.5781026721
Epoch:   400  |  train loss: 0.5324443936
Epoch:   500  |  train loss: 0.5058801830
Epoch:   600  |  train loss: 0.4822495759
Epoch:   700  |  train loss: 0.4604140878
Epoch:   800  |  train loss: 0.4404209554
Epoch:   900  |  train loss: 0.4224819303
Epoch:  1000  |  train loss: 0.4079609215
Processing class: 74
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.9346414328
Epoch:   200  |  train loss: 0.8919852018
Epoch:   300  |  train loss: 0.7946746469
Epoch:   400  |  train loss: 0.7165534377
Epoch:   500  |  train loss: 0.6500909090
Epoch:   600  |  train loss: 0.6030552745
Epoch:   700  |  train loss: 0.5685619593
Epoch:   800  |  train loss: 0.5355851114
Epoch:   900  |  train loss: 0.5096543491
Epoch:  1000  |  train loss: 0.4881602049
Processing class: 75
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.8361283302
Epoch:   200  |  train loss: 0.7432080865
Epoch:   300  |  train loss: 0.6421630740
Epoch:   400  |  train loss: 0.5926426053
Epoch:   500  |  train loss: 0.5486130834
Epoch:   600  |  train loss: 0.5134356260
Epoch:   700  |  train loss: 0.4853363037
Epoch:   800  |  train loss: 0.4633673787
Epoch:   900  |  train loss: 0.4457359374
Epoch:  1000  |  train loss: 0.4285366535
Processing class: 76
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.4786380053
Epoch:   200  |  train loss: 0.3986562073
Epoch:   300  |  train loss: 0.3407386661
Epoch:   400  |  train loss: 0.3146176279
Epoch:   500  |  train loss: 0.3001808703
Epoch:   600  |  train loss: 0.2851134926
Epoch:   700  |  train loss: 0.2773003280
Epoch:   800  |  train loss: 0.2642114103
Epoch:   900  |  train loss: 0.2571541905
Epoch:  1000  |  train loss: 0.2504051507
Processing class: 77
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.7333715320
Epoch:   200  |  train loss: 0.6438679814
Epoch:   300  |  train loss: 0.5566740632
Epoch:   400  |  train loss: 0.5038895547
Epoch:   500  |  train loss: 0.4496286571
Epoch:   600  |  train loss: 0.4084720850
Epoch:   700  |  train loss: 0.3802644849
Epoch:   800  |  train loss: 0.3584544420
Epoch:   900  |  train loss: 0.3421665311
Epoch:  1000  |  train loss: 0.3249582112
Processing class: 78
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.7702044606
Epoch:   200  |  train loss: 0.6168523192
Epoch:   300  |  train loss: 0.5643102407
Epoch:   400  |  train loss: 0.5103283107
Epoch:   500  |  train loss: 0.4622871995
Epoch:   600  |  train loss: 0.4312730491
Epoch:   700  |  train loss: 0.4093674541
Epoch:   800  |  train loss: 0.3887469113
Epoch:   900  |  train loss: 0.3755864143
Epoch:  1000  |  train loss: 0.3626096547
Processing class: 79
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.9615178823
Epoch:   200  |  train loss: 0.8565127730
Epoch:   300  |  train loss: 0.7833823442
Epoch:   400  |  train loss: 0.7115982294
Epoch:   500  |  train loss: 0.6619003773
Epoch:   600  |  train loss: 0.6180194139
Epoch:   700  |  train loss: 0.5805845022
Epoch:   800  |  train loss: 0.5448723078
Epoch:   900  |  train loss: 0.5153726816
Epoch:  1000  |  train loss: 0.4894603550
2024-03-05 10:23:03,667 [trainer.py] => CNN: {'total': 59.18, '00-09': 73.6, '10-19': 68.4, '20-29': 76.9, '30-39': 69.1, '40-49': 60.6, '50-59': 37.2, '60-69': 45.7, '70-79': 41.9, 'old': 61.64, 'new': 41.9}
2024-03-05 10:23:03,668 [trainer.py] => No NME accuracy
2024-03-05 10:23:03,668 [trainer.py] => FeCAM: {'total': 61.72, '00-09': 78.1, '10-19': 69.3, '20-29': 78.0, '30-39': 69.5, '40-49': 72.1, '50-59': 35.7, '60-69': 49.1, '70-79': 42.0, 'old': 64.54, 'new': 42.0}
2024-03-05 10:23:03,668 [trainer.py] => CNN top1 curve: [83.44, 71.75, 64.66, 59.18]
2024-03-05 10:23:03,668 [trainer.py] => CNN top5 curve: [96.5, 89.6, 86.54, 84.09]
2024-03-05 10:23:03,668 [trainer.py] => FeCAM top1 curve: [82.9, 72.78, 67.27, 61.72]
2024-03-05 10:23:03,668 [trainer.py] => FeCAM top5 curve: [95.84, 91.18, 87.21, 84.09]

2024-03-05 10:23:03,674 [fecam.py] => Learning on 80-90
Processing class: 80
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.5644236326
Epoch:   200  |  train loss: 0.4096475840
Epoch:   300  |  train loss: 0.3646926701
Epoch:   400  |  train loss: 0.3367411852
Epoch:   500  |  train loss: 0.3140720427
Epoch:   600  |  train loss: 0.3018897146
Epoch:   700  |  train loss: 0.2890664458
Epoch:   800  |  train loss: 0.2825271547
Epoch:   900  |  train loss: 0.2742275774
Epoch:  1000  |  train loss: 0.2667020023
Processing class: 81
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.7329139590
Epoch:   200  |  train loss: 0.6037338376
Epoch:   300  |  train loss: 0.5396505237
Epoch:   400  |  train loss: 0.4967582464
Epoch:   500  |  train loss: 0.4609564006
Epoch:   600  |  train loss: 0.4352208555
Epoch:   700  |  train loss: 0.4135689676
Epoch:   800  |  train loss: 0.3982749522
Epoch:   900  |  train loss: 0.3796853542
Epoch:  1000  |  train loss: 0.3632252872
Processing class: 82
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.6644457459
Epoch:   200  |  train loss: 0.5441388607
Epoch:   300  |  train loss: 0.4856758416
Epoch:   400  |  train loss: 0.4385206759
Epoch:   500  |  train loss: 0.4017649770
Epoch:   600  |  train loss: 0.3787306666
Epoch:   700  |  train loss: 0.3615142941
Epoch:   800  |  train loss: 0.3460462749
Epoch:   900  |  train loss: 0.3329816163
Epoch:  1000  |  train loss: 0.3203790665
Processing class: 83
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.6100918531
Epoch:   200  |  train loss: 0.4440761507
Epoch:   300  |  train loss: 0.3806396782
Epoch:   400  |  train loss: 0.3384785652
Epoch:   500  |  train loss: 0.3136492252
Epoch:   600  |  train loss: 0.2973696768
Epoch:   700  |  train loss: 0.2869022310
Epoch:   800  |  train loss: 0.2760855615
Epoch:   900  |  train loss: 0.2663054168
Epoch:  1000  |  train loss: 0.2589293808
Processing class: 84
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.8007519603
Epoch:   200  |  train loss: 0.6222326279
Epoch:   300  |  train loss: 0.5452827215
Epoch:   400  |  train loss: 0.4822980106
Epoch:   500  |  train loss: 0.4442752123
Epoch:   600  |  train loss: 0.4163786590
Epoch:   700  |  train loss: 0.3946300685
Epoch:   800  |  train loss: 0.3781061113
Epoch:   900  |  train loss: 0.3605217993
Epoch:  1000  |  train loss: 0.3452035129
Processing class: 85
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.8249564767
Epoch:   200  |  train loss: 0.6517797947
Epoch:   300  |  train loss: 0.5562296987
Epoch:   400  |  train loss: 0.4995035470
Epoch:   500  |  train loss: 0.4653221190
Epoch:   600  |  train loss: 0.4353515565
Epoch:   700  |  train loss: 0.4149810672
Epoch:   800  |  train loss: 0.3963222146
Epoch:   900  |  train loss: 0.3792003572
Epoch:  1000  |  train loss: 0.3654183984
Processing class: 86
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.9505194902
Epoch:   200  |  train loss: 0.8278390884
Epoch:   300  |  train loss: 0.6905811906
Epoch:   400  |  train loss: 0.6150516510
Epoch:   500  |  train loss: 0.5646927595
Epoch:   600  |  train loss: 0.5243144691
Epoch:   700  |  train loss: 0.4919389009
Epoch:   800  |  train loss: 0.4659930170
Epoch:   900  |  train loss: 0.4461244941
Epoch:  1000  |  train loss: 0.4300401926
Processing class: 87
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.6641729712
Epoch:   200  |  train loss: 0.5272226512
Epoch:   300  |  train loss: 0.4841952085
Epoch:   400  |  train loss: 0.4320607424
Epoch:   500  |  train loss: 0.4026053309
Epoch:   600  |  train loss: 0.3868179560
Epoch:   700  |  train loss: 0.3712974072
Epoch:   800  |  train loss: 0.3562427104
Epoch:   900  |  train loss: 0.3431661785
Epoch:  1000  |  train loss: 0.3331928790
Processing class: 88
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.7967614174
Epoch:   200  |  train loss: 0.6934460521
Epoch:   300  |  train loss: 0.6110947490
Epoch:   400  |  train loss: 0.5486505985
Epoch:   500  |  train loss: 0.4941987872
Epoch:   600  |  train loss: 0.4537523448
Epoch:   700  |  train loss: 0.4283099055
Epoch:   800  |  train loss: 0.4069853127
Epoch:   900  |  train loss: 0.3906513631
Epoch:  1000  |  train loss: 0.3737156153
Processing class: 89
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.7488927126
Epoch:   200  |  train loss: 0.6290697575
Epoch:   300  |  train loss: 0.5304724574
Epoch:   400  |  train loss: 0.4859204829
Epoch:   500  |  train loss: 0.4436751604
Epoch:   600  |  train loss: 0.4092178047
Epoch:   700  |  train loss: 0.3841204941
Epoch:   800  |  train loss: 0.3651072741
Epoch:   900  |  train loss: 0.3467469394
Epoch:  1000  |  train loss: 0.3314991474
2024-03-05 10:31:52,870 [trainer.py] => CNN: {'total': 54.08, '00-09': 67.9, '10-19': 63.6, '20-29': 73.2, '30-39': 68.8, '40-49': 57.2, '50-59': 32.6, '60-69': 40.9, '70-79': 38.1, '80-89': 44.4, 'old': 55.29, 'new': 44.4}
2024-03-05 10:31:54,673 [trainer.py] => No NME accuracy
2024-03-05 10:31:54,673 [trainer.py] => FeCAM: {'total': 57.66, '00-09': 76.1, '10-19': 65.8, '20-29': 75.3, '30-39': 67.9, '40-49': 68.5, '50-59': 33.0, '60-69': 45.0, '70-79': 39.0, '80-89': 48.3, 'old': 58.82, 'new': 48.3}
2024-03-05 10:31:54,673 [trainer.py] => CNN top1 curve: [83.44, 71.75, 64.66, 59.18, 54.08]
2024-03-05 10:31:54,673 [trainer.py] => CNN top5 curve: [96.5, 89.6, 86.54, 84.09, 81.74]
2024-03-05 10:31:54,673 [trainer.py] => FeCAM top1 curve: [82.9, 72.78, 67.27, 61.72, 57.66]
2024-03-05 10:31:54,673 [trainer.py] => FeCAM top5 curve: [95.84, 91.18, 87.21, 84.09, 81.63]

2024-03-05 10:31:54,685 [fecam.py] => Learning on 90-100
Processing class: 90
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.8212098002
Epoch:   200  |  train loss: 0.7351971507
Epoch:   300  |  train loss: 0.6257952929
Epoch:   400  |  train loss: 0.5472735047
Epoch:   500  |  train loss: 0.4927172124
Epoch:   600  |  train loss: 0.4485366344
Epoch:   700  |  train loss: 0.4140574276
Epoch:   800  |  train loss: 0.3926677644
Epoch:   900  |  train loss: 0.3765052378
Epoch:  1000  |  train loss: 0.3610298812
Processing class: 91
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.4625874996
Epoch:   200  |  train loss: 0.4166629016
Epoch:   300  |  train loss: 0.3679574609
Epoch:   400  |  train loss: 0.3479105234
Epoch:   500  |  train loss: 0.3216191232
Epoch:   600  |  train loss: 0.3035653710
Epoch:   700  |  train loss: 0.2820068836
Epoch:   800  |  train loss: 0.2650315285
Epoch:   900  |  train loss: 0.2530369580
Epoch:  1000  |  train loss: 0.2456093848
Processing class: 92
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.7230792284
Epoch:   200  |  train loss: 0.5568315744
Epoch:   300  |  train loss: 0.5132132649
Epoch:   400  |  train loss: 0.4733936310
Epoch:   500  |  train loss: 0.4267651439
Epoch:   600  |  train loss: 0.3839846075
Epoch:   700  |  train loss: 0.3571795583
Epoch:   800  |  train loss: 0.3391992927
Epoch:   900  |  train loss: 0.3248410225
Epoch:  1000  |  train loss: 0.3120552301
Processing class: 93
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.7762139440
Epoch:   200  |  train loss: 0.6014680028
Epoch:   300  |  train loss: 0.5159932077
Epoch:   400  |  train loss: 0.4710063875
Epoch:   500  |  train loss: 0.4308889449
Epoch:   600  |  train loss: 0.3983506083
Epoch:   700  |  train loss: 0.3753964305
Epoch:   800  |  train loss: 0.3563887417
Epoch:   900  |  train loss: 0.3407100201
Epoch:  1000  |  train loss: 0.3239718318
Processing class: 94
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.6959572196
Epoch:   200  |  train loss: 0.5965021729
Epoch:   300  |  train loss: 0.4950672150
Epoch:   400  |  train loss: 0.4439086080
Epoch:   500  |  train loss: 0.4092683971
Epoch:   600  |  train loss: 0.3852572501
Epoch:   700  |  train loss: 0.3655294359
Epoch:   800  |  train loss: 0.3451234996
Epoch:   900  |  train loss: 0.3311125934
Epoch:  1000  |  train loss: 0.3175552011
Processing class: 95
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.8646784186
Epoch:   200  |  train loss: 0.7821076989
Epoch:   300  |  train loss: 0.6958633065
Epoch:   400  |  train loss: 0.6119190097
Epoch:   500  |  train loss: 0.5489664674
Epoch:   600  |  train loss: 0.5096917748
Epoch:   700  |  train loss: 0.4753793836
Epoch:   800  |  train loss: 0.4484901607
Epoch:   900  |  train loss: 0.4257843554
Epoch:  1000  |  train loss: 0.4070998132
Processing class: 96
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.8814848661
Epoch:   200  |  train loss: 0.7781563997
Epoch:   300  |  train loss: 0.6633486629
Epoch:   400  |  train loss: 0.6032241940
Epoch:   500  |  train loss: 0.5519876242
Epoch:   600  |  train loss: 0.5160511851
Epoch:   700  |  train loss: 0.4857225120
Epoch:   800  |  train loss: 0.4584015965
Epoch:   900  |  train loss: 0.4322657287
Epoch:  1000  |  train loss: 0.4133579731
Processing class: 97
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.7667319536
Epoch:   200  |  train loss: 0.6065124035
Epoch:   300  |  train loss: 0.5131727815
Epoch:   400  |  train loss: 0.4704500318
Epoch:   500  |  train loss: 0.4333679795
Epoch:   600  |  train loss: 0.4061832547
Epoch:   700  |  train loss: 0.3827014625
Epoch:   800  |  train loss: 0.3652816713
Epoch:   900  |  train loss: 0.3481913149
Epoch:  1000  |  train loss: 0.3324439943
Processing class: 98
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.6518614650
Epoch:   200  |  train loss: 0.5554292321
Epoch:   300  |  train loss: 0.4651766896
Epoch:   400  |  train loss: 0.4177994251
Epoch:   500  |  train loss: 0.3843853593
Epoch:   600  |  train loss: 0.3580975056
Epoch:   700  |  train loss: 0.3395128131
Epoch:   800  |  train loss: 0.3262721896
Epoch:   900  |  train loss: 0.3141756296
Epoch:  1000  |  train loss: 0.3067833722
Processing class: 99
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.8089359879
Epoch:   200  |  train loss: 0.6947038531
Epoch:   300  |  train loss: 0.5682940841
Epoch:   400  |  train loss: 0.5001845717
Epoch:   500  |  train loss: 0.4507952332
Epoch:   600  |  train loss: 0.4205671310
Epoch:   700  |  train loss: 0.3981766343
Epoch:   800  |  train loss: 0.3804697931
Epoch:   900  |  train loss: 0.3642950833
Epoch:  1000  |  train loss: 0.3496010780
2024-03-05 10:42:19,104 [trainer.py] => CNN: {'total': 50.36, '00-09': 57.7, '10-19': 63.6, '20-29': 71.4, '30-39': 68.5, '40-49': 56.8, '50-59': 29.6, '60-69': 38.9, '70-79': 36.6, '80-89': 42.7, '90-99': 37.8, 'old': 51.76, 'new': 37.8}
2024-03-05 10:42:20,030 [trainer.py] => No NME accuracy
2024-03-05 10:42:20,030 [trainer.py] => FeCAM: {'total': 54.74, '00-09': 73.3, '10-19': 65.1, '20-29': 74.1, '30-39': 67.2, '40-49': 67.1, '50-59': 30.8, '60-69': 43.3, '70-79': 37.2, '80-89': 46.9, '90-99': 42.4, 'old': 56.11, 'new': 42.4}
2024-03-05 10:42:20,032 [trainer.py] => CNN top1 curve: [83.44, 71.75, 64.66, 59.18, 54.08, 50.36]
2024-03-05 10:42:20,032 [trainer.py] => CNN top5 curve: [96.5, 89.6, 86.54, 84.09, 81.74, 79.63]
2024-03-05 10:42:20,032 [trainer.py] => FeCAM top1 curve: [82.9, 72.78, 67.27, 61.72, 57.66, 54.74]
2024-03-05 10:42:20,033 [trainer.py] => FeCAM top5 curve: [95.84, 91.18, 87.21, 84.09, 81.63, 79.21]

=========================================
2024-03-05 10:42:37,728 [trainer.py] => config: ./exps/FeCAM_cifar100.json
2024-03-05 10:42:37,728 [trainer.py] => prefix: train
2024-03-05 10:42:37,729 [trainer.py] => dataset: cifar100
2024-03-05 10:42:37,729 [trainer.py] => memory_size: 0
2024-03-05 10:42:37,729 [trainer.py] => shuffle: True
2024-03-05 10:42:37,729 [trainer.py] => init_cls: 50
2024-03-05 10:42:37,729 [trainer.py] => increment: 10
2024-03-05 10:42:37,729 [trainer.py] => model_name: fecam
2024-03-05 10:42:37,729 [trainer.py] => convnet_type: resnet18
2024-03-05 10:42:37,729 [trainer.py] => device: [device(type='cuda', index=0)]
2024-03-05 10:42:37,729 [trainer.py] => seed: 1993
2024-03-05 10:42:37,729 [trainer.py] => init_epochs: 200
2024-03-05 10:42:37,729 [trainer.py] => init_lr: 0.1
2024-03-05 10:42:37,729 [trainer.py] => init_weight_decay: 0.0005
2024-03-05 10:42:37,729 [trainer.py] => batch_size: 128
2024-03-05 10:42:37,729 [trainer.py] => num_workers: 8
2024-03-05 10:42:37,729 [trainer.py] => T: 5
2024-03-05 10:42:37,729 [trainer.py] => beta: 0.5
2024-03-05 10:42:37,729 [trainer.py] => alpha1: 1
2024-03-05 10:42:37,729 [trainer.py] => alpha2: 1
2024-03-05 10:42:37,729 [trainer.py] => ncm: False
2024-03-05 10:42:37,729 [trainer.py] => tukey: False
2024-03-05 10:42:37,729 [trainer.py] => diagonal: False
2024-03-05 10:42:37,729 [trainer.py] => per_class: True
2024-03-05 10:42:37,729 [trainer.py] => full_cov: True
2024-03-05 10:42:37,729 [trainer.py] => shrink: True
2024-03-05 10:42:37,730 [trainer.py] => norm_cov: False
2024-03-05 10:42:37,730 [trainer.py] => vecnorm: False
2024-03-05 10:42:37,730 [trainer.py] => ae_type: wae
2024-03-05 10:42:37,730 [trainer.py] => epochs: 1000
2024-03-05 10:42:37,730 [trainer.py] => ae_latent_dim: 32
2024-03-05 10:42:37,730 [trainer.py] => wae_sigma: 5
2024-03-05 10:42:37,730 [trainer.py] => wae_C: 1
Files already downloaded and verified
Files already downloaded and verified
2024-03-05 10:42:39,400 [data_manager.py] => [68, 56, 78, 8, 23, 84, 90, 65, 74, 76, 40, 89, 3, 92, 55, 9, 26, 80, 43, 38, 58, 70, 77, 1, 85, 19, 17, 50, 28, 53, 13, 81, 45, 82, 6, 59, 83, 16, 15, 44, 91, 41, 72, 60, 79, 52, 20, 10, 31, 54, 37, 95, 14, 71, 96, 98, 97, 2, 64, 66, 42, 22, 35, 86, 24, 34, 87, 21, 99, 0, 88, 27, 18, 94, 11, 12, 47, 25, 30, 46, 62, 69, 36, 61, 7, 63, 75, 5, 32, 4, 51, 48, 73, 93, 39, 67, 29, 49, 57, 33]
2024-03-05 10:42:39,665 [fecam.py] => Learning on 0-50
/home/z1165703/miniconda3/lib/python3.11/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
/home/z1165703/miniconda3/lib/python3.11/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
Processing class: 0
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.5644880772
Epoch:   200  |  train loss: 0.5217736363
Epoch:   300  |  train loss: 0.5197298169
Epoch:   400  |  train loss: 0.5031067073
Epoch:   500  |  train loss: 0.4932003260
Epoch:   600  |  train loss: 0.4901203096
Epoch:   700  |  train loss: 0.4846808016
Epoch:   800  |  train loss: 0.4824366748
Epoch:   900  |  train loss: 0.4757012725
Epoch:  1000  |  train loss: 0.4691341043
Processing class: 1
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.6215434551
Epoch:   200  |  train loss: 0.6133142114
Epoch:   300  |  train loss: 0.5889731288
Epoch:   400  |  train loss: 0.5726539612
Epoch:   500  |  train loss: 0.5522305131
Epoch:   600  |  train loss: 0.5490424871
Epoch:   700  |  train loss: 0.5377809525
Epoch:   800  |  train loss: 0.5297019005
Epoch:   900  |  train loss: 0.5246633530
Epoch:  1000  |  train loss: 0.5137546062
Processing class: 2
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.6534837842
Epoch:   200  |  train loss: 0.6210039139
Epoch:   300  |  train loss: 0.5908524156
Epoch:   400  |  train loss: 0.5632872343
Epoch:   500  |  train loss: 0.5373054862
Epoch:   600  |  train loss: 0.5193548203
Epoch:   700  |  train loss: 0.5026718140
Epoch:   800  |  train loss: 0.4934978664
Epoch:   900  |  train loss: 0.4788733244
Epoch:  1000  |  train loss: 0.4666105747
Processing class: 3
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.5580690145
Epoch:   200  |  train loss: 0.5603909135
Epoch:   300  |  train loss: 0.5436569333
Epoch:   400  |  train loss: 0.5150385916
Epoch:   500  |  train loss: 0.4979513824
Epoch:   600  |  train loss: 0.4930938721
Epoch:   700  |  train loss: 0.4767762005
Epoch:   800  |  train loss: 0.4663900316
Epoch:   900  |  train loss: 0.4537742078
Epoch:  1000  |  train loss: 0.4426770687
Processing class: 4
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.6202483416
Epoch:   200  |  train loss: 0.6087242603
Epoch:   300  |  train loss: 0.5739391208
Epoch:   400  |  train loss: 0.5514255404
Epoch:   500  |  train loss: 0.5346879721
Epoch:   600  |  train loss: 0.5136352897
Epoch:   700  |  train loss: 0.5080730855
Epoch:   800  |  train loss: 0.5010464549
Epoch:   900  |  train loss: 0.4929150403
Epoch:  1000  |  train loss: 0.4855301380
Processing class: 5
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.6615930438
Epoch:   200  |  train loss: 0.6135369182
Epoch:   300  |  train loss: 0.5904839993
Epoch:   400  |  train loss: 0.5598696589
Epoch:   500  |  train loss: 0.5483309507
Epoch:   600  |  train loss: 0.5379127741
Epoch:   700  |  train loss: 0.5191982448
Epoch:   800  |  train loss: 0.5036436319
Epoch:   900  |  train loss: 0.4907573640
Epoch:  1000  |  train loss: 0.4833713055
Processing class: 6
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.6146829724
Epoch:   200  |  train loss: 0.6144945264
Epoch:   300  |  train loss: 0.5839064956
Epoch:   400  |  train loss: 0.5561143637
Epoch:   500  |  train loss: 0.5441676140
Epoch:   600  |  train loss: 0.5357604980
Epoch:   700  |  train loss: 0.5268886805
Epoch:   800  |  train loss: 0.5159337163
Epoch:   900  |  train loss: 0.4996791720
Epoch:  1000  |  train loss: 0.4932391226
Processing class: 7
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.6560686350
Epoch:   200  |  train loss: 0.6237516284
Epoch:   300  |  train loss: 0.6055479765
Epoch:   400  |  train loss: 0.5749435663
Epoch:   500  |  train loss: 0.5523543954
Epoch:   600  |  train loss: 0.5384499431
Epoch:   700  |  train loss: 0.5268598855
Epoch:   800  |  train loss: 0.5191240966
Epoch:   900  |  train loss: 0.5042137146
Epoch:  1000  |  train loss: 0.4958714783
Processing class: 8
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.6356083155
Epoch:   200  |  train loss: 0.6148185015
Epoch:   300  |  train loss: 0.5715265870
Epoch:   400  |  train loss: 0.5549223304
Epoch:   500  |  train loss: 0.5388502359
Epoch:   600  |  train loss: 0.5212124348
Epoch:   700  |  train loss: 0.5121261775
Epoch:   800  |  train loss: 0.4991443157
Epoch:   900  |  train loss: 0.4911719024
Epoch:  1000  |  train loss: 0.4785937786
Processing class: 9
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.6085722566
Epoch:   200  |  train loss: 0.6072178125
Epoch:   300  |  train loss: 0.5959595442
Epoch:   400  |  train loss: 0.5724419475
Epoch:   500  |  train loss: 0.5648497581
Epoch:   600  |  train loss: 0.5558690310
Epoch:   700  |  train loss: 0.5518842578
Epoch:   800  |  train loss: 0.5464891553
Epoch:   900  |  train loss: 0.5404156804
Epoch:  1000  |  train loss: 0.5306004167
Processing class: 10
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.6539460301
Epoch:   200  |  train loss: 0.6197627068
Epoch:   300  |  train loss: 0.6042049050
Epoch:   400  |  train loss: 0.5884284854
Epoch:   500  |  train loss: 0.5638756037
Epoch:   600  |  train loss: 0.5512516975
Epoch:   700  |  train loss: 0.5399394870
Epoch:   800  |  train loss: 0.5283766627
Epoch:   900  |  train loss: 0.5212048411
Epoch:  1000  |  train loss: 0.5136704803
Processing class: 11
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.6629840016
Epoch:   200  |  train loss: 0.6168328643
Epoch:   300  |  train loss: 0.5964706779
Epoch:   400  |  train loss: 0.5757091999
Epoch:   500  |  train loss: 0.5528101444
Epoch:   600  |  train loss: 0.5428486586
Epoch:   700  |  train loss: 0.5340766072
Epoch:   800  |  train loss: 0.5231465042
Epoch:   900  |  train loss: 0.5125035167
Epoch:  1000  |  train loss: 0.5032235682
Processing class: 12
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.6353409886
Epoch:   200  |  train loss: 0.6206403375
Epoch:   300  |  train loss: 0.5937440515
Epoch:   400  |  train loss: 0.5648937106
Epoch:   500  |  train loss: 0.5466690898
Epoch:   600  |  train loss: 0.5268381476
Epoch:   700  |  train loss: 0.5149573267
Epoch:   800  |  train loss: 0.5080054522
Epoch:   900  |  train loss: 0.5002794862
Epoch:  1000  |  train loss: 0.4904136479
Processing class: 13
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.6183615208
Epoch:   200  |  train loss: 0.5851341486
Epoch:   300  |  train loss: 0.5573702693
Epoch:   400  |  train loss: 0.5315541983
Epoch:   500  |  train loss: 0.5152185082
Epoch:   600  |  train loss: 0.5045627952
Epoch:   700  |  train loss: 0.4947541058
Epoch:   800  |  train loss: 0.4892859101
Epoch:   900  |  train loss: 0.4838583708
Epoch:  1000  |  train loss: 0.4767910421
Processing class: 14
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.6848187327
Epoch:   200  |  train loss: 0.6824774265
Epoch:   300  |  train loss: 0.6545903683
Epoch:   400  |  train loss: 0.6273782611
Epoch:   500  |  train loss: 0.6099484563
Epoch:   600  |  train loss: 0.5943235159
Epoch:   700  |  train loss: 0.5861023188
Epoch:   800  |  train loss: 0.5801086426
Epoch:   900  |  train loss: 0.5724219441
Epoch:  1000  |  train loss: 0.5637525320
Processing class: 15
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.6041890502
Epoch:   200  |  train loss: 0.5891991496
Epoch:   300  |  train loss: 0.5523039222
Epoch:   400  |  train loss: 0.5362429857
Epoch:   500  |  train loss: 0.5229754329
Epoch:   600  |  train loss: 0.5115988970
Epoch:   700  |  train loss: 0.5076890290
Epoch:   800  |  train loss: 0.4977414608
Epoch:   900  |  train loss: 0.4966657698
Epoch:  1000  |  train loss: 0.4893324077
Processing class: 16
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.6236296535
Epoch:   200  |  train loss: 0.5795816660
Epoch:   300  |  train loss: 0.5618217826
Epoch:   400  |  train loss: 0.5462073803
Epoch:   500  |  train loss: 0.5423295617
Epoch:   600  |  train loss: 0.5310829401
Epoch:   700  |  train loss: 0.5193154633
Epoch:   800  |  train loss: 0.5135899842
Epoch:   900  |  train loss: 0.5018797994
Epoch:  1000  |  train loss: 0.5012902617
Processing class: 17
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.6175462008
Epoch:   200  |  train loss: 0.6152274728
Epoch:   300  |  train loss: 0.6098009109
Epoch:   400  |  train loss: 0.5870172024
Epoch:   500  |  train loss: 0.5534713984
Epoch:   600  |  train loss: 0.5415140629
Epoch:   700  |  train loss: 0.5307963133
Epoch:   800  |  train loss: 0.5212502956
Epoch:   900  |  train loss: 0.5130155325
Epoch:  1000  |  train loss: 0.5062928498
Processing class: 18
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.6148946166
Epoch:   200  |  train loss: 0.6122722983
Epoch:   300  |  train loss: 0.5526625156
Epoch:   400  |  train loss: 0.5306720376
Epoch:   500  |  train loss: 0.5056168437
Epoch:   600  |  train loss: 0.4846205294
Epoch:   700  |  train loss: 0.4782025337
Epoch:   800  |  train loss: 0.4657190084
Epoch:   900  |  train loss: 0.4579395890
Epoch:  1000  |  train loss: 0.4509825349
Processing class: 19
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.5833422780
Epoch:   200  |  train loss: 0.5640786052
Epoch:   300  |  train loss: 0.5403346539
Epoch:   400  |  train loss: 0.5325541615
Epoch:   500  |  train loss: 0.5093244731
Epoch:   600  |  train loss: 0.5007594407
Epoch:   700  |  train loss: 0.4925729930
Epoch:   800  |  train loss: 0.4805983722
Epoch:   900  |  train loss: 0.4745495081
Epoch:  1000  |  train loss: 0.4672540724
Processing class: 20
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.6168389201
Epoch:   200  |  train loss: 0.5855929494
Epoch:   300  |  train loss: 0.5774533153
Epoch:   400  |  train loss: 0.5517012715
Epoch:   500  |  train loss: 0.5480132222
Epoch:   600  |  train loss: 0.5333824515
Epoch:   700  |  train loss: 0.5209466696
Epoch:   800  |  train loss: 0.5162990272
Epoch:   900  |  train loss: 0.5085488141
Epoch:  1000  |  train loss: 0.5063395441
Processing class: 21
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.6247795463
Epoch:   200  |  train loss: 0.5927610636
Epoch:   300  |  train loss: 0.5396222591
Epoch:   400  |  train loss: 0.5132110775
Epoch:   500  |  train loss: 0.4961915016
Epoch:   600  |  train loss: 0.4862704515
Epoch:   700  |  train loss: 0.4773970306
Epoch:   800  |  train loss: 0.4668000340
Epoch:   900  |  train loss: 0.4591357768
Epoch:  1000  |  train loss: 0.4474924147
Processing class: 22
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.6534794211
Epoch:   200  |  train loss: 0.6481310606
Epoch:   300  |  train loss: 0.6343895435
Epoch:   400  |  train loss: 0.5980483174
Epoch:   500  |  train loss: 0.5811696172
Epoch:   600  |  train loss: 0.5618964195
Epoch:   700  |  train loss: 0.5473359108
Epoch:   800  |  train loss: 0.5327442408
Epoch:   900  |  train loss: 0.5222896695
Epoch:  1000  |  train loss: 0.5190435648
Processing class: 23
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.6028646111
Epoch:   200  |  train loss: 0.6001137018
Epoch:   300  |  train loss: 0.5780307412
Epoch:   400  |  train loss: 0.5547760963
Epoch:   500  |  train loss: 0.5415478945
Epoch:   600  |  train loss: 0.5192899466
Epoch:   700  |  train loss: 0.5063023925
Epoch:   800  |  train loss: 0.4920721948
Epoch:   900  |  train loss: 0.4837653160
Epoch:  1000  |  train loss: 0.4707826436
Processing class: 24
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.6192776442
Epoch:   200  |  train loss: 0.6163491011
Epoch:   300  |  train loss: 0.6153973460
Epoch:   400  |  train loss: 0.5858876109
Epoch:   500  |  train loss: 0.5699971914
Epoch:   600  |  train loss: 0.5592180490
Epoch:   700  |  train loss: 0.5436955214
Epoch:   800  |  train loss: 0.5368602633
Epoch:   900  |  train loss: 0.5291384697
Epoch:  1000  |  train loss: 0.5181496799
Processing class: 25
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.6356345057
Epoch:   200  |  train loss: 0.6107479572
Epoch:   300  |  train loss: 0.5929395556
Epoch:   400  |  train loss: 0.5589359999
Epoch:   500  |  train loss: 0.5394578338
Epoch:   600  |  train loss: 0.5247986436
Epoch:   700  |  train loss: 0.5181929052
Epoch:   800  |  train loss: 0.5062507689
Epoch:   900  |  train loss: 0.4952749789
Epoch:  1000  |  train loss: 0.4907877862
Processing class: 26
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.5994999170
Epoch:   200  |  train loss: 0.5993554950
Epoch:   300  |  train loss: 0.6033026695
Epoch:   400  |  train loss: 0.5859364748
Epoch:   500  |  train loss: 0.5690303683
Epoch:   600  |  train loss: 0.5400509238
Epoch:   700  |  train loss: 0.5318494678
Epoch:   800  |  train loss: 0.5249748111
Epoch:   900  |  train loss: 0.5133093297
Epoch:  1000  |  train loss: 0.5084288597
Processing class: 27
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.6544148445
Epoch:   200  |  train loss: 0.6166831255
Epoch:   300  |  train loss: 0.6060906410
Epoch:   400  |  train loss: 0.5766207337
Epoch:   500  |  train loss: 0.5534439921
Epoch:   600  |  train loss: 0.5444070101
Epoch:   700  |  train loss: 0.5378368020
Epoch:   800  |  train loss: 0.5287713051
Epoch:   900  |  train loss: 0.5184450865
Epoch:  1000  |  train loss: 0.5022040069
Processing class: 28
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.6287330866
Epoch:   200  |  train loss: 0.6239672661
Epoch:   300  |  train loss: 0.5931276560
Epoch:   400  |  train loss: 0.5700145483
Epoch:   500  |  train loss: 0.5521215439
Epoch:   600  |  train loss: 0.5445384741
Epoch:   700  |  train loss: 0.5393651605
Epoch:   800  |  train loss: 0.5324938536
Epoch:   900  |  train loss: 0.5273926139
Epoch:  1000  |  train loss: 0.5189224124
Processing class: 29
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.6289829373
Epoch:   200  |  train loss: 0.5953696370
Epoch:   300  |  train loss: 0.5666107416
Epoch:   400  |  train loss: 0.5370221376
Epoch:   500  |  train loss: 0.5236285925
Epoch:   600  |  train loss: 0.5124961913
Epoch:   700  |  train loss: 0.4972377956
Epoch:   800  |  train loss: 0.4816215098
Epoch:   900  |  train loss: 0.4730839372
Epoch:  1000  |  train loss: 0.4649422407
Processing class: 30
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.5959983230
Epoch:   200  |  train loss: 0.5925791979
Epoch:   300  |  train loss: 0.5534291506
Epoch:   400  |  train loss: 0.5445440292
Epoch:   500  |  train loss: 0.5400785327
Epoch:   600  |  train loss: 0.5289606988
Epoch:   700  |  train loss: 0.5137914300
Epoch:   800  |  train loss: 0.5070036888
Epoch:   900  |  train loss: 0.5006741226
Epoch:  1000  |  train loss: 0.4924625099
Processing class: 31
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.6343293905
Epoch:   200  |  train loss: 0.6039921522
Epoch:   300  |  train loss: 0.5774440169
Epoch:   400  |  train loss: 0.5729553699
Epoch:   500  |  train loss: 0.5571065426
Epoch:   600  |  train loss: 0.5495877028
Epoch:   700  |  train loss: 0.5384283423
Epoch:   800  |  train loss: 0.5323425055
Epoch:   900  |  train loss: 0.5240720034
Epoch:  1000  |  train loss: 0.5137317955
Processing class: 32
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.6687849402
Epoch:   200  |  train loss: 0.6597457886
Epoch:   300  |  train loss: 0.6362026095
Epoch:   400  |  train loss: 0.6085781336
Epoch:   500  |  train loss: 0.5869860530
Epoch:   600  |  train loss: 0.5620694399
Epoch:   700  |  train loss: 0.5487736106
Epoch:   800  |  train loss: 0.5393302858
Epoch:   900  |  train loss: 0.5288707793
Epoch:  1000  |  train loss: 0.5195610285
Processing class: 33
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.6352361798
Epoch:   200  |  train loss: 0.6262532353
Epoch:   300  |  train loss: 0.5835142374
Epoch:   400  |  train loss: 0.5569513917
Epoch:   500  |  train loss: 0.5401969790
Epoch:   600  |  train loss: 0.5262060404
Epoch:   700  |  train loss: 0.5137255430
Epoch:   800  |  train loss: 0.5050569475
Epoch:   900  |  train loss: 0.4949449480
Epoch:  1000  |  train loss: 0.4841184855
Processing class: 34
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.6472009182
Epoch:   200  |  train loss: 0.6357606411
Epoch:   300  |  train loss: 0.5971193314
Epoch:   400  |  train loss: 0.5795529246
Epoch:   500  |  train loss: 0.5578688264
Epoch:   600  |  train loss: 0.5425141454
Epoch:   700  |  train loss: 0.5266949773
Epoch:   800  |  train loss: 0.5184383571
Epoch:   900  |  train loss: 0.5094090939
Epoch:  1000  |  train loss: 0.5005524039
Processing class: 35
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.7041050792
Epoch:   200  |  train loss: 0.6517092466
Epoch:   300  |  train loss: 0.6309208035
Epoch:   400  |  train loss: 0.6175266981
Epoch:   500  |  train loss: 0.5970445633
Epoch:   600  |  train loss: 0.5917353868
Epoch:   700  |  train loss: 0.5798531413
Epoch:   800  |  train loss: 0.5688256264
Epoch:   900  |  train loss: 0.5602801561
Epoch:  1000  |  train loss: 0.5500762105
Processing class: 36
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.5785141587
Epoch:   200  |  train loss: 0.5602594733
Epoch:   300  |  train loss: 0.5391984463
Epoch:   400  |  train loss: 0.5296792507
Epoch:   500  |  train loss: 0.5162409067
Epoch:   600  |  train loss: 0.4982695878
Epoch:   700  |  train loss: 0.4830089152
Epoch:   800  |  train loss: 0.4755274117
Epoch:   900  |  train loss: 0.4622897089
Epoch:  1000  |  train loss: 0.4550893068
Processing class: 37
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.6677315354
Epoch:   200  |  train loss: 0.6515692830
Epoch:   300  |  train loss: 0.6195398450
Epoch:   400  |  train loss: 0.5953009367
Epoch:   500  |  train loss: 0.5792427540
Epoch:   600  |  train loss: 0.5635786295
Epoch:   700  |  train loss: 0.5517408609
Epoch:   800  |  train loss: 0.5443752885
Epoch:   900  |  train loss: 0.5302501440
Epoch:  1000  |  train loss: 0.5233793855
Processing class: 38
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.5997576118
Epoch:   200  |  train loss: 0.5948178649
Epoch:   300  |  train loss: 0.5817380786
Epoch:   400  |  train loss: 0.5729235530
Epoch:   500  |  train loss: 0.5519995689
Epoch:   600  |  train loss: 0.5332373738
Epoch:   700  |  train loss: 0.5159541667
Epoch:   800  |  train loss: 0.5117343664
Epoch:   900  |  train loss: 0.5018490493
Epoch:  1000  |  train loss: 0.4981919289
Processing class: 39
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.6299023390
Epoch:   200  |  train loss: 0.6141342282
Epoch:   300  |  train loss: 0.5960549593
Epoch:   400  |  train loss: 0.5775700450
Epoch:   500  |  train loss: 0.5556683421
Epoch:   600  |  train loss: 0.5445831895
Epoch:   700  |  train loss: 0.5324659824
Epoch:   800  |  train loss: 0.5221617460
Epoch:   900  |  train loss: 0.5105062962
Epoch:  1000  |  train loss: 0.5008803487
Processing class: 40
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.6152735233
Epoch:   200  |  train loss: 0.5785913944
Epoch:   300  |  train loss: 0.5711783648
Epoch:   400  |  train loss: 0.5570416689
Epoch:   500  |  train loss: 0.5459408760
Epoch:   600  |  train loss: 0.5229336500
Epoch:   700  |  train loss: 0.5172949731
Epoch:   800  |  train loss: 0.5085981011
Epoch:   900  |  train loss: 0.5007085085
Epoch:  1000  |  train loss: 0.4966546655
Processing class: 41
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.5798730850
Epoch:   200  |  train loss: 0.5825396419
Epoch:   300  |  train loss: 0.5442887068
Epoch:   400  |  train loss: 0.5183525622
Epoch:   500  |  train loss: 0.5050463140
Epoch:   600  |  train loss: 0.4956413150
Epoch:   700  |  train loss: 0.4884648800
Epoch:   800  |  train loss: 0.4800756812
Epoch:   900  |  train loss: 0.4727627993
Epoch:  1000  |  train loss: 0.4667992532
Processing class: 42
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.6486713290
Epoch:   200  |  train loss: 0.6215689540
Epoch:   300  |  train loss: 0.5945792317
Epoch:   400  |  train loss: 0.5693807840
Epoch:   500  |  train loss: 0.5498972893
Epoch:   600  |  train loss: 0.5300970078
Epoch:   700  |  train loss: 0.5178026080
Epoch:   800  |  train loss: 0.5046184599
Epoch:   900  |  train loss: 0.4913733125
Epoch:  1000  |  train loss: 0.4812846541
Processing class: 43
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.6650904536
Epoch:   200  |  train loss: 0.6390930772
Epoch:   300  |  train loss: 0.6007616639
Epoch:   400  |  train loss: 0.5919135809
Epoch:   500  |  train loss: 0.5829904675
Epoch:   600  |  train loss: 0.5785045743
Epoch:   700  |  train loss: 0.5759787083
Epoch:   800  |  train loss: 0.5699631810
Epoch:   900  |  train loss: 0.5596966386
Epoch:  1000  |  train loss: 0.5557314754
Processing class: 44
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.6683108091
Epoch:   200  |  train loss: 0.6298758984
Epoch:   300  |  train loss: 0.6147116303
Epoch:   400  |  train loss: 0.5896165133
Epoch:   500  |  train loss: 0.5781392813
Epoch:   600  |  train loss: 0.5675069928
Epoch:   700  |  train loss: 0.5597388148
Epoch:   800  |  train loss: 0.5489402533
Epoch:   900  |  train loss: 0.5367544293
Epoch:  1000  |  train loss: 0.5291368604
Processing class: 45
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.6193549037
Epoch:   200  |  train loss: 0.5968344450
Epoch:   300  |  train loss: 0.5665344834
Epoch:   400  |  train loss: 0.5719199538
Epoch:   500  |  train loss: 0.5666525006
Epoch:   600  |  train loss: 0.5582837701
Epoch:   700  |  train loss: 0.5479038715
Epoch:   800  |  train loss: 0.5417527199
Epoch:   900  |  train loss: 0.5398006320
Epoch:  1000  |  train loss: 0.5360662937
Processing class: 46
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.5919717431
Epoch:   200  |  train loss: 0.5787896991
Epoch:   300  |  train loss: 0.5522526026
Epoch:   400  |  train loss: 0.5501953721
Epoch:   500  |  train loss: 0.5437359929
Epoch:   600  |  train loss: 0.5394681573
Epoch:   700  |  train loss: 0.5327484846
Epoch:   800  |  train loss: 0.5275402904
Epoch:   900  |  train loss: 0.5178531826
Epoch:  1000  |  train loss: 0.5137348354
Processing class: 47
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.6558199763
Epoch:   200  |  train loss: 0.6308846831
Epoch:   300  |  train loss: 0.5877569675
Epoch:   400  |  train loss: 0.5680044293
Epoch:   500  |  train loss: 0.5515771627
Epoch:   600  |  train loss: 0.5332753956
Epoch:   700  |  train loss: 0.5191942811
Epoch:   800  |  train loss: 0.5063753426
Epoch:   900  |  train loss: 0.4976411819
Epoch:  1000  |  train loss: 0.4893054903
Processing class: 48
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.6460379839
Epoch:   200  |  train loss: 0.6358816147
Epoch:   300  |  train loss: 0.5825903893
Epoch:   400  |  train loss: 0.5611554861
Epoch:   500  |  train loss: 0.5610957503
Epoch:   600  |  train loss: 0.5458574533
Epoch:   700  |  train loss: 0.5357239366
Epoch:   800  |  train loss: 0.5303353310
Epoch:   900  |  train loss: 0.5283292770
Epoch:  1000  |  train loss: 0.5214258909
Processing class: 49
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.6579794884
Epoch:   200  |  train loss: 0.6349208832
Epoch:   300  |  train loss: 0.6062932968
Epoch:   400  |  train loss: 0.5874407649
Epoch:   500  |  train loss: 0.5667115927
Epoch:   600  |  train loss: 0.5558393478
Epoch:   700  |  train loss: 0.5420749307
Epoch:   800  |  train loss: 0.5330638885
Epoch:   900  |  train loss: 0.5245067120
Epoch:  1000  |  train loss: 0.5162599385
/home/z1165703/FeCAM/models/base.py:87: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  vec = torch.tensor(vec).cuda()
2024-03-05 11:00:30,517 [trainer.py] => CNN: {'total': 83.44, '00-09': 87.7, '10-19': 79.6, '20-29': 84.9, '30-39': 81.0, '40-49': 84.0, 'old': 0, 'new': 83.44}
2024-03-05 11:00:30,518 [trainer.py] => No NME accuracy
2024-03-05 11:00:30,518 [trainer.py] => FeCAM: {'total': 82.9, '00-09': 86.2, '10-19': 79.7, '20-29': 85.0, '30-39': 80.6, '40-49': 83.0, 'old': 0, 'new': 82.9}
2024-03-05 11:00:30,518 [trainer.py] => CNN top1 curve: [83.44]
2024-03-05 11:00:30,518 [trainer.py] => CNN top5 curve: [96.5]
2024-03-05 11:00:30,518 [trainer.py] => FeCAM top1 curve: [82.9]
2024-03-05 11:00:30,518 [trainer.py] => FeCAM top5 curve: [95.84]

2024-03-05 11:00:30,527 [fecam.py] => Learning on 50-60
Processing class: 50
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.7407961488
Epoch:   200  |  train loss: 0.6984344244
Epoch:   300  |  train loss: 0.6730044127
Epoch:   400  |  train loss: 0.6486043692
Epoch:   500  |  train loss: 0.6214911461
Epoch:   600  |  train loss: 0.5992169023
Epoch:   700  |  train loss: 0.5846386909
Epoch:   800  |  train loss: 0.5718671322
Epoch:   900  |  train loss: 0.5630263448
Epoch:  1000  |  train loss: 0.5539471626
Processing class: 51
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.7475192666
Epoch:   200  |  train loss: 0.7011870265
Epoch:   300  |  train loss: 0.6745079875
Epoch:   400  |  train loss: 0.6543620229
Epoch:   500  |  train loss: 0.6241059303
Epoch:   600  |  train loss: 0.5996663451
Epoch:   700  |  train loss: 0.5800322652
Epoch:   800  |  train loss: 0.5688812613
Epoch:   900  |  train loss: 0.5611374021
Epoch:  1000  |  train loss: 0.5464295268
Processing class: 52
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.7883248091
Epoch:   200  |  train loss: 0.7684731603
Epoch:   300  |  train loss: 0.7396762967
Epoch:   400  |  train loss: 0.7157351255
Epoch:   500  |  train loss: 0.6928029776
Epoch:   600  |  train loss: 0.6703020930
Epoch:   700  |  train loss: 0.6558296323
Epoch:   800  |  train loss: 0.6401619315
Epoch:   900  |  train loss: 0.6250589132
Epoch:  1000  |  train loss: 0.6087858200
Processing class: 53
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.6932227254
Epoch:   200  |  train loss: 0.6489824891
Epoch:   300  |  train loss: 0.6278909206
Epoch:   400  |  train loss: 0.6055143952
Epoch:   500  |  train loss: 0.5958420515
Epoch:   600  |  train loss: 0.5872537613
Epoch:   700  |  train loss: 0.5781530619
Epoch:   800  |  train loss: 0.5623246789
Epoch:   900  |  train loss: 0.5572371721
Epoch:  1000  |  train loss: 0.5469226599
Processing class: 54
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.6480626702
Epoch:   200  |  train loss: 0.6125930905
Epoch:   300  |  train loss: 0.5784717441
Epoch:   400  |  train loss: 0.5612555861
Epoch:   500  |  train loss: 0.5456799805
Epoch:   600  |  train loss: 0.5344633698
Epoch:   700  |  train loss: 0.5224348783
Epoch:   800  |  train loss: 0.5143455803
Epoch:   900  |  train loss: 0.5070924878
Epoch:  1000  |  train loss: 0.4956628859
Processing class: 55
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.8031748772
Epoch:   200  |  train loss: 0.7886332512
Epoch:   300  |  train loss: 0.7689310789
Epoch:   400  |  train loss: 0.7533725858
Epoch:   500  |  train loss: 0.7380189657
Epoch:   600  |  train loss: 0.7171712399
Epoch:   700  |  train loss: 0.7009086132
Epoch:   800  |  train loss: 0.6877932191
Epoch:   900  |  train loss: 0.6668404698
Epoch:  1000  |  train loss: 0.6556822658
Processing class: 56
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.7272480011
Epoch:   200  |  train loss: 0.6715878963
Epoch:   300  |  train loss: 0.6135471106
Epoch:   400  |  train loss: 0.5831618667
Epoch:   500  |  train loss: 0.5565158844
Epoch:   600  |  train loss: 0.5337763548
Epoch:   700  |  train loss: 0.5171393871
Epoch:   800  |  train loss: 0.5002141178
Epoch:   900  |  train loss: 0.4907410443
Epoch:  1000  |  train loss: 0.4815756381
Processing class: 57
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.8001269341
Epoch:   200  |  train loss: 0.7863361001
Epoch:   300  |  train loss: 0.7656762242
Epoch:   400  |  train loss: 0.7442805052
Epoch:   500  |  train loss: 0.7249631643
Epoch:   600  |  train loss: 0.7059900045
Epoch:   700  |  train loss: 0.6960735798
Epoch:   800  |  train loss: 0.6804418206
Epoch:   900  |  train loss: 0.6673049688
Epoch:  1000  |  train loss: 0.6474344611
Processing class: 58
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.7496108770
Epoch:   200  |  train loss: 0.6668218255
Epoch:   300  |  train loss: 0.6294095516
Epoch:   400  |  train loss: 0.5946117640
Epoch:   500  |  train loss: 0.5743501306
Epoch:   600  |  train loss: 0.5607702255
Epoch:   700  |  train loss: 0.5437533021
Epoch:   800  |  train loss: 0.5337248206
Epoch:   900  |  train loss: 0.5215823293
Epoch:  1000  |  train loss: 0.5100483775
Processing class: 59
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.7866016746
Epoch:   200  |  train loss: 0.7588389397
Epoch:   300  |  train loss: 0.7181315184
Epoch:   400  |  train loss: 0.6857347131
Epoch:   500  |  train loss: 0.6617342830
Epoch:   600  |  train loss: 0.6453845501
Epoch:   700  |  train loss: 0.6315545559
Epoch:   800  |  train loss: 0.6195354819
Epoch:   900  |  train loss: 0.6067928553
Epoch:  1000  |  train loss: 0.5976896286
2024-03-05 11:06:12,744 [trainer.py] => CNN: {'total': 71.75, '00-09': 82.2, '10-19': 72.8, '20-29': 78.5, '30-39': 74.9, '40-49': 68.4, '50-59': 53.7, 'old': 75.36, 'new': 53.7}
2024-03-05 11:06:12,745 [trainer.py] => No NME accuracy
2024-03-05 11:06:12,745 [trainer.py] => FeCAM: {'total': 72.78, '00-09': 82.6, '10-19': 74.0, '20-29': 81.0, '30-39': 76.4, '40-49': 76.8, '50-59': 45.9, 'old': 78.16, 'new': 45.9}
2024-03-05 11:06:12,745 [trainer.py] => CNN top1 curve: [83.44, 71.75]
2024-03-05 11:06:12,745 [trainer.py] => CNN top5 curve: [96.5, 89.6]
2024-03-05 11:06:12,745 [trainer.py] => FeCAM top1 curve: [82.9, 72.78]
2024-03-05 11:06:12,745 [trainer.py] => FeCAM top5 curve: [95.84, 91.18]

2024-03-05 11:06:12,750 [fecam.py] => Learning on 60-70
Processing class: 60
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.7795625687
Epoch:   200  |  train loss: 0.7491053581
Epoch:   300  |  train loss: 0.7125087619
Epoch:   400  |  train loss: 0.6804853916
Epoch:   500  |  train loss: 0.6551432133
Epoch:   600  |  train loss: 0.6302490830
Epoch:   700  |  train loss: 0.6150512338
Epoch:   800  |  train loss: 0.5902690887
Epoch:   900  |  train loss: 0.5772715926
Epoch:  1000  |  train loss: 0.5649269104
Processing class: 61
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.7222665429
Epoch:   200  |  train loss: 0.6631457567
Epoch:   300  |  train loss: 0.6198986888
Epoch:   400  |  train loss: 0.5829043150
Epoch:   500  |  train loss: 0.5629803777
Epoch:   600  |  train loss: 0.5418289065
Epoch:   700  |  train loss: 0.5275581479
Epoch:   800  |  train loss: 0.5148268521
Epoch:   900  |  train loss: 0.4996299505
Epoch:  1000  |  train loss: 0.4920052826
Processing class: 62
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.8062927723
Epoch:   200  |  train loss: 0.7863630056
Epoch:   300  |  train loss: 0.7728062510
Epoch:   400  |  train loss: 0.7581321716
Epoch:   500  |  train loss: 0.7415812850
Epoch:   600  |  train loss: 0.7311680079
Epoch:   700  |  train loss: 0.7182623506
Epoch:   800  |  train loss: 0.7037830591
Epoch:   900  |  train loss: 0.6882163167
Epoch:  1000  |  train loss: 0.6767432451
Processing class: 63
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.7570589781
Epoch:   200  |  train loss: 0.7355286360
Epoch:   300  |  train loss: 0.6823212266
Epoch:   400  |  train loss: 0.6444611192
Epoch:   500  |  train loss: 0.6165745735
Epoch:   600  |  train loss: 0.5999146461
Epoch:   700  |  train loss: 0.5791343451
Epoch:   800  |  train loss: 0.5725919008
Epoch:   900  |  train loss: 0.5556771755
Epoch:  1000  |  train loss: 0.5463722229
Processing class: 64
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.7352259874
Epoch:   200  |  train loss: 0.6459186554
Epoch:   300  |  train loss: 0.6079238415
Epoch:   400  |  train loss: 0.5801106453
Epoch:   500  |  train loss: 0.5502204061
Epoch:   600  |  train loss: 0.5279820800
Epoch:   700  |  train loss: 0.5094199300
Epoch:   800  |  train loss: 0.4988296330
Epoch:   900  |  train loss: 0.4910737991
Epoch:  1000  |  train loss: 0.4774578571
Processing class: 65
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.7251202941
Epoch:   200  |  train loss: 0.6636194110
Epoch:   300  |  train loss: 0.6094552279
Epoch:   400  |  train loss: 0.5795805097
Epoch:   500  |  train loss: 0.5561175466
Epoch:   600  |  train loss: 0.5402816772
Epoch:   700  |  train loss: 0.5238879085
Epoch:   800  |  train loss: 0.5137828410
Epoch:   900  |  train loss: 0.4982328892
Epoch:  1000  |  train loss: 0.4913287878
Processing class: 66
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.7650667667
Epoch:   200  |  train loss: 0.7288645625
Epoch:   300  |  train loss: 0.6933897614
Epoch:   400  |  train loss: 0.6673896551
Epoch:   500  |  train loss: 0.6542358041
Epoch:   600  |  train loss: 0.6392260909
Epoch:   700  |  train loss: 0.6306036472
Epoch:   800  |  train loss: 0.6218498349
Epoch:   900  |  train loss: 0.6117720723
Epoch:  1000  |  train loss: 0.6037825823
Processing class: 67
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.7697146893
Epoch:   200  |  train loss: 0.7392500162
Epoch:   300  |  train loss: 0.7098960280
Epoch:   400  |  train loss: 0.6886723042
Epoch:   500  |  train loss: 0.6707374811
Epoch:   600  |  train loss: 0.6550771117
Epoch:   700  |  train loss: 0.6409235597
Epoch:   800  |  train loss: 0.6302669287
Epoch:   900  |  train loss: 0.6174798727
Epoch:  1000  |  train loss: 0.6069664717
Processing class: 68
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.7108557940
Epoch:   200  |  train loss: 0.6863598228
Epoch:   300  |  train loss: 0.6318334937
Epoch:   400  |  train loss: 0.6080616355
Epoch:   500  |  train loss: 0.5913543582
Epoch:   600  |  train loss: 0.5746742487
Epoch:   700  |  train loss: 0.5629288197
Epoch:   800  |  train loss: 0.5498875618
Epoch:   900  |  train loss: 0.5395054221
Epoch:  1000  |  train loss: 0.5344567060
Processing class: 69
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.5556086063
Epoch:   200  |  train loss: 0.5218008280
Epoch:   300  |  train loss: 0.4886737466
Epoch:   400  |  train loss: 0.4666319728
Epoch:   500  |  train loss: 0.4518812656
Epoch:   600  |  train loss: 0.4407880366
Epoch:   700  |  train loss: 0.4368032992
Epoch:   800  |  train loss: 0.4296298087
Epoch:   900  |  train loss: 0.4300889432
Epoch:  1000  |  train loss: 0.4205977917
2024-03-05 11:12:46,798 [trainer.py] => CNN: {'total': 64.66, '00-09': 75.0, '10-19': 70.1, '20-29': 77.1, '30-39': 70.6, '40-49': 64.0, '50-59': 45.8, '60-69': 50.0, 'old': 67.1, 'new': 50.0}
2024-03-05 11:12:46,799 [trainer.py] => No NME accuracy
2024-03-05 11:12:46,799 [trainer.py] => FeCAM: {'total': 67.27, '00-09': 79.9, '10-19': 70.6, '20-29': 79.1, '30-39': 71.9, '40-49': 74.1, '50-59': 41.2, '60-69': 54.1, 'old': 69.47, 'new': 54.1}
2024-03-05 11:12:46,799 [trainer.py] => CNN top1 curve: [83.44, 71.75, 64.66]
2024-03-05 11:12:46,799 [trainer.py] => CNN top5 curve: [96.5, 89.6, 86.54]
2024-03-05 11:12:46,799 [trainer.py] => FeCAM top1 curve: [82.9, 72.78, 67.27]
2024-03-05 11:12:46,799 [trainer.py] => FeCAM top5 curve: [95.84, 91.18, 87.21]

2024-03-05 11:12:46,803 [fecam.py] => Learning on 70-80
Processing class: 70
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.7420804858
Epoch:   200  |  train loss: 0.7090053201
Epoch:   300  |  train loss: 0.6622569680
Epoch:   400  |  train loss: 0.6375147820
Epoch:   500  |  train loss: 0.6092921972
Epoch:   600  |  train loss: 0.5900726199
Epoch:   700  |  train loss: 0.5716350436
Epoch:   800  |  train loss: 0.5491824985
Epoch:   900  |  train loss: 0.5416600823
Epoch:  1000  |  train loss: 0.5307704806
Processing class: 71
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.7788335323
Epoch:   200  |  train loss: 0.7543048620
Epoch:   300  |  train loss: 0.7063138247
Epoch:   400  |  train loss: 0.6792116523
Epoch:   500  |  train loss: 0.6550201058
Epoch:   600  |  train loss: 0.6338433266
Epoch:   700  |  train loss: 0.6137404799
Epoch:   800  |  train loss: 0.5975401640
Epoch:   900  |  train loss: 0.5837918401
Epoch:  1000  |  train loss: 0.5683187842
Processing class: 72
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.7627663136
Epoch:   200  |  train loss: 0.7342948079
Epoch:   300  |  train loss: 0.7001928210
Epoch:   400  |  train loss: 0.6711052775
Epoch:   500  |  train loss: 0.6537549734
Epoch:   600  |  train loss: 0.6328353643
Epoch:   700  |  train loss: 0.6183058262
Epoch:   800  |  train loss: 0.6021910906
Epoch:   900  |  train loss: 0.5891461611
Epoch:  1000  |  train loss: 0.5799290657
Processing class: 73
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.7630345941
Epoch:   200  |  train loss: 0.7229738474
Epoch:   300  |  train loss: 0.6775400519
Epoch:   400  |  train loss: 0.6524693012
Epoch:   500  |  train loss: 0.6443414927
Epoch:   600  |  train loss: 0.6310535550
Epoch:   700  |  train loss: 0.6154970050
Epoch:   800  |  train loss: 0.6039616704
Epoch:   900  |  train loss: 0.5919868588
Epoch:  1000  |  train loss: 0.5837109327
Processing class: 74
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.8007102489
Epoch:   200  |  train loss: 0.7919954777
Epoch:   300  |  train loss: 0.7614879847
Epoch:   400  |  train loss: 0.7317951202
Epoch:   500  |  train loss: 0.7068518162
Epoch:   600  |  train loss: 0.6846175909
Epoch:   700  |  train loss: 0.6733246326
Epoch:   800  |  train loss: 0.6520771146
Epoch:   900  |  train loss: 0.6386318564
Epoch:  1000  |  train loss: 0.6295796633
Processing class: 75
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.7690887690
Epoch:   200  |  train loss: 0.7379768491
Epoch:   300  |  train loss: 0.7023897290
Epoch:   400  |  train loss: 0.6788800001
Epoch:   500  |  train loss: 0.6602885485
Epoch:   600  |  train loss: 0.6443487048
Epoch:   700  |  train loss: 0.6214166999
Epoch:   800  |  train loss: 0.6132441640
Epoch:   900  |  train loss: 0.6018554926
Epoch:  1000  |  train loss: 0.5931870222
Processing class: 76
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.5661577702
Epoch:   200  |  train loss: 0.5192044795
Epoch:   300  |  train loss: 0.4876983941
Epoch:   400  |  train loss: 0.4724606812
Epoch:   500  |  train loss: 0.4631226182
Epoch:   600  |  train loss: 0.4541302145
Epoch:   700  |  train loss: 0.4497854829
Epoch:   800  |  train loss: 0.4362437427
Epoch:   900  |  train loss: 0.4331036508
Epoch:  1000  |  train loss: 0.4294145703
Processing class: 77
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.7213814974
Epoch:   200  |  train loss: 0.6839374065
Epoch:   300  |  train loss: 0.6409106135
Epoch:   400  |  train loss: 0.6137987018
Epoch:   500  |  train loss: 0.5819983602
Epoch:   600  |  train loss: 0.5565690041
Epoch:   700  |  train loss: 0.5386708975
Epoch:   800  |  train loss: 0.5259788036
Epoch:   900  |  train loss: 0.5124080300
Epoch:  1000  |  train loss: 0.4993294299
Processing class: 78
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.7455672503
Epoch:   200  |  train loss: 0.6883239985
Epoch:   300  |  train loss: 0.6635628939
Epoch:   400  |  train loss: 0.6344281435
Epoch:   500  |  train loss: 0.6053541660
Epoch:   600  |  train loss: 0.5886558652
Epoch:   700  |  train loss: 0.5745664477
Epoch:   800  |  train loss: 0.5611413479
Epoch:   900  |  train loss: 0.5524259448
Epoch:  1000  |  train loss: 0.5427959561
Processing class: 79
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.8044450164
Epoch:   200  |  train loss: 0.7771656513
Epoch:   300  |  train loss: 0.7604993701
Epoch:   400  |  train loss: 0.7351718903
Epoch:   500  |  train loss: 0.7163634300
Epoch:   600  |  train loss: 0.7006074309
Epoch:   700  |  train loss: 0.6834220648
Epoch:   800  |  train loss: 0.6668805599
Epoch:   900  |  train loss: 0.6536691070
Epoch:  1000  |  train loss: 0.6363290310
2024-03-05 11:20:31,107 [trainer.py] => CNN: {'total': 59.18, '00-09': 73.6, '10-19': 68.4, '20-29': 76.9, '30-39': 69.1, '40-49': 60.6, '50-59': 37.2, '60-69': 45.7, '70-79': 41.9, 'old': 61.64, 'new': 41.9}
2024-03-05 11:20:31,107 [trainer.py] => No NME accuracy
2024-03-05 11:20:31,108 [trainer.py] => FeCAM: {'total': 61.72, '00-09': 78.1, '10-19': 69.3, '20-29': 78.0, '30-39': 69.5, '40-49': 72.1, '50-59': 35.7, '60-69': 49.1, '70-79': 42.0, 'old': 64.54, 'new': 42.0}
2024-03-05 11:20:31,108 [trainer.py] => CNN top1 curve: [83.44, 71.75, 64.66, 59.18]
2024-03-05 11:20:31,108 [trainer.py] => CNN top5 curve: [96.5, 89.6, 86.54, 84.09]
2024-03-05 11:20:31,108 [trainer.py] => FeCAM top1 curve: [82.9, 72.78, 67.27, 61.72]
2024-03-05 11:20:31,108 [trainer.py] => FeCAM top5 curve: [95.84, 91.18, 87.21, 84.09]

2024-03-05 11:20:31,115 [fecam.py] => Learning on 80-90
Processing class: 80
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.6400726795
Epoch:   200  |  train loss: 0.5547331810
Epoch:   300  |  train loss: 0.5252807498
Epoch:   400  |  train loss: 0.5072260499
Epoch:   500  |  train loss: 0.4898808837
Epoch:   600  |  train loss: 0.4804291308
Epoch:   700  |  train loss: 0.4670728981
Epoch:   800  |  train loss: 0.4652815044
Epoch:   900  |  train loss: 0.4580763459
Epoch:  1000  |  train loss: 0.4531992018
Processing class: 81
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.7258948088
Epoch:   200  |  train loss: 0.6810147166
Epoch:   300  |  train loss: 0.6461880445
Epoch:   400  |  train loss: 0.6273342609
Epoch:   500  |  train loss: 0.6059387445
Epoch:   600  |  train loss: 0.5928363681
Epoch:   700  |  train loss: 0.5772128105
Epoch:   800  |  train loss: 0.5692014575
Epoch:   900  |  train loss: 0.5555198669
Epoch:  1000  |  train loss: 0.5433398366
Processing class: 82
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.6896922708
Epoch:   200  |  train loss: 0.6403335333
Epoch:   300  |  train loss: 0.6092428803
Epoch:   400  |  train loss: 0.5809800625
Epoch:   500  |  train loss: 0.5583250165
Epoch:   600  |  train loss: 0.5418804169
Epoch:   700  |  train loss: 0.5295572996
Epoch:   800  |  train loss: 0.5198252201
Epoch:   900  |  train loss: 0.5091805816
Epoch:  1000  |  train loss: 0.4992845893
Processing class: 83
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.6696935177
Epoch:   200  |  train loss: 0.5882980466
Epoch:   300  |  train loss: 0.5448065042
Epoch:   400  |  train loss: 0.5146677613
Epoch:   500  |  train loss: 0.4975319088
Epoch:   600  |  train loss: 0.4819758475
Epoch:   700  |  train loss: 0.4760046661
Epoch:   800  |  train loss: 0.4701758206
Epoch:   900  |  train loss: 0.4601093411
Epoch:  1000  |  train loss: 0.4515357554
Processing class: 84
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.7579113960
Epoch:   200  |  train loss: 0.6882509589
Epoch:   300  |  train loss: 0.6537770271
Epoch:   400  |  train loss: 0.6193728447
Epoch:   500  |  train loss: 0.5977846265
Epoch:   600  |  train loss: 0.5779685259
Epoch:   700  |  train loss: 0.5658383608
Epoch:   800  |  train loss: 0.5567567229
Epoch:   900  |  train loss: 0.5408576250
Epoch:  1000  |  train loss: 0.5294028699
Processing class: 85
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.7638625026
Epoch:   200  |  train loss: 0.7030120015
Epoch:   300  |  train loss: 0.6612541318
Epoch:   400  |  train loss: 0.6294031978
Epoch:   500  |  train loss: 0.6106737137
Epoch:   600  |  train loss: 0.5936263919
Epoch:   700  |  train loss: 0.5837960720
Epoch:   800  |  train loss: 0.5684016109
Epoch:   900  |  train loss: 0.5573985696
Epoch:  1000  |  train loss: 0.5494593620
Processing class: 86
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.8044683218
Epoch:   200  |  train loss: 0.7715160608
Epoch:   300  |  train loss: 0.7311144710
Epoch:   400  |  train loss: 0.6971870542
Epoch:   500  |  train loss: 0.6744759202
Epoch:   600  |  train loss: 0.6548077822
Epoch:   700  |  train loss: 0.6372232795
Epoch:   800  |  train loss: 0.6216202378
Epoch:   900  |  train loss: 0.6093761683
Epoch:  1000  |  train loss: 0.6023892283
Processing class: 87
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.6977598906
Epoch:   200  |  train loss: 0.6295898676
Epoch:   300  |  train loss: 0.6025573134
Epoch:   400  |  train loss: 0.5703020096
Epoch:   500  |  train loss: 0.5495539546
Epoch:   600  |  train loss: 0.5410846949
Epoch:   700  |  train loss: 0.5307498336
Epoch:   800  |  train loss: 0.5196254551
Epoch:   900  |  train loss: 0.5077415049
Epoch:  1000  |  train loss: 0.5037270367
Processing class: 88
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.7465372920
Epoch:   200  |  train loss: 0.7087353706
Epoch:   300  |  train loss: 0.6698208690
Epoch:   400  |  train loss: 0.6389947772
Epoch:   500  |  train loss: 0.6083136559
Epoch:   600  |  train loss: 0.5821027279
Epoch:   700  |  train loss: 0.5682436109
Epoch:   800  |  train loss: 0.5537653804
Epoch:   900  |  train loss: 0.5430176735
Epoch:  1000  |  train loss: 0.5303106606
Processing class: 89
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.7356138587
Epoch:   200  |  train loss: 0.6882189393
Epoch:   300  |  train loss: 0.6421577573
Epoch:   400  |  train loss: 0.6192106724
Epoch:   500  |  train loss: 0.5970856905
Epoch:   600  |  train loss: 0.5744913578
Epoch:   700  |  train loss: 0.5557030082
Epoch:   800  |  train loss: 0.5467427611
Epoch:   900  |  train loss: 0.5319054008
Epoch:  1000  |  train loss: 0.5215157628
2024-03-05 11:29:22,552 [trainer.py] => CNN: {'total': 54.08, '00-09': 67.9, '10-19': 63.6, '20-29': 73.2, '30-39': 68.8, '40-49': 57.2, '50-59': 32.6, '60-69': 40.9, '70-79': 38.1, '80-89': 44.4, 'old': 55.29, 'new': 44.4}
2024-03-05 11:29:22,553 [trainer.py] => No NME accuracy
2024-03-05 11:29:22,553 [trainer.py] => FeCAM: {'total': 57.66, '00-09': 76.1, '10-19': 65.8, '20-29': 75.3, '30-39': 67.9, '40-49': 68.5, '50-59': 33.0, '60-69': 45.0, '70-79': 39.0, '80-89': 48.3, 'old': 58.82, 'new': 48.3}
2024-03-05 11:29:22,553 [trainer.py] => CNN top1 curve: [83.44, 71.75, 64.66, 59.18, 54.08]
2024-03-05 11:29:22,553 [trainer.py] => CNN top5 curve: [96.5, 89.6, 86.54, 84.09, 81.74]
2024-03-05 11:29:22,553 [trainer.py] => FeCAM top1 curve: [82.9, 72.78, 67.27, 61.72, 57.66]
2024-03-05 11:29:22,554 [trainer.py] => FeCAM top5 curve: [95.84, 91.18, 87.21, 84.09, 81.63]

2024-03-05 11:29:22,561 [fecam.py] => Learning on 90-100
Processing class: 90
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.7618502855
Epoch:   200  |  train loss: 0.7312222958
Epoch:   300  |  train loss: 0.6796302080
Epoch:   400  |  train loss: 0.6411987305
Epoch:   500  |  train loss: 0.6104716182
Epoch:   600  |  train loss: 0.5854146361
Epoch:   700  |  train loss: 0.5616872311
Epoch:   800  |  train loss: 0.5475072026
Epoch:   900  |  train loss: 0.5377449155
Epoch:  1000  |  train loss: 0.5289599657
Processing class: 91
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.5718039751
Epoch:   200  |  train loss: 0.5411030710
Epoch:   300  |  train loss: 0.5145193040
Epoch:   400  |  train loss: 0.5033543706
Epoch:   500  |  train loss: 0.4854074121
Epoch:   600  |  train loss: 0.4735135198
Epoch:   700  |  train loss: 0.4561664104
Epoch:   800  |  train loss: 0.4429775715
Epoch:   900  |  train loss: 0.4330796301
Epoch:  1000  |  train loss: 0.4267084599
Processing class: 92
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.7259396672
Epoch:   200  |  train loss: 0.6480496645
Epoch:   300  |  train loss: 0.6199774742
Epoch:   400  |  train loss: 0.5981550336
Epoch:   500  |  train loss: 0.5713156581
Epoch:   600  |  train loss: 0.5435595274
Epoch:   700  |  train loss: 0.5242179871
Epoch:   800  |  train loss: 0.5058971226
Epoch:   900  |  train loss: 0.4954681695
Epoch:  1000  |  train loss: 0.4854475677
Processing class: 93
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.7466448188
Epoch:   200  |  train loss: 0.6791815877
Epoch:   300  |  train loss: 0.6396644473
Epoch:   400  |  train loss: 0.6151316762
Epoch:   500  |  train loss: 0.5885338902
Epoch:   600  |  train loss: 0.5642790675
Epoch:   700  |  train loss: 0.5520949006
Epoch:   800  |  train loss: 0.5366174459
Epoch:   900  |  train loss: 0.5253028393
Epoch:  1000  |  train loss: 0.5111547649
Processing class: 94
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.6801333427
Epoch:   200  |  train loss: 0.6373731613
Epoch:   300  |  train loss: 0.5825234771
Epoch:   400  |  train loss: 0.5532598615
Epoch:   500  |  train loss: 0.5272377789
Epoch:   600  |  train loss: 0.5138899565
Epoch:   700  |  train loss: 0.5025362253
Epoch:   800  |  train loss: 0.4826841354
Epoch:   900  |  train loss: 0.4759309232
Epoch:  1000  |  train loss: 0.4646600068
Processing class: 95
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.7762838840
Epoch:   200  |  train loss: 0.7565555692
Epoch:   300  |  train loss: 0.7261084437
Epoch:   400  |  train loss: 0.6915920973
Epoch:   500  |  train loss: 0.6656798482
Epoch:   600  |  train loss: 0.6447106957
Epoch:   700  |  train loss: 0.6269113660
Epoch:   800  |  train loss: 0.6094828486
Epoch:   900  |  train loss: 0.5969202042
Epoch:  1000  |  train loss: 0.5863340020
Processing class: 96
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.7867376804
Epoch:   200  |  train loss: 0.7493044853
Epoch:   300  |  train loss: 0.7080693722
Epoch:   400  |  train loss: 0.6853854895
Epoch:   500  |  train loss: 0.6589377761
Epoch:   600  |  train loss: 0.6395234823
Epoch:   700  |  train loss: 0.6221999049
Epoch:   800  |  train loss: 0.6037982583
Epoch:   900  |  train loss: 0.5904079199
Epoch:  1000  |  train loss: 0.5764068604
Processing class: 97
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.7461766601
Epoch:   200  |  train loss: 0.6800655961
Epoch:   300  |  train loss: 0.6342475772
Epoch:   400  |  train loss: 0.6095795155
Epoch:   500  |  train loss: 0.5892843008
Epoch:   600  |  train loss: 0.5722719669
Epoch:   700  |  train loss: 0.5579101086
Epoch:   800  |  train loss: 0.5443135977
Epoch:   900  |  train loss: 0.5320677042
Epoch:  1000  |  train loss: 0.5203193843
Processing class: 98
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.6826115847
Epoch:   200  |  train loss: 0.6361655235
Epoch:   300  |  train loss: 0.5881906033
Epoch:   400  |  train loss: 0.5589530826
Epoch:   500  |  train loss: 0.5345752597
Epoch:   600  |  train loss: 0.5165425062
Epoch:   700  |  train loss: 0.5001162052
Epoch:   800  |  train loss: 0.4916048467
Epoch:   900  |  train loss: 0.4827587426
Epoch:  1000  |  train loss: 0.4748306870
Processing class: 99
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.7632519960
Epoch:   200  |  train loss: 0.7193574309
Epoch:   300  |  train loss: 0.6679716110
Epoch:   400  |  train loss: 0.6345294595
Epoch:   500  |  train loss: 0.6039032459
Epoch:   600  |  train loss: 0.5828138947
Epoch:   700  |  train loss: 0.5673103690
Epoch:   800  |  train loss: 0.5567702532
Epoch:   900  |  train loss: 0.5426863551
Epoch:  1000  |  train loss: 0.5321492195
2024-03-05 11:39:41,036 [trainer.py] => CNN: {'total': 50.36, '00-09': 57.7, '10-19': 63.6, '20-29': 71.4, '30-39': 68.5, '40-49': 56.8, '50-59': 29.6, '60-69': 38.9, '70-79': 36.6, '80-89': 42.7, '90-99': 37.8, 'old': 51.76, 'new': 37.8}
2024-03-05 11:39:41,037 [trainer.py] => No NME accuracy
2024-03-05 11:39:41,037 [trainer.py] => FeCAM: {'total': 54.74, '00-09': 73.3, '10-19': 65.1, '20-29': 74.1, '30-39': 67.2, '40-49': 67.1, '50-59': 30.8, '60-69': 43.3, '70-79': 37.2, '80-89': 46.9, '90-99': 42.4, 'old': 56.11, 'new': 42.4}
2024-03-05 11:39:41,037 [trainer.py] => CNN top1 curve: [83.44, 71.75, 64.66, 59.18, 54.08, 50.36]
2024-03-05 11:39:41,037 [trainer.py] => CNN top5 curve: [96.5, 89.6, 86.54, 84.09, 81.74, 79.63]
2024-03-05 11:39:41,037 [trainer.py] => FeCAM top1 curve: [82.9, 72.78, 67.27, 61.72, 57.66, 54.74]
2024-03-05 11:39:41,037 [trainer.py] => FeCAM top5 curve: [95.84, 91.18, 87.21, 84.09, 81.63, 79.21]

=========================================
2024-03-05 11:39:49,670 [trainer.py] => config: ./exps/FeCAM_cifar100.json
2024-03-05 11:39:49,670 [trainer.py] => prefix: train
2024-03-05 11:39:49,670 [trainer.py] => dataset: cifar100
2024-03-05 11:39:49,670 [trainer.py] => memory_size: 0
2024-03-05 11:39:49,671 [trainer.py] => shuffle: True
2024-03-05 11:39:49,671 [trainer.py] => init_cls: 50
2024-03-05 11:39:49,671 [trainer.py] => increment: 10
2024-03-05 11:39:49,671 [trainer.py] => model_name: fecam
2024-03-05 11:39:49,671 [trainer.py] => convnet_type: resnet18
2024-03-05 11:39:49,671 [trainer.py] => device: [device(type='cuda', index=0)]
2024-03-05 11:39:49,671 [trainer.py] => seed: 1993
2024-03-05 11:39:49,671 [trainer.py] => init_epochs: 200
2024-03-05 11:39:49,671 [trainer.py] => init_lr: 0.1
2024-03-05 11:39:49,671 [trainer.py] => init_weight_decay: 0.0005
2024-03-05 11:39:49,671 [trainer.py] => batch_size: 128
2024-03-05 11:39:49,671 [trainer.py] => num_workers: 8
2024-03-05 11:39:49,671 [trainer.py] => T: 5
2024-03-05 11:39:49,671 [trainer.py] => beta: 0.5
2024-03-05 11:39:49,671 [trainer.py] => alpha1: 1
2024-03-05 11:39:49,671 [trainer.py] => alpha2: 1
2024-03-05 11:39:49,671 [trainer.py] => ncm: False
2024-03-05 11:39:49,671 [trainer.py] => tukey: False
2024-03-05 11:39:49,671 [trainer.py] => diagonal: False
2024-03-05 11:39:49,671 [trainer.py] => per_class: True
2024-03-05 11:39:49,671 [trainer.py] => full_cov: True
2024-03-05 11:39:49,671 [trainer.py] => shrink: True
2024-03-05 11:39:49,671 [trainer.py] => norm_cov: False
2024-03-05 11:39:49,671 [trainer.py] => vecnorm: False
2024-03-05 11:39:49,671 [trainer.py] => ae_type: wae
2024-03-05 11:39:49,671 [trainer.py] => epochs: 1000
2024-03-05 11:39:49,671 [trainer.py] => ae_latent_dim: 32
2024-03-05 11:39:49,671 [trainer.py] => wae_sigma: 10
2024-03-05 11:39:49,671 [trainer.py] => wae_C: 1
Files already downloaded and verified
Files already downloaded and verified
2024-03-05 11:39:51,332 [data_manager.py] => [68, 56, 78, 8, 23, 84, 90, 65, 74, 76, 40, 89, 3, 92, 55, 9, 26, 80, 43, 38, 58, 70, 77, 1, 85, 19, 17, 50, 28, 53, 13, 81, 45, 82, 6, 59, 83, 16, 15, 44, 91, 41, 72, 60, 79, 52, 20, 10, 31, 54, 37, 95, 14, 71, 96, 98, 97, 2, 64, 66, 42, 22, 35, 86, 24, 34, 87, 21, 99, 0, 88, 27, 18, 94, 11, 12, 47, 25, 30, 46, 62, 69, 36, 61, 7, 63, 75, 5, 32, 4, 51, 48, 73, 93, 39, 67, 29, 49, 57, 33]
2024-03-05 11:39:51,595 [fecam.py] => Learning on 0-50
/home/z1165703/miniconda3/lib/python3.11/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
/home/z1165703/miniconda3/lib/python3.11/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
Processing class: 0
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.5325630426
Epoch:   200  |  train loss: 0.5083062828
Epoch:   300  |  train loss: 0.5102808833
Epoch:   400  |  train loss: 0.5018021762
Epoch:   500  |  train loss: 0.4945253789
Epoch:   600  |  train loss: 0.4932181895
Epoch:   700  |  train loss: 0.4907746613
Epoch:   800  |  train loss: 0.4897591770
Epoch:   900  |  train loss: 0.4843676865
Epoch:  1000  |  train loss: 0.4800216675
Processing class: 1
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.5658501625
Epoch:   200  |  train loss: 0.5601901054
Epoch:   300  |  train loss: 0.5530001402
Epoch:   400  |  train loss: 0.5456632018
Epoch:   500  |  train loss: 0.5300220430
Epoch:   600  |  train loss: 0.5328982592
Epoch:   700  |  train loss: 0.5241405964
Epoch:   800  |  train loss: 0.5215308666
Epoch:   900  |  train loss: 0.5217016578
Epoch:  1000  |  train loss: 0.5116488993
Processing class: 2
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.5844037771
Epoch:   200  |  train loss: 0.5665408969
Epoch:   300  |  train loss: 0.5544102073
Epoch:   400  |  train loss: 0.5369421005
Epoch:   500  |  train loss: 0.5152588069
Epoch:   600  |  train loss: 0.5090104401
Epoch:   700  |  train loss: 0.4976079524
Epoch:   800  |  train loss: 0.4948590159
Epoch:   900  |  train loss: 0.4830541670
Epoch:  1000  |  train loss: 0.4740366876
Processing class: 3
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.5337284803
Epoch:   200  |  train loss: 0.5382728577
Epoch:   300  |  train loss: 0.5307119310
Epoch:   400  |  train loss: 0.5148530304
Epoch:   500  |  train loss: 0.5028523505
Epoch:   600  |  train loss: 0.5030207455
Epoch:   700  |  train loss: 0.4950006902
Epoch:   800  |  train loss: 0.4859848976
Epoch:   900  |  train loss: 0.4793658257
Epoch:  1000  |  train loss: 0.4724801064
Processing class: 4
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.5643096924
Epoch:   200  |  train loss: 0.5557392836
Epoch:   300  |  train loss: 0.5413617134
Epoch:   400  |  train loss: 0.5277258158
Epoch:   500  |  train loss: 0.5195956886
Epoch:   600  |  train loss: 0.5054159343
Epoch:   700  |  train loss: 0.5043045402
Epoch:   800  |  train loss: 0.5004567802
Epoch:   900  |  train loss: 0.4962030292
Epoch:  1000  |  train loss: 0.4941270411
Processing class: 5
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.5901750922
Epoch:   200  |  train loss: 0.5627395272
Epoch:   300  |  train loss: 0.5504051685
Epoch:   400  |  train loss: 0.5335112095
Epoch:   500  |  train loss: 0.5263794184
Epoch:   600  |  train loss: 0.5253069639
Epoch:   700  |  train loss: 0.5149651170
Epoch:   800  |  train loss: 0.5025985777
Epoch:   900  |  train loss: 0.4934354961
Epoch:  1000  |  train loss: 0.4908145130
Processing class: 6
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.5621526003
Epoch:   200  |  train loss: 0.5637821317
Epoch:   300  |  train loss: 0.5514708519
Epoch:   400  |  train loss: 0.5350794435
Epoch:   500  |  train loss: 0.5313040018
Epoch:   600  |  train loss: 0.5246633172
Epoch:   700  |  train loss: 0.5180292606
Epoch:   800  |  train loss: 0.5127226233
Epoch:   900  |  train loss: 0.5002710402
Epoch:  1000  |  train loss: 0.4978981256
Processing class: 7
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.5840690374
Epoch:   200  |  train loss: 0.5676032186
Epoch:   300  |  train loss: 0.5588485360
Epoch:   400  |  train loss: 0.5426355004
Epoch:   500  |  train loss: 0.5306817412
Epoch:   600  |  train loss: 0.5212726712
Epoch:   700  |  train loss: 0.5162143350
Epoch:   800  |  train loss: 0.5145902276
Epoch:   900  |  train loss: 0.5020981848
Epoch:  1000  |  train loss: 0.4989609480
Processing class: 8
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.5729133487
Epoch:   200  |  train loss: 0.5644860148
Epoch:   300  |  train loss: 0.5393722773
Epoch:   400  |  train loss: 0.5342613816
Epoch:   500  |  train loss: 0.5228284955
Epoch:   600  |  train loss: 0.5130934358
Epoch:   700  |  train loss: 0.5108147681
Epoch:   800  |  train loss: 0.4997177720
Epoch:   900  |  train loss: 0.4966735601
Epoch:  1000  |  train loss: 0.4868683875
Processing class: 9
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.5545362353
Epoch:   200  |  train loss: 0.5563241720
Epoch:   300  |  train loss: 0.5549428701
Epoch:   400  |  train loss: 0.5379935265
Epoch:   500  |  train loss: 0.5375345826
Epoch:   600  |  train loss: 0.5297053695
Epoch:   700  |  train loss: 0.5290789008
Epoch:   800  |  train loss: 0.5270496845
Epoch:   900  |  train loss: 0.5235529900
Epoch:  1000  |  train loss: 0.5150547504
Processing class: 10
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.5793527246
Epoch:   200  |  train loss: 0.5597558618
Epoch:   300  |  train loss: 0.5525587678
Epoch:   400  |  train loss: 0.5487523913
Epoch:   500  |  train loss: 0.5289148927
Epoch:   600  |  train loss: 0.5231925428
Epoch:   700  |  train loss: 0.5181476116
Epoch:   800  |  train loss: 0.5096348882
Epoch:   900  |  train loss: 0.5067214191
Epoch:  1000  |  train loss: 0.5018227041
Processing class: 11
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.5908221126
Epoch:   200  |  train loss: 0.5671989679
Epoch:   300  |  train loss: 0.5534751296
Epoch:   400  |  train loss: 0.5441183805
Epoch:   500  |  train loss: 0.5302387714
Epoch:   600  |  train loss: 0.5266685009
Epoch:   700  |  train loss: 0.5253144503
Epoch:   800  |  train loss: 0.5183562815
Epoch:   900  |  train loss: 0.5116834700
Epoch:  1000  |  train loss: 0.5077077985
Processing class: 12
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.5693579555
Epoch:   200  |  train loss: 0.5683898091
Epoch:   300  |  train loss: 0.5532999516
Epoch:   400  |  train loss: 0.5389763117
Epoch:   500  |  train loss: 0.5289751530
Epoch:   600  |  train loss: 0.5152990103
Epoch:   700  |  train loss: 0.5089815974
Epoch:   800  |  train loss: 0.5057646990
Epoch:   900  |  train loss: 0.5031594932
Epoch:  1000  |  train loss: 0.4952932298
Processing class: 13
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.5607021570
Epoch:   200  |  train loss: 0.5467488170
Epoch:   300  |  train loss: 0.5340328217
Epoch:   400  |  train loss: 0.5190987349
Epoch:   500  |  train loss: 0.5113442659
Epoch:   600  |  train loss: 0.5026205242
Epoch:   700  |  train loss: 0.4978185713
Epoch:   800  |  train loss: 0.4963448524
Epoch:   900  |  train loss: 0.4941545963
Epoch:  1000  |  train loss: 0.4905423939
Processing class: 14
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.6022246122
Epoch:   200  |  train loss: 0.6031955481
Epoch:   300  |  train loss: 0.5901438117
Epoch:   400  |  train loss: 0.5787071943
Epoch:   500  |  train loss: 0.5673515677
Epoch:   600  |  train loss: 0.5582528830
Epoch:   700  |  train loss: 0.5561977983
Epoch:   800  |  train loss: 0.5566678286
Epoch:   900  |  train loss: 0.5509066939
Epoch:  1000  |  train loss: 0.5432305098
Processing class: 15
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.5551060438
Epoch:   200  |  train loss: 0.5490093827
Epoch:   300  |  train loss: 0.5301468015
Epoch:   400  |  train loss: 0.5216688275
Epoch:   500  |  train loss: 0.5162653565
Epoch:   600  |  train loss: 0.5065495074
Epoch:   700  |  train loss: 0.5073791385
Epoch:   800  |  train loss: 0.4986325264
Epoch:   900  |  train loss: 0.5012943208
Epoch:  1000  |  train loss: 0.4958604872
Processing class: 16
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.5665637612
Epoch:   200  |  train loss: 0.5492021799
Epoch:   300  |  train loss: 0.5420277476
Epoch:   400  |  train loss: 0.5311700583
Epoch:   500  |  train loss: 0.5314963341
Epoch:   600  |  train loss: 0.5232671142
Epoch:   700  |  train loss: 0.5152573049
Epoch:   800  |  train loss: 0.5155252099
Epoch:   900  |  train loss: 0.5045839190
Epoch:  1000  |  train loss: 0.5078341305
Processing class: 17
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.5571646690
Epoch:   200  |  train loss: 0.5586970091
Epoch:   300  |  train loss: 0.5598304987
Epoch:   400  |  train loss: 0.5483831167
Epoch:   500  |  train loss: 0.5239410162
Epoch:   600  |  train loss: 0.5194239736
Epoch:   700  |  train loss: 0.5121365786
Epoch:   800  |  train loss: 0.5064914823
Epoch:   900  |  train loss: 0.5022498667
Epoch:  1000  |  train loss: 0.4982763350
Processing class: 18
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.5592712164
Epoch:   200  |  train loss: 0.5587754965
Epoch:   300  |  train loss: 0.5286521614
Epoch:   400  |  train loss: 0.5156530023
Epoch:   500  |  train loss: 0.5037457585
Epoch:   600  |  train loss: 0.4887650013
Epoch:   700  |  train loss: 0.4897817314
Epoch:   800  |  train loss: 0.4811566293
Epoch:   900  |  train loss: 0.4769220948
Epoch:  1000  |  train loss: 0.4749184489
Processing class: 19
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.5409005523
Epoch:   200  |  train loss: 0.5363338232
Epoch:   300  |  train loss: 0.5232593179
Epoch:   400  |  train loss: 0.5184820175
Epoch:   500  |  train loss: 0.5025014997
Epoch:   600  |  train loss: 0.4983284533
Epoch:   700  |  train loss: 0.4945094764
Epoch:   800  |  train loss: 0.4861118972
Epoch:   900  |  train loss: 0.4872603059
Epoch:  1000  |  train loss: 0.4837302148
Processing class: 20
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.5628335357
Epoch:   200  |  train loss: 0.5479461193
Epoch:   300  |  train loss: 0.5453919530
Epoch:   400  |  train loss: 0.5319956422
Epoch:   500  |  train loss: 0.5333146214
Epoch:   600  |  train loss: 0.5227471828
Epoch:   700  |  train loss: 0.5159964561
Epoch:   800  |  train loss: 0.5133774161
Epoch:   900  |  train loss: 0.5096863985
Epoch:  1000  |  train loss: 0.5106083751
Processing class: 21
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.5679299593
Epoch:   200  |  train loss: 0.5538868785
Epoch:   300  |  train loss: 0.5243020415
Epoch:   400  |  train loss: 0.5089442611
Epoch:   500  |  train loss: 0.4996986032
Epoch:   600  |  train loss: 0.4950374544
Epoch:   700  |  train loss: 0.4891772330
Epoch:   800  |  train loss: 0.4810277045
Epoch:   900  |  train loss: 0.4786381781
Epoch:  1000  |  train loss: 0.4698121548
Processing class: 22
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.5834613800
Epoch:   200  |  train loss: 0.5809332490
Epoch:   300  |  train loss: 0.5779039979
Epoch:   400  |  train loss: 0.5616595507
Epoch:   500  |  train loss: 0.5517488360
Epoch:   600  |  train loss: 0.5388781786
Epoch:   700  |  train loss: 0.5316976666
Epoch:   800  |  train loss: 0.5214144588
Epoch:   900  |  train loss: 0.5140582502
Epoch:  1000  |  train loss: 0.5169059753
Processing class: 23
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.5568657637
Epoch:   200  |  train loss: 0.5582094312
Epoch:   300  |  train loss: 0.5502719641
Epoch:   400  |  train loss: 0.5337109089
Epoch:   500  |  train loss: 0.5253519416
Epoch:   600  |  train loss: 0.5100008130
Epoch:   700  |  train loss: 0.5033936322
Epoch:   800  |  train loss: 0.4934696376
Epoch:   900  |  train loss: 0.4901939034
Epoch:  1000  |  train loss: 0.4798519731
Processing class: 24
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.5656266928
Epoch:   200  |  train loss: 0.5633182645
Epoch:   300  |  train loss: 0.5603161335
Epoch:   400  |  train loss: 0.5484650970
Epoch:   500  |  train loss: 0.5376293302
Epoch:   600  |  train loss: 0.5335781574
Epoch:   700  |  train loss: 0.5200873494
Epoch:   800  |  train loss: 0.5176393747
Epoch:   900  |  train loss: 0.5156903625
Epoch:  1000  |  train loss: 0.5075622916
Processing class: 25
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.5723228574
Epoch:   200  |  train loss: 0.5613545656
Epoch:   300  |  train loss: 0.5544425249
Epoch:   400  |  train loss: 0.5321730018
Epoch:   500  |  train loss: 0.5236324430
Epoch:   600  |  train loss: 0.5131305397
Epoch:   700  |  train loss: 0.5125529051
Epoch:   800  |  train loss: 0.5038395464
Epoch:   900  |  train loss: 0.4956533790
Epoch:  1000  |  train loss: 0.4940938950
Processing class: 26
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.5562043667
Epoch:   200  |  train loss: 0.5596114755
Epoch:   300  |  train loss: 0.5602426529
Epoch:   400  |  train loss: 0.5542550206
Epoch:   500  |  train loss: 0.5478234768
Epoch:   600  |  train loss: 0.5263304472
Epoch:   700  |  train loss: 0.5227487206
Epoch:   800  |  train loss: 0.5205548584
Epoch:   900  |  train loss: 0.5124590576
Epoch:  1000  |  train loss: 0.5122300506
Processing class: 27
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.5851201773
Epoch:   200  |  train loss: 0.5623566747
Epoch:   300  |  train loss: 0.5584141493
Epoch:   400  |  train loss: 0.5427976727
Epoch:   500  |  train loss: 0.5313574553
Epoch:   600  |  train loss: 0.5265086651
Epoch:   700  |  train loss: 0.5230767250
Epoch:   800  |  train loss: 0.5175727367
Epoch:   900  |  train loss: 0.5126151681
Epoch:  1000  |  train loss: 0.4991385996
Processing class: 28
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.5732276201
Epoch:   200  |  train loss: 0.5735614657
Epoch:   300  |  train loss: 0.5591351271
Epoch:   400  |  train loss: 0.5474263668
Epoch:   500  |  train loss: 0.5384359121
Epoch:   600  |  train loss: 0.5336054564
Epoch:   700  |  train loss: 0.5316893339
Epoch:   800  |  train loss: 0.5289216757
Epoch:   900  |  train loss: 0.5265118718
Epoch:  1000  |  train loss: 0.5193263531
Processing class: 29
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.5702916622
Epoch:   200  |  train loss: 0.5533526421
Epoch:   300  |  train loss: 0.5366892815
Epoch:   400  |  train loss: 0.5178472996
Epoch:   500  |  train loss: 0.5127340674
Epoch:   600  |  train loss: 0.5067710280
Epoch:   700  |  train loss: 0.4972804725
Epoch:   800  |  train loss: 0.4855753839
Epoch:   900  |  train loss: 0.4805838168
Epoch:  1000  |  train loss: 0.4745450854
Processing class: 30
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.5547929406
Epoch:   200  |  train loss: 0.5558796287
Epoch:   300  |  train loss: 0.5393355250
Epoch:   400  |  train loss: 0.5380260944
Epoch:   500  |  train loss: 0.5337125063
Epoch:   600  |  train loss: 0.5261956394
Epoch:   700  |  train loss: 0.5161261320
Epoch:   800  |  train loss: 0.5149645686
Epoch:   900  |  train loss: 0.5130501151
Epoch:  1000  |  train loss: 0.5053517282
Processing class: 31
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.5759608746
Epoch:   200  |  train loss: 0.5635646701
Epoch:   300  |  train loss: 0.5483952641
Epoch:   400  |  train loss: 0.5494098306
Epoch:   500  |  train loss: 0.5419545293
Epoch:   600  |  train loss: 0.5399099708
Epoch:   700  |  train loss: 0.5324309826
Epoch:   800  |  train loss: 0.5291823745
Epoch:   900  |  train loss: 0.5267228007
Epoch:  1000  |  train loss: 0.5174095511
Processing class: 32
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.5931482434
Epoch:   200  |  train loss: 0.5920065045
Epoch:   300  |  train loss: 0.5786466360
Epoch:   400  |  train loss: 0.5602856278
Epoch:   500  |  train loss: 0.5541116476
Epoch:   600  |  train loss: 0.5372604609
Epoch:   700  |  train loss: 0.5336737752
Epoch:   800  |  train loss: 0.5290605783
Epoch:   900  |  train loss: 0.5217297256
Epoch:  1000  |  train loss: 0.5146456778
Processing class: 33
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.5680556774
Epoch:   200  |  train loss: 0.5738139153
Epoch:   300  |  train loss: 0.5438886404
Epoch:   400  |  train loss: 0.5290913820
Epoch:   500  |  train loss: 0.5191679955
Epoch:   600  |  train loss: 0.5155549526
Epoch:   700  |  train loss: 0.5054440320
Epoch:   800  |  train loss: 0.5010805488
Epoch:   900  |  train loss: 0.4942920029
Epoch:  1000  |  train loss: 0.4865123868
Processing class: 34
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.5815202713
Epoch:   200  |  train loss: 0.5740079165
Epoch:   300  |  train loss: 0.5565652251
Epoch:   400  |  train loss: 0.5445725560
Epoch:   500  |  train loss: 0.5317704558
Epoch:   600  |  train loss: 0.5258945704
Epoch:   700  |  train loss: 0.5135131478
Epoch:   800  |  train loss: 0.5113443315
Epoch:   900  |  train loss: 0.5067921042
Epoch:  1000  |  train loss: 0.5021774411
Processing class: 35
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.6101500988
Epoch:   200  |  train loss: 0.5865921855
Epoch:   300  |  train loss: 0.5783707619
Epoch:   400  |  train loss: 0.5716971278
Epoch:   500  |  train loss: 0.5579502821
Epoch:   600  |  train loss: 0.5572605610
Epoch:   700  |  train loss: 0.5509549260
Epoch:   800  |  train loss: 0.5443088055
Epoch:   900  |  train loss: 0.5400786281
Epoch:  1000  |  train loss: 0.5326930165
Processing class: 36
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.5389208198
Epoch:   200  |  train loss: 0.5292955518
Epoch:   300  |  train loss: 0.5191777825
Epoch:   400  |  train loss: 0.5134951949
Epoch:   500  |  train loss: 0.5088476002
Epoch:   600  |  train loss: 0.4962036073
Epoch:   700  |  train loss: 0.4847856879
Epoch:   800  |  train loss: 0.4838734567
Epoch:   900  |  train loss: 0.4719652891
Epoch:  1000  |  train loss: 0.4694767535
Processing class: 37
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.5953364491
Epoch:   200  |  train loss: 0.5858917832
Epoch:   300  |  train loss: 0.5680995941
Epoch:   400  |  train loss: 0.5568903089
Epoch:   500  |  train loss: 0.5493412256
Epoch:   600  |  train loss: 0.5378459573
Epoch:   700  |  train loss: 0.5309911847
Epoch:   800  |  train loss: 0.5278392076
Epoch:   900  |  train loss: 0.5172226906
Epoch:  1000  |  train loss: 0.5145831466
Processing class: 38
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.5524287939
Epoch:   200  |  train loss: 0.5509580016
Epoch:   300  |  train loss: 0.5464119554
Epoch:   400  |  train loss: 0.5437678695
Epoch:   500  |  train loss: 0.5320746303
Epoch:   600  |  train loss: 0.5196646571
Epoch:   700  |  train loss: 0.5096279621
Epoch:   800  |  train loss: 0.5098514557
Epoch:   900  |  train loss: 0.5001094162
Epoch:  1000  |  train loss: 0.5000805199
Processing class: 39
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.5704108238
Epoch:   200  |  train loss: 0.5605323672
Epoch:   300  |  train loss: 0.5528058648
Epoch:   400  |  train loss: 0.5440205336
Epoch:   500  |  train loss: 0.5305954218
Epoch:   600  |  train loss: 0.5251874208
Epoch:   700  |  train loss: 0.5189438581
Epoch:   800  |  train loss: 0.5144266486
Epoch:   900  |  train loss: 0.5078262806
Epoch:  1000  |  train loss: 0.5025117576
Processing class: 40
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.5628840089
Epoch:   200  |  train loss: 0.5392028809
Epoch:   300  |  train loss: 0.5406065702
Epoch:   400  |  train loss: 0.5342727184
Epoch:   500  |  train loss: 0.5291542411
Epoch:   600  |  train loss: 0.5132460892
Epoch:   700  |  train loss: 0.5115034401
Epoch:   800  |  train loss: 0.5060975611
Epoch:   900  |  train loss: 0.4998883784
Epoch:  1000  |  train loss: 0.5022657931
Processing class: 41
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.5406384468
Epoch:   200  |  train loss: 0.5429266572
Epoch:   300  |  train loss: 0.5238789082
Epoch:   400  |  train loss: 0.5064721406
Epoch:   500  |  train loss: 0.4979974985
Epoch:   600  |  train loss: 0.4923250020
Epoch:   700  |  train loss: 0.4902919412
Epoch:   800  |  train loss: 0.4838125169
Epoch:   900  |  train loss: 0.4784361482
Epoch:  1000  |  train loss: 0.4745135963
Processing class: 42
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.5794552445
Epoch:   200  |  train loss: 0.5691894412
Epoch:   300  |  train loss: 0.5565082431
Epoch:   400  |  train loss: 0.5424829364
Epoch:   500  |  train loss: 0.5324021697
Epoch:   600  |  train loss: 0.5216626763
Epoch:   700  |  train loss: 0.5154005826
Epoch:   800  |  train loss: 0.5076184511
Epoch:   900  |  train loss: 0.4976047277
Epoch:  1000  |  train loss: 0.4910097182
Processing class: 43
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.5905642986
Epoch:   200  |  train loss: 0.5762575626
Epoch:   300  |  train loss: 0.5601403117
Epoch:   400  |  train loss: 0.5604304552
Epoch:   500  |  train loss: 0.5566663027
Epoch:   600  |  train loss: 0.5528686881
Epoch:   700  |  train loss: 0.5534854054
Epoch:   800  |  train loss: 0.5489890218
Epoch:   900  |  train loss: 0.5396855593
Epoch:  1000  |  train loss: 0.5409433722
Processing class: 44
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.5901122451
Epoch:   200  |  train loss: 0.5742236614
Epoch:   300  |  train loss: 0.5643842340
Epoch:   400  |  train loss: 0.5511416674
Epoch:   500  |  train loss: 0.5463290453
Epoch:   600  |  train loss: 0.5390604734
Epoch:   700  |  train loss: 0.5374841332
Epoch:   800  |  train loss: 0.5302167296
Epoch:   900  |  train loss: 0.5217572808
Epoch:  1000  |  train loss: 0.5201191068
Processing class: 45
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.5656225562
Epoch:   200  |  train loss: 0.5593500495
Epoch:   300  |  train loss: 0.5396244407
Epoch:   400  |  train loss: 0.5465549111
Epoch:   500  |  train loss: 0.5428023219
Epoch:   600  |  train loss: 0.5423751116
Epoch:   700  |  train loss: 0.5371124387
Epoch:   800  |  train loss: 0.5323345065
Epoch:   900  |  train loss: 0.5318253875
Epoch:  1000  |  train loss: 0.5316114545
Processing class: 46
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.5508278847
Epoch:   200  |  train loss: 0.5427173138
Epoch:   300  |  train loss: 0.5291370392
Epoch:   400  |  train loss: 0.5294021845
Epoch:   500  |  train loss: 0.5261210322
Epoch:   600  |  train loss: 0.5241749763
Epoch:   700  |  train loss: 0.5209894001
Epoch:   800  |  train loss: 0.5209314108
Epoch:   900  |  train loss: 0.5129271269
Epoch:  1000  |  train loss: 0.5123692453
Processing class: 47
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.5837189198
Epoch:   200  |  train loss: 0.5740521550
Epoch:   300  |  train loss: 0.5480409741
Epoch:   400  |  train loss: 0.5399459839
Epoch:   500  |  train loss: 0.5301636219
Epoch:   600  |  train loss: 0.5188725650
Epoch:   700  |  train loss: 0.5110541463
Epoch:   800  |  train loss: 0.5011216462
Epoch:   900  |  train loss: 0.4964575469
Epoch:  1000  |  train loss: 0.4897818089
Processing class: 48
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.5799629331
Epoch:   200  |  train loss: 0.5779394031
Epoch:   300  |  train loss: 0.5480466723
Epoch:   400  |  train loss: 0.5323859572
Epoch:   500  |  train loss: 0.5372506261
Epoch:   600  |  train loss: 0.5282886505
Epoch:   700  |  train loss: 0.5195776165
Epoch:   800  |  train loss: 0.5164762855
Epoch:   900  |  train loss: 0.5200213552
Epoch:  1000  |  train loss: 0.5166473448
Processing class: 49
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.5860043645
Epoch:   200  |  train loss: 0.5751270771
Epoch:   300  |  train loss: 0.5650446415
Epoch:   400  |  train loss: 0.5550741076
Epoch:   500  |  train loss: 0.5444466114
Epoch:   600  |  train loss: 0.5385238528
Epoch:   700  |  train loss: 0.5302838564
Epoch:   800  |  train loss: 0.5235871315
Epoch:   900  |  train loss: 0.5179414153
Epoch:  1000  |  train loss: 0.5138209820
/home/z1165703/FeCAM/models/base.py:87: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  vec = torch.tensor(vec).cuda()
2024-03-05 11:57:53,643 [trainer.py] => CNN: {'total': 83.44, '00-09': 87.7, '10-19': 79.6, '20-29': 84.9, '30-39': 81.0, '40-49': 84.0, 'old': 0, 'new': 83.44}
2024-03-05 11:57:56,625 [trainer.py] => No NME accuracy
2024-03-05 11:57:56,625 [trainer.py] => FeCAM: {'total': 82.9, '00-09': 86.2, '10-19': 79.7, '20-29': 85.0, '30-39': 80.6, '40-49': 83.0, 'old': 0, 'new': 82.9}
2024-03-05 11:57:56,627 [trainer.py] => CNN top1 curve: [83.44]
2024-03-05 11:57:56,627 [trainer.py] => CNN top5 curve: [96.5]
2024-03-05 11:57:56,627 [trainer.py] => FeCAM top1 curve: [82.9]
2024-03-05 11:57:56,627 [trainer.py] => FeCAM top5 curve: [95.84]

2024-03-05 11:57:56,636 [fecam.py] => Learning on 50-60
Processing class: 50
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.6271429420
Epoch:   200  |  train loss: 0.6074266076
Epoch:   300  |  train loss: 0.5944905996
Epoch:   400  |  train loss: 0.5849616528
Epoch:   500  |  train loss: 0.5713930726
Epoch:   600  |  train loss: 0.5567482948
Epoch:   700  |  train loss: 0.5513174534
Epoch:   800  |  train loss: 0.5453053951
Epoch:   900  |  train loss: 0.5421098590
Epoch:  1000  |  train loss: 0.5371293783
Processing class: 51
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.6323795199
Epoch:   200  |  train loss: 0.6058351159
Epoch:   300  |  train loss: 0.5980254889
Epoch:   400  |  train loss: 0.5881481290
Epoch:   500  |  train loss: 0.5721925616
Epoch:   600  |  train loss: 0.5599167943
Epoch:   700  |  train loss: 0.5459380388
Epoch:   800  |  train loss: 0.5398956418
Epoch:   900  |  train loss: 0.5390368104
Epoch:  1000  |  train loss: 0.5279979825
Processing class: 52
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.6531474829
Epoch:   200  |  train loss: 0.6423141241
Epoch:   300  |  train loss: 0.6311793804
Epoch:   400  |  train loss: 0.6212006927
Epoch:   500  |  train loss: 0.6089069247
Epoch:   600  |  train loss: 0.5948253989
Epoch:   700  |  train loss: 0.5907160401
Epoch:   800  |  train loss: 0.5824652433
Epoch:   900  |  train loss: 0.5748860359
Epoch:  1000  |  train loss: 0.5635860562
Processing class: 53
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.6032343507
Epoch:   200  |  train loss: 0.5810491562
Epoch:   300  |  train loss: 0.5660482526
Epoch:   400  |  train loss: 0.5550785303
Epoch:   500  |  train loss: 0.5513158560
Epoch:   600  |  train loss: 0.5491092563
Epoch:   700  |  train loss: 0.5465900660
Epoch:   800  |  train loss: 0.5313256383
Epoch:   900  |  train loss: 0.5314979911
Epoch:  1000  |  train loss: 0.5228108287
Processing class: 54
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.5671269178
Epoch:   200  |  train loss: 0.5486622214
Epoch:   300  |  train loss: 0.5319632411
Epoch:   400  |  train loss: 0.5225181222
Epoch:   500  |  train loss: 0.5148202300
Epoch:   600  |  train loss: 0.5114415884
Epoch:   700  |  train loss: 0.5050966978
Epoch:   800  |  train loss: 0.5027015746
Epoch:   900  |  train loss: 0.5000911951
Epoch:  1000  |  train loss: 0.4916502833
Processing class: 55
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.6574603558
Epoch:   200  |  train loss: 0.6514092088
Epoch:   300  |  train loss: 0.6402493119
Epoch:   400  |  train loss: 0.6372780561
Epoch:   500  |  train loss: 0.6352643013
Epoch:   600  |  train loss: 0.6258685827
Epoch:   700  |  train loss: 0.6193937063
Epoch:   800  |  train loss: 0.6165604711
Epoch:   900  |  train loss: 0.6004001260
Epoch:  1000  |  train loss: 0.5972892761
Processing class: 56
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.6187922001
Epoch:   200  |  train loss: 0.5921901703
Epoch:   300  |  train loss: 0.5561996698
Epoch:   400  |  train loss: 0.5437896371
Epoch:   500  |  train loss: 0.5288170874
Epoch:   600  |  train loss: 0.5152513504
Epoch:   700  |  train loss: 0.5059355438
Epoch:   800  |  train loss: 0.4936921895
Epoch:   900  |  train loss: 0.4900243461
Epoch:  1000  |  train loss: 0.4855719864
Processing class: 57
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.6534951925
Epoch:   200  |  train loss: 0.6520331144
Epoch:   300  |  train loss: 0.6410709739
Epoch:   400  |  train loss: 0.6312510610
Epoch:   500  |  train loss: 0.6248574018
Epoch:   600  |  train loss: 0.6118590355
Epoch:   700  |  train loss: 0.6117537498
Epoch:   800  |  train loss: 0.6032993317
Epoch:   900  |  train loss: 0.5991569281
Epoch:  1000  |  train loss: 0.5841840625
Processing class: 58
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.6307306647
Epoch:   200  |  train loss: 0.5871117830
Epoch:   300  |  train loss: 0.5695814252
Epoch:   400  |  train loss: 0.5536210060
Epoch:   500  |  train loss: 0.5406733632
Epoch:   600  |  train loss: 0.5354221940
Epoch:   700  |  train loss: 0.5239342809
Epoch:   800  |  train loss: 0.5202652454
Epoch:   900  |  train loss: 0.5128407896
Epoch:  1000  |  train loss: 0.5054529250
Processing class: 59
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.6535739660
Epoch:   200  |  train loss: 0.6460398793
Epoch:   300  |  train loss: 0.6230488181
Epoch:   400  |  train loss: 0.6079292178
Epoch:   500  |  train loss: 0.5980044127
Epoch:   600  |  train loss: 0.5896435857
Epoch:   700  |  train loss: 0.5836973906
Epoch:   800  |  train loss: 0.5774594784
Epoch:   900  |  train loss: 0.5695605278
Epoch:  1000  |  train loss: 0.5638837814
2024-03-05 12:03:42,729 [trainer.py] => CNN: {'total': 71.75, '00-09': 82.2, '10-19': 72.8, '20-29': 78.5, '30-39': 74.9, '40-49': 68.4, '50-59': 53.7, 'old': 75.36, 'new': 53.7}
2024-03-05 12:03:42,974 [trainer.py] => No NME accuracy
2024-03-05 12:03:42,974 [trainer.py] => FeCAM: {'total': 72.78, '00-09': 82.6, '10-19': 74.0, '20-29': 81.0, '30-39': 76.4, '40-49': 76.8, '50-59': 45.9, 'old': 78.16, 'new': 45.9}
2024-03-05 12:03:42,974 [trainer.py] => CNN top1 curve: [83.44, 71.75]
2024-03-05 12:03:42,974 [trainer.py] => CNN top5 curve: [96.5, 89.6]
2024-03-05 12:03:42,974 [trainer.py] => FeCAM top1 curve: [82.9, 72.78]
2024-03-05 12:03:42,974 [trainer.py] => FeCAM top5 curve: [95.84, 91.18]

2024-03-05 12:03:42,981 [fecam.py] => Learning on 60-70
Processing class: 60
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.6515875816
Epoch:   200  |  train loss: 0.6371936083
Epoch:   300  |  train loss: 0.6163551807
Epoch:   400  |  train loss: 0.5989437222
Epoch:   500  |  train loss: 0.5891551018
Epoch:   600  |  train loss: 0.5754399300
Epoch:   700  |  train loss: 0.5747114658
Epoch:   800  |  train loss: 0.5554571271
Epoch:   900  |  train loss: 0.5484082103
Epoch:  1000  |  train loss: 0.5431015849
Processing class: 61
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.6159499168
Epoch:   200  |  train loss: 0.5826727390
Epoch:   300  |  train loss: 0.5626194358
Epoch:   400  |  train loss: 0.5417061687
Epoch:   500  |  train loss: 0.5344839454
Epoch:   600  |  train loss: 0.5185175657
Epoch:   700  |  train loss: 0.5131626248
Epoch:   800  |  train loss: 0.5078231096
Epoch:   900  |  train loss: 0.4942245066
Epoch:  1000  |  train loss: 0.4913966954
Processing class: 62
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.6622094274
Epoch:   200  |  train loss: 0.6508611202
Epoch:   300  |  train loss: 0.6480422020
Epoch:   400  |  train loss: 0.6425511241
Epoch:   500  |  train loss: 0.6320615768
Epoch:   600  |  train loss: 0.6313591719
Epoch:   700  |  train loss: 0.6268001795
Epoch:   800  |  train loss: 0.6200420856
Epoch:   900  |  train loss: 0.6104019761
Epoch:  1000  |  train loss: 0.6059529424
Processing class: 63
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.6321287155
Epoch:   200  |  train loss: 0.6237003922
Epoch:   300  |  train loss: 0.6037054420
Epoch:   400  |  train loss: 0.5837755322
Epoch:   500  |  train loss: 0.5695799589
Epoch:   600  |  train loss: 0.5643044710
Epoch:   700  |  train loss: 0.5475459695
Epoch:   800  |  train loss: 0.5499789953
Epoch:   900  |  train loss: 0.5359049439
Epoch:  1000  |  train loss: 0.5317224860
Processing class: 64
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.6300606370
Epoch:   200  |  train loss: 0.5798653126
Epoch:   300  |  train loss: 0.5649108768
Epoch:   400  |  train loss: 0.5493151188
Epoch:   500  |  train loss: 0.5312413156
Epoch:   600  |  train loss: 0.5174986243
Epoch:   700  |  train loss: 0.5028922558
Epoch:   800  |  train loss: 0.4981549859
Epoch:   900  |  train loss: 0.4963112593
Epoch:  1000  |  train loss: 0.4862272024
Processing class: 65
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.6190712452
Epoch:   200  |  train loss: 0.5856362939
Epoch:   300  |  train loss: 0.5627360344
Epoch:   400  |  train loss: 0.5478267908
Epoch:   500  |  train loss: 0.5326155663
Epoch:   600  |  train loss: 0.5278815627
Epoch:   700  |  train loss: 0.5181649923
Epoch:   800  |  train loss: 0.5161127150
Epoch:   900  |  train loss: 0.5038525224
Epoch:  1000  |  train loss: 0.5037001491
Processing class: 66
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.6438240170
Epoch:   200  |  train loss: 0.6286135554
Epoch:   300  |  train loss: 0.6130382657
Epoch:   400  |  train loss: 0.5989869595
Epoch:   500  |  train loss: 0.5953703523
Epoch:   600  |  train loss: 0.5872671962
Epoch:   700  |  train loss: 0.5858110666
Epoch:   800  |  train loss: 0.5813116431
Epoch:   900  |  train loss: 0.5749857426
Epoch:  1000  |  train loss: 0.5702825546
Processing class: 67
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.6387064219
Epoch:   200  |  train loss: 0.6274693012
Epoch:   300  |  train loss: 0.6127225518
Epoch:   400  |  train loss: 0.6027316332
Epoch:   500  |  train loss: 0.5931578636
Epoch:   600  |  train loss: 0.5850920916
Epoch:   700  |  train loss: 0.5786659479
Epoch:   800  |  train loss: 0.5750446200
Epoch:   900  |  train loss: 0.5674261451
Epoch:  1000  |  train loss: 0.5612919092
Processing class: 68
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.6078470111
Epoch:   200  |  train loss: 0.5958967328
Epoch:   300  |  train loss: 0.5645989180
Epoch:   400  |  train loss: 0.5522949457
Epoch:   500  |  train loss: 0.5434183359
Epoch:   600  |  train loss: 0.5329856873
Epoch:   700  |  train loss: 0.5278364062
Epoch:   800  |  train loss: 0.5184361994
Epoch:   900  |  train loss: 0.5129176259
Epoch:  1000  |  train loss: 0.5141564310
Processing class: 69
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.5184428930
Epoch:   200  |  train loss: 0.5010238826
Epoch:   300  |  train loss: 0.4851888537
Epoch:   400  |  train loss: 0.4731900334
Epoch:   500  |  train loss: 0.4637281775
Epoch:   600  |  train loss: 0.4533264935
Epoch:   700  |  train loss: 0.4531714082
Epoch:   800  |  train loss: 0.4482274413
Epoch:   900  |  train loss: 0.4530617535
Epoch:  1000  |  train loss: 0.4423216462
2024-03-05 12:10:23,653 [trainer.py] => CNN: {'total': 64.66, '00-09': 75.0, '10-19': 70.1, '20-29': 77.1, '30-39': 70.6, '40-49': 64.0, '50-59': 45.8, '60-69': 50.0, 'old': 67.1, 'new': 50.0}
2024-03-05 12:10:23,653 [trainer.py] => No NME accuracy
2024-03-05 12:10:23,653 [trainer.py] => FeCAM: {'total': 67.27, '00-09': 79.9, '10-19': 70.6, '20-29': 79.1, '30-39': 71.9, '40-49': 74.1, '50-59': 41.2, '60-69': 54.1, 'old': 69.47, 'new': 54.1}
2024-03-05 12:10:23,653 [trainer.py] => CNN top1 curve: [83.44, 71.75, 64.66]
2024-03-05 12:10:23,653 [trainer.py] => CNN top5 curve: [96.5, 89.6, 86.54]
2024-03-05 12:10:23,653 [trainer.py] => FeCAM top1 curve: [82.9, 72.78, 67.27]
2024-03-05 12:10:23,653 [trainer.py] => FeCAM top5 curve: [95.84, 91.18, 87.21]

2024-03-05 12:10:23,660 [fecam.py] => Learning on 70-80
Processing class: 70
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.6261016130
Epoch:   200  |  train loss: 0.6080289483
Epoch:   300  |  train loss: 0.5853853583
Epoch:   400  |  train loss: 0.5742171884
Epoch:   500  |  train loss: 0.5559762239
Epoch:   600  |  train loss: 0.5488338828
Epoch:   700  |  train loss: 0.5417037606
Epoch:   800  |  train loss: 0.5238558412
Epoch:   900  |  train loss: 0.5224207163
Epoch:  1000  |  train loss: 0.5145384252
Processing class: 71
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.6469510555
Epoch:   200  |  train loss: 0.6375224113
Epoch:   300  |  train loss: 0.6109862447
Epoch:   400  |  train loss: 0.6001053572
Epoch:   500  |  train loss: 0.5893702388
Epoch:   600  |  train loss: 0.5784361362
Epoch:   700  |  train loss: 0.5679612398
Epoch:   800  |  train loss: 0.5598860741
Epoch:   900  |  train loss: 0.5551765442
Epoch:  1000  |  train loss: 0.5453806877
Processing class: 72
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.6353295803
Epoch:   200  |  train loss: 0.6234080076
Epoch:   300  |  train loss: 0.6075218201
Epoch:   400  |  train loss: 0.5920124888
Epoch:   500  |  train loss: 0.5846807241
Epoch:   600  |  train loss: 0.5711566091
Epoch:   700  |  train loss: 0.5647896409
Epoch:   800  |  train loss: 0.5548403502
Epoch:   900  |  train loss: 0.5484462857
Epoch:  1000  |  train loss: 0.5458862066
Processing class: 73
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.6409798384
Epoch:   200  |  train loss: 0.6206761241
Epoch:   300  |  train loss: 0.6025236011
Epoch:   400  |  train loss: 0.5869902015
Epoch:   500  |  train loss: 0.5881510854
Epoch:   600  |  train loss: 0.5808017731
Epoch:   700  |  train loss: 0.5694082856
Epoch:   800  |  train loss: 0.5634375095
Epoch:   900  |  train loss: 0.5567621708
Epoch:  1000  |  train loss: 0.5535910010
Processing class: 74
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.6606663704
Epoch:   200  |  train loss: 0.6594955444
Epoch:   300  |  train loss: 0.6413179159
Epoch:   400  |  train loss: 0.6250393391
Epoch:   500  |  train loss: 0.6150743008
Epoch:   600  |  train loss: 0.6023108482
Epoch:   700  |  train loss: 0.6018996239
Epoch:   800  |  train loss: 0.5857007384
Epoch:   900  |  train loss: 0.5792035222
Epoch:  1000  |  train loss: 0.5779453039
Processing class: 75
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.6409507036
Epoch:   200  |  train loss: 0.6254119158
Epoch:   300  |  train loss: 0.6122389674
Epoch:   400  |  train loss: 0.5992265224
Epoch:   500  |  train loss: 0.5925566316
Epoch:   600  |  train loss: 0.5869608283
Epoch:   700  |  train loss: 0.5666099191
Epoch:   800  |  train loss: 0.5672273040
Epoch:   900  |  train loss: 0.5603151798
Epoch:  1000  |  train loss: 0.5584571481
Processing class: 76
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.5166115046
Epoch:   200  |  train loss: 0.4896348000
Epoch:   300  |  train loss: 0.4757231593
Epoch:   400  |  train loss: 0.4693162560
Epoch:   500  |  train loss: 0.4648152947
Epoch:   600  |  train loss: 0.4615891695
Epoch:   700  |  train loss: 0.4599268436
Epoch:   800  |  train loss: 0.4478695989
Epoch:   900  |  train loss: 0.4496051371
Epoch:  1000  |  train loss: 0.4491870999
Processing class: 77
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.6156822443
Epoch:   200  |  train loss: 0.5972395778
Epoch:   300  |  train loss: 0.5713535905
Epoch:   400  |  train loss: 0.5581365108
Epoch:   500  |  train loss: 0.5404471755
Epoch:   600  |  train loss: 0.5256597519
Epoch:   700  |  train loss: 0.5160650611
Epoch:   800  |  train loss: 0.5116660655
Epoch:   900  |  train loss: 0.5018683553
Epoch:  1000  |  train loss: 0.4935366035
Processing class: 78
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.6320085883
Epoch:   200  |  train loss: 0.6064049125
Epoch:   300  |  train loss: 0.5959605217
Epoch:   400  |  train loss: 0.5799260616
Epoch:   500  |  train loss: 0.5627965808
Epoch:   600  |  train loss: 0.5561704636
Epoch:   700  |  train loss: 0.5483405590
Epoch:   800  |  train loss: 0.5412540436
Epoch:   900  |  train loss: 0.5362636685
Epoch:  1000  |  train loss: 0.5300028563
Processing class: 79
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.6593775034
Epoch:   200  |  train loss: 0.6449331403
Epoch:   300  |  train loss: 0.6417008758
Epoch:   400  |  train loss: 0.6293030500
Epoch:   500  |  train loss: 0.6205713034
Epoch:   600  |  train loss: 0.6163645148
Epoch:   700  |  train loss: 0.6078483105
Epoch:   800  |  train loss: 0.6007946253
Epoch:   900  |  train loss: 0.5967725277
Epoch:  1000  |  train loss: 0.5846777439
2024-03-05 12:18:01,930 [trainer.py] => CNN: {'total': 59.18, '00-09': 73.6, '10-19': 68.4, '20-29': 76.9, '30-39': 69.1, '40-49': 60.6, '50-59': 37.2, '60-69': 45.7, '70-79': 41.9, 'old': 61.64, 'new': 41.9}
2024-03-05 12:18:04,776 [trainer.py] => No NME accuracy
2024-03-05 12:18:04,776 [trainer.py] => FeCAM: {'total': 61.72, '00-09': 78.1, '10-19': 69.3, '20-29': 78.0, '30-39': 69.5, '40-49': 72.1, '50-59': 35.7, '60-69': 49.1, '70-79': 42.0, 'old': 64.54, 'new': 42.0}
2024-03-05 12:18:04,778 [trainer.py] => CNN top1 curve: [83.44, 71.75, 64.66, 59.18]
2024-03-05 12:18:04,778 [trainer.py] => CNN top5 curve: [96.5, 89.6, 86.54, 84.09]
2024-03-05 12:18:04,778 [trainer.py] => FeCAM top1 curve: [82.9, 72.78, 67.27, 61.72]
2024-03-05 12:18:04,778 [trainer.py] => FeCAM top5 curve: [95.84, 91.18, 87.21, 84.09]

2024-03-05 12:18:04,786 [fecam.py] => Learning on 80-90
Processing class: 80
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.5690629125
Epoch:   200  |  train loss: 0.5260255575
Epoch:   300  |  train loss: 0.5101884544
Epoch:   400  |  train loss: 0.5006394029
Epoch:   500  |  train loss: 0.4898654819
Epoch:   600  |  train loss: 0.4840388119
Epoch:   700  |  train loss: 0.4726553321
Epoch:   800  |  train loss: 0.4758363545
Epoch:   900  |  train loss: 0.4703785598
Epoch:  1000  |  train loss: 0.4694424033
Processing class: 81
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.6187337399
Epoch:   200  |  train loss: 0.6022426128
Epoch:   300  |  train loss: 0.5803967357
Epoch:   400  |  train loss: 0.5746757030
Epoch:   500  |  train loss: 0.5616881371
Epoch:   600  |  train loss: 0.5576404572
Epoch:   700  |  train loss: 0.5478053808
Epoch:   800  |  train loss: 0.5455357671
Epoch:   900  |  train loss: 0.5373705387
Epoch:  1000  |  train loss: 0.5307327628
Processing class: 82
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.5968638182
Epoch:   200  |  train loss: 0.5744924784
Epoch:   300  |  train loss: 0.5580394745
Epoch:   400  |  train loss: 0.5416942835
Epoch:   500  |  train loss: 0.5295198083
Epoch:   600  |  train loss: 0.5191318810
Epoch:   700  |  train loss: 0.5117556989
Epoch:   800  |  train loss: 0.5073135078
Epoch:   900  |  train loss: 0.5007926941
Epoch:  1000  |  train loss: 0.4949056327
Processing class: 83
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.5894854069
Epoch:   200  |  train loss: 0.5496617198
Epoch:   300  |  train loss: 0.5227492452
Epoch:   400  |  train loss: 0.5061047852
Epoch:   500  |  train loss: 0.4987978160
Epoch:   600  |  train loss: 0.4866527021
Epoch:   700  |  train loss: 0.4859717309
Epoch:   800  |  train loss: 0.4862089872
Epoch:   900  |  train loss: 0.4783482373
Epoch:  1000  |  train loss: 0.4708368957
Processing class: 84
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.6380125403
Epoch:   200  |  train loss: 0.6030794501
Epoch:   300  |  train loss: 0.5883085608
Epoch:   400  |  train loss: 0.5699887037
Epoch:   500  |  train loss: 0.5601154447
Epoch:   600  |  train loss: 0.5463527441
Epoch:   700  |  train loss: 0.5422293186
Epoch:   800  |  train loss: 0.5395529032
Epoch:   900  |  train loss: 0.5266312480
Epoch:  1000  |  train loss: 0.5194684148
Processing class: 85
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.6393316150
Epoch:   200  |  train loss: 0.6092574596
Epoch:   300  |  train loss: 0.5913313389
Epoch:   400  |  train loss: 0.5726218820
Epoch:   500  |  train loss: 0.5641018748
Epoch:   600  |  train loss: 0.5568909049
Epoch:   700  |  train loss: 0.5551755309
Epoch:   800  |  train loss: 0.5432519674
Epoch:   900  |  train loss: 0.5374584794
Epoch:  1000  |  train loss: 0.5354621291
Processing class: 86
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.6620246053
Epoch:   200  |  train loss: 0.6441111326
Epoch:   300  |  train loss: 0.6308699012
Epoch:   400  |  train loss: 0.6120216727
Epoch:   500  |  train loss: 0.6009274006
Epoch:   600  |  train loss: 0.5920636058
Epoch:   700  |  train loss: 0.5830705404
Epoch:   800  |  train loss: 0.5740941525
Epoch:   900  |  train loss: 0.5673941016
Epoch:  1000  |  train loss: 0.5674028993
Processing class: 87
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.6026885033
Epoch:   200  |  train loss: 0.5684764147
Epoch:   300  |  train loss: 0.5530477166
Epoch:   400  |  train loss: 0.5362220526
Epoch:   500  |  train loss: 0.5235171795
Epoch:   600  |  train loss: 0.5204524577
Epoch:   700  |  train loss: 0.5150277376
Epoch:   800  |  train loss: 0.5082404852
Epoch:   900  |  train loss: 0.4988241732
Epoch:  1000  |  train loss: 0.5005855739
Processing class: 88
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.6280734658
Epoch:   200  |  train loss: 0.6084581137
Epoch:   300  |  train loss: 0.5877979279
Epoch:   400  |  train loss: 0.5723824024
Epoch:   500  |  train loss: 0.5551550269
Epoch:   600  |  train loss: 0.5384998798
Epoch:   700  |  train loss: 0.5331170201
Epoch:   800  |  train loss: 0.5243566990
Epoch:   900  |  train loss: 0.5187223315
Epoch:  1000  |  train loss: 0.5096585929
Processing class: 89
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.6254089713
Epoch:   200  |  train loss: 0.6041556358
Epoch:   300  |  train loss: 0.5812221766
Epoch:   400  |  train loss: 0.5697614789
Epoch:   500  |  train loss: 0.5606629968
Epoch:   600  |  train loss: 0.5476636767
Epoch:   700  |  train loss: 0.5346349716
Epoch:   800  |  train loss: 0.5345702410
Epoch:   900  |  train loss: 0.5244830370
Epoch:  1000  |  train loss: 0.5196662545
2024-03-05 12:27:05,929 [trainer.py] => CNN: {'total': 54.08, '00-09': 67.9, '10-19': 63.6, '20-29': 73.2, '30-39': 68.8, '40-49': 57.2, '50-59': 32.6, '60-69': 40.9, '70-79': 38.1, '80-89': 44.4, 'old': 55.29, 'new': 44.4}
2024-03-05 12:27:05,932 [trainer.py] => No NME accuracy
2024-03-05 12:27:05,932 [trainer.py] => FeCAM: {'total': 57.66, '00-09': 76.1, '10-19': 65.8, '20-29': 75.3, '30-39': 67.9, '40-49': 68.5, '50-59': 33.0, '60-69': 45.0, '70-79': 39.0, '80-89': 48.3, 'old': 58.82, 'new': 48.3}
2024-03-05 12:27:05,932 [trainer.py] => CNN top1 curve: [83.44, 71.75, 64.66, 59.18, 54.08]
2024-03-05 12:27:05,932 [trainer.py] => CNN top5 curve: [96.5, 89.6, 86.54, 84.09, 81.74]
2024-03-05 12:27:05,932 [trainer.py] => FeCAM top1 curve: [82.9, 72.78, 67.27, 61.72, 57.66]
2024-03-05 12:27:05,932 [trainer.py] => FeCAM top5 curve: [95.84, 91.18, 87.21, 84.09, 81.63]

2024-03-05 12:27:05,945 [fecam.py] => Learning on 90-100
Processing class: 90
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.6387005448
Epoch:   200  |  train loss: 0.6244542956
Epoch:   300  |  train loss: 0.5947207451
Epoch:   400  |  train loss: 0.5729895711
Epoch:   500  |  train loss: 0.5551392317
Epoch:   600  |  train loss: 0.5424348593
Epoch:   700  |  train loss: 0.5277251482
Epoch:   800  |  train loss: 0.5201280594
Epoch:   900  |  train loss: 0.5159544289
Epoch:  1000  |  train loss: 0.5135988295
Processing class: 91
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.5258362532
Epoch:   200  |  train loss: 0.5065229595
Epoch:   300  |  train loss: 0.4940403938
Epoch:   400  |  train loss: 0.4901528418
Epoch:   500  |  train loss: 0.4808815360
Epoch:   600  |  train loss: 0.4749335349
Epoch:   700  |  train loss: 0.4643849015
Epoch:   800  |  train loss: 0.4571462452
Epoch:   900  |  train loss: 0.4521133125
Epoch:  1000  |  train loss: 0.4485063195
Processing class: 92
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.6237542152
Epoch:   200  |  train loss: 0.5795624137
Epoch:   300  |  train loss: 0.5603395462
Epoch:   400  |  train loss: 0.5503616810
Epoch:   500  |  train loss: 0.5366065383
Epoch:   600  |  train loss: 0.5222837865
Epoch:   700  |  train loss: 0.5107691109
Epoch:   800  |  train loss: 0.4953212261
Epoch:   900  |  train loss: 0.4897898614
Epoch:  1000  |  train loss: 0.4840304554
Processing class: 93
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.6299203515
Epoch:   200  |  train loss: 0.5987837911
Epoch:   300  |  train loss: 0.5841815829
Epoch:   400  |  train loss: 0.5721496820
Epoch:   500  |  train loss: 0.5562610149
Epoch:   600  |  train loss: 0.5397733688
Epoch:   700  |  train loss: 0.5374867916
Epoch:   800  |  train loss: 0.5274464369
Epoch:   900  |  train loss: 0.5210638821
Epoch:  1000  |  train loss: 0.5116175234
Processing class: 94
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.5790876985
Epoch:   200  |  train loss: 0.5562529922
Epoch:   300  |  train loss: 0.5217436254
Epoch:   400  |  train loss: 0.5062197268
Epoch:   500  |  train loss: 0.4859833837
Epoch:   600  |  train loss: 0.4806575179
Epoch:   700  |  train loss: 0.4763338268
Epoch:   800  |  train loss: 0.4584109902
Epoch:   900  |  train loss: 0.4582030952
Epoch:  1000  |  train loss: 0.4501874208
Processing class: 95
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.6438543439
Epoch:   200  |  train loss: 0.6391052961
Epoch:   300  |  train loss: 0.6228398204
Epoch:   400  |  train loss: 0.6049764872
Epoch:   500  |  train loss: 0.5962893128
Epoch:   600  |  train loss: 0.5848121166
Epoch:   700  |  train loss: 0.5769179344
Epoch:   800  |  train loss: 0.5660525918
Epoch:   900  |  train loss: 0.5617792726
Epoch:  1000  |  train loss: 0.5581535935
Processing class: 96
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.6535026312
Epoch:   200  |  train loss: 0.6305984259
Epoch:   300  |  train loss: 0.6140786171
Epoch:   400  |  train loss: 0.6078144908
Epoch:   500  |  train loss: 0.5922559857
Epoch:   600  |  train loss: 0.5816485405
Epoch:   700  |  train loss: 0.5725627542
Epoch:   800  |  train loss: 0.5602706313
Epoch:   900  |  train loss: 0.5572193980
Epoch:  1000  |  train loss: 0.5487299204
Processing class: 97
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.6330304861
Epoch:   200  |  train loss: 0.5999068856
Epoch:   300  |  train loss: 0.5763484955
Epoch:   400  |  train loss: 0.5637964129
Epoch:   500  |  train loss: 0.5552980781
Epoch:   600  |  train loss: 0.5471336603
Epoch:   700  |  train loss: 0.5408193588
Epoch:   800  |  train loss: 0.5324934006
Epoch:   900  |  train loss: 0.5262773752
Epoch:  1000  |  train loss: 0.5191428900
Processing class: 98
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.5934481502
Epoch:   200  |  train loss: 0.5676628351
Epoch:   300  |  train loss: 0.5416910172
Epoch:   400  |  train loss: 0.5260257840
Epoch:   500  |  train loss: 0.5105683267
Epoch:   600  |  train loss: 0.5013714433
Epoch:   700  |  train loss: 0.4883468270
Epoch:   800  |  train loss: 0.4852990031
Epoch:   900  |  train loss: 0.4800868630
Epoch:  1000  |  train loss: 0.4736548841
Processing class: 99
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.6440882206
Epoch:   200  |  train loss: 0.6192946315
Epoch:   300  |  train loss: 0.5982253075
Epoch:   400  |  train loss: 0.5844851375
Epoch:   500  |  train loss: 0.5662568331
Epoch:   600  |  train loss: 0.5532036901
Epoch:   700  |  train loss: 0.5436357141
Epoch:   800  |  train loss: 0.5403206587
Epoch:   900  |  train loss: 0.5299300432
Epoch:  1000  |  train loss: 0.5244499564
2024-03-05 12:37:23,279 [trainer.py] => CNN: {'total': 50.36, '00-09': 57.7, '10-19': 63.6, '20-29': 71.4, '30-39': 68.5, '40-49': 56.8, '50-59': 29.6, '60-69': 38.9, '70-79': 36.6, '80-89': 42.7, '90-99': 37.8, 'old': 51.76, 'new': 37.8}
2024-03-05 12:37:23,631 [trainer.py] => No NME accuracy
2024-03-05 12:37:23,631 [trainer.py] => FeCAM: {'total': 54.74, '00-09': 73.3, '10-19': 65.1, '20-29': 74.1, '30-39': 67.2, '40-49': 67.1, '50-59': 30.8, '60-69': 43.3, '70-79': 37.2, '80-89': 46.9, '90-99': 42.4, 'old': 56.11, 'new': 42.4}
2024-03-05 12:37:23,633 [trainer.py] => CNN top1 curve: [83.44, 71.75, 64.66, 59.18, 54.08, 50.36]
2024-03-05 12:37:23,633 [trainer.py] => CNN top5 curve: [96.5, 89.6, 86.54, 84.09, 81.74, 79.63]
2024-03-05 12:37:23,633 [trainer.py] => FeCAM top1 curve: [82.9, 72.78, 67.27, 61.72, 57.66, 54.74]
2024-03-05 12:37:23,633 [trainer.py] => FeCAM top5 curve: [95.84, 91.18, 87.21, 84.09, 81.63, 79.21]

=========================================
2024-03-05 12:37:40,916 [trainer.py] => config: ./exps/FeCAM_cifar100.json
2024-03-05 12:37:40,917 [trainer.py] => prefix: train
2024-03-05 12:37:40,917 [trainer.py] => dataset: cifar100
2024-03-05 12:37:40,917 [trainer.py] => memory_size: 0
2024-03-05 12:37:40,917 [trainer.py] => shuffle: True
2024-03-05 12:37:40,917 [trainer.py] => init_cls: 50
2024-03-05 12:37:40,917 [trainer.py] => increment: 10
2024-03-05 12:37:40,917 [trainer.py] => model_name: fecam
2024-03-05 12:37:40,917 [trainer.py] => convnet_type: resnet18
2024-03-05 12:37:40,917 [trainer.py] => device: [device(type='cuda', index=0)]
2024-03-05 12:37:40,917 [trainer.py] => seed: 1993
2024-03-05 12:37:40,917 [trainer.py] => init_epochs: 200
2024-03-05 12:37:40,917 [trainer.py] => init_lr: 0.1
2024-03-05 12:37:40,917 [trainer.py] => init_weight_decay: 0.0005
2024-03-05 12:37:40,917 [trainer.py] => batch_size: 128
2024-03-05 12:37:40,917 [trainer.py] => num_workers: 8
2024-03-05 12:37:40,917 [trainer.py] => T: 5
2024-03-05 12:37:40,917 [trainer.py] => beta: 0.5
2024-03-05 12:37:40,917 [trainer.py] => alpha1: 1
2024-03-05 12:37:40,917 [trainer.py] => alpha2: 1
2024-03-05 12:37:40,917 [trainer.py] => ncm: False
2024-03-05 12:37:40,917 [trainer.py] => tukey: False
2024-03-05 12:37:40,917 [trainer.py] => diagonal: False
2024-03-05 12:37:40,917 [trainer.py] => per_class: True
2024-03-05 12:37:40,917 [trainer.py] => full_cov: True
2024-03-05 12:37:40,917 [trainer.py] => shrink: True
2024-03-05 12:37:40,917 [trainer.py] => norm_cov: False
2024-03-05 12:37:40,917 [trainer.py] => vecnorm: False
2024-03-05 12:37:40,917 [trainer.py] => ae_type: wae
2024-03-05 12:37:40,917 [trainer.py] => epochs: 1000
2024-03-05 12:37:40,917 [trainer.py] => ae_latent_dim: 32
2024-03-05 12:37:40,917 [trainer.py] => wae_sigma: 20
2024-03-05 12:37:40,917 [trainer.py] => wae_C: 1
Files already downloaded and verified
Files already downloaded and verified
2024-03-05 12:37:42,575 [data_manager.py] => [68, 56, 78, 8, 23, 84, 90, 65, 74, 76, 40, 89, 3, 92, 55, 9, 26, 80, 43, 38, 58, 70, 77, 1, 85, 19, 17, 50, 28, 53, 13, 81, 45, 82, 6, 59, 83, 16, 15, 44, 91, 41, 72, 60, 79, 52, 20, 10, 31, 54, 37, 95, 14, 71, 96, 98, 97, 2, 64, 66, 42, 22, 35, 86, 24, 34, 87, 21, 99, 0, 88, 27, 18, 94, 11, 12, 47, 25, 30, 46, 62, 69, 36, 61, 7, 63, 75, 5, 32, 4, 51, 48, 73, 93, 39, 67, 29, 49, 57, 33]
2024-03-05 12:37:42,847 [fecam.py] => Learning on 0-50
/home/z1165703/miniconda3/lib/python3.11/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
/home/z1165703/miniconda3/lib/python3.11/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
Processing class: 0
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.4337566435
Epoch:   200  |  train loss: 0.4274637938
Epoch:   300  |  train loss: 0.4324843824
Epoch:   400  |  train loss: 0.4310828388
Epoch:   500  |  train loss: 0.4262882590
Epoch:   600  |  train loss: 0.4261197209
Epoch:   700  |  train loss: 0.4267088056
Epoch:   800  |  train loss: 0.4268824339
Epoch:   900  |  train loss: 0.4229617178
Epoch:  1000  |  train loss: 0.4211048424
Processing class: 1
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.4434159696
Epoch:   200  |  train loss: 0.4399015188
Epoch:   300  |  train loss: 0.4465909660
Epoch:   400  |  train loss: 0.4464724898
Epoch:   500  |  train loss: 0.4355619788
Epoch:   600  |  train loss: 0.4435146391
Epoch:   700  |  train loss: 0.4372929513
Epoch:   800  |  train loss: 0.4397606671
Epoch:   900  |  train loss: 0.4449850321
Epoch:  1000  |  train loss: 0.4361837745
Processing class: 2
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.4488593578
Epoch:   200  |  train loss: 0.4425536752
Epoch:   300  |  train loss: 0.4448983729
Epoch:   400  |  train loss: 0.4373092830
Epoch:   500  |  train loss: 0.4202173829
Epoch:   600  |  train loss: 0.4253224850
Epoch:   700  |  train loss: 0.4198835671
Epoch:   800  |  train loss: 0.4240623951
Epoch:   900  |  train loss: 0.4161556661
Epoch:  1000  |  train loss: 0.4110158682
Processing class: 3
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.4437311113
Epoch:   200  |  train loss: 0.4508471251
Epoch:   300  |  train loss: 0.4508113384
Epoch:   400  |  train loss: 0.4463104665
Epoch:   500  |  train loss: 0.4394751370
Epoch:   600  |  train loss: 0.4448899508
Epoch:   700  |  train loss: 0.4453978479
Epoch:   800  |  train loss: 0.4380185068
Epoch:   900  |  train loss: 0.4382281601
Epoch:  1000  |  train loss: 0.4363016129
Processing class: 4
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.4413583279
Epoch:   200  |  train loss: 0.4356468499
Epoch:   300  |  train loss: 0.4383074105
Epoch:   400  |  train loss: 0.4334607005
Epoch:   500  |  train loss: 0.4334612250
Epoch:   600  |  train loss: 0.4266660094
Epoch:   700  |  train loss: 0.4295447946
Epoch:   800  |  train loss: 0.4289812267
Epoch:   900  |  train loss: 0.4288152635
Epoch:  1000  |  train loss: 0.4323090434
Processing class: 5
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.4522996306
Epoch:   200  |  train loss: 0.4414180219
Epoch:   300  |  train loss: 0.4391835093
Epoch:   400  |  train loss: 0.4335697472
Epoch:   500  |  train loss: 0.4304620147
Epoch:   600  |  train loss: 0.4381287336
Epoch:   700  |  train loss: 0.4358794034
Epoch:   800  |  train loss: 0.4273990035
Epoch:   900  |  train loss: 0.4228535235
Epoch:  1000  |  train loss: 0.4253796816
Processing class: 6
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.4428096592
Epoch:   200  |  train loss: 0.4461555421
Epoch:   300  |  train loss: 0.4495256603
Epoch:   400  |  train loss: 0.4429002225
Epoch:   500  |  train loss: 0.4472600877
Epoch:   600  |  train loss: 0.4423644304
Epoch:   700  |  train loss: 0.4379742324
Epoch:   800  |  train loss: 0.4379658282
Epoch:   900  |  train loss: 0.4303180099
Epoch:  1000  |  train loss: 0.4319154322
Processing class: 7
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.4461061954
Epoch:   200  |  train loss: 0.4420249224
Epoch:   300  |  train loss: 0.4413194180
Epoch:   400  |  train loss: 0.4374812782
Epoch:   500  |  train loss: 0.4359517932
Epoch:   600  |  train loss: 0.4306175947
Epoch:   700  |  train loss: 0.4318844438
Epoch:   800  |  train loss: 0.4365482211
Epoch:   900  |  train loss: 0.4270519316
Epoch:  1000  |  train loss: 0.4297852457
Processing class: 8
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.4437619984
Epoch:   200  |  train loss: 0.4450635016
Epoch:   300  |  train loss: 0.4347247243
Epoch:   400  |  train loss: 0.4399732172
Epoch:   500  |  train loss: 0.4328412235
Epoch:   600  |  train loss: 0.4309194028
Epoch:   700  |  train loss: 0.4351111293
Epoch:   800  |  train loss: 0.4265497744
Epoch:   900  |  train loss: 0.4284606576
Epoch:  1000  |  train loss: 0.4220337272
Processing class: 9
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.4331156850
Epoch:   200  |  train loss: 0.4377208054
Epoch:   300  |  train loss: 0.4434718668
Epoch:   400  |  train loss: 0.4321899593
Epoch:   500  |  train loss: 0.4378068447
Epoch:   600  |  train loss: 0.4312701166
Epoch:   700  |  train loss: 0.4332803428
Epoch:   800  |  train loss: 0.4341854990
Epoch:   900  |  train loss: 0.4330699682
Epoch:  1000  |  train loss: 0.4262225449
Processing class: 10
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.4389950693
Epoch:   200  |  train loss: 0.4316941381
Epoch:   300  |  train loss: 0.4312473238
Epoch:   400  |  train loss: 0.4369341671
Epoch:   500  |  train loss: 0.4219775498
Epoch:   600  |  train loss: 0.4228045225
Epoch:   700  |  train loss: 0.4234576523
Epoch:   800  |  train loss: 0.4184534550
Epoch:   900  |  train loss: 0.4194042683
Epoch:  1000  |  train loss: 0.4172769666
Processing class: 11
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.4527210414
Epoch:   200  |  train loss: 0.4470408797
Epoch:   300  |  train loss: 0.4392935753
Epoch:   400  |  train loss: 0.4392956316
Epoch:   500  |  train loss: 0.4343137622
Epoch:   600  |  train loss: 0.4369701087
Epoch:   700  |  train loss: 0.4425668895
Epoch:   800  |  train loss: 0.4391417444
Epoch:   900  |  train loss: 0.4368075132
Epoch:  1000  |  train loss: 0.4383398950
Processing class: 12
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.4365994334
Epoch:   200  |  train loss: 0.4462524652
Epoch:   300  |  train loss: 0.4412606180
Epoch:   400  |  train loss: 0.4406358719
Epoch:   500  |  train loss: 0.4380101800
Epoch:   600  |  train loss: 0.4303210735
Epoch:   700  |  train loss: 0.4295791268
Epoch:   800  |  train loss: 0.4299860239
Epoch:   900  |  train loss: 0.4322653949
Epoch:  1000  |  train loss: 0.4269515395
Processing class: 13
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.4368511379
Epoch:   200  |  train loss: 0.4393872023
Epoch:   300  |  train loss: 0.4402443230
Epoch:   400  |  train loss: 0.4361367047
Epoch:   500  |  train loss: 0.4374543309
Epoch:   600  |  train loss: 0.4305942416
Epoch:   700  |  train loss: 0.4312643886
Epoch:   800  |  train loss: 0.4335865021
Epoch:   900  |  train loss: 0.4344771206
Epoch:  1000  |  train loss: 0.4346840322
Processing class: 14
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.4550755501
Epoch:   200  |  train loss: 0.4584852874
Epoch:   300  |  train loss: 0.4562380910
Epoch:   400  |  train loss: 0.4574244738
Epoch:   500  |  train loss: 0.4518647373
Epoch:   600  |  train loss: 0.4485090852
Epoch:   700  |  train loss: 0.4511253953
Epoch:   800  |  train loss: 0.4575097680
Epoch:   900  |  train loss: 0.4533187449
Epoch:  1000  |  train loss: 0.4465062261
Processing class: 15
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.4380041897
Epoch:   200  |  train loss: 0.4390152991
Epoch:   300  |  train loss: 0.4381124258
Epoch:   400  |  train loss: 0.4372679532
Epoch:   500  |  train loss: 0.4389074028
Epoch:   600  |  train loss: 0.4316509128
Epoch:   700  |  train loss: 0.4371533453
Epoch:   800  |  train loss: 0.4292595625
Epoch:   900  |  train loss: 0.4354388773
Epoch:  1000  |  train loss: 0.4315153658
Processing class: 16
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.4433608115
Epoch:   200  |  train loss: 0.4494978786
Epoch:   300  |  train loss: 0.4520250797
Epoch:   400  |  train loss: 0.4452506900
Epoch:   500  |  train loss: 0.4497441590
Epoch:   600  |  train loss: 0.4434679866
Epoch:   700  |  train loss: 0.4391070962
Epoch:   800  |  train loss: 0.4453971326
Epoch:   900  |  train loss: 0.4355128288
Epoch:  1000  |  train loss: 0.4424580753
Processing class: 17
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.4307991385
Epoch:   200  |  train loss: 0.4351871252
Epoch:   300  |  train loss: 0.4412056983
Epoch:   400  |  train loss: 0.4384452105
Epoch:   500  |  train loss: 0.4229910254
Epoch:   600  |  train loss: 0.4253497243
Epoch:   700  |  train loss: 0.4214727283
Epoch:   800  |  train loss: 0.4197780967
Epoch:   900  |  train loss: 0.4196664691
Epoch:  1000  |  train loss: 0.4179426372
Processing class: 18
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.4368164480
Epoch:   200  |  train loss: 0.4381270885
Epoch:   300  |  train loss: 0.4325133085
Epoch:   400  |  train loss: 0.4274268270
Epoch:   500  |  train loss: 0.4286456287
Epoch:   600  |  train loss: 0.4206690133
Epoch:   700  |  train loss: 0.4291841745
Epoch:   800  |  train loss: 0.4252782166
Epoch:   900  |  train loss: 0.4250759542
Epoch:  1000  |  train loss: 0.4288215876
Processing class: 19
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.4321817100
Epoch:   200  |  train loss: 0.4407739699
Epoch:   300  |  train loss: 0.4363616526
Epoch:   400  |  train loss: 0.4352925837
Epoch:   500  |  train loss: 0.4261098564
Epoch:   600  |  train loss: 0.4256114900
Epoch:   700  |  train loss: 0.4264035463
Epoch:   800  |  train loss: 0.4216973543
Epoch:   900  |  train loss: 0.4306919932
Epoch:  1000  |  train loss: 0.4311555862
Processing class: 20
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.4414369464
Epoch:   200  |  train loss: 0.4398466349
Epoch:   300  |  train loss: 0.4424453795
Epoch:   400  |  train loss: 0.4399863362
Epoch:   500  |  train loss: 0.4455811381
Epoch:   600  |  train loss: 0.4387467861
Epoch:   700  |  train loss: 0.4373905063
Epoch:   800  |  train loss: 0.4365821421
Epoch:   900  |  train loss: 0.4372086525
Epoch:  1000  |  train loss: 0.4410699368
Processing class: 21
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.4440190732
Epoch:   200  |  train loss: 0.4439045906
Epoch:   300  |  train loss: 0.4355569065
Epoch:   400  |  train loss: 0.4313940644
Epoch:   500  |  train loss: 0.4299880505
Epoch:   600  |  train loss: 0.4310330033
Epoch:   700  |  train loss: 0.4280982971
Epoch:   800  |  train loss: 0.4234522402
Epoch:   900  |  train loss: 0.4267567337
Epoch:  1000  |  train loss: 0.4218545198
Processing class: 22
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.4468298018
Epoch:   200  |  train loss: 0.4472247899
Epoch:   300  |  train loss: 0.4513849854
Epoch:   400  |  train loss: 0.4514879823
Epoch:   500  |  train loss: 0.4486449540
Epoch:   600  |  train loss: 0.4415164351
Epoch:   700  |  train loss: 0.4410938919
Epoch:   800  |  train loss: 0.4355760455
Epoch:   900  |  train loss: 0.4310062408
Epoch:  1000  |  train loss: 0.4393492341
Processing class: 23
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.4432477295
Epoch:   200  |  train loss: 0.4483208060
Epoch:   300  |  train loss: 0.4508550584
Epoch:   400  |  train loss: 0.4400086105
Epoch:   500  |  train loss: 0.4366002202
Epoch:   600  |  train loss: 0.4282555938
Epoch:   700  |  train loss: 0.4285460413
Epoch:   800  |  train loss: 0.4227699757
Epoch:   900  |  train loss: 0.4246223629
Epoch:  1000  |  train loss: 0.4177362382
Processing class: 24
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.4441095769
Epoch:   200  |  train loss: 0.4430286348
Epoch:   300  |  train loss: 0.4375611305
Epoch:   400  |  train loss: 0.4389899552
Epoch:   500  |  train loss: 0.4327758670
Epoch:   600  |  train loss: 0.4344386339
Epoch:   700  |  train loss: 0.4231231809
Epoch:   800  |  train loss: 0.4244627476
Epoch:   900  |  train loss: 0.4276572824
Epoch:  1000  |  train loss: 0.4229129434
Processing class: 25
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.4425494850
Epoch:   200  |  train loss: 0.4427314699
Epoch:   300  |  train loss: 0.4455295503
Epoch:   400  |  train loss: 0.4339522123
Epoch:   500  |  train loss: 0.4362712145
Epoch:   600  |  train loss: 0.4298283339
Epoch:   700  |  train loss: 0.4349103868
Epoch:   800  |  train loss: 0.4296245515
Epoch:   900  |  train loss: 0.4247888625
Epoch:  1000  |  train loss: 0.4261265218
Processing class: 26
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.4454231501
Epoch:   200  |  train loss: 0.4525503278
Epoch:   300  |  train loss: 0.4494931042
Epoch:   400  |  train loss: 0.4524029553
Epoch:   500  |  train loss: 0.4548370957
Epoch:   600  |  train loss: 0.4402650535
Epoch:   700  |  train loss: 0.4413847685
Epoch:   800  |  train loss: 0.4430703640
Epoch:   900  |  train loss: 0.4388651550
Epoch:  1000  |  train loss: 0.4428693354
Processing class: 27
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.4486980796
Epoch:   200  |  train loss: 0.4385475814
Epoch:   300  |  train loss: 0.4395652652
Epoch:   400  |  train loss: 0.4357935250
Epoch:   500  |  train loss: 0.4348478436
Epoch:   600  |  train loss: 0.4334456265
Epoch:   700  |  train loss: 0.4335900366
Epoch:   800  |  train loss: 0.4314385116
Epoch:   900  |  train loss: 0.4313367307
Epoch:  1000  |  train loss: 0.4213610649
Processing class: 28
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.4496318102
Epoch:   200  |  train loss: 0.4547981799
Epoch:   300  |  train loss: 0.4549244821
Epoch:   400  |  train loss: 0.4533678114
Epoch:   500  |  train loss: 0.4519255817
Epoch:   600  |  train loss: 0.4500997365
Epoch:   700  |  train loss: 0.4507235765
Epoch:   800  |  train loss: 0.4514365852
Epoch:   900  |  train loss: 0.4515733242
Epoch:  1000  |  train loss: 0.4452702761
Processing class: 29
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.4437907219
Epoch:   200  |  train loss: 0.4411458135
Epoch:   300  |  train loss: 0.4350349545
Epoch:   400  |  train loss: 0.4262353122
Epoch:   500  |  train loss: 0.4288321018
Epoch:   600  |  train loss: 0.4284950852
Epoch:   700  |  train loss: 0.4248412848
Epoch:   800  |  train loss: 0.4175152004
Epoch:   900  |  train loss: 0.4164186537
Epoch:  1000  |  train loss: 0.4130329967
Processing class: 30
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.4450305820
Epoch:   200  |  train loss: 0.4506350994
Epoch:   300  |  train loss: 0.4529432893
Epoch:   400  |  train loss: 0.4589498460
Epoch:   500  |  train loss: 0.4544677079
Epoch:   600  |  train loss: 0.4508576274
Epoch:   700  |  train loss: 0.4460406721
Epoch:   800  |  train loss: 0.4501734853
Epoch:   900  |  train loss: 0.4524349153
Epoch:  1000  |  train loss: 0.4460627854
Processing class: 31
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.4507107258
Epoch:   200  |  train loss: 0.4530319870
Epoch:   300  |  train loss: 0.4480696380
Epoch:   400  |  train loss: 0.4543463945
Epoch:   500  |  train loss: 0.4539420307
Epoch:   600  |  train loss: 0.4575244665
Epoch:   700  |  train loss: 0.4536687851
Epoch:   800  |  train loss: 0.4529468298
Epoch:   900  |  train loss: 0.4558139026
Epoch:  1000  |  train loss: 0.4479402184
Processing class: 32
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.4520653903
Epoch:   200  |  train loss: 0.4569365025
Epoch:   300  |  train loss: 0.4513884425
Epoch:   400  |  train loss: 0.4406286657
Epoch:   500  |  train loss: 0.4473426700
Epoch:   600  |  train loss: 0.4387137532
Epoch:   700  |  train loss: 0.4440044284
Epoch:   800  |  train loss: 0.4444592357
Epoch:   900  |  train loss: 0.4403976440
Epoch:  1000  |  train loss: 0.4358999670
Processing class: 33
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.4347432852
Epoch:   200  |  train loss: 0.4499382377
Epoch:   300  |  train loss: 0.4309153557
Epoch:   400  |  train loss: 0.4268377542
Epoch:   500  |  train loss: 0.4240322113
Epoch:   600  |  train loss: 0.4296987951
Epoch:   700  |  train loss: 0.4223804295
Epoch:   800  |  train loss: 0.4223417878
Epoch:   900  |  train loss: 0.4191955566
Epoch:  1000  |  train loss: 0.4145584941
Processing class: 34
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.4494121790
Epoch:   200  |  train loss: 0.4454235971
Epoch:   300  |  train loss: 0.4440596879
Epoch:   400  |  train loss: 0.4373222768
Epoch:   500  |  train loss: 0.4324666798
Epoch:   600  |  train loss: 0.4347071409
Epoch:   700  |  train loss: 0.4263557017
Epoch:   800  |  train loss: 0.4301160753
Epoch:   900  |  train loss: 0.4299295604
Epoch:  1000  |  train loss: 0.4294059157
Processing class: 35
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.4541206062
Epoch:   200  |  train loss: 0.4520282567
Epoch:   300  |  train loss: 0.4535028100
Epoch:   400  |  train loss: 0.4528564870
Epoch:   500  |  train loss: 0.4447570324
Epoch:   600  |  train loss: 0.4476239502
Epoch:   700  |  train loss: 0.4467139781
Epoch:   800  |  train loss: 0.4439192116
Epoch:   900  |  train loss: 0.4433632910
Epoch:  1000  |  train loss: 0.4387400985
Processing class: 36
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.4342823327
Epoch:   200  |  train loss: 0.4322064042
Epoch:   300  |  train loss: 0.4320891500
Epoch:   400  |  train loss: 0.4301063895
Epoch:   500  |  train loss: 0.4341046691
Epoch:   600  |  train loss: 0.4266899526
Epoch:   700  |  train loss: 0.4195297778
Epoch:   800  |  train loss: 0.4255213320
Epoch:   900  |  train loss: 0.4155970335
Epoch:  1000  |  train loss: 0.4183825791
Processing class: 37
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.4562246084
Epoch:   200  |  train loss: 0.4520482898
Epoch:   300  |  train loss: 0.4455712616
Epoch:   400  |  train loss: 0.4447914898
Epoch:   500  |  train loss: 0.4447302282
Epoch:   600  |  train loss: 0.4374075234
Epoch:   700  |  train loss: 0.4355320930
Epoch:   800  |  train loss: 0.4361601114
Epoch:   900  |  train loss: 0.4294271529
Epoch:  1000  |  train loss: 0.4305126369
Processing class: 38
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.4375450253
Epoch:   200  |  train loss: 0.4391402304
Epoch:   300  |  train loss: 0.4421801150
Epoch:   400  |  train loss: 0.4436323583
Epoch:   500  |  train loss: 0.4407207787
Epoch:   600  |  train loss: 0.4347678840
Epoch:   700  |  train loss: 0.4318214715
Epoch:   800  |  train loss: 0.4361967266
Epoch:   900  |  train loss: 0.4273038626
Epoch:  1000  |  train loss: 0.4308552206
Processing class: 39
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.4452146947
Epoch:   200  |  train loss: 0.4401858389
Epoch:   300  |  train loss: 0.4409669161
Epoch:   400  |  train loss: 0.4405148208
Epoch:   500  |  train loss: 0.4343088984
Epoch:   600  |  train loss: 0.4340033174
Epoch:   700  |  train loss: 0.4336029112
Epoch:   800  |  train loss: 0.4348418176
Epoch:   900  |  train loss: 0.4337315559
Epoch:  1000  |  train loss: 0.4328492045
Processing class: 40
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.4435741782
Epoch:   200  |  train loss: 0.4319295406
Epoch:   300  |  train loss: 0.4410729945
Epoch:   400  |  train loss: 0.4409954607
Epoch:   500  |  train loss: 0.4415559530
Epoch:   600  |  train loss: 0.4330654740
Epoch:   700  |  train loss: 0.4347724795
Epoch:   800  |  train loss: 0.4325244963
Epoch:   900  |  train loss: 0.4280608058
Epoch:  1000  |  train loss: 0.4364903510
Processing class: 41
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.4336976707
Epoch:   200  |  train loss: 0.4354634523
Epoch:   300  |  train loss: 0.4329828560
Epoch:   400  |  train loss: 0.4247353375
Epoch:   500  |  train loss: 0.4215669632
Epoch:   600  |  train loss: 0.4198582768
Epoch:   700  |  train loss: 0.4227525353
Epoch:   800  |  train loss: 0.4183361411
Epoch:   900  |  train loss: 0.4152498186
Epoch:  1000  |  train loss: 0.4138345301
Processing class: 42
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.4439416885
Epoch:   200  |  train loss: 0.4465706468
Epoch:   300  |  train loss: 0.4460872233
Epoch:   400  |  train loss: 0.4424345553
Epoch:   500  |  train loss: 0.4411141038
Epoch:   600  |  train loss: 0.4389117658
Epoch:   700  |  train loss: 0.4389189839
Epoch:   800  |  train loss: 0.4373402059
Epoch:   900  |  train loss: 0.4312757134
Epoch:  1000  |  train loss: 0.4284215212
Processing class: 43
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.4500838757
Epoch:   200  |  train loss: 0.4447555006
Epoch:   300  |  train loss: 0.4485546172
Epoch:   400  |  train loss: 0.4571295381
Epoch:   500  |  train loss: 0.4579149187
Epoch:   600  |  train loss: 0.4545881569
Epoch:   700  |  train loss: 0.4580640018
Epoch:   800  |  train loss: 0.4549649477
Epoch:   900  |  train loss: 0.4469474852
Epoch:  1000  |  train loss: 0.4530330658
Processing class: 44
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.4472896039
Epoch:   200  |  train loss: 0.4489660800
Epoch:   300  |  train loss: 0.4434392869
Epoch:   400  |  train loss: 0.4402447581
Epoch:   500  |  train loss: 0.4413223863
Epoch:   600  |  train loss: 0.4368323445
Epoch:   700  |  train loss: 0.4407198071
Epoch:   800  |  train loss: 0.4368268132
Epoch:   900  |  train loss: 0.4319722235
Epoch:  1000  |  train loss: 0.4360696137
Processing class: 45
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.4447382927
Epoch:   200  |  train loss: 0.4513043463
Epoch:   300  |  train loss: 0.4429904103
Epoch:   400  |  train loss: 0.4508738399
Epoch:   500  |  train loss: 0.4485444486
Epoch:   600  |  train loss: 0.4551634371
Epoch:   700  |  train loss: 0.4548199654
Epoch:   800  |  train loss: 0.4513075888
Epoch:   900  |  train loss: 0.4519572198
Epoch:  1000  |  train loss: 0.4553345978
Processing class: 46
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.4419477046
Epoch:   200  |  train loss: 0.4395581782
Epoch:   300  |  train loss: 0.4377813995
Epoch:   400  |  train loss: 0.4400098085
Epoch:   500  |  train loss: 0.4393873394
Epoch:   600  |  train loss: 0.4390377223
Epoch:   700  |  train loss: 0.4386636496
Epoch:   800  |  train loss: 0.4438662469
Epoch:   900  |  train loss: 0.4377285957
Epoch:  1000  |  train loss: 0.4401174366
Processing class: 47
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.4453512430
Epoch:   200  |  train loss: 0.4474365473
Epoch:   300  |  train loss: 0.4359804809
Epoch:   400  |  train loss: 0.4384625196
Epoch:   500  |  train loss: 0.4350971341
Epoch:   600  |  train loss: 0.4309027195
Epoch:   700  |  train loss: 0.4295132339
Epoch:   800  |  train loss: 0.4230700076
Epoch:   900  |  train loss: 0.4228631079
Epoch:  1000  |  train loss: 0.4186288595
Processing class: 48
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.4471172392
Epoch:   200  |  train loss: 0.4525075614
Epoch:   300  |  train loss: 0.4415149987
Epoch:   400  |  train loss: 0.4313171685
Epoch:   500  |  train loss: 0.4399321914
Epoch:   600  |  train loss: 0.4360569537
Epoch:   700  |  train loss: 0.4294896901
Epoch:   800  |  train loss: 0.4284130812
Epoch:   900  |  train loss: 0.4365519345
Epoch:  1000  |  train loss: 0.4369141996
Processing class: 49
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.4481452167
Epoch:   200  |  train loss: 0.4464491665
Epoch:   300  |  train loss: 0.4516812146
Epoch:   400  |  train loss: 0.4497006655
Epoch:   500  |  train loss: 0.4480086207
Epoch:   600  |  train loss: 0.4462666750
Epoch:   700  |  train loss: 0.4432623684
Epoch:   800  |  train loss: 0.4392691612
Epoch:   900  |  train loss: 0.4363724947
Epoch:  1000  |  train loss: 0.4360349834
/home/z1165703/FeCAM/models/base.py:87: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  vec = torch.tensor(vec).cuda()
2024-03-05 12:55:25,140 [trainer.py] => CNN: {'total': 83.44, '00-09': 87.7, '10-19': 79.6, '20-29': 84.9, '30-39': 81.0, '40-49': 84.0, 'old': 0, 'new': 83.44}
2024-03-05 12:55:25,142 [trainer.py] => No NME accuracy
2024-03-05 12:55:25,142 [trainer.py] => FeCAM: {'total': 82.9, '00-09': 86.2, '10-19': 79.7, '20-29': 85.0, '30-39': 80.6, '40-49': 83.0, 'old': 0, 'new': 82.9}
2024-03-05 12:55:25,142 [trainer.py] => CNN top1 curve: [83.44]
2024-03-05 12:55:25,142 [trainer.py] => CNN top5 curve: [96.5]
2024-03-05 12:55:25,142 [trainer.py] => FeCAM top1 curve: [82.9]
2024-03-05 12:55:25,142 [trainer.py] => FeCAM top5 curve: [95.84]

2024-03-05 12:55:25,156 [fecam.py] => Learning on 50-60
Processing class: 50
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.4580725670
Epoch:   200  |  train loss: 0.4538546801
Epoch:   300  |  train loss: 0.4506217182
Epoch:   400  |  train loss: 0.4524111569
Epoch:   500  |  train loss: 0.4504732549
Epoch:   600  |  train loss: 0.4426076055
Epoch:   700  |  train loss: 0.4456207395
Epoch:   800  |  train loss: 0.4458558738
Epoch:   900  |  train loss: 0.4477775693
Epoch:  1000  |  train loss: 0.4470278800
Processing class: 51
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.4619429946
Epoch:   200  |  train loss: 0.4487798810
Epoch:   300  |  train loss: 0.4545746207
Epoch:   400  |  train loss: 0.4529666781
Epoch:   500  |  train loss: 0.4488331556
Epoch:   600  |  train loss: 0.4474332869
Epoch:   700  |  train loss: 0.4386897504
Epoch:   800  |  train loss: 0.4376398504
Epoch:   900  |  train loss: 0.4431816578
Epoch:  1000  |  train loss: 0.4366017401
Processing class: 52
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.4697424293
Epoch:   200  |  train loss: 0.4646235049
Epoch:   300  |  train loss: 0.4647857130
Epoch:   400  |  train loss: 0.4643786848
Epoch:   500  |  train loss: 0.4598469853
Epoch:   600  |  train loss: 0.4522505641
Epoch:   700  |  train loss: 0.4558850467
Epoch:   800  |  train loss: 0.4536642671
Epoch:   900  |  train loss: 0.4524198592
Epoch:  1000  |  train loss: 0.4453924119
Processing class: 53
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.4498472810
Epoch:   200  |  train loss: 0.4448916912
Epoch:   300  |  train loss: 0.4350694060
Epoch:   400  |  train loss: 0.4335397422
Epoch:   500  |  train loss: 0.4348094046
Epoch:   600  |  train loss: 0.4382183015
Epoch:   700  |  train loss: 0.4417436004
Epoch:   800  |  train loss: 0.4277957857
Epoch:   900  |  train loss: 0.4323156655
Epoch:  1000  |  train loss: 0.4259718955
Processing class: 54
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.4236832142
Epoch:   200  |  train loss: 0.4192601025
Epoch:   300  |  train loss: 0.4178931355
Epoch:   400  |  train loss: 0.4159759045
Epoch:   500  |  train loss: 0.4155108154
Epoch:   600  |  train loss: 0.4194662452
Epoch:   700  |  train loss: 0.4186632335
Epoch:   800  |  train loss: 0.4218717873
Epoch:   900  |  train loss: 0.4237768292
Epoch:  1000  |  train loss: 0.4191562593
Processing class: 55
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.4680995166
Epoch:   200  |  train loss: 0.4671904445
Epoch:   300  |  train loss: 0.4612457454
Epoch:   400  |  train loss: 0.4661279321
Epoch:   500  |  train loss: 0.4729495585
Epoch:   600  |  train loss: 0.4711437404
Epoch:   700  |  train loss: 0.4715742528
Epoch:   800  |  train loss: 0.4762793124
Epoch:   900  |  train loss: 0.4643710732
Epoch:  1000  |  train loss: 0.4674704790
Processing class: 56
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.4532149255
Epoch:   200  |  train loss: 0.4474233508
Epoch:   300  |  train loss: 0.4306474268
Epoch:   400  |  train loss: 0.4334013343
Epoch:   500  |  train loss: 0.4302229643
Epoch:   600  |  train loss: 0.4265161693
Epoch:   700  |  train loss: 0.4254113197
Epoch:   800  |  train loss: 0.4184004843
Epoch:   900  |  train loss: 0.4211003423
Epoch:  1000  |  train loss: 0.4218567014
Processing class: 57
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.4638701141
Epoch:   200  |  train loss: 0.4690684855
Epoch:   300  |  train loss: 0.4639668703
Epoch:   400  |  train loss: 0.4613764942
Epoch:   500  |  train loss: 0.4635005951
Epoch:   600  |  train loss: 0.4547753692
Epoch:   700  |  train loss: 0.4617783427
Epoch:   800  |  train loss: 0.4589199603
Epoch:   900  |  train loss: 0.4615766346
Epoch:  1000  |  train loss: 0.4509241998
Processing class: 58
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.4578872681
Epoch:   200  |  train loss: 0.4424926639
Epoch:   300  |  train loss: 0.4416665137
Epoch:   400  |  train loss: 0.4421134830
Epoch:   500  |  train loss: 0.4361375570
Epoch:   600  |  train loss: 0.4384917796
Epoch:   700  |  train loss: 0.4330703259
Epoch:   800  |  train loss: 0.4351909101
Epoch:   900  |  train loss: 0.4325312138
Epoch:  1000  |  train loss: 0.4298901260
Processing class: 59
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.4712866783
Epoch:   200  |  train loss: 0.4762553513
Epoch:   300  |  train loss: 0.4662110567
Epoch:   400  |  train loss: 0.4643528283
Epoch:   500  |  train loss: 0.4649869442
Epoch:   600  |  train loss: 0.4630709469
Epoch:   700  |  train loss: 0.4636665761
Epoch:   800  |  train loss: 0.4626012862
Epoch:   900  |  train loss: 0.4590685248
Epoch:  1000  |  train loss: 0.4560387433
2024-03-05 13:01:04,683 [trainer.py] => CNN: {'total': 71.75, '00-09': 82.2, '10-19': 72.8, '20-29': 78.5, '30-39': 74.9, '40-49': 68.4, '50-59': 53.7, 'old': 75.36, 'new': 53.7}
2024-03-05 13:01:05,829 [trainer.py] => No NME accuracy
2024-03-05 13:01:05,830 [trainer.py] => FeCAM: {'total': 72.78, '00-09': 82.6, '10-19': 74.0, '20-29': 81.0, '30-39': 76.4, '40-49': 76.8, '50-59': 45.9, 'old': 78.16, 'new': 45.9}
2024-03-05 13:01:05,830 [trainer.py] => CNN top1 curve: [83.44, 71.75]
2024-03-05 13:01:05,830 [trainer.py] => CNN top5 curve: [96.5, 89.6]
2024-03-05 13:01:05,830 [trainer.py] => FeCAM top1 curve: [82.9, 72.78]
2024-03-05 13:01:05,830 [trainer.py] => FeCAM top5 curve: [95.84, 91.18]

2024-03-05 13:01:05,836 [fecam.py] => Learning on 60-70
Processing class: 60
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.4729146540
Epoch:   200  |  train loss: 0.4682851315
Epoch:   300  |  train loss: 0.4588088751
Epoch:   400  |  train loss: 0.4523112357
Epoch:   500  |  train loss: 0.4552368402
Epoch:   600  |  train loss: 0.4510397255
Epoch:   700  |  train loss: 0.4634959519
Epoch:   800  |  train loss: 0.4499290884
Epoch:   900  |  train loss: 0.4485686243
Epoch:  1000  |  train loss: 0.4500894666
Processing class: 61
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.4522016704
Epoch:   200  |  train loss: 0.4377330244
Epoch:   300  |  train loss: 0.4370873034
Epoch:   400  |  train loss: 0.4308831811
Epoch:   500  |  train loss: 0.4355103850
Epoch:   600  |  train loss: 0.4250821292
Epoch:   700  |  train loss: 0.4290375054
Epoch:   800  |  train loss: 0.4309261084
Epoch:   900  |  train loss: 0.4202240825
Epoch:  1000  |  train loss: 0.4218644023
Processing class: 62
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.4737274349
Epoch:   200  |  train loss: 0.4672990143
Epoch:   300  |  train loss: 0.4710642278
Epoch:   400  |  train loss: 0.4715706587
Epoch:   500  |  train loss: 0.4651951134
Epoch:   600  |  train loss: 0.4715227067
Epoch:   700  |  train loss: 0.4724045753
Epoch:   800  |  train loss: 0.4716077149
Epoch:   900  |  train loss: 0.4667715549
Epoch:  1000  |  train loss: 0.4674018145
Processing class: 63
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.4551119030
Epoch:   200  |  train loss: 0.4546168387
Epoch:   300  |  train loss: 0.4577729881
Epoch:   400  |  train loss: 0.4520454705
Epoch:   500  |  train loss: 0.4500338018
Epoch:   600  |  train loss: 0.4543708265
Epoch:   700  |  train loss: 0.4419386744
Epoch:   800  |  train loss: 0.4524683475
Epoch:   900  |  train loss: 0.4415179431
Epoch:  1000  |  train loss: 0.4418754339
Processing class: 64
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.4662932932
Epoch:   200  |  train loss: 0.4458020329
Epoch:   300  |  train loss: 0.4507556856
Epoch:   400  |  train loss: 0.4462717354
Epoch:   500  |  train loss: 0.4403628707
Epoch:   600  |  train loss: 0.4352964759
Epoch:   700  |  train loss: 0.4255544782
Epoch:   800  |  train loss: 0.4266190410
Epoch:   900  |  train loss: 0.4306839764
Epoch:  1000  |  train loss: 0.4251658797
Processing class: 65
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.4562605917
Epoch:   200  |  train loss: 0.4444717288
Epoch:   300  |  train loss: 0.4468971491
Epoch:   400  |  train loss: 0.4457217932
Epoch:   500  |  train loss: 0.4388380766
Epoch:   600  |  train loss: 0.4449710250
Epoch:   700  |  train loss: 0.4417896450
Epoch:   800  |  train loss: 0.4481240809
Epoch:   900  |  train loss: 0.4394163370
Epoch:  1000  |  train loss: 0.4464822531
Processing class: 66
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.4692292333
Epoch:   200  |  train loss: 0.4685457885
Epoch:   300  |  train loss: 0.4680585265
Epoch:   400  |  train loss: 0.4636385560
Epoch:   500  |  train loss: 0.4676273644
Epoch:   600  |  train loss: 0.4655729890
Epoch:   700  |  train loss: 0.4704717875
Epoch:   800  |  train loss: 0.4696711600
Epoch:   900  |  train loss: 0.4671172500
Epoch:  1000  |  train loss: 0.4650833547
Processing class: 67
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.4583561540
Epoch:   200  |  train loss: 0.4586323559
Epoch:   300  |  train loss: 0.4541665077
Epoch:   400  |  train loss: 0.4523534298
Epoch:   500  |  train loss: 0.4494543672
Epoch:   600  |  train loss: 0.4469535291
Epoch:   700  |  train loss: 0.4463545620
Epoch:   800  |  train loss: 0.4485566795
Epoch:   900  |  train loss: 0.4453175008
Epoch:  1000  |  train loss: 0.4430041194
Processing class: 68
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.4473378718
Epoch:   200  |  train loss: 0.4438601732
Epoch:   300  |  train loss: 0.4319407701
Epoch:   400  |  train loss: 0.4298657060
Epoch:   500  |  train loss: 0.4276124179
Epoch:   600  |  train loss: 0.4231181145
Epoch:   700  |  train loss: 0.4239050925
Epoch:   800  |  train loss: 0.4182088077
Epoch:   900  |  train loss: 0.4177728474
Epoch:  1000  |  train loss: 0.4246096849
Processing class: 69
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.4116942286
Epoch:   200  |  train loss: 0.4099136055
Epoch:   300  |  train loss: 0.4124777913
Epoch:   400  |  train loss: 0.4116813779
Epoch:   500  |  train loss: 0.4085652113
Epoch:   600  |  train loss: 0.3993767917
Epoch:   700  |  train loss: 0.4039385438
Epoch:   800  |  train loss: 0.4017068863
Epoch:   900  |  train loss: 0.4107281804
Epoch:  1000  |  train loss: 0.3996489704
2024-03-05 13:07:42,948 [trainer.py] => CNN: {'total': 64.66, '00-09': 75.0, '10-19': 70.1, '20-29': 77.1, '30-39': 70.6, '40-49': 64.0, '50-59': 45.8, '60-69': 50.0, 'old': 67.1, 'new': 50.0}
2024-03-05 13:07:42,949 [trainer.py] => No NME accuracy
2024-03-05 13:07:42,949 [trainer.py] => FeCAM: {'total': 67.27, '00-09': 79.9, '10-19': 70.6, '20-29': 79.1, '30-39': 71.9, '40-49': 74.1, '50-59': 41.2, '60-69': 54.1, 'old': 69.47, 'new': 54.1}
2024-03-05 13:07:42,949 [trainer.py] => CNN top1 curve: [83.44, 71.75, 64.66]
2024-03-05 13:07:42,949 [trainer.py] => CNN top5 curve: [96.5, 89.6, 86.54]
2024-03-05 13:07:42,949 [trainer.py] => FeCAM top1 curve: [82.9, 72.78, 67.27]
2024-03-05 13:07:42,949 [trainer.py] => FeCAM top5 curve: [95.84, 91.18, 87.21]

2024-03-05 13:07:42,953 [fecam.py] => Learning on 70-80
Processing class: 70
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.4553486168
Epoch:   200  |  train loss: 0.4475367308
Epoch:   300  |  train loss: 0.4430554986
Epoch:   400  |  train loss: 0.4424387038
Epoch:   500  |  train loss: 0.4338158190
Epoch:   600  |  train loss: 0.4365306914
Epoch:   700  |  train loss: 0.4398044050
Epoch:   800  |  train loss: 0.4273138046
Epoch:   900  |  train loss: 0.4314436018
Epoch:  1000  |  train loss: 0.4269306183
Processing class: 71
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.4658746183
Epoch:   200  |  train loss: 0.4655502200
Epoch:   300  |  train loss: 0.4533785999
Epoch:   400  |  train loss: 0.4543073714
Epoch:   500  |  train loss: 0.4539569795
Epoch:   600  |  train loss: 0.4511790276
Epoch:   700  |  train loss: 0.4486281574
Epoch:   800  |  train loss: 0.4474170566
Epoch:   900  |  train loss: 0.4508570552
Epoch:  1000  |  train loss: 0.4464844644
Processing class: 72
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.4577884734
Epoch:   200  |  train loss: 0.4565204859
Epoch:   300  |  train loss: 0.4530915260
Epoch:   400  |  train loss: 0.4481307745
Epoch:   500  |  train loss: 0.4490062356
Epoch:   600  |  train loss: 0.4413912535
Epoch:   700  |  train loss: 0.4415965617
Epoch:   800  |  train loss: 0.4368856370
Epoch:   900  |  train loss: 0.4362709165
Epoch:  1000  |  train loss: 0.4397442639
Processing class: 73
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.4658646643
Epoch:   200  |  train loss: 0.4586234987
Epoch:   300  |  train loss: 0.4597140074
Epoch:   400  |  train loss: 0.4519406617
Epoch:   500  |  train loss: 0.4594637692
Epoch:   600  |  train loss: 0.4569157660
Epoch:   700  |  train loss: 0.4496021271
Epoch:   800  |  train loss: 0.4479844093
Epoch:   900  |  train loss: 0.4463403940
Epoch:  1000  |  train loss: 0.4478970945
Processing class: 74
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.4747055829
Epoch:   200  |  train loss: 0.4779343784
Epoch:   300  |  train loss: 0.4672492981
Epoch:   400  |  train loss: 0.4600424170
Epoch:   500  |  train loss: 0.4603636384
Epoch:   600  |  train loss: 0.4546563923
Epoch:   700  |  train loss: 0.4619728565
Epoch:   800  |  train loss: 0.4502321303
Epoch:   900  |  train loss: 0.4491628945
Epoch:  1000  |  train loss: 0.4543911397
Processing class: 75
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.4615604579
Epoch:   200  |  train loss: 0.4560520470
Epoch:   300  |  train loss: 0.4583004177
Epoch:   400  |  train loss: 0.4535005212
Epoch:   500  |  train loss: 0.4558409035
Epoch:   600  |  train loss: 0.4584322393
Epoch:   700  |  train loss: 0.4409086585
Epoch:   800  |  train loss: 0.4483247042
Epoch:   900  |  train loss: 0.4455029845
Epoch:  1000  |  train loss: 0.4496273398
Processing class: 76
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.4046687365
Epoch:   200  |  train loss: 0.3978471637
Epoch:   300  |  train loss: 0.4003917098
Epoch:   400  |  train loss: 0.4029491484
Epoch:   500  |  train loss: 0.4033969045
Epoch:   600  |  train loss: 0.4060626686
Epoch:   700  |  train loss: 0.4066613734
Epoch:   800  |  train loss: 0.3969261765
Epoch:   900  |  train loss: 0.4037309349
Epoch:  1000  |  train loss: 0.4064653218
Processing class: 77
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.4533070803
Epoch:   200  |  train loss: 0.4488043845
Epoch:   300  |  train loss: 0.4370706916
Epoch:   400  |  train loss: 0.4355142057
Epoch:   500  |  train loss: 0.4309230208
Epoch:   600  |  train loss: 0.4260564029
Epoch:   700  |  train loss: 0.4243436575
Epoch:   800  |  train loss: 0.4277259231
Epoch:   900  |  train loss: 0.4220291078
Epoch:  1000  |  train loss: 0.4186048925
Processing class: 78
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.4626631796
Epoch:   200  |  train loss: 0.4594595253
Epoch:   300  |  train loss: 0.4605972350
Epoch:   400  |  train loss: 0.4554320633
Epoch:   500  |  train loss: 0.4489509106
Epoch:   600  |  train loss: 0.4509668589
Epoch:   700  |  train loss: 0.4494018078
Epoch:   800  |  train loss: 0.4483987093
Epoch:   900  |  train loss: 0.4472105443
Epoch:  1000  |  train loss: 0.4440476954
Processing class: 79
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.4703246832
Epoch:   200  |  train loss: 0.4630208433
Epoch:   300  |  train loss: 0.4677006006
Epoch:   400  |  train loss: 0.4642325878
Epoch:   500  |  train loss: 0.4620028019
Epoch:   600  |  train loss: 0.4663027167
Epoch:   700  |  train loss: 0.4638356566
Epoch:   800  |  train loss: 0.4642247498
Epoch:   900  |  train loss: 0.4673138976
Epoch:  1000  |  train loss: 0.4598823786
2024-03-05 13:15:25,000 [trainer.py] => CNN: {'total': 59.18, '00-09': 73.6, '10-19': 68.4, '20-29': 76.9, '30-39': 69.1, '40-49': 60.6, '50-59': 37.2, '60-69': 45.7, '70-79': 41.9, 'old': 61.64, 'new': 41.9}
2024-03-05 13:15:25,002 [trainer.py] => No NME accuracy
2024-03-05 13:15:25,002 [trainer.py] => FeCAM: {'total': 61.72, '00-09': 78.1, '10-19': 69.3, '20-29': 78.0, '30-39': 69.5, '40-49': 72.1, '50-59': 35.7, '60-69': 49.1, '70-79': 42.0, 'old': 64.54, 'new': 42.0}
2024-03-05 13:15:25,002 [trainer.py] => CNN top1 curve: [83.44, 71.75, 64.66, 59.18]
2024-03-05 13:15:25,002 [trainer.py] => CNN top5 curve: [96.5, 89.6, 86.54, 84.09]
2024-03-05 13:15:25,002 [trainer.py] => FeCAM top1 curve: [82.9, 72.78, 67.27, 61.72]
2024-03-05 13:15:25,003 [trainer.py] => FeCAM top5 curve: [95.84, 91.18, 87.21, 84.09]

2024-03-05 13:15:25,011 [fecam.py] => Learning on 80-90
Processing class: 80
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.4334627450
Epoch:   200  |  train loss: 0.4284175992
Epoch:   300  |  train loss: 0.4264568031
Epoch:   400  |  train loss: 0.4253841937
Epoch:   500  |  train loss: 0.4215116322
Epoch:   600  |  train loss: 0.4196498930
Epoch:   700  |  train loss: 0.4112265587
Epoch:   800  |  train loss: 0.4193930089
Epoch:   900  |  train loss: 0.4159071565
Epoch:  1000  |  train loss: 0.4193140984
Processing class: 81
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.4537945926
Epoch:   200  |  train loss: 0.4568873346
Epoch:   300  |  train loss: 0.4458514214
Epoch:   400  |  train loss: 0.4511051416
Epoch:   500  |  train loss: 0.4454537868
Epoch:   600  |  train loss: 0.4492194355
Epoch:   700  |  train loss: 0.4452719212
Epoch:   800  |  train loss: 0.4479438543
Epoch:   900  |  train loss: 0.4455814302
Epoch:  1000  |  train loss: 0.4448487639
Processing class: 82
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.4435878456
Epoch:   200  |  train loss: 0.4413646817
Epoch:   300  |  train loss: 0.4372145474
Epoch:   400  |  train loss: 0.4315573215
Epoch:   500  |  train loss: 0.4292176485
Epoch:   600  |  train loss: 0.4247212768
Epoch:   700  |  train loss: 0.4224389434
Epoch:   800  |  train loss: 0.4225705326
Epoch:   900  |  train loss: 0.4208855689
Epoch:  1000  |  train loss: 0.4191583991
Processing class: 83
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.4448798299
Epoch:   200  |  train loss: 0.4392356813
Epoch:   300  |  train loss: 0.4290547013
Epoch:   400  |  train loss: 0.4260634959
Epoch:   500  |  train loss: 0.4291816294
Epoch:   600  |  train loss: 0.4214816093
Epoch:   700  |  train loss: 0.4265996277
Epoch:   800  |  train loss: 0.4329759598
Epoch:   900  |  train loss: 0.4278662384
Epoch:  1000  |  train loss: 0.4228144586
Processing class: 84
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.4645547628
Epoch:   200  |  train loss: 0.4537987828
Epoch:   300  |  train loss: 0.4545097053
Epoch:   400  |  train loss: 0.4494250834
Epoch:   500  |  train loss: 0.4501827478
Epoch:   600  |  train loss: 0.4419505119
Epoch:   700  |  train loss: 0.4451654732
Epoch:   800  |  train loss: 0.4482678950
Epoch:   900  |  train loss: 0.4387866199
Epoch:  1000  |  train loss: 0.4357002914
Processing class: 85
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.4627203345
Epoch:   200  |  train loss: 0.4531616747
Epoch:   300  |  train loss: 0.4530984700
Epoch:   400  |  train loss: 0.4453891814
Epoch:   500  |  train loss: 0.4459575713
Epoch:   600  |  train loss: 0.4476026773
Epoch:   700  |  train loss: 0.4529355168
Epoch:   800  |  train loss: 0.4444421470
Epoch:   900  |  train loss: 0.4433996379
Epoch:  1000  |  train loss: 0.4472124636
Processing class: 86
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.4747690201
Epoch:   200  |  train loss: 0.4653568268
Epoch:   300  |  train loss: 0.4688921988
Epoch:   400  |  train loss: 0.4611626029
Epoch:   500  |  train loss: 0.4583458006
Epoch:   600  |  train loss: 0.4581557572
Epoch:   700  |  train loss: 0.4559875906
Epoch:   800  |  train loss: 0.4526685297
Epoch:   900  |  train loss: 0.4506756485
Epoch:  1000  |  train loss: 0.4569011390
Processing class: 87
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.4461764753
Epoch:   200  |  train loss: 0.4401823461
Epoch:   300  |  train loss: 0.4355995774
Epoch:   400  |  train loss: 0.4333411932
Epoch:   500  |  train loss: 0.4287460446
Epoch:   600  |  train loss: 0.4306651354
Epoch:   700  |  train loss: 0.4303338110
Epoch:   800  |  train loss: 0.4281669080
Epoch:   900  |  train loss: 0.4220385730
Epoch:  1000  |  train loss: 0.4296182692
Processing class: 88
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.4568415225
Epoch:   200  |  train loss: 0.4494423330
Epoch:   300  |  train loss: 0.4433110774
Epoch:   400  |  train loss: 0.4402792215
Epoch:   500  |  train loss: 0.4346681476
Epoch:   600  |  train loss: 0.4273390651
Epoch:   700  |  train loss: 0.4296920657
Epoch:   800  |  train loss: 0.4269619584
Epoch:   900  |  train loss: 0.4263618946
Epoch:  1000  |  train loss: 0.4209131062
Processing class: 89
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.4589776754
Epoch:   200  |  train loss: 0.4573222697
Epoch:   300  |  train loss: 0.4528221607
Epoch:   400  |  train loss: 0.4507377863
Epoch:   500  |  train loss: 0.4526159465
Epoch:   600  |  train loss: 0.4488113403
Epoch:   700  |  train loss: 0.4408699334
Epoch:   800  |  train loss: 0.4493524015
Epoch:   900  |  train loss: 0.4440116107
Epoch:  1000  |  train loss: 0.4448616385
2024-03-05 13:24:30,902 [trainer.py] => CNN: {'total': 54.08, '00-09': 67.9, '10-19': 63.6, '20-29': 73.2, '30-39': 68.8, '40-49': 57.2, '50-59': 32.6, '60-69': 40.9, '70-79': 38.1, '80-89': 44.4, 'old': 55.29, 'new': 44.4}
2024-03-05 13:24:30,903 [trainer.py] => No NME accuracy
2024-03-05 13:24:30,903 [trainer.py] => FeCAM: {'total': 57.66, '00-09': 76.1, '10-19': 65.8, '20-29': 75.3, '30-39': 67.9, '40-49': 68.5, '50-59': 33.0, '60-69': 45.0, '70-79': 39.0, '80-89': 48.3, 'old': 58.82, 'new': 48.3}
2024-03-05 13:24:30,903 [trainer.py] => CNN top1 curve: [83.44, 71.75, 64.66, 59.18, 54.08]
2024-03-05 13:24:30,903 [trainer.py] => CNN top5 curve: [96.5, 89.6, 86.54, 84.09, 81.74]
2024-03-05 13:24:30,903 [trainer.py] => FeCAM top1 curve: [82.9, 72.78, 67.27, 61.72, 57.66]
2024-03-05 13:24:30,903 [trainer.py] => FeCAM top5 curve: [95.84, 91.18, 87.21, 84.09, 81.63]

2024-03-05 13:24:30,908 [fecam.py] => Learning on 90-100
Processing class: 90
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.4635059953
Epoch:   200  |  train loss: 0.4604964197
Epoch:   300  |  train loss: 0.4471188605
Epoch:   400  |  train loss: 0.4390667319
Epoch:   500  |  train loss: 0.4320377648
Epoch:   600  |  train loss: 0.4302825987
Epoch:   700  |  train loss: 0.4243701875
Epoch:   800  |  train loss: 0.4232112408
Epoch:   900  |  train loss: 0.4243637383
Epoch:  1000  |  train loss: 0.4282474339
Processing class: 91
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.4137194037
Epoch:   200  |  train loss: 0.4062840998
Epoch:   300  |  train loss: 0.4068588912
Epoch:   400  |  train loss: 0.4098028541
Epoch:   500  |  train loss: 0.4100877583
Epoch:   600  |  train loss: 0.4101923108
Epoch:   700  |  train loss: 0.4075621963
Epoch:   800  |  train loss: 0.4072138786
Epoch:   900  |  train loss: 0.4081495464
Epoch:  1000  |  train loss: 0.4077618778
Processing class: 92
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.4627038658
Epoch:   200  |  train loss: 0.4443163335
Epoch:   300  |  train loss: 0.4329336762
Epoch:   400  |  train loss: 0.4333173811
Epoch:   500  |  train loss: 0.4314504564
Epoch:   600  |  train loss: 0.4307722211
Epoch:   700  |  train loss: 0.4270851552
Epoch:   800  |  train loss: 0.4158082306
Epoch:   900  |  train loss: 0.4155044198
Epoch:  1000  |  train loss: 0.4144789875
Processing class: 93
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.4584827363
Epoch:   200  |  train loss: 0.4529737651
Epoch:   300  |  train loss: 0.4589077294
Epoch:   400  |  train loss: 0.4572342217
Epoch:   500  |  train loss: 0.4515536845
Epoch:   600  |  train loss: 0.4429383755
Epoch:   700  |  train loss: 0.4504354835
Epoch:   800  |  train loss: 0.4463628590
Epoch:   900  |  train loss: 0.4450821340
Epoch:  1000  |  train loss: 0.4409346402
Processing class: 94
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.4248335779
Epoch:   200  |  train loss: 0.4172558427
Epoch:   300  |  train loss: 0.4003895402
Epoch:   400  |  train loss: 0.3977695286
Epoch:   500  |  train loss: 0.3834644079
Epoch:   600  |  train loss: 0.3856395185
Epoch:   700  |  train loss: 0.3881038666
Epoch:   800  |  train loss: 0.3729652107
Epoch:   900  |  train loss: 0.3789355755
Epoch:  1000  |  train loss: 0.3744601309
Processing class: 95
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.4619835436
Epoch:   200  |  train loss: 0.4663590074
Epoch:   300  |  train loss: 0.4591186106
Epoch:   400  |  train loss: 0.4526621640
Epoch:   500  |  train loss: 0.4570422709
Epoch:   600  |  train loss: 0.4530382097
Epoch:   700  |  train loss: 0.4532798111
Epoch:   800  |  train loss: 0.4479504943
Epoch:   900  |  train loss: 0.4510203302
Epoch:  1000  |  train loss: 0.4537783325
Processing class: 96
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.4712086380
Epoch:   200  |  train loss: 0.4577310622
Epoch:   300  |  train loss: 0.4582881570
Epoch:   400  |  train loss: 0.4644669831
Epoch:   500  |  train loss: 0.4574322462
Epoch:   600  |  train loss: 0.4538181365
Epoch:   700  |  train loss: 0.4520290136
Epoch:   800  |  train loss: 0.4453081548
Epoch:   900  |  train loss: 0.4518290937
Epoch:  1000  |  train loss: 0.4489298403
Processing class: 97
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.4634497762
Epoch:   200  |  train loss: 0.4543421924
Epoch:   300  |  train loss: 0.4486607552
Epoch:   400  |  train loss: 0.4467172742
Epoch:   500  |  train loss: 0.4488728523
Epoch:   600  |  train loss: 0.4492738426
Epoch:   700  |  train loss: 0.4504642129
Epoch:   800  |  train loss: 0.4483080864
Epoch:   900  |  train loss: 0.4481081724
Epoch:  1000  |  train loss: 0.4454521120
Processing class: 98
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.4433857560
Epoch:   200  |  train loss: 0.4346698642
Epoch:   300  |  train loss: 0.4278207481
Epoch:   400  |  train loss: 0.4251265526
Epoch:   500  |  train loss: 0.4191010714
Epoch:   600  |  train loss: 0.4189317286
Epoch:   700  |  train loss: 0.4099751472
Epoch:   800  |  train loss: 0.4128601372
Epoch:   900  |  train loss: 0.4113775849
Epoch:  1000  |  train loss: 0.4070785940
Processing class: 99
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.4708802104
Epoch:   200  |  train loss: 0.4591120303
Epoch:   300  |  train loss: 0.4609448075
Epoch:   400  |  train loss: 0.4630386710
Epoch:   500  |  train loss: 0.4561334670
Epoch:   600  |  train loss: 0.4510134220
Epoch:   700  |  train loss: 0.4469000518
Epoch:   800  |  train loss: 0.4506335497
Epoch:   900  |  train loss: 0.4447969735
Epoch:  1000  |  train loss: 0.4441764355
2024-03-05 13:35:06,096 [trainer.py] => CNN: {'total': 50.36, '00-09': 57.7, '10-19': 63.6, '20-29': 71.4, '30-39': 68.5, '40-49': 56.8, '50-59': 29.6, '60-69': 38.9, '70-79': 36.6, '80-89': 42.7, '90-99': 37.8, 'old': 51.76, 'new': 37.8}
2024-03-05 13:35:06,098 [trainer.py] => No NME accuracy
2024-03-05 13:35:06,098 [trainer.py] => FeCAM: {'total': 54.74, '00-09': 73.3, '10-19': 65.1, '20-29': 74.1, '30-39': 67.2, '40-49': 67.1, '50-59': 30.8, '60-69': 43.3, '70-79': 37.2, '80-89': 46.9, '90-99': 42.4, 'old': 56.11, 'new': 42.4}
2024-03-05 13:35:06,098 [trainer.py] => CNN top1 curve: [83.44, 71.75, 64.66, 59.18, 54.08, 50.36]
2024-03-05 13:35:06,098 [trainer.py] => CNN top5 curve: [96.5, 89.6, 86.54, 84.09, 81.74, 79.63]
2024-03-05 13:35:06,098 [trainer.py] => FeCAM top1 curve: [82.9, 72.78, 67.27, 61.72, 57.66, 54.74]
2024-03-05 13:35:06,098 [trainer.py] => FeCAM top5 curve: [95.84, 91.18, 87.21, 84.09, 81.63, 79.21]

=========================================
2024-03-05 13:35:14,895 [trainer.py] => config: ./exps/FeCAM_cifar100.json
2024-03-05 13:35:14,895 [trainer.py] => prefix: train
2024-03-05 13:35:14,895 [trainer.py] => dataset: cifar100
2024-03-05 13:35:14,895 [trainer.py] => memory_size: 0
2024-03-05 13:35:14,895 [trainer.py] => shuffle: True
2024-03-05 13:35:14,895 [trainer.py] => init_cls: 50
2024-03-05 13:35:14,895 [trainer.py] => increment: 10
2024-03-05 13:35:14,895 [trainer.py] => model_name: fecam
2024-03-05 13:35:14,895 [trainer.py] => convnet_type: resnet18
2024-03-05 13:35:14,895 [trainer.py] => device: [device(type='cuda', index=0)]
2024-03-05 13:35:14,895 [trainer.py] => seed: 1993
2024-03-05 13:35:14,895 [trainer.py] => init_epochs: 200
2024-03-05 13:35:14,895 [trainer.py] => init_lr: 0.1
2024-03-05 13:35:14,896 [trainer.py] => init_weight_decay: 0.0005
2024-03-05 13:35:14,896 [trainer.py] => batch_size: 128
2024-03-05 13:35:14,896 [trainer.py] => num_workers: 8
2024-03-05 13:35:14,896 [trainer.py] => T: 5
2024-03-05 13:35:14,896 [trainer.py] => beta: 0.5
2024-03-05 13:35:14,896 [trainer.py] => alpha1: 1
2024-03-05 13:35:14,896 [trainer.py] => alpha2: 1
2024-03-05 13:35:14,896 [trainer.py] => ncm: False
2024-03-05 13:35:14,896 [trainer.py] => tukey: False
2024-03-05 13:35:14,896 [trainer.py] => diagonal: False
2024-03-05 13:35:14,896 [trainer.py] => per_class: True
2024-03-05 13:35:14,896 [trainer.py] => full_cov: True
2024-03-05 13:35:14,896 [trainer.py] => shrink: True
2024-03-05 13:35:14,896 [trainer.py] => norm_cov: False
2024-03-05 13:35:14,896 [trainer.py] => vecnorm: False
2024-03-05 13:35:14,896 [trainer.py] => ae_type: wae
2024-03-05 13:35:14,896 [trainer.py] => epochs: 1000
2024-03-05 13:35:14,896 [trainer.py] => ae_latent_dim: 32
2024-03-05 13:35:14,896 [trainer.py] => wae_sigma: 30
2024-03-05 13:35:14,896 [trainer.py] => wae_C: 1
Files already downloaded and verified
Files already downloaded and verified
2024-03-05 13:35:16,561 [data_manager.py] => [68, 56, 78, 8, 23, 84, 90, 65, 74, 76, 40, 89, 3, 92, 55, 9, 26, 80, 43, 38, 58, 70, 77, 1, 85, 19, 17, 50, 28, 53, 13, 81, 45, 82, 6, 59, 83, 16, 15, 44, 91, 41, 72, 60, 79, 52, 20, 10, 31, 54, 37, 95, 14, 71, 96, 98, 97, 2, 64, 66, 42, 22, 35, 86, 24, 34, 87, 21, 99, 0, 88, 27, 18, 94, 11, 12, 47, 25, 30, 46, 62, 69, 36, 61, 7, 63, 75, 5, 32, 4, 51, 48, 73, 93, 39, 67, 29, 49, 57, 33]
2024-03-05 13:35:16,838 [fecam.py] => Learning on 0-50
/home/z1165703/miniconda3/lib/python3.11/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
/home/z1165703/miniconda3/lib/python3.11/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
Processing class: 0
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.3550341427
Epoch:   200  |  train loss: 0.3571251631
Epoch:   300  |  train loss: 0.3632074654
Epoch:   400  |  train loss: 0.3648229897
Epoch:   500  |  train loss: 0.3613643885
Epoch:   600  |  train loss: 0.3613930821
Epoch:   700  |  train loss: 0.3634736538
Epoch:   800  |  train loss: 0.3642034531
Epoch:   900  |  train loss: 0.3610866249
Epoch:  1000  |  train loss: 0.3603795826
Processing class: 1
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.3540241897
Epoch:   200  |  train loss: 0.3513086438
Epoch:   300  |  train loss: 0.3635958612
Epoch:   400  |  train loss: 0.3660525799
Epoch:   500  |  train loss: 0.3579394221
Epoch:   600  |  train loss: 0.3674682558
Epoch:   700  |  train loss: 0.3627479255
Epoch:   800  |  train loss: 0.3673531950
Epoch:   900  |  train loss: 0.3745109081
Epoch:  1000  |  train loss: 0.3668097615
Processing class: 2
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.3534460306
Epoch:   200  |  train loss: 0.3517212629
Epoch:   300  |  train loss: 0.3596875131
Epoch:   400  |  train loss: 0.3567556560
Epoch:   500  |  train loss: 0.3426004291
Epoch:   600  |  train loss: 0.3522558093
Epoch:   700  |  train loss: 0.3498495877
Epoch:   800  |  train loss: 0.3571831584
Epoch:   900  |  train loss: 0.3516863883
Epoch:  1000  |  train loss: 0.3485229492
Processing class: 3
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.3690400541
Epoch:   200  |  train loss: 0.3772855639
Epoch:   300  |  train loss: 0.3804889917
Epoch:   400  |  train loss: 0.3809626520
Epoch:   500  |  train loss: 0.3767056286
Epoch:   600  |  train loss: 0.3847200871
Epoch:   700  |  train loss: 0.3891737998
Epoch:   800  |  train loss: 0.3828615487
Epoch:   900  |  train loss: 0.3864122570
Epoch:  1000  |  train loss: 0.3871478200
Processing class: 4
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.3515537262
Epoch:   200  |  train loss: 0.3473128736
Epoch:   300  |  train loss: 0.3567751586
Epoch:   400  |  train loss: 0.3561981320
Epoch:   500  |  train loss: 0.3597364902
Epoch:   600  |  train loss: 0.3568102956
Epoch:   700  |  train loss: 0.3612106800
Epoch:   800  |  train loss: 0.3621390998
Epoch:   900  |  train loss: 0.3639045656
Epoch:  1000  |  train loss: 0.3698785067
Processing class: 5
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.3555344582
Epoch:   200  |  train loss: 0.3515455782
Epoch:   300  |  train loss: 0.3539290547
Epoch:   400  |  train loss: 0.3530227602
Epoch:   500  |  train loss: 0.3516817570
Epoch:   600  |  train loss: 0.3628303289
Epoch:   700  |  train loss: 0.3641860664
Epoch:   800  |  train loss: 0.3579743624
Epoch:   900  |  train loss: 0.3559737623
Epoch:  1000  |  train loss: 0.3608144283
Processing class: 6
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.3547016442
Epoch:   200  |  train loss: 0.3587285578
Epoch:   300  |  train loss: 0.3685163319
Epoch:   400  |  train loss: 0.3660739362
Epoch:   500  |  train loss: 0.3741436660
Epoch:   600  |  train loss: 0.3702615380
Epoch:   700  |  train loss: 0.3670238197
Epoch:   800  |  train loss: 0.3691793740
Epoch:   900  |  train loss: 0.3644778609
Epoch:  1000  |  train loss: 0.3677125514
Processing class: 7
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.3497153640
Epoch:   200  |  train loss: 0.3506607771
Epoch:   300  |  train loss: 0.3532398701
Epoch:   400  |  train loss: 0.3545073330
Epoch:   500  |  train loss: 0.3578094602
Epoch:   600  |  train loss: 0.3542754889
Epoch:   700  |  train loss: 0.3582902312
Epoch:   800  |  train loss: 0.3657098770
Epoch:   900  |  train loss: 0.3580911934
Epoch:  1000  |  train loss: 0.3637119234
Processing class: 8
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.3514080584
Epoch:   200  |  train loss: 0.3564380109
Epoch:   300  |  train loss: 0.3522862196
Epoch:   400  |  train loss: 0.3617544711
Epoch:   500  |  train loss: 0.3565877020
Epoch:   600  |  train loss: 0.3583779156
Epoch:   700  |  train loss: 0.3651587725
Epoch:   800  |  train loss: 0.3582944572
Epoch:   900  |  train loss: 0.3622200012
Epoch:  1000  |  train loss: 0.3576927662
Processing class: 9
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.3442403316
Epoch:   200  |  train loss: 0.3498629868
Epoch:   300  |  train loss: 0.3579252660
Epoch:   400  |  train loss: 0.3493606865
Epoch:   500  |  train loss: 0.3572178364
Epoch:   600  |  train loss: 0.3516661465
Epoch:   700  |  train loss: 0.3545728385
Epoch:   800  |  train loss: 0.3566575229
Epoch:   900  |  train loss: 0.3566060066
Epoch:  1000  |  train loss: 0.3509743631
Processing class: 10
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.3417706192
Epoch:   200  |  train loss: 0.3397851944
Epoch:   300  |  train loss: 0.3422003090
Epoch:   400  |  train loss: 0.3510789931
Epoch:   500  |  train loss: 0.3389681518
Epoch:   600  |  train loss: 0.3426792860
Epoch:   700  |  train loss: 0.3456704676
Epoch:   800  |  train loss: 0.3426506162
Epoch:   900  |  train loss: 0.3450735331
Epoch:  1000  |  train loss: 0.3442150235
Processing class: 11
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.3560172737
Epoch:   200  |  train loss: 0.3576040387
Epoch:   300  |  train loss: 0.3527977705
Epoch:   400  |  train loss: 0.3564316571
Epoch:   500  |  train loss: 0.3557108760
Epoch:   600  |  train loss: 0.3611395180
Epoch:   700  |  train loss: 0.3694976747
Epoch:   800  |  train loss: 0.3674890339
Epoch:   900  |  train loss: 0.3673599362
Epoch:  1000  |  train loss: 0.3713481605
Processing class: 12
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.3427297831
Epoch:   200  |  train loss: 0.3559512615
Epoch:   300  |  train loss: 0.3555148542
Epoch:   400  |  train loss: 0.3609894872
Epoch:   500  |  train loss: 0.3613478184
Epoch:   600  |  train loss: 0.3565662980
Epoch:   700  |  train loss: 0.3585030079
Epoch:   800  |  train loss: 0.3604616761
Epoch:   900  |  train loss: 0.3646587551
Epoch:  1000  |  train loss: 0.3608447552
Processing class: 13
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.3472152174
Epoch:   200  |  train loss: 0.3563737273
Epoch:   300  |  train loss: 0.3629278839
Epoch:   400  |  train loss: 0.3638517559
Epoch:   500  |  train loss: 0.3696116805
Epoch:   600  |  train loss: 0.3636993170
Epoch:   700  |  train loss: 0.3671229959
Epoch:   800  |  train loss: 0.3709317327
Epoch:   900  |  train loss: 0.3730922163
Epoch:  1000  |  train loss: 0.3752487838
Processing class: 14
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.3547529936
Epoch:   200  |  train loss: 0.3588136852
Epoch:   300  |  train loss: 0.3606084824
Epoch:   400  |  train loss: 0.3667282939
Epoch:   500  |  train loss: 0.3641391218
Epoch:   600  |  train loss: 0.3634210348
Epoch:   700  |  train loss: 0.3673986077
Epoch:   800  |  train loss: 0.3761623740
Epoch:   900  |  train loss: 0.3727305830
Epoch:  1000  |  train loss: 0.3664229274
Processing class: 15
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.3505755961
Epoch:   200  |  train loss: 0.3544896305
Epoch:   300  |  train loss: 0.3617010593
Epoch:   400  |  train loss: 0.3646263897
Epoch:   500  |  train loss: 0.3689129531
Epoch:   600  |  train loss: 0.3634517550
Epoch:   700  |  train loss: 0.3709086597
Epoch:   800  |  train loss: 0.3635595560
Epoch:   900  |  train loss: 0.3709855616
Epoch:  1000  |  train loss: 0.3676064909
Processing class: 16
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.3537812531
Epoch:   200  |  train loss: 0.3693909407
Epoch:   300  |  train loss: 0.3760865688
Epoch:   400  |  train loss: 0.3713093877
Epoch:   500  |  train loss: 0.3776379287
Epoch:   600  |  train loss: 0.3719808340
Epoch:   700  |  train loss: 0.3695138812
Epoch:   800  |  train loss: 0.3784302652
Epoch:   900  |  train loss: 0.3693614125
Epoch:  1000  |  train loss: 0.3776436985
Processing class: 17
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.3401449084
Epoch:   200  |  train loss: 0.3452570558
Epoch:   300  |  train loss: 0.3528736174
Epoch:   400  |  train loss: 0.3534504533
Epoch:   500  |  train loss: 0.3429182887
Epoch:   600  |  train loss: 0.3481031060
Epoch:   700  |  train loss: 0.3459654927
Epoch:   800  |  train loss: 0.3461320400
Epoch:   900  |  train loss: 0.3479336143
Epoch:  1000  |  train loss: 0.3469364345
Processing class: 18
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.3475043476
Epoch:   200  |  train loss: 0.3495754123
Epoch:   300  |  train loss: 0.3539808035
Epoch:   400  |  train loss: 0.3523026705
Epoch:   500  |  train loss: 0.3593747675
Epoch:   600  |  train loss: 0.3551243246
Epoch:   700  |  train loss: 0.3667457819
Epoch:   800  |  train loss: 0.3654037535
Epoch:   900  |  train loss: 0.3672900856
Epoch:  1000  |  train loss: 0.3736628294
Processing class: 19
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.3491942942
Epoch:   200  |  train loss: 0.3631148517
Epoch:   300  |  train loss: 0.3623328507
Epoch:   400  |  train loss: 0.3633515060
Epoch:   500  |  train loss: 0.3575199783
Epoch:   600  |  train loss: 0.3584321558
Epoch:   700  |  train loss: 0.3614266872
Epoch:   800  |  train loss: 0.3585697055
Epoch:   900  |  train loss: 0.3711580992
Epoch:  1000  |  train loss: 0.3734262824
Processing class: 20
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.3523075819
Epoch:   200  |  train loss: 0.3560558677
Epoch:   300  |  train loss: 0.3607408583
Epoch:   400  |  train loss: 0.3632120967
Epoch:   500  |  train loss: 0.3703291774
Epoch:   600  |  train loss: 0.3653341651
Epoch:   700  |  train loss: 0.3663548827
Epoch:   800  |  train loss: 0.3662966192
Epoch:   900  |  train loss: 0.3689089060
Epoch:  1000  |  train loss: 0.3739470124
Processing class: 21
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.3537654340
Epoch:   200  |  train loss: 0.3588142991
Epoch:   300  |  train loss: 0.3600049555
Epoch:   400  |  train loss: 0.3611338258
Epoch:   500  |  train loss: 0.3632380962
Epoch:   600  |  train loss: 0.3669883251
Epoch:   700  |  train loss: 0.3653106689
Epoch:   800  |  train loss: 0.3627724111
Epoch:   900  |  train loss: 0.3686307967
Epoch:  1000  |  train loss: 0.3660494447
Processing class: 22
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.3508901298
Epoch:   200  |  train loss: 0.3528539121
Epoch:   300  |  train loss: 0.3590877295
Epoch:   400  |  train loss: 0.3654578209
Epoch:   500  |  train loss: 0.3662687838
Epoch:   600  |  train loss: 0.3617732644
Epoch:   700  |  train loss: 0.3643372118
Epoch:   800  |  train loss: 0.3613677621
Epoch:   900  |  train loss: 0.3579979897
Epoch:  1000  |  train loss: 0.3682124734
Processing class: 23
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.3575084269
Epoch:   200  |  train loss: 0.3639944315
Epoch:   300  |  train loss: 0.3700511515
Epoch:   400  |  train loss: 0.3620272219
Epoch:   500  |  train loss: 0.3611128211
Epoch:   600  |  train loss: 0.3562079668
Epoch:   700  |  train loss: 0.3598189771
Epoch:   800  |  train loss: 0.3559117317
Epoch:   900  |  train loss: 0.3600077093
Epoch:  1000  |  train loss: 0.3551949680
Processing class: 24
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.3544343650
Epoch:   200  |  train loss: 0.3542601287
Epoch:   300  |  train loss: 0.3480180144
Epoch:   400  |  train loss: 0.3541693628
Epoch:   500  |  train loss: 0.3502356648
Epoch:   600  |  train loss: 0.3541056156
Epoch:   700  |  train loss: 0.3443641305
Epoch:   800  |  train loss: 0.3471987844
Epoch:   900  |  train loss: 0.3523575783
Epoch:  1000  |  train loss: 0.3495754600
Processing class: 25
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.3500083506
Epoch:   200  |  train loss: 0.3543889701
Epoch:   300  |  train loss: 0.3611354649
Epoch:   400  |  train loss: 0.3545926332
Epoch:   500  |  train loss: 0.3619076848
Epoch:   600  |  train loss: 0.3573393106
Epoch:   700  |  train loss: 0.3647216380
Epoch:   800  |  train loss: 0.3611561120
Epoch:   900  |  train loss: 0.3583409965
Epoch:  1000  |  train loss: 0.3608285248
Processing class: 26
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.3608977318
Epoch:   200  |  train loss: 0.3695518136
Epoch:   300  |  train loss: 0.3647555888
Epoch:   400  |  train loss: 0.3710893214
Epoch:   500  |  train loss: 0.3771399379
Epoch:   600  |  train loss: 0.3661734164
Epoch:   700  |  train loss: 0.3694487333
Epoch:   800  |  train loss: 0.3724654317
Epoch:   900  |  train loss: 0.3703378499
Epoch:  1000  |  train loss: 0.3759768546
Processing class: 27
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.3525647044
Epoch:   200  |  train loss: 0.3481832922
Epoch:   300  |  train loss: 0.3508961558
Epoch:   400  |  train loss: 0.3520231903
Epoch:   500  |  train loss: 0.3555214882
Epoch:   600  |  train loss: 0.3554396689
Epoch:   700  |  train loss: 0.3575213969
Epoch:   800  |  train loss: 0.3568702161
Epoch:   900  |  train loss: 0.3588426888
Epoch:  1000  |  train loss: 0.3511344433
Processing class: 28
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.3591088891
Epoch:   200  |  train loss: 0.3661450088
Epoch:   300  |  train loss: 0.3725869477
Epoch:   400  |  train loss: 0.3753953874
Epoch:   500  |  train loss: 0.3769694865
Epoch:   600  |  train loss: 0.3768693745
Epoch:   700  |  train loss: 0.3783692122
Epoch:   800  |  train loss: 0.3804628074
Epoch:   900  |  train loss: 0.3816678762
Epoch:  1000  |  train loss: 0.3757718921
Processing class: 29
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.3520815849
Epoch:   200  |  train loss: 0.3557748318
Epoch:   300  |  train loss: 0.3542157173
Epoch:   400  |  train loss: 0.3500294745
Epoch:   500  |  train loss: 0.3556344509
Epoch:   600  |  train loss: 0.3580169559
Epoch:   700  |  train loss: 0.3571559191
Epoch:   800  |  train loss: 0.3521754920
Epoch:   900  |  train loss: 0.3529516995
Epoch:  1000  |  train loss: 0.3509960413
Processing class: 30
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.3604660988
Epoch:   200  |  train loss: 0.3680877566
Epoch:   300  |  train loss: 0.3779705048
Epoch:   400  |  train loss: 0.3871310651
Epoch:   500  |  train loss: 0.3825877607
Epoch:   600  |  train loss: 0.3813299894
Epoch:   700  |  train loss: 0.3790991962
Epoch:   800  |  train loss: 0.3853132367
Epoch:   900  |  train loss: 0.3892196834
Epoch:  1000  |  train loss: 0.3839636624
Processing class: 31
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.3597658873
Epoch:   200  |  train loss: 0.3677525103
Epoch:   300  |  train loss: 0.3675718248
Epoch:   400  |  train loss: 0.3759290218
Epoch:   500  |  train loss: 0.3783018768
Epoch:   600  |  train loss: 0.3846221805
Epoch:   700  |  train loss: 0.3824401975
Epoch:   800  |  train loss: 0.3827040434
Epoch:   900  |  train loss: 0.3877454102
Epoch:  1000  |  train loss: 0.3810162067
Processing class: 32
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.3542023599
Epoch:   200  |  train loss: 0.3612208843
Epoch:   300  |  train loss: 0.3590481877
Epoch:   400  |  train loss: 0.3518478930
Epoch:   500  |  train loss: 0.3631939173
Epoch:   600  |  train loss: 0.3587698340
Epoch:   700  |  train loss: 0.3674236655
Epoch:   800  |  train loss: 0.3703050852
Epoch:   900  |  train loss: 0.3678385973
Epoch:  1000  |  train loss: 0.3648570955
Processing class: 33
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.3411811471
Epoch:   200  |  train loss: 0.3582946658
Epoch:   300  |  train loss: 0.3447427034
Epoch:   400  |  train loss: 0.3453767419
Epoch:   500  |  train loss: 0.3459303260
Epoch:   600  |  train loss: 0.3550270498
Epoch:   700  |  train loss: 0.3493967950
Epoch:   800  |  train loss: 0.3512727141
Epoch:   900  |  train loss: 0.3499419928
Epoch:  1000  |  train loss: 0.3468355179
Processing class: 34
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.3553290844
Epoch:   200  |  train loss: 0.3530997694
Epoch:   300  |  train loss: 0.3577440679
Epoch:   400  |  train loss: 0.3537001312
Epoch:   500  |  train loss: 0.3524166644
Epoch:   600  |  train loss: 0.3576841712
Epoch:   700  |  train loss: 0.3518088639
Epoch:   800  |  train loss: 0.3581380546
Epoch:   900  |  train loss: 0.3598755300
Epoch:  1000  |  train loss: 0.3609664083
Processing class: 35
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.3505143106
Epoch:   200  |  train loss: 0.3563399613
Epoch:   300  |  train loss: 0.3613834620
Epoch:   400  |  train loss: 0.3633674085
Epoch:   500  |  train loss: 0.3579688549
Epoch:   600  |  train loss: 0.3618506491
Epoch:   700  |  train loss: 0.3635382593
Epoch:   800  |  train loss: 0.3623727381
Epoch:   900  |  train loss: 0.3631799281
Epoch:  1000  |  train loss: 0.3599561572
Processing class: 36
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.3532883108
Epoch:   200  |  train loss: 0.3547740936
Epoch:   300  |  train loss: 0.3590082049
Epoch:   400  |  train loss: 0.3588013649
Epoch:   500  |  train loss: 0.3666261554
Epoch:   600  |  train loss: 0.3619886935
Epoch:   700  |  train loss: 0.3571281612
Epoch:   800  |  train loss: 0.3662401497
Epoch:   900  |  train loss: 0.3577587485
Epoch:  1000  |  train loss: 0.3630031884
Processing class: 37
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.3588537216
Epoch:   200  |  train loss: 0.3570228457
Epoch:   300  |  train loss: 0.3551696599
Epoch:   400  |  train loss: 0.3583889067
Epoch:   500  |  train loss: 0.3614147604
Epoch:   600  |  train loss: 0.3562852085
Epoch:   700  |  train loss: 0.3567339540
Epoch:   800  |  train loss: 0.3588063955
Epoch:   900  |  train loss: 0.3542423546
Epoch:  1000  |  train loss: 0.3566510379
Processing class: 38
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.3512946248
Epoch:   200  |  train loss: 0.3542387426
Epoch:   300  |  train loss: 0.3605623305
Epoch:   400  |  train loss: 0.3630177677
Epoch:   500  |  train loss: 0.3642728865
Epoch:   600  |  train loss: 0.3615156353
Epoch:   700  |  train loss: 0.3617696106
Epoch:   800  |  train loss: 0.3677282155
Epoch:   900  |  train loss: 0.3598689079
Epoch:  1000  |  train loss: 0.3648969233
Processing class: 39
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.3546954453
Epoch:   200  |  train loss: 0.3519311130
Epoch:   300  |  train loss: 0.3561650753
Epoch:   400  |  train loss: 0.3591802895
Epoch:   500  |  train loss: 0.3561468124
Epoch:   600  |  train loss: 0.3579643369
Epoch:   700  |  train loss: 0.3602839887
Epoch:   800  |  train loss: 0.3640364349
Epoch:   900  |  train loss: 0.3656536102
Epoch:  1000  |  train loss: 0.3668382525
Processing class: 40
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.3553126216
Epoch:   200  |  train loss: 0.3494491220
Epoch:   300  |  train loss: 0.3615257084
Epoch:   400  |  train loss: 0.3637666166
Epoch:   500  |  train loss: 0.3667984605
Epoch:   600  |  train loss: 0.3622593999
Epoch:   700  |  train loss: 0.3652453899
Epoch:   800  |  train loss: 0.3645504773
Epoch:   900  |  train loss: 0.3610613704
Epoch:  1000  |  train loss: 0.3718264520
Processing class: 41
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.3510006845
Epoch:   200  |  train loss: 0.3523995161
Epoch:   300  |  train loss: 0.3568654954
Epoch:   400  |  train loss: 0.3533779442
Epoch:   500  |  train loss: 0.3529668450
Epoch:   600  |  train loss: 0.3530620575
Epoch:   700  |  train loss: 0.3579524517
Epoch:   800  |  train loss: 0.3546091318
Epoch:   900  |  train loss: 0.3528895080
Epoch:  1000  |  train loss: 0.3528514683
Processing class: 42
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.3487997293
Epoch:   200  |  train loss: 0.3562513709
Epoch:   300  |  train loss: 0.3607598245
Epoch:   400  |  train loss: 0.3617985785
Epoch:   500  |  train loss: 0.3642181516
Epoch:   600  |  train loss: 0.3656210124
Epoch:   700  |  train loss: 0.3685281038
Epoch:   800  |  train loss: 0.3701807439
Epoch:   900  |  train loss: 0.3663049579
Epoch:  1000  |  train loss: 0.3652927876
Processing class: 43
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.3525839806
Epoch:   200  |  train loss: 0.3511097729
Epoch:   300  |  train loss: 0.3631588757
Epoch:   400  |  train loss: 0.3746670842
Epoch:   500  |  train loss: 0.3773246586
Epoch:   600  |  train loss: 0.3742997944
Epoch:   700  |  train loss: 0.3789587200
Epoch:   800  |  train loss: 0.3765624523
Epoch:   900  |  train loss: 0.3695976675
Epoch:  1000  |  train loss: 0.3775645256
Processing class: 44
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.3491785228
Epoch:   200  |  train loss: 0.3574627101
Epoch:   300  |  train loss: 0.3538408935
Epoch:   400  |  train loss: 0.3549528837
Epoch:   500  |  train loss: 0.3583787441
Epoch:   600  |  train loss: 0.3552587271
Epoch:   700  |  train loss: 0.3611841321
Epoch:   800  |  train loss: 0.3589120746
Epoch:   900  |  train loss: 0.3558468401
Epoch:  1000  |  train loss: 0.3622373998
Processing class: 45
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.3557010651
Epoch:   200  |  train loss: 0.3670070231
Epoch:   300  |  train loss: 0.3647630095
Epoch:   400  |  train loss: 0.3724331021
Epoch:   500  |  train loss: 0.3708815157
Epoch:   600  |  train loss: 0.3802537262
Epoch:   700  |  train loss: 0.3820845962
Epoch:   800  |  train loss: 0.3793181241
Epoch:   900  |  train loss: 0.3803842366
Epoch:  1000  |  train loss: 0.3852995813
Processing class: 46
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.3580188215
Epoch:   200  |  train loss: 0.3586030304
Epoch:   300  |  train loss: 0.3621867120
Epoch:   400  |  train loss: 0.3651281714
Epoch:   500  |  train loss: 0.3657158434
Epoch:   600  |  train loss: 0.3658637941
Epoch:   700  |  train loss: 0.3667098761
Epoch:   800  |  train loss: 0.3741656959
Epoch:   900  |  train loss: 0.3692426920
Epoch:  1000  |  train loss: 0.3726450264
Processing class: 47
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.3488123655
Epoch:   200  |  train loss: 0.3552804112
Epoch:   300  |  train loss: 0.3503887713
Epoch:   400  |  train loss: 0.3570737123
Epoch:   500  |  train loss: 0.3565864205
Epoch:   600  |  train loss: 0.3557269573
Epoch:   700  |  train loss: 0.3574315012
Epoch:   800  |  train loss: 0.3529705346
Epoch:   900  |  train loss: 0.3548413098
Epoch:  1000  |  train loss: 0.3521822453
Processing class: 48
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.3528245986
Epoch:   200  |  train loss: 0.3611657798
Epoch:   300  |  train loss: 0.3580905020
Epoch:   400  |  train loss: 0.3508370340
Epoch:   500  |  train loss: 0.3603468537
Epoch:   600  |  train loss: 0.3585352242
Epoch:   700  |  train loss: 0.3536549747
Epoch:   800  |  train loss: 0.3533644319
Epoch:   900  |  train loss: 0.3628249705
Epoch:  1000  |  train loss: 0.3649785221
Processing class: 49
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.3518646300
Epoch:   200  |  train loss: 0.3537046492
Epoch:   300  |  train loss: 0.3648147166
Epoch:   400  |  train loss: 0.3661884427
Epoch:   500  |  train loss: 0.3683754563
Epoch:   600  |  train loss: 0.3683327317
Epoch:   700  |  train loss: 0.3677427113
Epoch:   800  |  train loss: 0.3653361917
Epoch:   900  |  train loss: 0.3637581944
Epoch:  1000  |  train loss: 0.3649248064
/home/z1165703/FeCAM/models/base.py:87: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  vec = torch.tensor(vec).cuda()
2024-03-05 13:52:46,150 [trainer.py] => CNN: {'total': 83.44, '00-09': 87.7, '10-19': 79.6, '20-29': 84.9, '30-39': 81.0, '40-49': 84.0, 'old': 0, 'new': 83.44}
2024-03-05 13:52:46,152 [trainer.py] => No NME accuracy
2024-03-05 13:52:46,152 [trainer.py] => FeCAM: {'total': 82.9, '00-09': 86.2, '10-19': 79.7, '20-29': 85.0, '30-39': 80.6, '40-49': 83.0, 'old': 0, 'new': 82.9}
2024-03-05 13:52:46,152 [trainer.py] => CNN top1 curve: [83.44]
2024-03-05 13:52:46,152 [trainer.py] => CNN top5 curve: [96.5]
2024-03-05 13:52:46,152 [trainer.py] => FeCAM top1 curve: [82.9]
2024-03-05 13:52:46,152 [trainer.py] => FeCAM top5 curve: [95.84]

2024-03-05 13:52:46,159 [fecam.py] => Learning on 50-60
Processing class: 50
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.3498460412
Epoch:   200  |  train loss: 0.3512010455
Epoch:   300  |  train loss: 0.3519737184
Epoch:   400  |  train loss: 0.3577761471
Epoch:   500  |  train loss: 0.3607569039
Epoch:   600  |  train loss: 0.3560604095
Epoch:   700  |  train loss: 0.3626014352
Epoch:   800  |  train loss: 0.3655398548
Epoch:   900  |  train loss: 0.3694819331
Epoch:  1000  |  train loss: 0.3707874119
Processing class: 51
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.3530087590
Epoch:   200  |  train loss: 0.3451867342
Epoch:   300  |  train loss: 0.3553408623
Epoch:   400  |  train loss: 0.3570573211
Epoch:   500  |  train loss: 0.3578378558
Epoch:   600  |  train loss: 0.3610662997
Epoch:   700  |  train loss: 0.3548765242
Epoch:   800  |  train loss: 0.3560844600
Epoch:   900  |  train loss: 0.3640741229
Epoch:  1000  |  train loss: 0.3601581514
Processing class: 52
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.3559829235
Epoch:   200  |  train loss: 0.3530800164
Epoch:   300  |  train loss: 0.3571330607
Epoch:   400  |  train loss: 0.3599896848
Epoch:   500  |  train loss: 0.3585151672
Epoch:   600  |  train loss: 0.3538972974
Epoch:   700  |  train loss: 0.3600699246
Epoch:   800  |  train loss: 0.3601929784
Epoch:   900  |  train loss: 0.3615230620
Epoch:  1000  |  train loss: 0.3564466178
Processing class: 53
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.3471431255
Epoch:   200  |  train loss: 0.3488320827
Epoch:   300  |  train loss: 0.3417957067
Epoch:   400  |  train loss: 0.3440026581
Epoch:   500  |  train loss: 0.3472490251
Epoch:   600  |  train loss: 0.3526540577
Epoch:   700  |  train loss: 0.3586714983
Epoch:   800  |  train loss: 0.3462273180
Epoch:   900  |  train loss: 0.3521287978
Epoch:  1000  |  train loss: 0.3474916279
Processing class: 54
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.3266815662
Epoch:   200  |  train loss: 0.3278246224
Epoch:   300  |  train loss: 0.3328405738
Epoch:   400  |  train loss: 0.3344985604
Epoch:   500  |  train loss: 0.3371246159
Epoch:   600  |  train loss: 0.3440803170
Epoch:   700  |  train loss: 0.3457752883
Epoch:   800  |  train loss: 0.3514179170
Epoch:   900  |  train loss: 0.3552880883
Epoch:  1000  |  train loss: 0.3529237568
Processing class: 55
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.3526213229
Epoch:   200  |  train loss: 0.3534985423
Epoch:   300  |  train loss: 0.3497177899
Epoch:   400  |  train loss: 0.3569741488
Epoch:   500  |  train loss: 0.3665400088
Epoch:   600  |  train loss: 0.3673826993
Epoch:   700  |  train loss: 0.3703150690
Epoch:   800  |  train loss: 0.3774727523
Epoch:   900  |  train loss: 0.3681215763
Epoch:  1000  |  train loss: 0.3733462334
Processing class: 56
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.3463523567
Epoch:   200  |  train loss: 0.3482505798
Epoch:   300  |  train loss: 0.3403222501
Epoch:   400  |  train loss: 0.3485681176
Epoch:   500  |  train loss: 0.3509195924
Epoch:   600  |  train loss: 0.3521538675
Epoch:   700  |  train loss: 0.3550713539
Epoch:   800  |  train loss: 0.3507788837
Epoch:   900  |  train loss: 0.3564208031
Epoch:  1000  |  train loss: 0.3596252561
Processing class: 57
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.3487753570
Epoch:   200  |  train loss: 0.3554248393
Epoch:   300  |  train loss: 0.3525892138
Epoch:   400  |  train loss: 0.3525176704
Epoch:   500  |  train loss: 0.3573336244
Epoch:   600  |  train loss: 0.3506765366
Epoch:   700  |  train loss: 0.3598349333
Epoch:   800  |  train loss: 0.3593655765
Epoch:   900  |  train loss: 0.3642650068
Epoch:  1000  |  train loss: 0.3560647547
Processing class: 58
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.3481700659
Epoch:   200  |  train loss: 0.3437996268
Epoch:   300  |  train loss: 0.3496721566
Epoch:   400  |  train loss: 0.3569662929
Epoch:   500  |  train loss: 0.3543732882
Epoch:   600  |  train loss: 0.3597275674
Epoch:   700  |  train loss: 0.3575458050
Epoch:   800  |  train loss: 0.3619405329
Epoch:   900  |  train loss: 0.3614469528
Epoch:  1000  |  train loss: 0.3613108218
Processing class: 59
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.3576151133
Epoch:   200  |  train loss: 0.3663673103
Epoch:   300  |  train loss: 0.3619872212
Epoch:   400  |  train loss: 0.3654642880
Epoch:   500  |  train loss: 0.3698724389
Epoch:   600  |  train loss: 0.3704927504
Epoch:   700  |  train loss: 0.3736544907
Epoch:   800  |  train loss: 0.3748409331
Epoch:   900  |  train loss: 0.3734073400
Epoch:  1000  |  train loss: 0.3713187516
2024-03-05 13:58:27,385 [trainer.py] => CNN: {'total': 71.75, '00-09': 82.2, '10-19': 72.8, '20-29': 78.5, '30-39': 74.9, '40-49': 68.4, '50-59': 53.7, 'old': 75.36, 'new': 53.7}
2024-03-05 13:58:27,385 [trainer.py] => No NME accuracy
2024-03-05 13:58:27,385 [trainer.py] => FeCAM: {'total': 72.78, '00-09': 82.6, '10-19': 74.0, '20-29': 81.0, '30-39': 76.4, '40-49': 76.8, '50-59': 45.9, 'old': 78.16, 'new': 45.9}
2024-03-05 13:58:27,385 [trainer.py] => CNN top1 curve: [83.44, 71.75]
2024-03-05 13:58:27,385 [trainer.py] => CNN top5 curve: [96.5, 89.6]
2024-03-05 13:58:27,385 [trainer.py] => FeCAM top1 curve: [82.9, 72.78]
2024-03-05 13:58:27,385 [trainer.py] => FeCAM top5 curve: [95.84, 91.18]

2024-03-05 13:58:27,390 [fecam.py] => Learning on 60-70
Processing class: 60
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.3606412947
Epoch:   200  |  train loss: 0.3592359900
Epoch:   300  |  train loss: 0.3543969631
Epoch:   400  |  train loss: 0.3522936046
Epoch:   500  |  train loss: 0.3601686597
Epoch:   600  |  train loss: 0.3598775566
Epoch:   700  |  train loss: 0.3774843037
Epoch:   800  |  train loss: 0.3670966089
Epoch:   900  |  train loss: 0.3683433712
Epoch:  1000  |  train loss: 0.3728065372
Processing class: 61
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.3459991157
Epoch:   200  |  train loss: 0.3390731633
Epoch:   300  |  train loss: 0.3462292969
Epoch:   400  |  train loss: 0.3464797378
Epoch:   500  |  train loss: 0.3558334112
Epoch:   600  |  train loss: 0.3485908806
Epoch:   700  |  train loss: 0.3567891419
Epoch:   800  |  train loss: 0.3617618918
Epoch:   900  |  train loss: 0.3531720877
Epoch:  1000  |  train loss: 0.3565919757
Processing class: 62
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.3580525339
Epoch:   200  |  train loss: 0.3536607325
Epoch:   300  |  train loss: 0.3593944609
Epoch:   400  |  train loss: 0.3618372917
Epoch:   500  |  train loss: 0.3574381292
Epoch:   600  |  train loss: 0.3660241663
Epoch:   700  |  train loss: 0.3685914636
Epoch:   800  |  train loss: 0.3702187121
Epoch:   900  |  train loss: 0.3675967097
Epoch:  1000  |  train loss: 0.3698982358
Processing class: 63
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.3442367256
Epoch:   200  |  train loss: 0.3463043392
Epoch:   300  |  train loss: 0.3573654354
Epoch:   400  |  train loss: 0.3574751318
Epoch:   500  |  train loss: 0.3607798040
Epoch:   600  |  train loss: 0.3684810936
Epoch:   700  |  train loss: 0.3586058855
Epoch:   800  |  train loss: 0.3721370220
Epoch:   900  |  train loss: 0.3630513489
Epoch:  1000  |  train loss: 0.3651383638
Processing class: 64
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.3592758596
Epoch:   200  |  train loss: 0.3511243105
Epoch:   300  |  train loss: 0.3639095128
Epoch:   400  |  train loss: 0.3641554773
Epoch:   500  |  train loss: 0.3642650366
Epoch:   600  |  train loss: 0.3632113338
Epoch:   700  |  train loss: 0.3563269019
Epoch:   800  |  train loss: 0.3597603559
Epoch:   900  |  train loss: 0.3663366377
Epoch:  1000  |  train loss: 0.3635242939
Processing class: 65
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.3503502190
Epoch:   200  |  train loss: 0.3476986647
Epoch:   300  |  train loss: 0.3600938201
Epoch:   400  |  train loss: 0.3648015261
Epoch:   500  |  train loss: 0.3620831251
Epoch:   600  |  train loss: 0.3729591012
Epoch:   700  |  train loss: 0.3727992475
Epoch:   800  |  train loss: 0.3828350961
Epoch:   900  |  train loss: 0.3761147618
Epoch:  1000  |  train loss: 0.3864296913
Processing class: 66
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.3583994865
Epoch:   200  |  train loss: 0.3628865540
Epoch:   300  |  train loss: 0.3681678534
Epoch:   400  |  train loss: 0.3679260015
Epoch:   500  |  train loss: 0.3746332228
Epoch:   600  |  train loss: 0.3752243519
Epoch:   700  |  train loss: 0.3826345205
Epoch:   800  |  train loss: 0.3833785474
Epoch:   900  |  train loss: 0.3828532934
Epoch:  1000  |  train loss: 0.3817789018
Processing class: 67
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.3465100646
Epoch:   200  |  train loss: 0.3501125395
Epoch:   300  |  train loss: 0.3495427132
Epoch:   400  |  train loss: 0.3508944154
Epoch:   500  |  train loss: 0.3508686066
Epoch:   600  |  train loss: 0.3503931105
Epoch:   700  |  train loss: 0.3518997133
Epoch:   800  |  train loss: 0.3563540161
Epoch:   900  |  train loss: 0.3550272763
Epoch:  1000  |  train loss: 0.3543935537
Processing class: 68
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.3425617635
Epoch:   200  |  train loss: 0.3422039151
Epoch:   300  |  train loss: 0.3387790322
Epoch:   400  |  train loss: 0.3409644723
Epoch:   500  |  train loss: 0.3414248884
Epoch:   600  |  train loss: 0.3398229599
Epoch:   700  |  train loss: 0.3429462731
Epoch:   800  |  train loss: 0.3393858016
Epoch:   900  |  train loss: 0.3414221227
Epoch:  1000  |  train loss: 0.3502560377
Processing class: 69
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.3296967864
Epoch:   200  |  train loss: 0.3347018421
Epoch:   300  |  train loss: 0.3457823753
Epoch:   400  |  train loss: 0.3503862858
Epoch:   500  |  train loss: 0.3505867958
Epoch:   600  |  train loss: 0.3423437297
Epoch:   700  |  train loss: 0.3493323207
Epoch:   800  |  train loss: 0.3485341549
Epoch:   900  |  train loss: 0.3590009570
Epoch:  1000  |  train loss: 0.3484932721
2024-03-05 14:05:00,770 [trainer.py] => CNN: {'total': 64.66, '00-09': 75.0, '10-19': 70.1, '20-29': 77.1, '30-39': 70.6, '40-49': 64.0, '50-59': 45.8, '60-69': 50.0, 'old': 67.1, 'new': 50.0}
2024-03-05 14:05:00,771 [trainer.py] => No NME accuracy
2024-03-05 14:05:00,771 [trainer.py] => FeCAM: {'total': 67.27, '00-09': 79.9, '10-19': 70.6, '20-29': 79.1, '30-39': 71.9, '40-49': 74.1, '50-59': 41.2, '60-69': 54.1, 'old': 69.47, 'new': 54.1}
2024-03-05 14:05:00,771 [trainer.py] => CNN top1 curve: [83.44, 71.75, 64.66]
2024-03-05 14:05:00,771 [trainer.py] => CNN top5 curve: [96.5, 89.6, 86.54]
2024-03-05 14:05:00,771 [trainer.py] => FeCAM top1 curve: [82.9, 72.78, 67.27]
2024-03-05 14:05:00,771 [trainer.py] => FeCAM top5 curve: [95.84, 91.18, 87.21]

2024-03-05 14:05:00,777 [fecam.py] => Learning on 70-80
Processing class: 70
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.3465819180
Epoch:   200  |  train loss: 0.3428354263
Epoch:   300  |  train loss: 0.3453799605
Epoch:   400  |  train loss: 0.3486083686
Epoch:   500  |  train loss: 0.3448449194
Epoch:   600  |  train loss: 0.3509860456
Epoch:   700  |  train loss: 0.3584999263
Epoch:   800  |  train loss: 0.3492665410
Epoch:   900  |  train loss: 0.3555044949
Epoch:  1000  |  train loss: 0.3528250337
Processing class: 71
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.3530525625
Epoch:   200  |  train loss: 0.3556500196
Epoch:   300  |  train loss: 0.3492902577
Epoch:   400  |  train loss: 0.3545505106
Epoch:   500  |  train loss: 0.3582032740
Epoch:   600  |  train loss: 0.3587326288
Epoch:   700  |  train loss: 0.3593509734
Epoch:   800  |  train loss: 0.3609272122
Epoch:   900  |  train loss: 0.3675548673
Epoch:  1000  |  train loss: 0.3656955063
Processing class: 72
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.3469352424
Epoch:   200  |  train loss: 0.3492574096
Epoch:   300  |  train loss: 0.3502336383
Epoch:   400  |  train loss: 0.3495575786
Epoch:   500  |  train loss: 0.3537834406
Epoch:   600  |  train loss: 0.3488360882
Epoch:   700  |  train loss: 0.3514521539
Epoch:   800  |  train loss: 0.3491135538
Epoch:   900  |  train loss: 0.3509307742
Epoch:  1000  |  train loss: 0.3568414271
Processing class: 73
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.3549725592
Epoch:   200  |  train loss: 0.3526915967
Epoch:   300  |  train loss: 0.3606308579
Epoch:   400  |  train loss: 0.3564200699
Epoch:   500  |  train loss: 0.3654676855
Epoch:   600  |  train loss: 0.3648719251
Epoch:   700  |  train loss: 0.3599149227
Epoch:   800  |  train loss: 0.3598585844
Epoch:   900  |  train loss: 0.3604847670
Epoch:  1000  |  train loss: 0.3640545666
Processing class: 74
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.3598952711
Epoch:   200  |  train loss: 0.3641652405
Epoch:   300  |  train loss: 0.3567394972
Epoch:   400  |  train loss: 0.3532128692
Epoch:   500  |  train loss: 0.3570083261
Epoch:   600  |  train loss: 0.3542569220
Epoch:   700  |  train loss: 0.3637132287
Epoch:   800  |  train loss: 0.3546223819
Epoch:   900  |  train loss: 0.3555901229
Epoch:  1000  |  train loss: 0.3631143034
Processing class: 75
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.3494496644
Epoch:   200  |  train loss: 0.3476112425
Epoch:   300  |  train loss: 0.3550673306
Epoch:   400  |  train loss: 0.3538048148
Epoch:   500  |  train loss: 0.3593503892
Epoch:   600  |  train loss: 0.3649000823
Epoch:   700  |  train loss: 0.3497272849
Epoch:   800  |  train loss: 0.3591340184
Epoch:   900  |  train loss: 0.3581970215
Epoch:  1000  |  train loss: 0.3645774722
Processing class: 76
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.3221014977
Epoch:   200  |  train loss: 0.3251074076
Epoch:   300  |  train loss: 0.3348122239
Epoch:   400  |  train loss: 0.3416177332
Epoch:   500  |  train loss: 0.3443508506
Epoch:   600  |  train loss: 0.3497707546
Epoch:   700  |  train loss: 0.3512102187
Epoch:   800  |  train loss: 0.3431969881
Epoch:   900  |  train loss: 0.3521072090
Epoch:  1000  |  train loss: 0.3561235130
Processing class: 77
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.3478991866
Epoch:   200  |  train loss: 0.3485450566
Epoch:   300  |  train loss: 0.3432366133
Epoch:   400  |  train loss: 0.3463165462
Epoch:   500  |  train loss: 0.3476450324
Epoch:   600  |  train loss: 0.3474337995
Epoch:   700  |  train loss: 0.3492398143
Epoch:   800  |  train loss: 0.3557318449
Epoch:   900  |  train loss: 0.3522309482
Epoch:  1000  |  train loss: 0.3512195408
Processing class: 78
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.3539820135
Epoch:   200  |  train loss: 0.3589608848
Epoch:   300  |  train loss: 0.3646906793
Epoch:   400  |  train loss: 0.3640237987
Epoch:   500  |  train loss: 0.3622579336
Epoch:   600  |  train loss: 0.3675582051
Epoch:   700  |  train loss: 0.3690105081
Epoch:   800  |  train loss: 0.3707903028
Epoch:   900  |  train loss: 0.3714693844
Epoch:  1000  |  train loss: 0.3696750820
Processing class: 79
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.3547855377
Epoch:   200  |  train loss: 0.3500418007
Epoch:   300  |  train loss: 0.3569385171
Epoch:   400  |  train loss: 0.3570967793
Epoch:   500  |  train loss: 0.3570336699
Epoch:   600  |  train loss: 0.3643350124
Epoch:   700  |  train loss: 0.3640331030
Epoch:   800  |  train loss: 0.3672575295
Epoch:   900  |  train loss: 0.3728378296
Epoch:  1000  |  train loss: 0.3677387595
2024-03-05 14:12:39,528 [trainer.py] => CNN: {'total': 59.18, '00-09': 73.6, '10-19': 68.4, '20-29': 76.9, '30-39': 69.1, '40-49': 60.6, '50-59': 37.2, '60-69': 45.7, '70-79': 41.9, 'old': 61.64, 'new': 41.9}
2024-03-05 14:12:39,529 [trainer.py] => No NME accuracy
2024-03-05 14:12:39,529 [trainer.py] => FeCAM: {'total': 61.72, '00-09': 78.1, '10-19': 69.3, '20-29': 78.0, '30-39': 69.5, '40-49': 72.1, '50-59': 35.7, '60-69': 49.1, '70-79': 42.0, 'old': 64.54, 'new': 42.0}
2024-03-05 14:12:39,530 [trainer.py] => CNN top1 curve: [83.44, 71.75, 64.66, 59.18]
2024-03-05 14:12:39,530 [trainer.py] => CNN top5 curve: [96.5, 89.6, 86.54, 84.09]
2024-03-05 14:12:39,530 [trainer.py] => FeCAM top1 curve: [82.9, 72.78, 67.27, 61.72]
2024-03-05 14:12:39,530 [trainer.py] => FeCAM top5 curve: [95.84, 91.18, 87.21, 84.09]

2024-03-05 14:12:39,538 [fecam.py] => Learning on 80-90
Processing class: 80
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.3388916075
Epoch:   200  |  train loss: 0.3500488639
Epoch:   300  |  train loss: 0.3546999633
Epoch:   400  |  train loss: 0.3575611770
Epoch:   500  |  train loss: 0.3570577800
Epoch:   600  |  train loss: 0.3571727335
Epoch:   700  |  train loss: 0.3507939935
Epoch:   800  |  train loss: 0.3609264553
Epoch:   900  |  train loss: 0.3585785031
Epoch:  1000  |  train loss: 0.3639999628
Processing class: 81
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.3469246447
Epoch:   200  |  train loss: 0.3565334141
Epoch:   300  |  train loss: 0.3505518317
Epoch:   400  |  train loss: 0.3600695610
Epoch:   500  |  train loss: 0.3577576041
Epoch:   600  |  train loss: 0.3644471943
Epoch:   700  |  train loss: 0.3634058714
Epoch:   800  |  train loss: 0.3678699136
Epoch:   900  |  train loss: 0.3683704197
Epoch:  1000  |  train loss: 0.3704541087
Processing class: 82
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.3418064654
Epoch:   200  |  train loss: 0.3470607996
Epoch:   300  |  train loss: 0.3480241001
Epoch:   400  |  train loss: 0.3470508218
Epoch:   500  |  train loss: 0.3491867661
Epoch:   600  |  train loss: 0.3474124432
Epoch:   700  |  train loss: 0.3476807833
Epoch:   800  |  train loss: 0.3494110167
Epoch:   900  |  train loss: 0.3502805173
Epoch:  1000  |  train loss: 0.3504968643
Processing class: 83
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.3459616303
Epoch:   200  |  train loss: 0.3539770901
Epoch:   300  |  train loss: 0.3520550251
Epoch:   400  |  train loss: 0.3552830398
Epoch:   500  |  train loss: 0.3631632268
Epoch:   600  |  train loss: 0.3581587791
Epoch:   700  |  train loss: 0.3659709275
Epoch:   800  |  train loss: 0.3748669863
Epoch:   900  |  train loss: 0.3712831318
Epoch:  1000  |  train loss: 0.3682518542
Processing class: 84
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.3545113087
Epoch:   200  |  train loss: 0.3528520346
Epoch:   300  |  train loss: 0.3595555007
Epoch:   400  |  train loss: 0.3598761261
Epoch:   500  |  train loss: 0.3651430368
Epoch:   600  |  train loss: 0.3595061302
Epoch:   700  |  train loss: 0.3656758726
Epoch:   800  |  train loss: 0.3710725486
Epoch:   900  |  train loss: 0.3636874497
Epoch:  1000  |  train loss: 0.3623775065
Processing class: 85
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.3515488982
Epoch:   200  |  train loss: 0.3496500671
Epoch:   300  |  train loss: 0.3561263859
Epoch:   400  |  train loss: 0.3531967461
Epoch:   500  |  train loss: 0.3575901806
Epoch:   600  |  train loss: 0.3628884554
Epoch:   700  |  train loss: 0.3708450198
Epoch:   800  |  train loss: 0.3642380893
Epoch:   900  |  train loss: 0.3651433647
Epoch:  1000  |  train loss: 0.3714713275
Processing class: 86
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.3595977187
Epoch:   200  |  train loss: 0.3534867287
Epoch:   300  |  train loss: 0.3621369183
Epoch:   400  |  train loss: 0.3590932012
Epoch:   500  |  train loss: 0.3591969788
Epoch:   600  |  train loss: 0.3625301778
Epoch:   700  |  train loss: 0.3629720747
Epoch:   800  |  train loss: 0.3621499836
Epoch:   900  |  train loss: 0.3620983064
Epoch:  1000  |  train loss: 0.3706499040
Processing class: 87
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.3427002251
Epoch:   200  |  train loss: 0.3483119071
Epoch:   300  |  train loss: 0.3486181736
Epoch:   400  |  train loss: 0.3527095079
Epoch:   500  |  train loss: 0.3519050837
Epoch:   600  |  train loss: 0.3558423042
Epoch:   700  |  train loss: 0.3579645097
Epoch:   800  |  train loss: 0.3580066979
Epoch:   900  |  train loss: 0.3539515436
Epoch:  1000  |  train loss: 0.3639339030
Processing class: 88
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.3481038988
Epoch:   200  |  train loss: 0.3453179419
Epoch:   300  |  train loss: 0.3451170623
Epoch:   400  |  train loss: 0.3469125152
Epoch:   500  |  train loss: 0.3462335706
Epoch:   600  |  train loss: 0.3435405850
Epoch:   700  |  train loss: 0.3490678787
Epoch:   800  |  train loss: 0.3494324803
Epoch:   900  |  train loss: 0.3510965228
Epoch:  1000  |  train loss: 0.3474548995
Processing class: 89
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.3516401768
Epoch:   200  |  train loss: 0.3575783670
Epoch:   300  |  train loss: 0.3606319308
Epoch:   400  |  train loss: 0.3623648047
Epoch:   500  |  train loss: 0.3684862077
Epoch:   600  |  train loss: 0.3690553308
Epoch:   700  |  train loss: 0.3633450687
Epoch:   800  |  train loss: 0.3753407776
Epoch:   900  |  train loss: 0.3722860157
Epoch:  1000  |  train loss: 0.3757146239
2024-03-05 14:21:33,234 [trainer.py] => CNN: {'total': 54.08, '00-09': 67.9, '10-19': 63.6, '20-29': 73.2, '30-39': 68.8, '40-49': 57.2, '50-59': 32.6, '60-69': 40.9, '70-79': 38.1, '80-89': 44.4, 'old': 55.29, 'new': 44.4}
2024-03-05 14:21:33,235 [trainer.py] => No NME accuracy
2024-03-05 14:21:33,235 [trainer.py] => FeCAM: {'total': 57.66, '00-09': 76.1, '10-19': 65.8, '20-29': 75.3, '30-39': 67.9, '40-49': 68.5, '50-59': 33.0, '60-69': 45.0, '70-79': 39.0, '80-89': 48.3, 'old': 58.82, 'new': 48.3}
2024-03-05 14:21:33,235 [trainer.py] => CNN top1 curve: [83.44, 71.75, 64.66, 59.18, 54.08]
2024-03-05 14:21:33,235 [trainer.py] => CNN top5 curve: [96.5, 89.6, 86.54, 84.09, 81.74]
2024-03-05 14:21:33,235 [trainer.py] => FeCAM top1 curve: [82.9, 72.78, 67.27, 61.72, 57.66]
2024-03-05 14:21:33,235 [trainer.py] => FeCAM top5 curve: [95.84, 91.18, 87.21, 84.09, 81.63]

2024-03-05 14:21:33,241 [fecam.py] => Learning on 90-100
Processing class: 90
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.3530839324
Epoch:   200  |  train loss: 0.3540211022
Epoch:   300  |  train loss: 0.3472281516
Epoch:   400  |  train loss: 0.3450784922
Epoch:   500  |  train loss: 0.3426433265
Epoch:   600  |  train loss: 0.3455342591
Epoch:   700  |  train loss: 0.3438169301
Epoch:   800  |  train loss: 0.3455861449
Epoch:   900  |  train loss: 0.3489366233
Epoch:  1000  |  train loss: 0.3554056823
Processing class: 91
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.3298819542
Epoch:   200  |  train loss: 0.3284465730
Epoch:   300  |  train loss: 0.3346482217
Epoch:   400  |  train loss: 0.3403584123
Epoch:   500  |  train loss: 0.3454013288
Epoch:   600  |  train loss: 0.3483115792
Epoch:   700  |  train loss: 0.3497949719
Epoch:   800  |  train loss: 0.3529759645
Epoch:   900  |  train loss: 0.3570629537
Epoch:  1000  |  train loss: 0.3582207859
Processing class: 92
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.3568158209
Epoch:   200  |  train loss: 0.3492854536
Epoch:   300  |  train loss: 0.3415912628
Epoch:   400  |  train loss: 0.3460310757
Epoch:   500  |  train loss: 0.3492205560
Epoch:   600  |  train loss: 0.3549159169
Epoch:   700  |  train loss: 0.3548190534
Epoch:   800  |  train loss: 0.3463579357
Epoch:   900  |  train loss: 0.3484810710
Epoch:  1000  |  train loss: 0.3497829258
Processing class: 93
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.3495262682
Epoch:   200  |  train loss: 0.3534449637
Epoch:   300  |  train loss: 0.3672690928
Epoch:   400  |  train loss: 0.3695781052
Epoch:   500  |  train loss: 0.3685211957
Epoch:   600  |  train loss: 0.3639222622
Epoch:   700  |  train loss: 0.3755167127
Epoch:   800  |  train loss: 0.3744746149
Epoch:   900  |  train loss: 0.3756657422
Epoch:  1000  |  train loss: 0.3741536081
Processing class: 94
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.3250056565
Epoch:   200  |  train loss: 0.3234931946
Epoch:   300  |  train loss: 0.3148213983
Epoch:   400  |  train loss: 0.3180526316
Epoch:   500  |  train loss: 0.3071525574
Epoch:   600  |  train loss: 0.3125104487
Epoch:   700  |  train loss: 0.3178170681
Epoch:   800  |  train loss: 0.3048772991
Epoch:   900  |  train loss: 0.3132804036
Epoch:  1000  |  train loss: 0.3105998099
Processing class: 95
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.3491022289
Epoch:   200  |  train loss: 0.3560584903
Epoch:   300  |  train loss: 0.3522191942
Epoch:   400  |  train loss: 0.3500581324
Epoch:   500  |  train loss: 0.3591684282
Epoch:   600  |  train loss: 0.3582010806
Epoch:   700  |  train loss: 0.3616522133
Epoch:   800  |  train loss: 0.3587696791
Epoch:   900  |  train loss: 0.3646516740
Epoch:  1000  |  train loss: 0.3701014340
Processing class: 96
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.3576420963
Epoch:   200  |  train loss: 0.3483750641
Epoch:   300  |  train loss: 0.3548167825
Epoch:   400  |  train loss: 0.3653149068
Epoch:   500  |  train loss: 0.3619283199
Epoch:   600  |  train loss: 0.3610119879
Epoch:   700  |  train loss: 0.3624342918
Epoch:   800  |  train loss: 0.3583558142
Epoch:   900  |  train loss: 0.3687138140
Epoch:  1000  |  train loss: 0.3684410989
Processing class: 97
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.3543896675
Epoch:   200  |  train loss: 0.3546201289
Epoch:   300  |  train loss: 0.3562526464
Epoch:   400  |  train loss: 0.3586509585
Epoch:   500  |  train loss: 0.3653151631
Epoch:   600  |  train loss: 0.3694711626
Epoch:   700  |  train loss: 0.3737870097
Epoch:   800  |  train loss: 0.3749499917
Epoch:   900  |  train loss: 0.3773992181
Epoch:  1000  |  train loss: 0.3766710460
Processing class: 98
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.3427916050
Epoch:   200  |  train loss: 0.3413513899
Epoch:   300  |  train loss: 0.3427135766
Epoch:   400  |  train loss: 0.3457927585
Epoch:   500  |  train loss: 0.3445273876
Epoch:   600  |  train loss: 0.3484016597
Epoch:   700  |  train loss: 0.3418593884
Epoch:   800  |  train loss: 0.3475710928
Epoch:   900  |  train loss: 0.3478540897
Epoch:  1000  |  train loss: 0.3448750913
Processing class: 99
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.3602853775
Epoch:   200  |  train loss: 0.3538244069
Epoch:   300  |  train loss: 0.3643374205
Epoch:   400  |  train loss: 0.3722094536
Epoch:   500  |  train loss: 0.3706168473
Epoch:   600  |  train loss: 0.3693645358
Epoch:   700  |  train loss: 0.3675932348
Epoch:   800  |  train loss: 0.3742657065
Epoch:   900  |  train loss: 0.3711801708
Epoch:  1000  |  train loss: 0.3725792527
2024-03-05 14:31:54,646 [trainer.py] => CNN: {'total': 50.36, '00-09': 57.7, '10-19': 63.6, '20-29': 71.4, '30-39': 68.5, '40-49': 56.8, '50-59': 29.6, '60-69': 38.9, '70-79': 36.6, '80-89': 42.7, '90-99': 37.8, 'old': 51.76, 'new': 37.8}
2024-03-05 14:31:54,649 [trainer.py] => No NME accuracy
2024-03-05 14:31:54,649 [trainer.py] => FeCAM: {'total': 54.74, '00-09': 73.3, '10-19': 65.1, '20-29': 74.1, '30-39': 67.2, '40-49': 67.1, '50-59': 30.8, '60-69': 43.3, '70-79': 37.2, '80-89': 46.9, '90-99': 42.4, 'old': 56.11, 'new': 42.4}
2024-03-05 14:31:54,649 [trainer.py] => CNN top1 curve: [83.44, 71.75, 64.66, 59.18, 54.08, 50.36]
2024-03-05 14:31:54,649 [trainer.py] => CNN top5 curve: [96.5, 89.6, 86.54, 84.09, 81.74, 79.63]
2024-03-05 14:31:54,649 [trainer.py] => FeCAM top1 curve: [82.9, 72.78, 67.27, 61.72, 57.66, 54.74]
2024-03-05 14:31:54,649 [trainer.py] => FeCAM top5 curve: [95.84, 91.18, 87.21, 84.09, 81.63, 79.21]

=========================================
2024-03-05 14:32:03,889 [trainer.py] => config: ./exps/FeCAM_cifar100.json
2024-03-05 14:32:03,889 [trainer.py] => prefix: train
2024-03-05 14:32:03,889 [trainer.py] => dataset: cifar100
2024-03-05 14:32:03,889 [trainer.py] => memory_size: 0
2024-03-05 14:32:03,890 [trainer.py] => shuffle: True
2024-03-05 14:32:03,890 [trainer.py] => init_cls: 50
2024-03-05 14:32:03,890 [trainer.py] => increment: 10
2024-03-05 14:32:03,890 [trainer.py] => model_name: fecam
2024-03-05 14:32:03,890 [trainer.py] => convnet_type: resnet18
2024-03-05 14:32:03,890 [trainer.py] => device: [device(type='cuda', index=0)]
2024-03-05 14:32:03,890 [trainer.py] => seed: 1993
2024-03-05 14:32:03,890 [trainer.py] => init_epochs: 200
2024-03-05 14:32:03,890 [trainer.py] => init_lr: 0.1
2024-03-05 14:32:03,890 [trainer.py] => init_weight_decay: 0.0005
2024-03-05 14:32:03,890 [trainer.py] => batch_size: 128
2024-03-05 14:32:03,890 [trainer.py] => num_workers: 8
2024-03-05 14:32:03,890 [trainer.py] => T: 5
2024-03-05 14:32:03,890 [trainer.py] => beta: 0.5
2024-03-05 14:32:03,890 [trainer.py] => alpha1: 1
2024-03-05 14:32:03,890 [trainer.py] => alpha2: 1
2024-03-05 14:32:03,890 [trainer.py] => ncm: False
2024-03-05 14:32:03,890 [trainer.py] => tukey: False
2024-03-05 14:32:03,890 [trainer.py] => diagonal: False
2024-03-05 14:32:03,890 [trainer.py] => per_class: True
2024-03-05 14:32:03,890 [trainer.py] => full_cov: True
2024-03-05 14:32:03,890 [trainer.py] => shrink: True
2024-03-05 14:32:03,890 [trainer.py] => norm_cov: False
2024-03-05 14:32:03,890 [trainer.py] => vecnorm: False
2024-03-05 14:32:03,890 [trainer.py] => ae_type: wae
2024-03-05 14:32:03,890 [trainer.py] => epochs: 1000
2024-03-05 14:32:03,890 [trainer.py] => ae_latent_dim: 32
2024-03-05 14:32:03,890 [trainer.py] => wae_sigma: 40
2024-03-05 14:32:03,890 [trainer.py] => wae_C: 1
Files already downloaded and verified
Files already downloaded and verified
2024-03-05 14:32:05,549 [data_manager.py] => [68, 56, 78, 8, 23, 84, 90, 65, 74, 76, 40, 89, 3, 92, 55, 9, 26, 80, 43, 38, 58, 70, 77, 1, 85, 19, 17, 50, 28, 53, 13, 81, 45, 82, 6, 59, 83, 16, 15, 44, 91, 41, 72, 60, 79, 52, 20, 10, 31, 54, 37, 95, 14, 71, 96, 98, 97, 2, 64, 66, 42, 22, 35, 86, 24, 34, 87, 21, 99, 0, 88, 27, 18, 94, 11, 12, 47, 25, 30, 46, 62, 69, 36, 61, 7, 63, 75, 5, 32, 4, 51, 48, 73, 93, 39, 67, 29, 49, 57, 33]
2024-03-05 14:32:05,849 [fecam.py] => Learning on 0-50
/home/z1165703/miniconda3/lib/python3.11/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
/home/z1165703/miniconda3/lib/python3.11/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
Processing class: 0
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.2966136396
Epoch:   200  |  train loss: 0.3031186819
Epoch:   300  |  train loss: 0.3095575392
Epoch:   400  |  train loss: 0.3126572669
Epoch:   500  |  train loss: 0.3100763440
Epoch:   600  |  train loss: 0.3100522637
Epoch:   700  |  train loss: 0.3129375577
Epoch:   800  |  train loss: 0.3139768481
Epoch:   900  |  train loss: 0.3113756001
Epoch:  1000  |  train loss: 0.3112502158
Processing class: 1
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.2901504815
Epoch:   200  |  train loss: 0.2878165126
Epoch:   300  |  train loss: 0.3026396334
Epoch:   400  |  train loss: 0.3060877085
Epoch:   500  |  train loss: 0.2999216795
Epoch:   600  |  train loss: 0.3097783625
Epoch:   700  |  train loss: 0.3061040938
Epoch:   800  |  train loss: 0.3116810739
Epoch:   900  |  train loss: 0.3195147395
Epoch:  1000  |  train loss: 0.3127996683
Processing class: 2
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.2864477992
Epoch:   200  |  train loss: 0.2869006991
Epoch:   300  |  train loss: 0.2973514259
Epoch:   400  |  train loss: 0.2969564617
Epoch:   500  |  train loss: 0.2850035191
Epoch:   600  |  train loss: 0.2965207577
Epoch:   700  |  train loss: 0.2958397567
Epoch:   800  |  train loss: 0.3046749234
Epoch:   900  |  train loss: 0.3007860720
Epoch:  1000  |  train loss: 0.2987268925
Processing class: 3
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.3124246538
Epoch:   200  |  train loss: 0.3211062551
Epoch:   300  |  train loss: 0.3259243608
Epoch:   400  |  train loss: 0.3289536297
Epoch:   500  |  train loss: 0.3261936963
Epoch:   600  |  train loss: 0.3355819345
Epoch:   700  |  train loss: 0.3420409739
Epoch:   800  |  train loss: 0.3365283072
Epoch:   900  |  train loss: 0.3418209136
Epoch:  1000  |  train loss: 0.3441239834
Processing class: 4
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.2874127150
Epoch:   200  |  train loss: 0.2841121972
Epoch:   300  |  train loss: 0.2966246545
Epoch:   400  |  train loss: 0.2984198213
Epoch:   500  |  train loss: 0.3036654353
Epoch:   600  |  train loss: 0.3030396223
Epoch:   700  |  train loss: 0.3080151677
Epoch:   800  |  train loss: 0.3096950114
Epoch:   900  |  train loss: 0.3124561846
Epoch:  1000  |  train loss: 0.3195792317
Processing class: 5
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.2876377106
Epoch:   200  |  train loss: 0.2871286094
Epoch:   300  |  train loss: 0.2919369936
Epoch:   400  |  train loss: 0.2933918297
Epoch:   500  |  train loss: 0.2929733992
Epoch:   600  |  train loss: 0.3055409670
Epoch:   700  |  train loss: 0.3087021530
Epoch:   800  |  train loss: 0.3039856195
Epoch:   900  |  train loss: 0.3035154641
Epoch:  1000  |  train loss: 0.3094318867
Processing class: 6
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.2914217770
Epoch:   200  |  train loss: 0.2957115114
Epoch:   300  |  train loss: 0.3084271014
Epoch:   400  |  train loss: 0.3081140459
Epoch:   500  |  train loss: 0.3180150807
Epoch:   600  |  train loss: 0.3148470998
Epoch:   700  |  train loss: 0.3123158514
Epoch:   800  |  train loss: 0.3154188931
Epoch:   900  |  train loss: 0.3126754761
Epoch:  1000  |  train loss: 0.3166282594
Processing class: 7
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.2822779775
Epoch:   200  |  train loss: 0.2856874704
Epoch:   300  |  train loss: 0.2897947907
Epoch:   400  |  train loss: 0.2935104787
Epoch:   500  |  train loss: 0.2993407965
Epoch:   600  |  train loss: 0.2967746615
Epoch:   700  |  train loss: 0.3020883083
Epoch:   800  |  train loss: 0.3107531071
Epoch:   900  |  train loss: 0.3044579089
Epoch:  1000  |  train loss: 0.3115935981
Processing class: 8
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.2861054122
Epoch:   200  |  train loss: 0.2927544653
Epoch:   300  |  train loss: 0.2917632937
Epoch:   400  |  train loss: 0.3030930817
Epoch:   500  |  train loss: 0.2990109861
Epoch:   600  |  train loss: 0.3028046668
Epoch:   700  |  train loss: 0.3106329679
Epoch:   800  |  train loss: 0.3050257623
Epoch:   900  |  train loss: 0.3097784519
Epoch:  1000  |  train loss: 0.3064573526
Processing class: 9
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.2808529019
Epoch:   200  |  train loss: 0.2867961228
Epoch:   300  |  train loss: 0.2955738008
Epoch:   400  |  train loss: 0.2886652052
Epoch:   500  |  train loss: 0.2973497033
Epoch:   600  |  train loss: 0.2926092327
Epoch:   700  |  train loss: 0.2958123147
Epoch:   800  |  train loss: 0.2984130681
Epoch:   900  |  train loss: 0.2989090562
Epoch:  1000  |  train loss: 0.2942302167
Processing class: 10
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.2741427720
Epoch:   200  |  train loss: 0.2748894930
Epoch:   300  |  train loss: 0.2787415683
Epoch:   400  |  train loss: 0.2886865079
Epoch:   500  |  train loss: 0.2785383165
Epoch:   600  |  train loss: 0.2836552024
Epoch:   700  |  train loss: 0.2876900017
Epoch:   800  |  train loss: 0.2859452844
Epoch:   900  |  train loss: 0.2889521837
Epoch:  1000  |  train loss: 0.2887504220
Processing class: 11
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.2881970346
Epoch:   200  |  train loss: 0.2932936192
Epoch:   300  |  train loss: 0.2902560115
Epoch:   400  |  train loss: 0.2954761446
Epoch:   500  |  train loss: 0.2971050024
Epoch:   600  |  train loss: 0.3038796246
Epoch:   700  |  train loss: 0.3133475125
Epoch:   800  |  train loss: 0.3120027006
Epoch:   900  |  train loss: 0.3131219983
Epoch:  1000  |  train loss: 0.3182902277
Processing class: 12
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.2768405914
Epoch:   200  |  train loss: 0.2912034392
Epoch:   300  |  train loss: 0.2932418406
Epoch:   400  |  train loss: 0.3017682433
Epoch:   500  |  train loss: 0.3034814835
Epoch:   600  |  train loss: 0.3003769875
Epoch:   700  |  train loss: 0.3037617087
Epoch:   800  |  train loss: 0.3064530134
Epoch:   900  |  train loss: 0.3114191592
Epoch:  1000  |  train loss: 0.3085883379
Processing class: 13
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.2835288227
Epoch:   200  |  train loss: 0.2956406116
Epoch:   300  |  train loss: 0.3048705995
Epoch:   400  |  train loss: 0.3084180534
Epoch:   500  |  train loss: 0.3165452838
Epoch:   600  |  train loss: 0.3112606764
Epoch:   700  |  train loss: 0.3161849499
Epoch:   800  |  train loss: 0.3205780029
Epoch:   900  |  train loss: 0.3233040392
Epoch:  1000  |  train loss: 0.3265397370
Processing class: 14
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.2852524400
Epoch:   200  |  train loss: 0.2893988669
Epoch:   300  |  train loss: 0.2930259705
Epoch:   400  |  train loss: 0.3013690472
Epoch:   500  |  train loss: 0.3005633652
Epoch:   600  |  train loss: 0.3012912273
Epoch:   700  |  train loss: 0.3055811286
Epoch:   800  |  train loss: 0.3153203368
Epoch:   900  |  train loss: 0.3123900235
Epoch:  1000  |  train loss: 0.3064796686
Processing class: 15
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.2875814140
Epoch:   200  |  train loss: 0.2928913891
Epoch:   300  |  train loss: 0.3041397095
Epoch:   400  |  train loss: 0.3091433346
Epoch:   500  |  train loss: 0.3144618690
Epoch:   600  |  train loss: 0.3103408575
Epoch:   700  |  train loss: 0.3186182082
Epoch:   800  |  train loss: 0.3117621422
Epoch:   900  |  train loss: 0.3195371926
Epoch:  1000  |  train loss: 0.3164190233
Processing class: 16
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.2898932159
Epoch:   200  |  train loss: 0.3096699715
Epoch:   300  |  train loss: 0.3183440089
Epoch:   400  |  train loss: 0.3147868037
Epoch:   500  |  train loss: 0.3219412863
Epoch:   600  |  train loss: 0.3165904760
Epoch:   700  |  train loss: 0.3152778029
Epoch:   800  |  train loss: 0.3253788531
Epoch:   900  |  train loss: 0.3170204639
Epoch:  1000  |  train loss: 0.3256886661
Processing class: 17
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.2760412693
Epoch:   200  |  train loss: 0.2812031627
Epoch:   300  |  train loss: 0.2893090427
Epoch:   400  |  train loss: 0.2913932204
Epoch:   500  |  train loss: 0.2840515018
Epoch:   600  |  train loss: 0.2904772878
Epoch:   700  |  train loss: 0.2893823624
Epoch:   800  |  train loss: 0.2905461431
Epoch:   900  |  train loss: 0.2933060884
Epoch:  1000  |  train loss: 0.2925576746
Processing class: 18
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.2838198841
Epoch:   200  |  train loss: 0.2862402558
Epoch:   300  |  train loss: 0.2954401970
Epoch:   400  |  train loss: 0.2954964519
Epoch:   500  |  train loss: 0.3054349005
Epoch:   600  |  train loss: 0.3033974349
Epoch:   700  |  train loss: 0.3163065076
Epoch:   800  |  train loss: 0.3164776146
Epoch:   900  |  train loss: 0.3195421517
Epoch:  1000  |  train loss: 0.3271328211
Processing class: 19
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.2887294233
Epoch:   200  |  train loss: 0.3049373686
Epoch:   300  |  train loss: 0.3059954822
Epoch:   400  |  train loss: 0.3082532108
Epoch:   500  |  train loss: 0.3044028819
Epoch:   600  |  train loss: 0.3059314668
Epoch:   700  |  train loss: 0.3100578427
Epoch:   800  |  train loss: 0.3083013296
Epoch:   900  |  train loss: 0.3225555301
Epoch:  1000  |  train loss: 0.3256937981
Processing class: 20
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.2885283589
Epoch:   200  |  train loss: 0.2947836876
Epoch:   300  |  train loss: 0.3003519475
Epoch:   400  |  train loss: 0.3054156780
Epoch:   500  |  train loss: 0.3130328774
Epoch:   600  |  train loss: 0.3091478348
Epoch:   700  |  train loss: 0.3113654733
Epoch:   800  |  train loss: 0.3116610706
Epoch:   900  |  train loss: 0.3152562618
Epoch:  1000  |  train loss: 0.3207623839
Processing class: 21
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.2893835843
Epoch:   200  |  train loss: 0.2966105700
Epoch:   300  |  train loss: 0.3027873337
Epoch:   400  |  train loss: 0.3067491651
Epoch:   500  |  train loss: 0.3106098890
Epoch:   600  |  train loss: 0.3157386541
Epoch:   700  |  train loss: 0.3146837950
Epoch:   800  |  train loss: 0.3134795487
Epoch:   900  |  train loss: 0.3205375373
Epoch:  1000  |  train loss: 0.3194285989
Processing class: 22
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.2836291015
Epoch:   200  |  train loss: 0.2865236580
Epoch:   300  |  train loss: 0.2932729840
Epoch:   400  |  train loss: 0.3023578286
Epoch:   500  |  train loss: 0.3053125322
Epoch:   600  |  train loss: 0.3022857785
Epoch:   700  |  train loss: 0.3063828170
Epoch:   800  |  train loss: 0.3049429536
Epoch:   900  |  train loss: 0.3022222042
Epoch:  1000  |  train loss: 0.3129446507
Processing class: 23
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.2953349173
Epoch:   200  |  train loss: 0.3023317218
Epoch:   300  |  train loss: 0.3096422255
Epoch:   400  |  train loss: 0.3033679903
Epoch:   500  |  train loss: 0.3039050579
Epoch:   600  |  train loss: 0.3009764433
Epoch:   700  |  train loss: 0.3063079655
Epoch:   800  |  train loss: 0.3034195662
Epoch:   900  |  train loss: 0.3085424840
Epoch:  1000  |  train loss: 0.3050908864
Processing class: 24
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.2900896013
Epoch:   200  |  train loss: 0.2905497491
Epoch:   300  |  train loss: 0.2842128396
Epoch:   400  |  train loss: 0.2922388971
Epoch:   500  |  train loss: 0.2896690369
Epoch:   600  |  train loss: 0.2944708467
Epoch:   700  |  train loss: 0.2860098124
Epoch:   800  |  train loss: 0.2894978642
Epoch:   900  |  train loss: 0.2954205990
Epoch:  1000  |  train loss: 0.2939052463
Processing class: 25
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.2846722662
Epoch:   200  |  train loss: 0.2908547699
Epoch:   300  |  train loss: 0.2993337929
Epoch:   400  |  train loss: 0.2956317902
Epoch:   500  |  train loss: 0.3054695010
Epoch:   600  |  train loss: 0.3019466281
Epoch:   700  |  train loss: 0.3102721989
Epoch:   800  |  train loss: 0.3077133119
Epoch:   900  |  train loss: 0.3062421143
Epoch:  1000  |  train loss: 0.3091821849
Processing class: 26
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.2992689967
Epoch:   200  |  train loss: 0.3084921002
Epoch:   300  |  train loss: 0.3028447926
Epoch:   400  |  train loss: 0.3106476963
Epoch:   500  |  train loss: 0.3183607221
Epoch:   600  |  train loss: 0.3097028911
Epoch:   700  |  train loss: 0.3140519261
Epoch:   800  |  train loss: 0.3175223351
Epoch:   900  |  train loss: 0.3166314423
Epoch:  1000  |  train loss: 0.3229282081
Processing class: 27
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.2850506425
Epoch:   200  |  train loss: 0.2838821590
Epoch:   300  |  train loss: 0.2872339606
Epoch:   400  |  train loss: 0.2907036364
Epoch:   500  |  train loss: 0.2963544846
Epoch:   600  |  train loss: 0.2968776047
Epoch:   700  |  train loss: 0.3000783861
Epoch:   800  |  train loss: 0.3002024710
Epoch:   900  |  train loss: 0.3031879961
Epoch:  1000  |  train loss: 0.2971044302
Processing class: 28
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.2943687320
Epoch:   200  |  train loss: 0.3020965159
Epoch:   300  |  train loss: 0.3116607487
Epoch:   400  |  train loss: 0.3166305006
Epoch:   500  |  train loss: 0.3195713818
Epoch:   600  |  train loss: 0.3205709159
Epoch:   700  |  train loss: 0.3223829985
Epoch:   800  |  train loss: 0.3250883758
Epoch:   900  |  train loss: 0.3267898440
Epoch:  1000  |  train loss: 0.3211988330
Processing class: 29
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.2868579507
Epoch:   200  |  train loss: 0.2938224196
Epoch:   300  |  train loss: 0.2946096659
Epoch:   400  |  train loss: 0.2929189622
Epoch:   500  |  train loss: 0.2997464776
Epoch:   600  |  train loss: 0.3035577416
Epoch:   700  |  train loss: 0.3042395592
Epoch:   800  |  train loss: 0.3006945193
Epoch:   900  |  train loss: 0.3024604976
Epoch:  1000  |  train loss: 0.3013528228
Processing class: 30
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.2985843062
Epoch:   200  |  train loss: 0.3071292400
Epoch:   300  |  train loss: 0.3204720974
Epoch:   400  |  train loss: 0.3310452282
Epoch:   500  |  train loss: 0.3265612662
Epoch:   600  |  train loss: 0.3268789291
Epoch:   700  |  train loss: 0.3261068642
Epoch:   800  |  train loss: 0.3331474662
Epoch:   900  |  train loss: 0.3377267659
Epoch:  1000  |  train loss: 0.3333451927
Processing class: 31
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.2948919892
Epoch:   200  |  train loss: 0.3053040445
Epoch:   300  |  train loss: 0.3077683747
Epoch:   400  |  train loss: 0.3169247985
Epoch:   500  |  train loss: 0.3205361664
Epoch:   600  |  train loss: 0.3282964706
Epoch:   700  |  train loss: 0.3270106435
Epoch:   800  |  train loss: 0.3277025461
Epoch:   900  |  train loss: 0.3336976349
Epoch:  1000  |  train loss: 0.3279007554
Processing class: 32
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.2858947456
Epoch:   200  |  train loss: 0.2936938763
Epoch:   300  |  train loss: 0.2933667302
Epoch:   400  |  train loss: 0.2882773697
Epoch:   500  |  train loss: 0.3013009310
Epoch:   600  |  train loss: 0.2993851423
Epoch:   700  |  train loss: 0.3093134284
Epoch:   800  |  train loss: 0.3134713888
Epoch:   900  |  train loss: 0.3118892908
Epoch:  1000  |  train loss: 0.3099251568
Processing class: 33
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.2757499814
Epoch:   200  |  train loss: 0.2925960898
Epoch:   300  |  train loss: 0.2824830890
Epoch:   400  |  train loss: 0.2855201006
Epoch:   500  |  train loss: 0.2878404498
Epoch:   600  |  train loss: 0.2982118070
Epoch:   700  |  train loss: 0.2937509477
Epoch:   800  |  train loss: 0.2965634346
Epoch:   900  |  train loss: 0.2962564468
Epoch:  1000  |  train loss: 0.2940246582
Processing class: 34
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.2888229489
Epoch:   200  |  train loss: 0.2876629412
Epoch:   300  |  train loss: 0.2949256122
Epoch:   400  |  train loss: 0.2925068438
Epoch:   500  |  train loss: 0.2931248963
Epoch:   600  |  train loss: 0.2995956421
Epoch:   700  |  train loss: 0.2954235375
Epoch:   800  |  train loss: 0.3029266536
Epoch:   900  |  train loss: 0.3056115925
Epoch:  1000  |  train loss: 0.3073813081
Processing class: 35
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.2796434581
Epoch:   200  |  train loss: 0.2889189422
Epoch:   300  |  train loss: 0.2954535127
Epoch:   400  |  train loss: 0.2987655461
Epoch:   500  |  train loss: 0.2949882984
Epoch:   600  |  train loss: 0.2990578711
Epoch:   700  |  train loss: 0.3021575987
Epoch:   800  |  train loss: 0.3018262684
Epoch:   900  |  train loss: 0.3031817257
Epoch:  1000  |  train loss: 0.3008115768
Processing class: 36
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.2937506735
Epoch:   200  |  train loss: 0.2972097039
Epoch:   300  |  train loss: 0.3035742998
Epoch:   400  |  train loss: 0.3043637156
Epoch:   500  |  train loss: 0.3140012026
Epoch:   600  |  train loss: 0.3111211479
Epoch:   700  |  train loss: 0.3076759994
Epoch:   800  |  train loss: 0.3182472050
Epoch:   900  |  train loss: 0.3108926058
Epoch:  1000  |  train loss: 0.3173274577
Processing class: 37
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.2906080008
Epoch:   200  |  train loss: 0.2900648594
Epoch:   300  |  train loss: 0.2905145943
Epoch:   400  |  train loss: 0.2955150187
Epoch:   500  |  train loss: 0.2999838173
Epoch:   600  |  train loss: 0.2962262571
Epoch:   700  |  train loss: 0.2978999853
Epoch:   800  |  train loss: 0.3005544424
Epoch:   900  |  train loss: 0.2973834574
Epoch:  1000  |  train loss: 0.3002575815
Processing class: 38
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.2889722824
Epoch:   200  |  train loss: 0.2925762951
Epoch:   300  |  train loss: 0.3004686058
Epoch:   400  |  train loss: 0.3030354559
Epoch:   500  |  train loss: 0.3065671384
Epoch:   600  |  train loss: 0.3056340277
Epoch:   700  |  train loss: 0.3075190842
Epoch:   800  |  train loss: 0.3140524209
Epoch:   900  |  train loss: 0.3071625471
Epoch:  1000  |  train loss: 0.3127920806
Processing class: 39
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.2902362049
Epoch:   200  |  train loss: 0.2887792766
Epoch:   300  |  train loss: 0.2946156144
Epoch:   400  |  train loss: 0.2992805064
Epoch:   500  |  train loss: 0.2979456782
Epoch:   600  |  train loss: 0.3007583380
Epoch:   700  |  train loss: 0.3044882238
Epoch:   800  |  train loss: 0.3094119012
Epoch:   900  |  train loss: 0.3125268340
Epoch:  1000  |  train loss: 0.3148002267
Processing class: 40
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.2918555737
Epoch:   200  |  train loss: 0.2892767429
Epoch:   300  |  train loss: 0.3024464905
Epoch:   400  |  train loss: 0.3056410253
Epoch:   500  |  train loss: 0.3098683715
Epoch:   600  |  train loss: 0.3077374578
Epoch:   700  |  train loss: 0.3111874342
Epoch:   800  |  train loss: 0.3114005506
Epoch:   900  |  train loss: 0.3085485101
Epoch:  1000  |  train loss: 0.3201722324
Processing class: 41
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.2904938400
Epoch:   200  |  train loss: 0.2916176438
Epoch:   300  |  train loss: 0.2994366467
Epoch:   400  |  train loss: 0.2987291992
Epoch:   500  |  train loss: 0.2999093533
Epoch:   600  |  train loss: 0.3008964658
Epoch:   700  |  train loss: 0.3066322565
Epoch:   800  |  train loss: 0.3039472342
Epoch:   900  |  train loss: 0.3031170785
Epoch:  1000  |  train loss: 0.3038948596
Processing class: 42
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.2821147323
Epoch:   200  |  train loss: 0.2916351438
Epoch:   300  |  train loss: 0.2984785020
Epoch:   400  |  train loss: 0.3019853652
Epoch:   500  |  train loss: 0.3061839104
Epoch:   600  |  train loss: 0.3093077242
Epoch:   700  |  train loss: 0.3136827230
Epoch:   800  |  train loss: 0.3171715081
Epoch:   900  |  train loss: 0.3146417379
Epoch:  1000  |  train loss: 0.3146602392
Processing class: 43
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.2845638394
Epoch:   200  |  train loss: 0.2851587832
Epoch:   300  |  train loss: 0.3010135829
Epoch:   400  |  train loss: 0.3134236813
Epoch:   500  |  train loss: 0.3169508398
Epoch:   600  |  train loss: 0.3141805589
Epoch:   700  |  train loss: 0.3193621576
Epoch:   800  |  train loss: 0.3173851013
Epoch:   900  |  train loss: 0.3113101900
Epoch:  1000  |  train loss: 0.3200014591
Processing class: 44
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.2810594022
Epoch:   200  |  train loss: 0.2921867073
Epoch:   300  |  train loss: 0.2896232665
Epoch:   400  |  train loss: 0.2929171085
Epoch:   500  |  train loss: 0.2973498583
Epoch:   600  |  train loss: 0.2951053500
Epoch:   700  |  train loss: 0.3017929435
Epoch:   800  |  train loss: 0.3004503012
Epoch:   900  |  train loss: 0.2984435737
Epoch:  1000  |  train loss: 0.3057598770
Processing class: 45
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.2918166041
Epoch:   200  |  train loss: 0.3050225556
Epoch:   300  |  train loss: 0.3063952088
Epoch:   400  |  train loss: 0.3135097384
Epoch:   500  |  train loss: 0.3124768674
Epoch:   600  |  train loss: 0.3229345262
Epoch:   700  |  train loss: 0.3258297682
Epoch:   800  |  train loss: 0.3235996544
Epoch:   900  |  train loss: 0.3248152673
Epoch:  1000  |  train loss: 0.3304043353
Processing class: 46
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.2965551674
Epoch:   200  |  train loss: 0.2988535702
Epoch:   300  |  train loss: 0.3052171528
Epoch:   400  |  train loss: 0.3084109426
Epoch:   500  |  train loss: 0.3096664965
Epoch:   600  |  train loss: 0.3099755824
Epoch:   700  |  train loss: 0.3114546418
Epoch:   800  |  train loss: 0.3198848665
Epoch:   900  |  train loss: 0.3158495069
Epoch:  1000  |  train loss: 0.3195865929
Processing class: 47
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.2813410640
Epoch:   200  |  train loss: 0.2896796584
Epoch:   300  |  train loss: 0.2883758128
Epoch:   400  |  train loss: 0.2968701363
Epoch:   500  |  train loss: 0.2978609324
Epoch:   600  |  train loss: 0.2987884283
Epoch:   700  |  train loss: 0.3021748722
Epoch:   800  |  train loss: 0.2989604056
Epoch:   900  |  train loss: 0.3018855155
Epoch:  1000  |  train loss: 0.3002783537
Processing class: 48
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.2863752663
Epoch:   200  |  train loss: 0.2959160388
Epoch:   300  |  train loss: 0.2968741953
Epoch:   400  |  train loss: 0.2915272772
Epoch:   500  |  train loss: 0.3009934425
Epoch:   600  |  train loss: 0.3002061546
Epoch:   700  |  train loss: 0.2966360152
Epoch:   800  |  train loss: 0.2966744900
Epoch:   900  |  train loss: 0.3063091815
Epoch:  1000  |  train loss: 0.3094036877
Processing class: 49
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.2844935834
Epoch:   200  |  train loss: 0.2879564583
Epoch:   300  |  train loss: 0.3015530050
Epoch:   400  |  train loss: 0.3045292854
Epoch:   500  |  train loss: 0.3086632729
Epoch:   600  |  train loss: 0.3094512582
Epoch:   700  |  train loss: 0.3101401865
Epoch:   800  |  train loss: 0.3087657332
Epoch:   900  |  train loss: 0.3079595566
Epoch:  1000  |  train loss: 0.3097754180
/home/z1165703/FeCAM/models/base.py:87: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  vec = torch.tensor(vec).cuda()
2024-03-05 14:49:37,694 [trainer.py] => CNN: {'total': 83.44, '00-09': 87.7, '10-19': 79.6, '20-29': 84.9, '30-39': 81.0, '40-49': 84.0, 'old': 0, 'new': 83.44}
2024-03-05 14:49:37,696 [trainer.py] => No NME accuracy
2024-03-05 14:49:37,696 [trainer.py] => FeCAM: {'total': 82.9, '00-09': 86.2, '10-19': 79.7, '20-29': 85.0, '30-39': 80.6, '40-49': 83.0, 'old': 0, 'new': 82.9}
2024-03-05 14:49:37,696 [trainer.py] => CNN top1 curve: [83.44]
2024-03-05 14:49:37,696 [trainer.py] => CNN top5 curve: [96.5]
2024-03-05 14:49:37,696 [trainer.py] => FeCAM top1 curve: [82.9]
2024-03-05 14:49:37,696 [trainer.py] => FeCAM top5 curve: [95.84]

2024-03-05 14:49:37,706 [fecam.py] => Learning on 50-60
Processing class: 50
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.2769163728
Epoch:   200  |  train loss: 0.2807419419
Epoch:   300  |  train loss: 0.2835174024
Epoch:   400  |  train loss: 0.2908985555
Epoch:   500  |  train loss: 0.2962977111
Epoch:   600  |  train loss: 0.2934183836
Epoch:   700  |  train loss: 0.3015566587
Epoch:   800  |  train loss: 0.3058224499
Epoch:   900  |  train loss: 0.3106028914
Epoch:  1000  |  train loss: 0.3130208552
Processing class: 51
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.2795931458
Epoch:   200  |  train loss: 0.2746517181
Epoch:   300  |  train loss: 0.2862127423
Epoch:   400  |  train loss: 0.2895039916
Epoch:   500  |  train loss: 0.2927345753
Epoch:   600  |  train loss: 0.2982044876
Epoch:   700  |  train loss: 0.2935427845
Epoch:   800  |  train loss: 0.2958980143
Epoch:   900  |  train loss: 0.3047894955
Epoch:  1000  |  train loss: 0.3026293099
Processing class: 52
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.2802190781
Epoch:   200  |  train loss: 0.2784859002
Epoch:   300  |  train loss: 0.2841767132
Epoch:   400  |  train loss: 0.2883372843
Epoch:   500  |  train loss: 0.2883840919
Epoch:   600  |  train loss: 0.2855253935
Epoch:   700  |  train loss: 0.2925364196
Epoch:   800  |  train loss: 0.2937092423
Epoch:   900  |  train loss: 0.2962480843
Epoch:  1000  |  train loss: 0.2923031390
Processing class: 53
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.2766866446
Epoch:   200  |  train loss: 0.2814183474
Epoch:   300  |  train loss: 0.2762289166
Epoch:   400  |  train loss: 0.2801461995
Epoch:   500  |  train loss: 0.2842601955
Epoch:   600  |  train loss: 0.2903434932
Epoch:   700  |  train loss: 0.2974544525
Epoch:   800  |  train loss: 0.2864375055
Epoch:   900  |  train loss: 0.2926869929
Epoch:  1000  |  train loss: 0.2893261135
Processing class: 54
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.2600510359
Epoch:   200  |  train loss: 0.2637556374
Epoch:   300  |  train loss: 0.2718004346
Epoch:   400  |  train loss: 0.2753700018
Epoch:   500  |  train loss: 0.2794903815
Epoch:   600  |  train loss: 0.2877376318
Epoch:   700  |  train loss: 0.2906951964
Epoch:   800  |  train loss: 0.2974585593
Epoch:   900  |  train loss: 0.3022551775
Epoch:  1000  |  train loss: 0.3013192952
Processing class: 55
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.2763237536
Epoch:   200  |  train loss: 0.2779714465
Epoch:   300  |  train loss: 0.2754307091
Epoch:   400  |  train loss: 0.2834257603
Epoch:   500  |  train loss: 0.2938518465
Epoch:   600  |  train loss: 0.2958265960
Epoch:   700  |  train loss: 0.2998666108
Epoch:   800  |  train loss: 0.3078071654
Epoch:   900  |  train loss: 0.3003098369
Epoch:  1000  |  train loss: 0.3062946081
Processing class: 56
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.2741648138
Epoch:   200  |  train loss: 0.2795172334
Epoch:   300  |  train loss: 0.2765201747
Epoch:   400  |  train loss: 0.2868551254
Epoch:   500  |  train loss: 0.2921003938
Epoch:   600  |  train loss: 0.2960963428
Epoch:   700  |  train loss: 0.3011287570
Epoch:   800  |  train loss: 0.2984479487
Epoch:   900  |  train loss: 0.3055123448
Epoch:  1000  |  train loss: 0.3099420667
Processing class: 57
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.2729741395
Epoch:   200  |  train loss: 0.2796878159
Epoch:   300  |  train loss: 0.2780665874
Epoch:   400  |  train loss: 0.2791445553
Epoch:   500  |  train loss: 0.2848943114
Epoch:   600  |  train loss: 0.2795812607
Epoch:   700  |  train loss: 0.2892717361
Epoch:   800  |  train loss: 0.2900760710
Epoch:   900  |  train loss: 0.2957014024
Epoch:  1000  |  train loss: 0.2892074883
Processing class: 58
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.2745274663
Epoch:   200  |  train loss: 0.2755527258
Epoch:   300  |  train loss: 0.2845034420
Epoch:   400  |  train loss: 0.2950447321
Epoch:   500  |  train loss: 0.2943822861
Epoch:   600  |  train loss: 0.3009943306
Epoch:   700  |  train loss: 0.3007507920
Epoch:   800  |  train loss: 0.3061110556
Epoch:   900  |  train loss: 0.3067240238
Epoch:  1000  |  train loss: 0.3080444515
Processing class: 59
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.2816938639
Epoch:   200  |  train loss: 0.2916724265
Epoch:   300  |  train loss: 0.2904484153
Epoch:   400  |  train loss: 0.2965134919
Epoch:   500  |  train loss: 0.3024821639
Epoch:   600  |  train loss: 0.3042702854
Epoch:   700  |  train loss: 0.3085692227
Epoch:   800  |  train loss: 0.3108877838
Epoch:   900  |  train loss: 0.3106799722
Epoch:  1000  |  train loss: 0.3090031445
2024-03-05 14:55:17,469 [trainer.py] => CNN: {'total': 71.75, '00-09': 82.2, '10-19': 72.8, '20-29': 78.5, '30-39': 74.9, '40-49': 68.4, '50-59': 53.7, 'old': 75.36, 'new': 53.7}
2024-03-05 14:55:17,469 [trainer.py] => No NME accuracy
2024-03-05 14:55:17,469 [trainer.py] => FeCAM: {'total': 72.78, '00-09': 82.6, '10-19': 74.0, '20-29': 81.0, '30-39': 76.4, '40-49': 76.8, '50-59': 45.9, 'old': 78.16, 'new': 45.9}
2024-03-05 14:55:17,469 [trainer.py] => CNN top1 curve: [83.44, 71.75]
2024-03-05 14:55:17,469 [trainer.py] => CNN top5 curve: [96.5, 89.6]
2024-03-05 14:55:17,469 [trainer.py] => FeCAM top1 curve: [82.9, 72.78]
2024-03-05 14:55:17,470 [trainer.py] => FeCAM top5 curve: [95.84, 91.18]

2024-03-05 14:55:17,476 [fecam.py] => Learning on 60-70
Processing class: 60
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.2853539050
Epoch:   200  |  train loss: 0.2853370905
Epoch:   300  |  train loss: 0.2829410672
Epoch:   400  |  train loss: 0.2830685556
Epoch:   500  |  train loss: 0.2931063533
Epoch:   600  |  train loss: 0.2946853936
Epoch:   700  |  train loss: 0.3143135726
Epoch:   800  |  train loss: 0.3060244977
Epoch:   900  |  train loss: 0.3086398900
Epoch:  1000  |  train loss: 0.3144661903
Processing class: 61
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.2740714252
Epoch:   200  |  train loss: 0.2710077703
Epoch:   300  |  train loss: 0.2817580640
Epoch:   400  |  train loss: 0.2852773547
Epoch:   500  |  train loss: 0.2965966940
Epoch:   600  |  train loss: 0.2914823234
Epoch:   700  |  train loss: 0.3017249286
Epoch:   800  |  train loss: 0.3081256151
Epoch:   900  |  train loss: 0.3010798812
Epoch:  1000  |  train loss: 0.3052579403
Processing class: 62
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.2812808692
Epoch:   200  |  train loss: 0.2780814707
Epoch:   300  |  train loss: 0.2844270647
Epoch:   400  |  train loss: 0.2875689149
Epoch:   500  |  train loss: 0.2844573557
Epoch:   600  |  train loss: 0.2937107503
Epoch:   700  |  train loss: 0.2968671918
Epoch:   800  |  train loss: 0.2996805012
Epoch:   900  |  train loss: 0.2983347535
Epoch:  1000  |  train loss: 0.3012342215
Processing class: 63
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.2703574359
Epoch:   200  |  train loss: 0.2734324157
Epoch:   300  |  train loss: 0.2875880778
Epoch:   400  |  train loss: 0.2906269610
Epoch:   500  |  train loss: 0.2966185153
Epoch:   600  |  train loss: 0.3054641902
Epoch:   700  |  train loss: 0.2973631263
Epoch:   800  |  train loss: 0.3119472504
Epoch:   900  |  train loss: 0.3041721523
Epoch:  1000  |  train loss: 0.3069761753
Processing class: 64
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.2863783300
Epoch:   200  |  train loss: 0.2846551180
Epoch:   300  |  train loss: 0.3008128941
Epoch:   400  |  train loss: 0.3033711731
Epoch:   500  |  train loss: 0.3068715453
Epoch:   600  |  train loss: 0.3079127073
Epoch:   700  |  train loss: 0.3029038072
Epoch:   800  |  train loss: 0.3073364019
Epoch:   900  |  train loss: 0.3150219142
Epoch:  1000  |  train loss: 0.3139461637
Processing class: 65
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.2783771574
Epoch:   200  |  train loss: 0.2804746628
Epoch:   300  |  train loss: 0.2973639965
Epoch:   400  |  train loss: 0.3049116373
Epoch:   500  |  train loss: 0.3045949697
Epoch:   600  |  train loss: 0.3176931858
Epoch:   700  |  train loss: 0.3191475093
Epoch:   800  |  train loss: 0.3308660328
Epoch:   900  |  train loss: 0.3254867435
Epoch:  1000  |  train loss: 0.3372915864
Processing class: 66
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.2838290453
Epoch:   200  |  train loss: 0.2905146420
Epoch:   300  |  train loss: 0.2983899713
Epoch:   400  |  train loss: 0.3003600240
Epoch:   500  |  train loss: 0.3080972850
Epoch:   600  |  train loss: 0.3100338340
Epoch:   700  |  train loss: 0.3184743881
Epoch:   800  |  train loss: 0.3199652016
Epoch:   900  |  train loss: 0.3206961155
Epoch:  1000  |  train loss: 0.3200093687
Processing class: 67
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.2722229719
Epoch:   200  |  train loss: 0.2768846214
Epoch:   300  |  train loss: 0.2781632662
Epoch:   400  |  train loss: 0.2810192227
Epoch:   500  |  train loss: 0.2824971080
Epoch:   600  |  train loss: 0.2829240739
Epoch:   700  |  train loss: 0.2853161991
Epoch:   800  |  train loss: 0.2907324970
Epoch:   900  |  train loss: 0.2904185712
Epoch:  1000  |  train loss: 0.2906580925
Processing class: 68
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.2714097440
Epoch:   200  |  train loss: 0.2724672079
Epoch:   300  |  train loss: 0.2735404372
Epoch:   400  |  train loss: 0.2777296782
Epoch:   500  |  train loss: 0.2794946730
Epoch:   600  |  train loss: 0.2795260191
Epoch:   700  |  train loss: 0.2836386025
Epoch:   800  |  train loss: 0.2815121472
Epoch:   900  |  train loss: 0.2848593414
Epoch:  1000  |  train loss: 0.2943468571
Processing class: 69
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.2703035593
Epoch:   200  |  train loss: 0.2785769403
Epoch:   300  |  train loss: 0.2938899636
Epoch:   400  |  train loss: 0.3013085723
Epoch:   500  |  train loss: 0.3033782959
Epoch:   600  |  train loss: 0.2959147751
Epoch:   700  |  train loss: 0.3041694760
Epoch:   800  |  train loss: 0.3042047501
Epoch:   900  |  train loss: 0.3150163770
Epoch:  1000  |  train loss: 0.3053055704
2024-03-05 15:01:47,063 [trainer.py] => CNN: {'total': 64.66, '00-09': 75.0, '10-19': 70.1, '20-29': 77.1, '30-39': 70.6, '40-49': 64.0, '50-59': 45.8, '60-69': 50.0, 'old': 67.1, 'new': 50.0}
2024-03-05 15:01:47,064 [trainer.py] => No NME accuracy
2024-03-05 15:01:47,064 [trainer.py] => FeCAM: {'total': 67.27, '00-09': 79.9, '10-19': 70.6, '20-29': 79.1, '30-39': 71.9, '40-49': 74.1, '50-59': 41.2, '60-69': 54.1, 'old': 69.47, 'new': 54.1}
2024-03-05 15:01:47,064 [trainer.py] => CNN top1 curve: [83.44, 71.75, 64.66]
2024-03-05 15:01:47,064 [trainer.py] => CNN top5 curve: [96.5, 89.6, 86.54]
2024-03-05 15:01:47,064 [trainer.py] => FeCAM top1 curve: [82.9, 72.78, 67.27]
2024-03-05 15:01:47,064 [trainer.py] => FeCAM top5 curve: [95.84, 91.18, 87.21]

2024-03-05 15:01:47,068 [fecam.py] => Learning on 70-80
Processing class: 70
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.2735086501
Epoch:   200  |  train loss: 0.2718545675
Epoch:   300  |  train loss: 0.2776839018
Epoch:   400  |  train loss: 0.2824998915
Epoch:   500  |  train loss: 0.2815887749
Epoch:   600  |  train loss: 0.2889573514
Epoch:   700  |  train loss: 0.2983305275
Epoch:   800  |  train loss: 0.2913298130
Epoch:   900  |  train loss: 0.2983723104
Epoch:  1000  |  train loss: 0.2968234420
Processing class: 71
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.2778119504
Epoch:   200  |  train loss: 0.2815271735
Epoch:   300  |  train loss: 0.2782974422
Epoch:   400  |  train loss: 0.2854180396
Epoch:   500  |  train loss: 0.2908844888
Epoch:   600  |  train loss: 0.2930395007
Epoch:   700  |  train loss: 0.2951481640
Epoch:   800  |  train loss: 0.2980734468
Epoch:   900  |  train loss: 0.3060168028
Epoch:  1000  |  train loss: 0.3055378854
Processing class: 72
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.2730349958
Epoch:   200  |  train loss: 0.2768305063
Epoch:   300  |  train loss: 0.2797198296
Epoch:   400  |  train loss: 0.2811729074
Epoch:   500  |  train loss: 0.2869580984
Epoch:   600  |  train loss: 0.2835466385
Epoch:   700  |  train loss: 0.2871235549
Epoch:   800  |  train loss: 0.2861040533
Epoch:   900  |  train loss: 0.2891123056
Epoch:  1000  |  train loss: 0.2960655153
Processing class: 73
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.2804943740
Epoch:   200  |  train loss: 0.2806516588
Epoch:   300  |  train loss: 0.2914746046
Epoch:   400  |  train loss: 0.2893035352
Epoch:   500  |  train loss: 0.2984229863
Epoch:   600  |  train loss: 0.2987843454
Epoch:   700  |  train loss: 0.2954277515
Epoch:   800  |  train loss: 0.2960248113
Epoch:   900  |  train loss: 0.2978116393
Epoch:  1000  |  train loss: 0.3023219883
Processing class: 74
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.2834681809
Epoch:   200  |  train loss: 0.2878572404
Epoch:   300  |  train loss: 0.2824215531
Epoch:   400  |  train loss: 0.2808506966
Epoch:   500  |  train loss: 0.2860055923
Epoch:   600  |  train loss: 0.2848358452
Epoch:   700  |  train loss: 0.2946755528
Epoch:   800  |  train loss: 0.2875000536
Epoch:   900  |  train loss: 0.2893773258
Epoch:  1000  |  train loss: 0.2976850450
Processing class: 75
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.2747197926
Epoch:   200  |  train loss: 0.2746297896
Epoch:   300  |  train loss: 0.2840921819
Epoch:   400  |  train loss: 0.2847129941
Epoch:   500  |  train loss: 0.2915302455
Epoch:   600  |  train loss: 0.2982233107
Epoch:   700  |  train loss: 0.2850870132
Epoch:   800  |  train loss: 0.2948951721
Epoch:   900  |  train loss: 0.2949837327
Epoch:  1000  |  train loss: 0.3022336245
Processing class: 76
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.2629799843
Epoch:   200  |  train loss: 0.2713363886
Epoch:   300  |  train loss: 0.2845055103
Epoch:   400  |  train loss: 0.2935251772
Epoch:   500  |  train loss: 0.2974005699
Epoch:   600  |  train loss: 0.3042224586
Epoch:   700  |  train loss: 0.3060059965
Epoch:   800  |  train loss: 0.2993109107
Epoch:   900  |  train loss: 0.3090594232
Epoch:  1000  |  train loss: 0.3136193812
Processing class: 77
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.2764222503
Epoch:   200  |  train loss: 0.2793495357
Epoch:   300  |  train loss: 0.2775845051
Epoch:   400  |  train loss: 0.2827777326
Epoch:   500  |  train loss: 0.2871605873
Epoch:   600  |  train loss: 0.2895114839
Epoch:   700  |  train loss: 0.2931045413
Epoch:   800  |  train loss: 0.3008868456
Epoch:   900  |  train loss: 0.2987392247
Epoch:  1000  |  train loss: 0.2990855396
Processing class: 78
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.2806104004
Epoch:   200  |  train loss: 0.2891580164
Epoch:   300  |  train loss: 0.2970141828
Epoch:   400  |  train loss: 0.2985627472
Epoch:   500  |  train loss: 0.2992611527
Epoch:   600  |  train loss: 0.3059221745
Epoch:   700  |  train loss: 0.3090021491
Epoch:   800  |  train loss: 0.3122345805
Epoch:   900  |  train loss: 0.3139425099
Epoch:  1000  |  train loss: 0.3128834784
Processing class: 79
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.2783471942
Epoch:   200  |  train loss: 0.2748902977
Epoch:   300  |  train loss: 0.2824130177
Epoch:   400  |  train loss: 0.2844321370
Epoch:   500  |  train loss: 0.2852741361
Epoch:   600  |  train loss: 0.2937275290
Epoch:   700  |  train loss: 0.2944251657
Epoch:   800  |  train loss: 0.2988756359
Epoch:   900  |  train loss: 0.3054068565
Epoch:  1000  |  train loss: 0.3017432690
2024-03-05 15:09:24,746 [trainer.py] => CNN: {'total': 59.18, '00-09': 73.6, '10-19': 68.4, '20-29': 76.9, '30-39': 69.1, '40-49': 60.6, '50-59': 37.2, '60-69': 45.7, '70-79': 41.9, 'old': 61.64, 'new': 41.9}
2024-03-05 15:09:24,749 [trainer.py] => No NME accuracy
2024-03-05 15:09:24,749 [trainer.py] => FeCAM: {'total': 61.72, '00-09': 78.1, '10-19': 69.3, '20-29': 78.0, '30-39': 69.5, '40-49': 72.1, '50-59': 35.7, '60-69': 49.1, '70-79': 42.0, 'old': 64.54, 'new': 42.0}
2024-03-05 15:09:24,750 [trainer.py] => CNN top1 curve: [83.44, 71.75, 64.66, 59.18]
2024-03-05 15:09:24,750 [trainer.py] => CNN top5 curve: [96.5, 89.6, 86.54, 84.09]
2024-03-05 15:09:24,750 [trainer.py] => FeCAM top1 curve: [82.9, 72.78, 67.27, 61.72]
2024-03-05 15:09:24,750 [trainer.py] => FeCAM top5 curve: [95.84, 91.18, 87.21, 84.09]

2024-03-05 15:09:24,755 [fecam.py] => Learning on 80-90
Processing class: 80
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.2728485405
Epoch:   200  |  train loss: 0.2918338299
Epoch:   300  |  train loss: 0.3000068367
Epoch:   400  |  train loss: 0.3048952639
Epoch:   500  |  train loss: 0.3062520325
Epoch:   600  |  train loss: 0.3074679792
Epoch:   700  |  train loss: 0.3025663376
Epoch:   800  |  train loss: 0.3134045899
Epoch:   900  |  train loss: 0.3118080497
Epoch:  1000  |  train loss: 0.3181976199
Processing class: 81
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.2745860636
Epoch:   200  |  train loss: 0.2866185606
Epoch:   300  |  train loss: 0.2835806608
Epoch:   400  |  train loss: 0.2949073553
Epoch:   500  |  train loss: 0.2944237709
Epoch:   600  |  train loss: 0.3022459090
Epoch:   700  |  train loss: 0.3028410077
Epoch:   800  |  train loss: 0.3079755425
Epoch:   900  |  train loss: 0.3100626409
Epoch:  1000  |  train loss: 0.3136087060
Processing class: 82
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.2722159564
Epoch:   200  |  train loss: 0.2807268143
Epoch:   300  |  train loss: 0.2842022717
Epoch:   400  |  train loss: 0.2856088400
Epoch:   500  |  train loss: 0.2900983334
Epoch:   600  |  train loss: 0.2897814989
Epoch:   700  |  train loss: 0.2915014267
Epoch:   800  |  train loss: 0.2937881887
Epoch:   900  |  train loss: 0.2961240709
Epoch:  1000  |  train loss: 0.2973590255
Processing class: 83
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.2774453998
Epoch:   200  |  train loss: 0.2917946756
Epoch:   300  |  train loss: 0.2945322156
Epoch:   400  |  train loss: 0.3009101450
Epoch:   500  |  train loss: 0.3111061156
Epoch:   600  |  train loss: 0.3078834414
Epoch:   700  |  train loss: 0.3169837892
Epoch:   800  |  train loss: 0.3269032836
Epoch:   900  |  train loss: 0.3242740095
Epoch:  1000  |  train loss: 0.3227713168
Processing class: 84
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.2805568695
Epoch:   200  |  train loss: 0.2832006335
Epoch:   300  |  train loss: 0.2926068842
Epoch:   400  |  train loss: 0.2955670059
Epoch:   500  |  train loss: 0.3029829860
Epoch:   600  |  train loss: 0.2988231301
Epoch:   700  |  train loss: 0.3062653005
Epoch:   800  |  train loss: 0.3126425803
Epoch:   900  |  train loss: 0.3067146719
Epoch:  1000  |  train loss: 0.3062902153
Processing class: 85
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.2771398664
Epoch:   200  |  train loss: 0.2789045990
Epoch:   300  |  train loss: 0.2881807864
Epoch:   400  |  train loss: 0.2877706945
Epoch:   500  |  train loss: 0.2939696372
Epoch:   600  |  train loss: 0.3009316683
Epoch:   700  |  train loss: 0.3098807812
Epoch:   800  |  train loss: 0.3045471847
Epoch:   900  |  train loss: 0.3063345611
Epoch:  1000  |  train loss: 0.3138309419
Processing class: 86
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.2830433607
Epoch:   200  |  train loss: 0.2787683487
Epoch:   300  |  train loss: 0.2891803801
Epoch:   400  |  train loss: 0.2886549711
Epoch:   500  |  train loss: 0.2900021732
Epoch:   600  |  train loss: 0.2950317442
Epoch:   700  |  train loss: 0.2966411769
Epoch:   800  |  train loss: 0.2971577466
Epoch:   900  |  train loss: 0.2980673611
Epoch:  1000  |  train loss: 0.3074548781
Processing class: 87
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.2721542299
Epoch:   200  |  train loss: 0.2833434165
Epoch:   300  |  train loss: 0.2862081289
Epoch:   400  |  train loss: 0.2934147358
Epoch:   500  |  train loss: 0.2946265459
Epoch:   600  |  train loss: 0.2994332552
Epoch:   700  |  train loss: 0.3028649509
Epoch:   800  |  train loss: 0.3040734351
Epoch:   900  |  train loss: 0.3014228404
Epoch:  1000  |  train loss: 0.3123729408
Processing class: 88
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.2750230968
Epoch:   200  |  train loss: 0.2744980633
Epoch:   300  |  train loss: 0.2771919668
Epoch:   400  |  train loss: 0.2812091231
Epoch:   500  |  train loss: 0.2829634190
Epoch:   600  |  train loss: 0.2829433084
Epoch:   700  |  train loss: 0.2898934007
Epoch:   800  |  train loss: 0.2920325518
Epoch:   900  |  train loss: 0.2948306084
Epoch:  1000  |  train loss: 0.2922657192
Processing class: 89
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.2789984107
Epoch:   200  |  train loss: 0.2884574831
Epoch:   300  |  train loss: 0.2951936245
Epoch:   400  |  train loss: 0.2987568021
Epoch:   500  |  train loss: 0.3067033827
Epoch:   600  |  train loss: 0.3096844077
Epoch:   700  |  train loss: 0.3051948965
Epoch:   800  |  train loss: 0.3186649740
Epoch:   900  |  train loss: 0.3169113100
Epoch:  1000  |  train loss: 0.3216284752
2024-03-05 15:18:30,157 [trainer.py] => CNN: {'total': 54.08, '00-09': 67.9, '10-19': 63.6, '20-29': 73.2, '30-39': 68.8, '40-49': 57.2, '50-59': 32.6, '60-69': 40.9, '70-79': 38.1, '80-89': 44.4, 'old': 55.29, 'new': 44.4}
2024-03-05 15:18:30,157 [trainer.py] => No NME accuracy
2024-03-05 15:18:30,157 [trainer.py] => FeCAM: {'total': 57.66, '00-09': 76.1, '10-19': 65.8, '20-29': 75.3, '30-39': 67.9, '40-49': 68.5, '50-59': 33.0, '60-69': 45.0, '70-79': 39.0, '80-89': 48.3, 'old': 58.82, 'new': 48.3}
2024-03-05 15:18:30,157 [trainer.py] => CNN top1 curve: [83.44, 71.75, 64.66, 59.18, 54.08]
2024-03-05 15:18:30,157 [trainer.py] => CNN top5 curve: [96.5, 89.6, 86.54, 84.09, 81.74]
2024-03-05 15:18:30,157 [trainer.py] => FeCAM top1 curve: [82.9, 72.78, 67.27, 61.72, 57.66]
2024-03-05 15:18:30,157 [trainer.py] => FeCAM top5 curve: [95.84, 91.18, 87.21, 84.09, 81.63]

2024-03-05 15:18:30,163 [fecam.py] => Learning on 90-100
Processing class: 90
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.2790933371
Epoch:   200  |  train loss: 0.2816887915
Epoch:   300  |  train loss: 0.2782727778
Epoch:   400  |  train loss: 0.2791952610
Epoch:   500  |  train loss: 0.2790989816
Epoch:   600  |  train loss: 0.2842429340
Epoch:   700  |  train loss: 0.2848559201
Epoch:   800  |  train loss: 0.2881258249
Epoch:   900  |  train loss: 0.2924417436
Epoch:  1000  |  train loss: 0.3000305474
Processing class: 91
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.2695959449
Epoch:   200  |  train loss: 0.2715567529
Epoch:   300  |  train loss: 0.2804497778
Epoch:   400  |  train loss: 0.2873417497
Epoch:   500  |  train loss: 0.2949661195
Epoch:   600  |  train loss: 0.2993047237
Epoch:   700  |  train loss: 0.3031108856
Epoch:   800  |  train loss: 0.3082432628
Epoch:   900  |  train loss: 0.3140893281
Epoch:  1000  |  train loss: 0.3160409272
Processing class: 92
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.2844565690
Epoch:   200  |  train loss: 0.2826726735
Epoch:   300  |  train loss: 0.2771113634
Epoch:   400  |  train loss: 0.2832826793
Epoch:   500  |  train loss: 0.2889062941
Epoch:   600  |  train loss: 0.2979005575
Epoch:   700  |  train loss: 0.2996639431
Epoch:   800  |  train loss: 0.2932105720
Epoch:   900  |  train loss: 0.2965465784
Epoch:  1000  |  train loss: 0.2991081536
Processing class: 93
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.2763886750
Epoch:   200  |  train loss: 0.2845310509
Epoch:   300  |  train loss: 0.3017224491
Epoch:   400  |  train loss: 0.3057895005
Epoch:   500  |  train loss: 0.3071334779
Epoch:   600  |  train loss: 0.3049353957
Epoch:   700  |  train loss: 0.3182659626
Epoch:   800  |  train loss: 0.3189550579
Epoch:   900  |  train loss: 0.3215033591
Epoch:  1000  |  train loss: 0.3214262426
Processing class: 94
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.2574629128
Epoch:   200  |  train loss: 0.2588852197
Epoch:   300  |  train loss: 0.2547907680
Epoch:   400  |  train loss: 0.2610334933
Epoch:   500  |  train loss: 0.2524257332
Epoch:   600  |  train loss: 0.2592688620
Epoch:   700  |  train loss: 0.2658289313
Epoch:   800  |  train loss: 0.2546733677
Epoch:   900  |  train loss: 0.2640395045
Epoch:  1000  |  train loss: 0.2623821199
Processing class: 95
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.2740799725
Epoch:   200  |  train loss: 0.2817138672
Epoch:   300  |  train loss: 0.2795989931
Epoch:   400  |  train loss: 0.2795256078
Epoch:   500  |  train loss: 0.2905364811
Epoch:   600  |  train loss: 0.2910873711
Epoch:   700  |  train loss: 0.2959940255
Epoch:   800  |  train loss: 0.2944654465
Epoch:   900  |  train loss: 0.3014669478
Epoch:  1000  |  train loss: 0.3081745803
Processing class: 96
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.2818256199
Epoch:   200  |  train loss: 0.2751429975
Epoch:   300  |  train loss: 0.2839502931
Epoch:   400  |  train loss: 0.2960187137
Epoch:   500  |  train loss: 0.2946054101
Epoch:   600  |  train loss: 0.2949277699
Epoch:   700  |  train loss: 0.2980065942
Epoch:   800  |  train loss: 0.2954468668
Epoch:   900  |  train loss: 0.3073921621
Epoch:  1000  |  train loss: 0.3085241020
Processing class: 97
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.2806898475
Epoch:   200  |  train loss: 0.2854181707
Epoch:   300  |  train loss: 0.2906405568
Epoch:   400  |  train loss: 0.2950603366
Epoch:   500  |  train loss: 0.3038690925
Epoch:   600  |  train loss: 0.3098346770
Epoch:   700  |  train loss: 0.3155836105
Epoch:   800  |  train loss: 0.3186815977
Epoch:   900  |  train loss: 0.3224199533
Epoch:  1000  |  train loss: 0.3226314127
Processing class: 98
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.2736706853
Epoch:   200  |  train loss: 0.2759384751
Epoch:   300  |  train loss: 0.2814031422
Epoch:   400  |  train loss: 0.2873729110
Epoch:   500  |  train loss: 0.2888195515
Epoch:   600  |  train loss: 0.2946543276
Epoch:   700  |  train loss: 0.2897276878
Epoch:   800  |  train loss: 0.2968604386
Epoch:   900  |  train loss: 0.2980952978
Epoch:  1000  |  train loss: 0.2959885180
Processing class: 99
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.2856032014
Epoch:   200  |  train loss: 0.2820168078
Epoch:   300  |  train loss: 0.2963789582
Epoch:   400  |  train loss: 0.3065474391
Epoch:   500  |  train loss: 0.3079213679
Epoch:   600  |  train loss: 0.3088590741
Epoch:   700  |  train loss: 0.3082535684
Epoch:   800  |  train loss: 0.3162034392
Epoch:   900  |  train loss: 0.3149390280
Epoch:  1000  |  train loss: 0.3172608852
2024-03-05 15:28:49,298 [trainer.py] => CNN: {'total': 50.36, '00-09': 57.7, '10-19': 63.6, '20-29': 71.4, '30-39': 68.5, '40-49': 56.8, '50-59': 29.6, '60-69': 38.9, '70-79': 36.6, '80-89': 42.7, '90-99': 37.8, 'old': 51.76, 'new': 37.8}
2024-03-05 15:28:49,299 [trainer.py] => No NME accuracy
2024-03-05 15:28:49,299 [trainer.py] => FeCAM: {'total': 54.74, '00-09': 73.3, '10-19': 65.1, '20-29': 74.1, '30-39': 67.2, '40-49': 67.1, '50-59': 30.8, '60-69': 43.3, '70-79': 37.2, '80-89': 46.9, '90-99': 42.4, 'old': 56.11, 'new': 42.4}
2024-03-05 15:28:49,301 [trainer.py] => CNN top1 curve: [83.44, 71.75, 64.66, 59.18, 54.08, 50.36]
2024-03-05 15:28:49,301 [trainer.py] => CNN top5 curve: [96.5, 89.6, 86.54, 84.09, 81.74, 79.63]
2024-03-05 15:28:49,301 [trainer.py] => FeCAM top1 curve: [82.9, 72.78, 67.27, 61.72, 57.66, 54.74]
2024-03-05 15:28:49,301 [trainer.py] => FeCAM top5 curve: [95.84, 91.18, 87.21, 84.09, 81.63, 79.21]

=========================================
2024-03-05 15:28:59,625 [trainer.py] => config: ./exps/FeCAM_cifar100.json
2024-03-05 15:28:59,625 [trainer.py] => prefix: train
2024-03-05 15:28:59,625 [trainer.py] => dataset: cifar100
2024-03-05 15:28:59,625 [trainer.py] => memory_size: 0
2024-03-05 15:28:59,625 [trainer.py] => shuffle: True
2024-03-05 15:28:59,625 [trainer.py] => init_cls: 50
2024-03-05 15:28:59,625 [trainer.py] => increment: 10
2024-03-05 15:28:59,625 [trainer.py] => model_name: fecam
2024-03-05 15:28:59,626 [trainer.py] => convnet_type: resnet18
2024-03-05 15:28:59,626 [trainer.py] => device: [device(type='cuda', index=0)]
2024-03-05 15:28:59,626 [trainer.py] => seed: 1993
2024-03-05 15:28:59,626 [trainer.py] => init_epochs: 200
2024-03-05 15:28:59,626 [trainer.py] => init_lr: 0.1
2024-03-05 15:28:59,626 [trainer.py] => init_weight_decay: 0.0005
2024-03-05 15:28:59,626 [trainer.py] => batch_size: 128
2024-03-05 15:28:59,626 [trainer.py] => num_workers: 8
2024-03-05 15:28:59,626 [trainer.py] => T: 5
2024-03-05 15:28:59,626 [trainer.py] => beta: 0.5
2024-03-05 15:28:59,626 [trainer.py] => alpha1: 1
2024-03-05 15:28:59,626 [trainer.py] => alpha2: 1
2024-03-05 15:28:59,626 [trainer.py] => ncm: False
2024-03-05 15:28:59,626 [trainer.py] => tukey: False
2024-03-05 15:28:59,626 [trainer.py] => diagonal: False
2024-03-05 15:28:59,626 [trainer.py] => per_class: True
2024-03-05 15:28:59,626 [trainer.py] => full_cov: True
2024-03-05 15:28:59,626 [trainer.py] => shrink: True
2024-03-05 15:28:59,626 [trainer.py] => norm_cov: False
2024-03-05 15:28:59,626 [trainer.py] => vecnorm: False
2024-03-05 15:28:59,626 [trainer.py] => ae_type: wae
2024-03-05 15:28:59,626 [trainer.py] => epochs: 1000
2024-03-05 15:28:59,626 [trainer.py] => ae_latent_dim: 32
2024-03-05 15:28:59,626 [trainer.py] => wae_sigma: 50
2024-03-05 15:28:59,626 [trainer.py] => wae_C: 1
Files already downloaded and verified
Files already downloaded and verified
2024-03-05 15:29:01,264 [data_manager.py] => [68, 56, 78, 8, 23, 84, 90, 65, 74, 76, 40, 89, 3, 92, 55, 9, 26, 80, 43, 38, 58, 70, 77, 1, 85, 19, 17, 50, 28, 53, 13, 81, 45, 82, 6, 59, 83, 16, 15, 44, 91, 41, 72, 60, 79, 52, 20, 10, 31, 54, 37, 95, 14, 71, 96, 98, 97, 2, 64, 66, 42, 22, 35, 86, 24, 34, 87, 21, 99, 0, 88, 27, 18, 94, 11, 12, 47, 25, 30, 46, 62, 69, 36, 61, 7, 63, 75, 5, 32, 4, 51, 48, 73, 93, 39, 67, 29, 49, 57, 33]
2024-03-05 15:29:01,548 [fecam.py] => Learning on 0-50
/home/z1165703/miniconda3/lib/python3.11/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
/home/z1165703/miniconda3/lib/python3.11/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
Processing class: 0
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.2526322573
Epoch:   200  |  train loss: 0.2615819693
Epoch:   300  |  train loss: 0.2680664390
Epoch:   400  |  train loss: 0.2719308436
Epoch:   500  |  train loss: 0.2699749589
Epoch:   600  |  train loss: 0.2698326737
Epoch:   700  |  train loss: 0.2731603980
Epoch:   800  |  train loss: 0.2743860215
Epoch:   900  |  train loss: 0.2721464097
Epoch:  1000  |  train loss: 0.2723316967
Processing class: 1
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.2432142317
Epoch:   200  |  train loss: 0.2411041558
Epoch:   300  |  train loss: 0.2570408642
Epoch:   400  |  train loss: 0.2608278960
Epoch:   500  |  train loss: 0.2560939789
Epoch:   600  |  train loss: 0.2657805502
Epoch:   700  |  train loss: 0.2628779948
Epoch:   800  |  train loss: 0.2688774049
Epoch:   900  |  train loss: 0.2768258691
Epoch:  1000  |  train loss: 0.2709691048
Processing class: 2
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.2378035933
Epoch:   200  |  train loss: 0.2393730044
Epoch:   300  |  train loss: 0.2509135187
Epoch:   400  |  train loss: 0.2520059556
Epoch:   500  |  train loss: 0.2417817146
Epoch:   600  |  train loss: 0.2539540976
Epoch:   700  |  train loss: 0.2543205678
Epoch:   800  |  train loss: 0.2638440400
Epoch:   900  |  train loss: 0.2610824287
Epoch:  1000  |  train loss: 0.2596823573
Processing class: 3
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.2691409290
Epoch:   200  |  train loss: 0.2779098272
Epoch:   300  |  train loss: 0.2835591316
Epoch:   400  |  train loss: 0.2879973590
Epoch:   500  |  train loss: 0.2861838281
Epoch:   600  |  train loss: 0.2962844253
Epoch:   700  |  train loss: 0.3037895620
Epoch:   800  |  train loss: 0.2989076555
Epoch:   900  |  train loss: 0.3051219404
Epoch:  1000  |  train loss: 0.3083916068
Processing class: 4
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.2403211325
Epoch:   200  |  train loss: 0.2376855940
Epoch:   300  |  train loss: 0.2515514791
Epoch:   400  |  train loss: 0.2547493607
Epoch:   500  |  train loss: 0.2608337283
Epoch:   600  |  train loss: 0.2616663456
Epoch:   700  |  train loss: 0.2667971492
Epoch:   800  |  train loss: 0.2688728392
Epoch:   900  |  train loss: 0.2721597373
Epoch:  1000  |  train loss: 0.2797846198
Processing class: 5
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.2383673728
Epoch:   200  |  train loss: 0.2397739559
Epoch:   300  |  train loss: 0.2459294617
Epoch:   400  |  train loss: 0.2486659795
Epoch:   500  |  train loss: 0.2487769425
Epoch:   600  |  train loss: 0.2618246794
Epoch:   700  |  train loss: 0.2659294307
Epoch:   800  |  train loss: 0.2622647047
Epoch:   900  |  train loss: 0.2627646983
Epoch:  1000  |  train loss: 0.2691540837
Processing class: 6
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.2447710812
Epoch:   200  |  train loss: 0.2491283327
Epoch:   300  |  train loss: 0.2631630510
Epoch:   400  |  train loss: 0.2640268266
Epoch:   500  |  train loss: 0.2748235285
Epoch:   600  |  train loss: 0.2721943259
Epoch:   700  |  train loss: 0.2701389015
Epoch:   800  |  train loss: 0.2736402333
Epoch:   900  |  train loss: 0.2722531915
Epoch:  1000  |  train loss: 0.2765054882
Processing class: 7
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.2334492087
Epoch:   200  |  train loss: 0.2381567866
Epoch:   300  |  train loss: 0.2429917276
Epoch:   400  |  train loss: 0.2479485869
Epoch:   500  |  train loss: 0.2551648349
Epoch:   600  |  train loss: 0.2531827629
Epoch:   700  |  train loss: 0.2591061115
Epoch:   800  |  train loss: 0.2682861328
Epoch:   900  |  train loss: 0.2629754364
Epoch:  1000  |  train loss: 0.2709118187
Processing class: 8
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.2384743601
Epoch:   200  |  train loss: 0.2458126366
Epoch:   300  |  train loss: 0.2465964139
Epoch:   400  |  train loss: 0.2587046027
Epoch:   500  |  train loss: 0.2552929372
Epoch:   600  |  train loss: 0.2602251828
Epoch:   700  |  train loss: 0.2683989644
Epoch:   800  |  train loss: 0.2637619436
Epoch:   900  |  train loss: 0.2688046217
Epoch:  1000  |  train loss: 0.2663064361
Processing class: 9
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.2344031245
Epoch:   200  |  train loss: 0.2403595030
Epoch:   300  |  train loss: 0.2492057711
Epoch:   400  |  train loss: 0.2434309810
Epoch:   500  |  train loss: 0.2523267835
Epoch:   600  |  train loss: 0.2482541978
Epoch:   700  |  train loss: 0.2515063226
Epoch:   800  |  train loss: 0.2543201953
Epoch:   900  |  train loss: 0.2551150143
Epoch:  1000  |  train loss: 0.2511900842
Processing class: 10
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.2254120648
Epoch:   200  |  train loss: 0.2276753932
Epoch:   300  |  train loss: 0.2322864443
Epoch:   400  |  train loss: 0.2424420744
Epoch:   500  |  train loss: 0.2337443739
Epoch:   600  |  train loss: 0.2395551145
Epoch:   700  |  train loss: 0.2440401405
Epoch:   800  |  train loss: 0.2431687504
Epoch:   900  |  train loss: 0.2463768244
Epoch:  1000  |  train loss: 0.2465420038
Processing class: 11
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.2389830232
Epoch:   200  |  train loss: 0.2458878011
Epoch:   300  |  train loss: 0.2440140396
Epoch:   400  |  train loss: 0.2499182880
Epoch:   500  |  train loss: 0.2529332876
Epoch:   600  |  train loss: 0.2603571236
Epoch:   700  |  train loss: 0.2701842844
Epoch:   800  |  train loss: 0.2691813052
Epoch:   900  |  train loss: 0.2710448384
Epoch:  1000  |  train loss: 0.2767787635
Processing class: 12
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.2290718764
Epoch:   200  |  train loss: 0.2435738117
Epoch:   300  |  train loss: 0.2470668614
Epoch:   400  |  train loss: 0.2571722656
Epoch:   500  |  train loss: 0.2595168591
Epoch:   600  |  train loss: 0.2574731588
Epoch:   700  |  train loss: 0.2616753697
Epoch:   800  |  train loss: 0.2647174150
Epoch:   900  |  train loss: 0.2699429691
Epoch:  1000  |  train loss: 0.2677995563
Processing class: 13
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.2369414717
Epoch:   200  |  train loss: 0.2503449142
Epoch:   300  |  train loss: 0.2608501852
Epoch:   400  |  train loss: 0.2658334434
Epoch:   500  |  train loss: 0.2752493262
Epoch:   600  |  train loss: 0.2704400301
Epoch:   700  |  train loss: 0.2762003303
Epoch:   800  |  train loss: 0.2807767272
Epoch:   900  |  train loss: 0.2837483346
Epoch:  1000  |  train loss: 0.2875954568
Processing class: 14
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.2352114320
Epoch:   200  |  train loss: 0.2392405897
Epoch:   300  |  train loss: 0.2437483042
Epoch:   400  |  train loss: 0.2531065404
Epoch:   500  |  train loss: 0.2534456104
Epoch:   600  |  train loss: 0.2550291508
Epoch:   700  |  train loss: 0.2592481256
Epoch:   800  |  train loss: 0.2693100095
Epoch:   900  |  train loss: 0.2667561591
Epoch:  1000  |  train loss: 0.2611865759
Processing class: 15
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.2410924941
Epoch:   200  |  train loss: 0.2471011013
Epoch:   300  |  train loss: 0.2604059219
Epoch:   400  |  train loss: 0.2666011155
Epoch:   500  |  train loss: 0.2722625196
Epoch:   600  |  train loss: 0.2691544175
Epoch:   700  |  train loss: 0.2777140915
Epoch:   800  |  train loss: 0.2713144779
Epoch:   900  |  train loss: 0.2790482700
Epoch:  1000  |  train loss: 0.2760912716
Processing class: 16
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.2430072486
Epoch:   200  |  train loss: 0.2645424128
Epoch:   300  |  train loss: 0.2741533160
Epoch:   400  |  train loss: 0.2714188337
Epoch:   500  |  train loss: 0.2789003909
Epoch:   600  |  train loss: 0.2737650990
Epoch:   700  |  train loss: 0.2732076883
Epoch:   800  |  train loss: 0.2837909997
Epoch:   900  |  train loss: 0.2760617018
Epoch:  1000  |  train loss: 0.2846963942
Processing class: 17
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.2293255299
Epoch:   200  |  train loss: 0.2342986017
Epoch:   300  |  train loss: 0.2424396217
Epoch:   400  |  train loss: 0.2452320218
Epoch:   500  |  train loss: 0.2401092410
Epoch:   600  |  train loss: 0.2470479101
Epoch:   700  |  train loss: 0.2466280252
Epoch:   800  |  train loss: 0.2483604997
Epoch:   900  |  train loss: 0.2516055644
Epoch:  1000  |  train loss: 0.2509345323
Processing class: 18
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.2371292591
Epoch:   200  |  train loss: 0.2397034049
Epoch:   300  |  train loss: 0.2513205975
Epoch:   400  |  train loss: 0.2523320258
Epoch:   500  |  train loss: 0.2636985242
Epoch:   600  |  train loss: 0.2630635947
Epoch:   700  |  train loss: 0.2764082313
Epoch:   800  |  train loss: 0.2775167048
Epoch:   900  |  train loss: 0.2812685311
Epoch:  1000  |  train loss: 0.2893870115
Processing class: 19
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.2437528521
Epoch:   200  |  train loss: 0.2608486712
Epoch:   300  |  train loss: 0.2629207909
Epoch:   400  |  train loss: 0.2659324259
Epoch:   500  |  train loss: 0.2633606017
Epoch:   600  |  train loss: 0.2651638925
Epoch:   700  |  train loss: 0.2698762178
Epoch:   800  |  train loss: 0.2688391805
Epoch:   900  |  train loss: 0.2837894678
Epoch:  1000  |  train loss: 0.2873456717
Processing class: 20
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.2416460931
Epoch:   200  |  train loss: 0.2491331071
Epoch:   300  |  train loss: 0.2550337970
Epoch:   400  |  train loss: 0.2615415335
Epoch:   500  |  train loss: 0.2692182899
Epoch:   600  |  train loss: 0.2660732746
Epoch:   700  |  train loss: 0.2689283848
Epoch:   800  |  train loss: 0.2693979919
Epoch:   900  |  train loss: 0.2734822512
Epoch:  1000  |  train loss: 0.2791272759
Processing class: 21
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.2421457589
Epoch:   200  |  train loss: 0.2502787828
Epoch:   300  |  train loss: 0.2592306197
Epoch:   400  |  train loss: 0.2647966176
Epoch:   500  |  train loss: 0.2695747375
Epoch:   600  |  train loss: 0.2754122615
Epoch:   700  |  train loss: 0.2746949822
Epoch:   800  |  train loss: 0.2743693650
Epoch:   900  |  train loss: 0.2819702923
Epoch:  1000  |  train loss: 0.2818409801
Processing class: 22
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.2348610163
Epoch:   200  |  train loss: 0.2383236706
Epoch:   300  |  train loss: 0.2450105280
Epoch:   400  |  train loss: 0.2552593321
Epoch:   500  |  train loss: 0.2595356196
Epoch:   600  |  train loss: 0.2574184537
Epoch:   700  |  train loss: 0.2623502672
Epoch:   800  |  train loss: 0.2618845105
Epoch:   900  |  train loss: 0.2595682591
Epoch:  1000  |  train loss: 0.2702282786
Processing class: 23
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.2492057681
Epoch:   200  |  train loss: 0.2563103795
Epoch:   300  |  train loss: 0.2639613211
Epoch:   400  |  train loss: 0.2588852346
Epoch:   500  |  train loss: 0.2603264719
Epoch:   600  |  train loss: 0.2586382121
Epoch:   700  |  train loss: 0.2648745120
Epoch:   800  |  train loss: 0.2626133800
Epoch:   900  |  train loss: 0.2681800663
Epoch:  1000  |  train loss: 0.2656730473
Processing class: 24
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.2427194744
Epoch:   200  |  train loss: 0.2436211526
Epoch:   300  |  train loss: 0.2374601483
Epoch:   400  |  train loss: 0.2461717039
Epoch:   500  |  train loss: 0.2444953918
Epoch:   600  |  train loss: 0.2496654898
Epoch:   700  |  train loss: 0.2422597617
Epoch:   800  |  train loss: 0.2460241944
Epoch:   900  |  train loss: 0.2521797419
Epoch:  1000  |  train loss: 0.2515270680
Processing class: 25
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.2370663136
Epoch:   200  |  train loss: 0.2440220863
Epoch:   300  |  train loss: 0.2532196790
Epoch:   400  |  train loss: 0.2512750417
Epoch:   500  |  train loss: 0.2624018431
Epoch:   600  |  train loss: 0.2595356464
Epoch:   700  |  train loss: 0.2681739986
Epoch:   800  |  train loss: 0.2662597001
Epoch:   900  |  train loss: 0.2657233655
Epoch:  1000  |  train loss: 0.2688056171
Processing class: 26
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.2533717036
Epoch:   200  |  train loss: 0.2627086997
Epoch:   300  |  train loss: 0.2566614687
Epoch:   400  |  train loss: 0.2650584638
Epoch:   500  |  train loss: 0.2735326409
Epoch:   600  |  train loss: 0.2664936960
Epoch:   700  |  train loss: 0.2713809252
Epoch:   800  |  train loss: 0.2749523044
Epoch:   900  |  train loss: 0.2748371422
Epoch:  1000  |  train loss: 0.2813577473
Processing class: 27
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.2360473990
Epoch:   200  |  train loss: 0.2368320882
Epoch:   300  |  train loss: 0.2403967768
Epoch:   400  |  train loss: 0.2450411379
Epoch:   500  |  train loss: 0.2517789811
Epoch:   600  |  train loss: 0.2526038468
Epoch:   700  |  train loss: 0.2564606398
Epoch:   800  |  train loss: 0.2570147574
Epoch:   900  |  train loss: 0.2605158418
Epoch:  1000  |  train loss: 0.2556454211
Processing class: 28
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.2467814595
Epoch:   200  |  train loss: 0.2546717674
Epoch:   300  |  train loss: 0.2658302128
Epoch:   400  |  train loss: 0.2719242752
Epoch:   500  |  train loss: 0.2754967153
Epoch:   600  |  train loss: 0.2772236407
Epoch:   700  |  train loss: 0.2791171551
Epoch:   800  |  train loss: 0.2820881307
Epoch:   900  |  train loss: 0.2840232253
Epoch:  1000  |  train loss: 0.2787001610
Processing class: 29
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.2391269863
Epoch:   200  |  train loss: 0.2478680253
Epoch:   300  |  train loss: 0.2499749005
Epoch:   400  |  train loss: 0.2497517288
Epoch:   500  |  train loss: 0.2570175022
Epoch:   600  |  train loss: 0.2615993708
Epoch:   700  |  train loss: 0.2632001042
Epoch:   800  |  train loss: 0.2606017500
Epoch:   900  |  train loss: 0.2629149139
Epoch:  1000  |  train loss: 0.2623449683
Processing class: 30
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.2524176478
Epoch:   200  |  train loss: 0.2613398582
Epoch:   300  |  train loss: 0.2762766600
Epoch:   400  |  train loss: 0.2874192536
Epoch:   500  |  train loss: 0.2830621421
Epoch:   600  |  train loss: 0.2844716311
Epoch:   700  |  train loss: 0.2845832050
Epoch:   800  |  train loss: 0.2918863773
Epoch:   900  |  train loss: 0.2967073262
Epoch:  1000  |  train loss: 0.2930070579
Processing class: 31
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.2472564518
Epoch:   200  |  train loss: 0.2586706817
Epoch:   300  |  train loss: 0.2627036840
Epoch:   400  |  train loss: 0.2720732570
Epoch:   500  |  train loss: 0.2762420714
Epoch:   600  |  train loss: 0.2847592235
Epoch:   700  |  train loss: 0.2839949012
Epoch:   800  |  train loss: 0.2848765731
Epoch:   900  |  train loss: 0.2912621796
Epoch:  1000  |  train loss: 0.2862239480
Processing class: 32
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.2364915609
Epoch:   200  |  train loss: 0.2444783121
Epoch:   300  |  train loss: 0.2452687144
Epoch:   400  |  train loss: 0.2415705949
Epoch:   500  |  train loss: 0.2550221205
Epoch:   600  |  train loss: 0.2547186375
Epoch:   700  |  train loss: 0.2650123954
Epoch:   800  |  train loss: 0.2698601842
Epoch:   900  |  train loss: 0.2688068390
Epoch:  1000  |  train loss: 0.2675682008
Processing class: 33
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.2284073234
Epoch:   200  |  train loss: 0.2442849010
Epoch:   300  |  train loss: 0.2365423501
Epoch:   400  |  train loss: 0.2408847153
Epoch:   500  |  train loss: 0.2441814333
Epoch:   600  |  train loss: 0.2549032390
Epoch:   700  |  train loss: 0.2513037413
Epoch:   800  |  train loss: 0.2545862883
Epoch:   900  |  train loss: 0.2548990011
Epoch:  1000  |  train loss: 0.2532164574
Processing class: 34
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.2403074712
Epoch:   200  |  train loss: 0.2398571104
Epoch:   300  |  train loss: 0.2482797801
Epoch:   400  |  train loss: 0.2469160408
Epoch:   500  |  train loss: 0.2486301273
Epoch:   600  |  train loss: 0.2555237353
Epoch:   700  |  train loss: 0.2525792181
Epoch:   800  |  train loss: 0.2605827749
Epoch:   900  |  train loss: 0.2637502015
Epoch:  1000  |  train loss: 0.2657933444
Processing class: 35
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.2290535152
Epoch:   200  |  train loss: 0.2398730457
Epoch:   300  |  train loss: 0.2470102876
Epoch:   400  |  train loss: 0.2510278761
Epoch:   500  |  train loss: 0.2483265430
Epoch:   600  |  train loss: 0.2522795767
Epoch:   700  |  train loss: 0.2561783373
Epoch:   800  |  train loss: 0.2563219368
Epoch:   900  |  train loss: 0.2578834802
Epoch:  1000  |  train loss: 0.2560841352
Processing class: 36
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.2491665930
Epoch:   200  |  train loss: 0.2537840247
Epoch:   300  |  train loss: 0.2612225533
Epoch:   400  |  train loss: 0.2626129150
Epoch:   500  |  train loss: 0.2730863810
Epoch:   600  |  train loss: 0.2714064896
Epoch:   700  |  train loss: 0.2688986480
Epoch:   800  |  train loss: 0.2801048934
Epoch:   900  |  train loss: 0.2736561775
Epoch:  1000  |  train loss: 0.2806488812
Processing class: 37
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.2411057293
Epoch:   200  |  train loss: 0.2413344145
Epoch:   300  |  train loss: 0.2430374205
Epoch:   400  |  train loss: 0.2488530964
Epoch:   500  |  train loss: 0.2540108085
Epoch:   600  |  train loss: 0.2511867404
Epoch:   700  |  train loss: 0.2535406619
Epoch:   800  |  train loss: 0.2564047575
Epoch:   900  |  train loss: 0.2541916192
Epoch:  1000  |  train loss: 0.2571793258
Processing class: 38
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.2428810090
Epoch:   200  |  train loss: 0.2468064129
Epoch:   300  |  train loss: 0.2554384679
Epoch:   400  |  train loss: 0.2578249991
Epoch:   500  |  train loss: 0.2626763999
Epoch:   600  |  train loss: 0.2628644288
Epoch:   700  |  train loss: 0.2656266272
Epoch:   800  |  train loss: 0.2722842515
Epoch:   900  |  train loss: 0.2662392378
Epoch:  1000  |  train loss: 0.2720601380
Processing class: 39
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.2429656446
Epoch:   200  |  train loss: 0.2423373878
Epoch:   300  |  train loss: 0.2489254355
Epoch:   400  |  train loss: 0.2544012606
Epoch:   500  |  train loss: 0.2540707499
Epoch:   600  |  train loss: 0.2573610991
Epoch:   700  |  train loss: 0.2618474901
Epoch:   800  |  train loss: 0.2673023641
Epoch:   900  |  train loss: 0.2712761402
Epoch:  1000  |  train loss: 0.2741568685
Processing class: 40
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.2450471878
Epoch:   200  |  train loss: 0.2444942027
Epoch:   300  |  train loss: 0.2579340488
Epoch:   400  |  train loss: 0.2615014255
Epoch:   500  |  train loss: 0.2663181424
Epoch:   600  |  train loss: 0.2657530665
Epoch:   700  |  train loss: 0.2693159461
Epoch:   800  |  train loss: 0.2701037467
Epoch:   900  |  train loss: 0.2677007407
Epoch:  1000  |  train loss: 0.2795137107
Processing class: 41
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.2453938872
Epoch:   200  |  train loss: 0.2463051915
Epoch:   300  |  train loss: 0.2558135539
Epoch:   400  |  train loss: 0.2568300217
Epoch:   500  |  train loss: 0.2589778900
Epoch:   600  |  train loss: 0.2604200959
Epoch:   700  |  train loss: 0.2664794207
Epoch:   800  |  train loss: 0.2642405868
Epoch:   900  |  train loss: 0.2640119016
Epoch:  1000  |  train loss: 0.2652946532
Processing class: 42
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.2337664783
Epoch:   200  |  train loss: 0.2441592634
Epoch:   300  |  train loss: 0.2521247625
Epoch:   400  |  train loss: 0.2570181668
Epoch:   500  |  train loss: 0.2620805264
Epoch:   600  |  train loss: 0.2660642207
Epoch:   700  |  train loss: 0.2711997986
Epoch:   800  |  train loss: 0.2757728755
Epoch:   900  |  train loss: 0.2741271615
Epoch:  1000  |  train loss: 0.2747636914
Processing class: 43
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.2353858173
Epoch:   200  |  train loss: 0.2372023374
Epoch:   300  |  train loss: 0.2548124760
Epoch:   400  |  train loss: 0.2672628164
Epoch:   500  |  train loss: 0.2711988151
Epoch:   600  |  train loss: 0.2686491907
Epoch:   700  |  train loss: 0.2740410745
Epoch:   800  |  train loss: 0.2723353982
Epoch:   900  |  train loss: 0.2670055568
Epoch:  1000  |  train loss: 0.2758956194
Processing class: 44
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.2319717407
Epoch:   200  |  train loss: 0.2443079442
Epoch:   300  |  train loss: 0.2424045235
Epoch:   400  |  train loss: 0.2468776971
Epoch:   500  |  train loss: 0.2517122507
Epoch:   600  |  train loss: 0.2500873446
Epoch:   700  |  train loss: 0.2569820046
Epoch:   800  |  train loss: 0.2562232733
Epoch:   900  |  train loss: 0.2548997074
Epoch:  1000  |  train loss: 0.2625219285
Processing class: 45
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.2447547048
Epoch:   200  |  train loss: 0.2586305439
Epoch:   300  |  train loss: 0.2622758985
Epoch:   400  |  train loss: 0.2687684894
Epoch:   500  |  train loss: 0.2681083620
Epoch:   600  |  train loss: 0.2788916171
Epoch:   700  |  train loss: 0.2823196054
Epoch:   800  |  train loss: 0.2804982245
Epoch:   900  |  train loss: 0.2817497432
Epoch:  1000  |  train loss: 0.2875992000
Processing class: 46
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.2506753474
Epoch:   200  |  train loss: 0.2540148556
Epoch:   300  |  train loss: 0.2618869603
Epoch:   400  |  train loss: 0.2651324153
Epoch:   500  |  train loss: 0.2667854130
Epoch:   600  |  train loss: 0.2671290815
Epoch:   700  |  train loss: 0.2689540744
Epoch:   800  |  train loss: 0.2777473152
Epoch:   900  |  train loss: 0.2743855596
Epoch:  1000  |  train loss: 0.2781775534
Processing class: 47
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.2325220942
Epoch:   200  |  train loss: 0.2416439176
Epoch:   300  |  train loss: 0.2424804062
Epoch:   400  |  train loss: 0.2517201453
Epoch:   500  |  train loss: 0.2535195172
Epoch:   600  |  train loss: 0.2554715157
Epoch:   700  |  train loss: 0.2598229587
Epoch:   800  |  train loss: 0.2574412644
Epoch:   900  |  train loss: 0.2609185755
Epoch:  1000  |  train loss: 0.2600348920
Processing class: 48
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.2380239308
Epoch:   200  |  train loss: 0.2479654312
Epoch:   300  |  train loss: 0.2511687815
Epoch:   400  |  train loss: 0.2471540242
Epoch:   500  |  train loss: 0.2562423706
Epoch:   600  |  train loss: 0.2560174137
Epoch:   700  |  train loss: 0.2534556419
Epoch:   800  |  train loss: 0.2536254406
Epoch:   900  |  train loss: 0.2629930794
Epoch:  1000  |  train loss: 0.2665957749
Processing class: 49
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.2356911331
Epoch:   200  |  train loss: 0.2399349242
Epoch:   300  |  train loss: 0.2545195758
Epoch:   400  |  train loss: 0.2582934141
Epoch:   500  |  train loss: 0.2634547949
Epoch:   600  |  train loss: 0.2646875024
Epoch:   700  |  train loss: 0.2660980046
Epoch:   800  |  train loss: 0.2654268622
Epoch:   900  |  train loss: 0.2651199818
Epoch:  1000  |  train loss: 0.2672074735
/home/z1165703/FeCAM/models/base.py:87: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  vec = torch.tensor(vec).cuda()
2024-03-05 15:46:25,258 [trainer.py] => CNN: {'total': 83.44, '00-09': 87.7, '10-19': 79.6, '20-29': 84.9, '30-39': 81.0, '40-49': 84.0, 'old': 0, 'new': 83.44}
2024-03-05 15:46:25,260 [trainer.py] => No NME accuracy
2024-03-05 15:46:25,260 [trainer.py] => FeCAM: {'total': 82.9, '00-09': 86.2, '10-19': 79.7, '20-29': 85.0, '30-39': 80.6, '40-49': 83.0, 'old': 0, 'new': 82.9}
2024-03-05 15:46:25,260 [trainer.py] => CNN top1 curve: [83.44]
2024-03-05 15:46:25,260 [trainer.py] => CNN top5 curve: [96.5]
2024-03-05 15:46:25,260 [trainer.py] => FeCAM top1 curve: [82.9]
2024-03-05 15:46:25,260 [trainer.py] => FeCAM top5 curve: [95.84]

2024-03-05 15:46:25,272 [fecam.py] => Learning on 50-60
Processing class: 50
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.2253339887
Epoch:   200  |  train loss: 0.2303301096
Epoch:   300  |  train loss: 0.2341814488
Epoch:   400  |  train loss: 0.2421278924
Epoch:   500  |  train loss: 0.2487765282
Epoch:   600  |  train loss: 0.2470499486
Epoch:   700  |  train loss: 0.2558846444
Epoch:   800  |  train loss: 0.2608259976
Epoch:   900  |  train loss: 0.2659151196
Epoch:  1000  |  train loss: 0.2689603627
Processing class: 51
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.2276402116
Epoch:   200  |  train loss: 0.2244789094
Epoch:   300  |  train loss: 0.2362883985
Epoch:   400  |  train loss: 0.2403639197
Epoch:   500  |  train loss: 0.2449137241
Epoch:   600  |  train loss: 0.2515082479
Epoch:   700  |  train loss: 0.2478596181
Epoch:   800  |  train loss: 0.2508226216
Epoch:   900  |  train loss: 0.2599299669
Epoch:  1000  |  train loss: 0.2589815199
Processing class: 52
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.2269859344
Epoch:   200  |  train loss: 0.2259585619
Epoch:   300  |  train loss: 0.2323508948
Epoch:   400  |  train loss: 0.2370204985
Epoch:   500  |  train loss: 0.2379074007
Epoch:   600  |  train loss: 0.2362017572
Epoch:   700  |  train loss: 0.2433736920
Epoch:   800  |  train loss: 0.2450318724
Epoch:   900  |  train loss: 0.2481602788
Epoch:  1000  |  train loss: 0.2449585468
Processing class: 53
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.2263286173
Epoch:   200  |  train loss: 0.2325340509
Epoch:   300  |  train loss: 0.2286596388
Epoch:   400  |  train loss: 0.2333888561
Epoch:   500  |  train loss: 0.2378766716
Epoch:   600  |  train loss: 0.2440868825
Epoch:   700  |  train loss: 0.2516419381
Epoch:   800  |  train loss: 0.2418841302
Epoch:   900  |  train loss: 0.2480712920
Epoch:  1000  |  train loss: 0.2456744105
Processing class: 54
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.2124567002
Epoch:   200  |  train loss: 0.2173961967
Epoch:   300  |  train loss: 0.2269295216
Epoch:   400  |  train loss: 0.2315679759
Epoch:   500  |  train loss: 0.2364373773
Epoch:   600  |  train loss: 0.2451904804
Epoch:   700  |  train loss: 0.2488123387
Epoch:   800  |  train loss: 0.2560664952
Epoch:   900  |  train loss: 0.2612911940
Epoch:  1000  |  train loss: 0.2612980783
Processing class: 55
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.2230003148
Epoch:   200  |  train loss: 0.2249935657
Epoch:   300  |  train loss: 0.2232552558
Epoch:   400  |  train loss: 0.2313540041
Epoch:   500  |  train loss: 0.2418969482
Epoch:   600  |  train loss: 0.2443746954
Epoch:   700  |  train loss: 0.2489299864
Epoch:   800  |  train loss: 0.2569821119
Epoch:   900  |  train loss: 0.2508931071
Epoch:  1000  |  train loss: 0.2570793867
Processing class: 56
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.2230590224
Epoch:   200  |  train loss: 0.2300581992
Epoch:   300  |  train loss: 0.2300679415
Epoch:   400  |  train loss: 0.2410728663
Epoch:   500  |  train loss: 0.2479016870
Epoch:   600  |  train loss: 0.2535220116
Epoch:   700  |  train loss: 0.2596937299
Epoch:   800  |  train loss: 0.2580457509
Epoch:   900  |  train loss: 0.2657809734
Epoch:  1000  |  train loss: 0.2708284020
Processing class: 57
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.2201222330
Epoch:   200  |  train loss: 0.2264505237
Epoch:   300  |  train loss: 0.2255735248
Epoch:   400  |  train loss: 0.2272379875
Epoch:   500  |  train loss: 0.2332360446
Epoch:   600  |  train loss: 0.2288896829
Epoch:   700  |  train loss: 0.2384774685
Epoch:   800  |  train loss: 0.2400287151
Epoch:   900  |  train loss: 0.2457714319
Epoch:  1000  |  train loss: 0.2405432135
Processing class: 58
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.2225882202
Epoch:   200  |  train loss: 0.2265238285
Epoch:   300  |  train loss: 0.2369258732
Epoch:   400  |  train loss: 0.2490428597
Epoch:   500  |  train loss: 0.2495700657
Epoch:   600  |  train loss: 0.2566673577
Epoch:   700  |  train loss: 0.2576492906
Epoch:   800  |  train loss: 0.2633974612
Epoch:   900  |  train loss: 0.2646087885
Epoch:  1000  |  train loss: 0.2668194234
Processing class: 59
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.2282518923
Epoch:   200  |  train loss: 0.2384739250
Epoch:   300  |  train loss: 0.2391847014
Epoch:   400  |  train loss: 0.2465853721
Epoch:   500  |  train loss: 0.2531915545
Epoch:   600  |  train loss: 0.2555432767
Epoch:   700  |  train loss: 0.2603423655
Epoch:   800  |  train loss: 0.2632591903
Epoch:   900  |  train loss: 0.2638317227
Epoch:  1000  |  train loss: 0.2623613179
2024-03-05 15:52:07,922 [trainer.py] => CNN: {'total': 71.75, '00-09': 82.2, '10-19': 72.8, '20-29': 78.5, '30-39': 74.9, '40-49': 68.4, '50-59': 53.7, 'old': 75.36, 'new': 53.7}
2024-03-05 15:52:07,922 [trainer.py] => No NME accuracy
2024-03-05 15:52:07,922 [trainer.py] => FeCAM: {'total': 72.78, '00-09': 82.6, '10-19': 74.0, '20-29': 81.0, '30-39': 76.4, '40-49': 76.8, '50-59': 45.9, 'old': 78.16, 'new': 45.9}
2024-03-05 15:52:07,922 [trainer.py] => CNN top1 curve: [83.44, 71.75]
2024-03-05 15:52:07,923 [trainer.py] => CNN top5 curve: [96.5, 89.6]
2024-03-05 15:52:07,923 [trainer.py] => FeCAM top1 curve: [82.9, 72.78]
2024-03-05 15:52:07,923 [trainer.py] => FeCAM top5 curve: [95.84, 91.18]

2024-03-05 15:52:07,928 [fecam.py] => Learning on 60-70
Processing class: 60
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.2321941376
Epoch:   200  |  train loss: 0.2328460664
Epoch:   300  |  train loss: 0.2318950564
Epoch:   400  |  train loss: 0.2332794756
Epoch:   500  |  train loss: 0.2442375779
Epoch:   600  |  train loss: 0.2467691720
Epoch:   700  |  train loss: 0.2670050323
Epoch:   800  |  train loss: 0.2602111757
Epoch:   900  |  train loss: 0.2635879815
Epoch:  1000  |  train loss: 0.2700302124
Processing class: 61
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.2230624437
Epoch:   200  |  train loss: 0.2222148120
Epoch:   300  |  train loss: 0.2346630037
Epoch:   400  |  train loss: 0.2399423689
Epoch:   500  |  train loss: 0.2519894600
Epoch:   600  |  train loss: 0.2483889520
Epoch:   700  |  train loss: 0.2595903814
Epoch:   800  |  train loss: 0.2666460633
Epoch:   900  |  train loss: 0.2607546568
Epoch:  1000  |  train loss: 0.2652355433
Processing class: 62
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.2274442106
Epoch:   200  |  train loss: 0.2250342727
Epoch:   300  |  train loss: 0.2314708114
Epoch:   400  |  train loss: 0.2348214269
Epoch:   500  |  train loss: 0.2326308370
Epoch:   600  |  train loss: 0.2419082314
Epoch:   700  |  train loss: 0.2452354819
Epoch:   800  |  train loss: 0.2486650705
Epoch:   900  |  train loss: 0.2481304497
Epoch:  1000  |  train loss: 0.2512051582
Processing class: 63
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.2185147434
Epoch:   200  |  train loss: 0.2219796032
Epoch:   300  |  train loss: 0.2372661054
Epoch:   400  |  train loss: 0.2419010997
Epoch:   500  |  train loss: 0.2493220419
Epoch:   600  |  train loss: 0.2583972156
Epoch:   700  |  train loss: 0.2516088396
Epoch:   800  |  train loss: 0.2663880825
Epoch:   900  |  train loss: 0.2595993459
Epoch:  1000  |  train loss: 0.2626878977
Processing class: 64
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.2344308376
Epoch:   200  |  train loss: 0.2364133328
Epoch:   300  |  train loss: 0.2539531767
Epoch:   400  |  train loss: 0.2577071488
Epoch:   500  |  train loss: 0.2632385850
Epoch:   600  |  train loss: 0.2654431939
Epoch:   700  |  train loss: 0.2617422700
Epoch:   800  |  train loss: 0.2665585041
Epoch:   900  |  train loss: 0.2746909320
Epoch:  1000  |  train loss: 0.2747812033
Processing class: 65
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.2271864653
Epoch:   200  |  train loss: 0.2319775909
Epoch:   300  |  train loss: 0.2509111404
Epoch:   400  |  train loss: 0.2598702878
Epoch:   500  |  train loss: 0.2610375613
Epoch:   600  |  train loss: 0.2751438498
Epoch:   700  |  train loss: 0.2775244057
Epoch:   800  |  train loss: 0.2899444163
Epoch:   900  |  train loss: 0.2855520964
Epoch:  1000  |  train loss: 0.2979756713
Processing class: 66
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.2310909986
Epoch:   200  |  train loss: 0.2387285769
Epoch:   300  |  train loss: 0.2478032976
Epoch:   400  |  train loss: 0.2510425895
Epoch:   500  |  train loss: 0.2590990007
Epoch:   600  |  train loss: 0.2617595077
Epoch:   700  |  train loss: 0.2705554247
Epoch:   800  |  train loss: 0.2724261343
Epoch:   900  |  train loss: 0.2739789486
Epoch:  1000  |  train loss: 0.2734509170
Processing class: 67
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.2201633483
Epoch:   200  |  train loss: 0.2250606269
Epoch:   300  |  train loss: 0.2273007214
Epoch:   400  |  train loss: 0.2309237570
Epoch:   500  |  train loss: 0.2332524478
Epoch:   600  |  train loss: 0.2341175497
Epoch:   700  |  train loss: 0.2368859679
Epoch:   800  |  train loss: 0.2426887900
Epoch:   900  |  train loss: 0.2429629058
Epoch:  1000  |  train loss: 0.2436901778
Processing class: 68
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.2208863378
Epoch:   200  |  train loss: 0.2226263493
Epoch:   300  |  train loss: 0.2262948632
Epoch:   400  |  train loss: 0.2314548045
Epoch:   500  |  train loss: 0.2338894606
Epoch:   600  |  train loss: 0.2349072009
Epoch:   700  |  train loss: 0.2394158155
Epoch:   800  |  train loss: 0.2383071244
Epoch:   900  |  train loss: 0.2423656583
Epoch:  1000  |  train loss: 0.2519348592
Processing class: 69
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.2264329642
Epoch:   200  |  train loss: 0.2363202065
Epoch:   300  |  train loss: 0.2537515521
Epoch:   400  |  train loss: 0.2626805902
Epoch:   500  |  train loss: 0.2658496380
Epoch:   600  |  train loss: 0.2590421796
Epoch:   700  |  train loss: 0.2679346442
Epoch:   800  |  train loss: 0.2684848785
Epoch:   900  |  train loss: 0.2791647673
Epoch:  1000  |  train loss: 0.2702624261
2024-03-05 15:58:40,977 [trainer.py] => CNN: {'total': 64.66, '00-09': 75.0, '10-19': 70.1, '20-29': 77.1, '30-39': 70.6, '40-49': 64.0, '50-59': 45.8, '60-69': 50.0, 'old': 67.1, 'new': 50.0}
2024-03-05 15:58:40,978 [trainer.py] => No NME accuracy
2024-03-05 15:58:40,978 [trainer.py] => FeCAM: {'total': 67.27, '00-09': 79.9, '10-19': 70.6, '20-29': 79.1, '30-39': 71.9, '40-49': 74.1, '50-59': 41.2, '60-69': 54.1, 'old': 69.47, 'new': 54.1}
2024-03-05 15:58:40,978 [trainer.py] => CNN top1 curve: [83.44, 71.75, 64.66]
2024-03-05 15:58:40,978 [trainer.py] => CNN top5 curve: [96.5, 89.6, 86.54]
2024-03-05 15:58:40,978 [trainer.py] => FeCAM top1 curve: [82.9, 72.78, 67.27]
2024-03-05 15:58:40,978 [trainer.py] => FeCAM top5 curve: [95.84, 91.18, 87.21]

2024-03-05 15:58:40,982 [fecam.py] => Learning on 70-80
Processing class: 70
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.2219473153
Epoch:   200  |  train loss: 0.2215014905
Epoch:   300  |  train loss: 0.2289699107
Epoch:   400  |  train loss: 0.2344269484
Epoch:   500  |  train loss: 0.2353058904
Epoch:   600  |  train loss: 0.2430067003
Epoch:   700  |  train loss: 0.2531532794
Epoch:   800  |  train loss: 0.2477664679
Epoch:   900  |  train loss: 0.2550451994
Epoch:  1000  |  train loss: 0.2542409092
Processing class: 71
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.2249233902
Epoch:   200  |  train loss: 0.2290434927
Epoch:   300  |  train loss: 0.2277176559
Epoch:   400  |  train loss: 0.2356350988
Epoch:   500  |  train loss: 0.2419470012
Epoch:   600  |  train loss: 0.2449650198
Epoch:   700  |  train loss: 0.2478150606
Epoch:   800  |  train loss: 0.2514423013
Epoch:   900  |  train loss: 0.2598672152
Epoch:  1000  |  train loss: 0.2602155626
Processing class: 72
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.2211125314
Epoch:   200  |  train loss: 0.2255288541
Epoch:   300  |  train loss: 0.2293046415
Epoch:   400  |  train loss: 0.2319126666
Epoch:   500  |  train loss: 0.2384281576
Epoch:   600  |  train loss: 0.2360055178
Epoch:   700  |  train loss: 0.2399315000
Epoch:   800  |  train loss: 0.2397183150
Epoch:   900  |  train loss: 0.2433402240
Epoch:  1000  |  train loss: 0.2506978929
Processing class: 73
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.2279037148
Epoch:   200  |  train loss: 0.2293925524
Epoch:   300  |  train loss: 0.2414347857
Epoch:   400  |  train loss: 0.2405494094
Epoch:   500  |  train loss: 0.2492479146
Epoch:   600  |  train loss: 0.2501171798
Epoch:   700  |  train loss: 0.2479085445
Epoch:   800  |  train loss: 0.2487901807
Epoch:   900  |  train loss: 0.2511969447
Epoch:  1000  |  train loss: 0.2561406672
Processing class: 74
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.2297551721
Epoch:   200  |  train loss: 0.2339573294
Epoch:   300  |  train loss: 0.2298877537
Epoch:   400  |  train loss: 0.2294791996
Epoch:   500  |  train loss: 0.2351389021
Epoch:   600  |  train loss: 0.2349129260
Epoch:   700  |  train loss: 0.2444919020
Epoch:   800  |  train loss: 0.2387724727
Epoch:   900  |  train loss: 0.2410801381
Epoch:  1000  |  train loss: 0.2495301217
Processing class: 75
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.2222414315
Epoch:   200  |  train loss: 0.2230814993
Epoch:   300  |  train loss: 0.2332498223
Epoch:   400  |  train loss: 0.2349666923
Epoch:   500  |  train loss: 0.2422447145
Epoch:   600  |  train loss: 0.2493100703
Epoch:   700  |  train loss: 0.2378948927
Epoch:   800  |  train loss: 0.2475036353
Epoch:   900  |  train loss: 0.2481948048
Epoch:  1000  |  train loss: 0.2557058096
Processing class: 76
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.2195641130
Epoch:   200  |  train loss: 0.2309708804
Epoch:   300  |  train loss: 0.2458439082
Epoch:   400  |  train loss: 0.2560432702
Epoch:   500  |  train loss: 0.2604939461
Epoch:   600  |  train loss: 0.2680338442
Epoch:   700  |  train loss: 0.2699503481
Epoch:   800  |  train loss: 0.2642818779
Epoch:   900  |  train loss: 0.2742715418
Epoch:  1000  |  train loss: 0.2790367424
Processing class: 77
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.2256715626
Epoch:   200  |  train loss: 0.2296687752
Epoch:   300  |  train loss: 0.2300314844
Epoch:   400  |  train loss: 0.2362178624
Epoch:   500  |  train loss: 0.2422591001
Epoch:   600  |  train loss: 0.2461248785
Epoch:   700  |  train loss: 0.2506739974
Epoch:   800  |  train loss: 0.2589269787
Epoch:   900  |  train loss: 0.2576760232
Epoch:  1000  |  train loss: 0.2588415921
Processing class: 78
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.2286427557
Epoch:   200  |  train loss: 0.2387904644
Epoch:   300  |  train loss: 0.2476637751
Epoch:   400  |  train loss: 0.2503871024
Epoch:   500  |  train loss: 0.2524778217
Epoch:   600  |  train loss: 0.2596679211
Epoch:   700  |  train loss: 0.2636636257
Epoch:   800  |  train loss: 0.2676945329
Epoch:   900  |  train loss: 0.2699974835
Epoch:  1000  |  train loss: 0.2693812430
Processing class: 79
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.2248749554
Epoch:   200  |  train loss: 0.2221673220
Epoch:   300  |  train loss: 0.2297321260
Epoch:   400  |  train loss: 0.2327878773
Epoch:   500  |  train loss: 0.2340370446
Epoch:   600  |  train loss: 0.2428560883
Epoch:   700  |  train loss: 0.2440772146
Epoch:   800  |  train loss: 0.2490452707
Epoch:   900  |  train loss: 0.2558891296
Epoch:  1000  |  train loss: 0.2531922340
2024-03-05 16:06:16,041 [trainer.py] => CNN: {'total': 59.18, '00-09': 73.6, '10-19': 68.4, '20-29': 76.9, '30-39': 69.1, '40-49': 60.6, '50-59': 37.2, '60-69': 45.7, '70-79': 41.9, 'old': 61.64, 'new': 41.9}
2024-03-05 16:06:16,043 [trainer.py] => No NME accuracy
2024-03-05 16:06:16,043 [trainer.py] => FeCAM: {'total': 61.72, '00-09': 78.1, '10-19': 69.3, '20-29': 78.0, '30-39': 69.5, '40-49': 72.1, '50-59': 35.7, '60-69': 49.1, '70-79': 42.0, 'old': 64.54, 'new': 42.0}
2024-03-05 16:06:16,043 [trainer.py] => CNN top1 curve: [83.44, 71.75, 64.66, 59.18]
2024-03-05 16:06:16,043 [trainer.py] => CNN top5 curve: [96.5, 89.6, 86.54, 84.09]
2024-03-05 16:06:16,043 [trainer.py] => FeCAM top1 curve: [82.9, 72.78, 67.27, 61.72]
2024-03-05 16:06:16,043 [trainer.py] => FeCAM top5 curve: [95.84, 91.18, 87.21, 84.09]

2024-03-05 16:06:16,054 [fecam.py] => Learning on 80-90
Processing class: 80
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.2251196921
Epoch:   200  |  train loss: 0.2480140626
Epoch:   300  |  train loss: 0.2581490189
Epoch:   400  |  train loss: 0.2641189516
Epoch:   500  |  train loss: 0.2665687740
Epoch:   600  |  train loss: 0.2684350908
Epoch:   700  |  train loss: 0.2646302104
Epoch:   800  |  train loss: 0.2755943596
Epoch:   900  |  train loss: 0.2745258808
Epoch:  1000  |  train loss: 0.2813584566
Processing class: 81
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.2233077586
Epoch:   200  |  train loss: 0.2361200333
Epoch:   300  |  train loss: 0.2349698126
Epoch:   400  |  train loss: 0.2470061064
Epoch:   500  |  train loss: 0.2476126194
Epoch:   600  |  train loss: 0.2557894319
Epoch:   700  |  train loss: 0.2573660970
Epoch:   800  |  train loss: 0.2627004504
Epoch:   900  |  train loss: 0.2657153547
Epoch:  1000  |  train loss: 0.2700402856
Processing class: 82
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.2225803524
Epoch:   200  |  train loss: 0.2325417489
Epoch:   300  |  train loss: 0.2373363316
Epoch:   400  |  train loss: 0.2400312215
Epoch:   500  |  train loss: 0.2458364248
Epoch:   600  |  train loss: 0.2463623822
Epoch:   700  |  train loss: 0.2489571869
Epoch:   800  |  train loss: 0.2513816297
Epoch:   900  |  train loss: 0.2545946032
Epoch:  1000  |  train loss: 0.2563921809
Processing class: 83
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.2281718433
Epoch:   200  |  train loss: 0.2455744237
Epoch:   300  |  train loss: 0.2511120677
Epoch:   400  |  train loss: 0.2591502726
Epoch:   500  |  train loss: 0.2704613268
Epoch:   600  |  train loss: 0.2684761047
Epoch:   700  |  train loss: 0.2781556666
Epoch:   800  |  train loss: 0.2884053111
Epoch:   900  |  train loss: 0.2864211023
Epoch:  1000  |  train loss: 0.2860574782
Processing class: 84
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.2283075899
Epoch:   200  |  train loss: 0.2331829727
Epoch:   300  |  train loss: 0.2438359618
Epoch:   400  |  train loss: 0.2481849879
Epoch:   500  |  train loss: 0.2566444010
Epoch:   600  |  train loss: 0.2534165949
Epoch:   700  |  train loss: 0.2613687694
Epoch:   800  |  train loss: 0.2681351364
Epoch:   900  |  train loss: 0.2632787645
Epoch:  1000  |  train loss: 0.2633258879
Processing class: 85
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.2247290254
Epoch:   200  |  train loss: 0.2284315735
Epoch:   300  |  train loss: 0.2389205456
Epoch:   400  |  train loss: 0.2399638772
Epoch:   500  |  train loss: 0.2470251590
Epoch:   600  |  train loss: 0.2547347993
Epoch:   700  |  train loss: 0.2639649659
Epoch:   800  |  train loss: 0.2595667064
Epoch:   900  |  train loss: 0.2617547452
Epoch:  1000  |  train loss: 0.2697723567
Processing class: 86
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.2292840481
Epoch:   200  |  train loss: 0.2261840582
Epoch:   300  |  train loss: 0.2370794326
Epoch:   400  |  train loss: 0.2380501986
Epoch:   500  |  train loss: 0.2399583012
Epoch:   600  |  train loss: 0.2458442986
Epoch:   700  |  train loss: 0.2479979932
Epoch:   800  |  train loss: 0.2492980242
Epoch:   900  |  train loss: 0.2507273942
Epoch:  1000  |  train loss: 0.2602900565
Processing class: 87
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.2219487935
Epoch:   200  |  train loss: 0.2359553397
Epoch:   300  |  train loss: 0.2402535766
Epoch:   400  |  train loss: 0.2490470141
Epoch:   500  |  train loss: 0.2514050752
Epoch:   600  |  train loss: 0.2565599680
Epoch:   700  |  train loss: 0.2607167542
Epoch:   800  |  train loss: 0.2625710070
Epoch:   900  |  train loss: 0.2609118640
Epoch:  1000  |  train loss: 0.2721550047
Processing class: 88
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.2234117955
Epoch:   200  |  train loss: 0.2241315573
Epoch:   300  |  train loss: 0.2283416152
Epoch:   400  |  train loss: 0.2334439665
Epoch:   500  |  train loss: 0.2364698410
Epoch:   600  |  train loss: 0.2381004184
Epoch:   700  |  train loss: 0.2456715733
Epoch:   800  |  train loss: 0.2488795012
Epoch:   900  |  train loss: 0.2522653818
Epoch:  1000  |  train loss: 0.2504057407
Processing class: 89
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.2274609447
Epoch:   200  |  train loss: 0.2386351794
Epoch:   300  |  train loss: 0.2472765476
Epoch:   400  |  train loss: 0.2517657906
Epoch:   500  |  train loss: 0.2604750454
Epoch:   600  |  train loss: 0.2648742557
Epoch:   700  |  train loss: 0.2611447275
Epoch:   800  |  train loss: 0.2751372516
Epoch:   900  |  train loss: 0.2741948545
Epoch:  1000  |  train loss: 0.2795677662
2024-03-05 16:15:25,137 [trainer.py] => CNN: {'total': 54.08, '00-09': 67.9, '10-19': 63.6, '20-29': 73.2, '30-39': 68.8, '40-49': 57.2, '50-59': 32.6, '60-69': 40.9, '70-79': 38.1, '80-89': 44.4, 'old': 55.29, 'new': 44.4}
2024-03-05 16:15:25,138 [trainer.py] => No NME accuracy
2024-03-05 16:15:25,138 [trainer.py] => FeCAM: {'total': 57.66, '00-09': 76.1, '10-19': 65.8, '20-29': 75.3, '30-39': 67.9, '40-49': 68.5, '50-59': 33.0, '60-69': 45.0, '70-79': 39.0, '80-89': 48.3, 'old': 58.82, 'new': 48.3}
2024-03-05 16:15:25,138 [trainer.py] => CNN top1 curve: [83.44, 71.75, 64.66, 59.18, 54.08]
2024-03-05 16:15:25,138 [trainer.py] => CNN top5 curve: [96.5, 89.6, 86.54, 84.09, 81.74]
2024-03-05 16:15:25,138 [trainer.py] => FeCAM top1 curve: [82.9, 72.78, 67.27, 61.72, 57.66]
2024-03-05 16:15:25,138 [trainer.py] => FeCAM top5 curve: [95.84, 91.18, 87.21, 84.09, 81.63]

2024-03-05 16:15:25,144 [fecam.py] => Learning on 90-100
Processing class: 90
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.2269124746
Epoch:   200  |  train loss: 0.2302242845
Epoch:   300  |  train loss: 0.2287596285
Epoch:   400  |  train loss: 0.2314194143
Epoch:   500  |  train loss: 0.2326198220
Epoch:   600  |  train loss: 0.2389000118
Epoch:   700  |  train loss: 0.2409100085
Epoch:   800  |  train loss: 0.2449841529
Epoch:   900  |  train loss: 0.2497028589
Epoch:  1000  |  train loss: 0.2577340931
Processing class: 91
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.2252234071
Epoch:   200  |  train loss: 0.2292125136
Epoch:   300  |  train loss: 0.2394096673
Epoch:   400  |  train loss: 0.2467655063
Epoch:   500  |  train loss: 0.2558375895
Epoch:   600  |  train loss: 0.2609215468
Epoch:   700  |  train loss: 0.2660955459
Epoch:   800  |  train loss: 0.2723432183
Epoch:   900  |  train loss: 0.2791909635
Epoch:  1000  |  train loss: 0.2815634787
Processing class: 92
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.2327867597
Epoch:   200  |  train loss: 0.2343754888
Epoch:   300  |  train loss: 0.2301832050
Epoch:   400  |  train loss: 0.2370631874
Epoch:   500  |  train loss: 0.2439024270
Epoch:   600  |  train loss: 0.2546637297
Epoch:   700  |  train loss: 0.2574480265
Epoch:   800  |  train loss: 0.2524728268
Epoch:   900  |  train loss: 0.2564241648
Epoch:  1000  |  train loss: 0.2597008556
Processing class: 93
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.2247902423
Epoch:   200  |  train loss: 0.2349323124
Epoch:   300  |  train loss: 0.2534964532
Epoch:   400  |  train loss: 0.2583609909
Epoch:   500  |  train loss: 0.2610341728
Epoch:   600  |  train loss: 0.2603889555
Epoch:   700  |  train loss: 0.2743453383
Epoch:   800  |  train loss: 0.2760872781
Epoch:   900  |  train loss: 0.2794259608
Epoch:  1000  |  train loss: 0.2801727951
Processing class: 94
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.2095959991
Epoch:   200  |  train loss: 0.2125677794
Epoch:   300  |  train loss: 0.2112653583
Epoch:   400  |  train loss: 0.2191359878
Epoch:   500  |  train loss: 0.2121803939
Epoch:   600  |  train loss: 0.2197151750
Epoch:   700  |  train loss: 0.2267956614
Epoch:   800  |  train loss: 0.2170911163
Epoch:   900  |  train loss: 0.2267587870
Epoch:  1000  |  train loss: 0.2257278204
Processing class: 95
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.2214921117
Epoch:   200  |  train loss: 0.2290983677
Epoch:   300  |  train loss: 0.2279838949
Epoch:   400  |  train loss: 0.2290472716
Epoch:   500  |  train loss: 0.2407477260
Epoch:   600  |  train loss: 0.2421326041
Epoch:   700  |  train loss: 0.2477099717
Epoch:   800  |  train loss: 0.2470183849
Epoch:   900  |  train loss: 0.2543953717
Epoch:  1000  |  train loss: 0.2616856515
Processing class: 96
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.2284675866
Epoch:   200  |  train loss: 0.2235586882
Epoch:   300  |  train loss: 0.2332884371
Epoch:   400  |  train loss: 0.2457813680
Epoch:   500  |  train loss: 0.2455577493
Epoch:   600  |  train loss: 0.2464907438
Epoch:   700  |  train loss: 0.2504803091
Epoch:   800  |  train loss: 0.2488805979
Epoch:   900  |  train loss: 0.2613857925
Epoch:  1000  |  train loss: 0.2633193910
Processing class: 97
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.2284633517
Epoch:   200  |  train loss: 0.2355470359
Epoch:   300  |  train loss: 0.2426609397
Epoch:   400  |  train loss: 0.2480513483
Epoch:   500  |  train loss: 0.2578945279
Epoch:   600  |  train loss: 0.2647417843
Epoch:   700  |  train loss: 0.2711453080
Epoch:   800  |  train loss: 0.2754141331
Epoch:   900  |  train loss: 0.2797955632
Epoch:  1000  |  train loss: 0.2804948390
Processing class: 98
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.2242070228
Epoch:   200  |  train loss: 0.2285163730
Epoch:   300  |  train loss: 0.2361560732
Epoch:   400  |  train loss: 0.2436362147
Epoch:   500  |  train loss: 0.2467244178
Epoch:   600  |  train loss: 0.2535191417
Epoch:   700  |  train loss: 0.2497435689
Epoch:   800  |  train loss: 0.2575791419
Epoch:   900  |  train loss: 0.2593634874
Epoch:  1000  |  train loss: 0.2578611702
Processing class: 99
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.2326492369
Epoch:   200  |  train loss: 0.2308266908
Epoch:   300  |  train loss: 0.2469182342
Epoch:   400  |  train loss: 0.2579115510
Epoch:   500  |  train loss: 0.2610787213
Epoch:   600  |  train loss: 0.2633464962
Epoch:   700  |  train loss: 0.2633675992
Epoch:   800  |  train loss: 0.2718219519
Epoch:   900  |  train loss: 0.2718150914
Epoch:  1000  |  train loss: 0.2745636106
2024-03-05 16:26:00,327 [trainer.py] => CNN: {'total': 50.36, '00-09': 57.7, '10-19': 63.6, '20-29': 71.4, '30-39': 68.5, '40-49': 56.8, '50-59': 29.6, '60-69': 38.9, '70-79': 36.6, '80-89': 42.7, '90-99': 37.8, 'old': 51.76, 'new': 37.8}
2024-03-05 16:26:00,328 [trainer.py] => No NME accuracy
2024-03-05 16:26:00,328 [trainer.py] => FeCAM: {'total': 54.74, '00-09': 73.3, '10-19': 65.1, '20-29': 74.1, '30-39': 67.2, '40-49': 67.1, '50-59': 30.8, '60-69': 43.3, '70-79': 37.2, '80-89': 46.9, '90-99': 42.4, 'old': 56.11, 'new': 42.4}
2024-03-05 16:26:00,329 [trainer.py] => CNN top1 curve: [83.44, 71.75, 64.66, 59.18, 54.08, 50.36]
2024-03-05 16:26:00,329 [trainer.py] => CNN top5 curve: [96.5, 89.6, 86.54, 84.09, 81.74, 79.63]
2024-03-05 16:26:00,329 [trainer.py] => FeCAM top1 curve: [82.9, 72.78, 67.27, 61.72, 57.66, 54.74]
2024-03-05 16:26:00,329 [trainer.py] => FeCAM top5 curve: [95.84, 91.18, 87.21, 84.09, 81.63, 79.21]

=========================================
2024-03-05 16:26:08,770 [trainer.py] => config: ./exps/FeCAM_cifar100.json
2024-03-05 16:26:08,770 [trainer.py] => prefix: train
2024-03-05 16:26:08,770 [trainer.py] => dataset: cifar100
2024-03-05 16:26:08,770 [trainer.py] => memory_size: 0
2024-03-05 16:26:08,770 [trainer.py] => shuffle: True
2024-03-05 16:26:08,771 [trainer.py] => init_cls: 50
2024-03-05 16:26:08,771 [trainer.py] => increment: 10
2024-03-05 16:26:08,771 [trainer.py] => model_name: fecam
2024-03-05 16:26:08,771 [trainer.py] => convnet_type: resnet18
2024-03-05 16:26:08,771 [trainer.py] => device: [device(type='cuda', index=0)]
2024-03-05 16:26:08,771 [trainer.py] => seed: 1993
2024-03-05 16:26:08,771 [trainer.py] => init_epochs: 200
2024-03-05 16:26:08,771 [trainer.py] => init_lr: 0.1
2024-03-05 16:26:08,771 [trainer.py] => init_weight_decay: 0.0005
2024-03-05 16:26:08,771 [trainer.py] => batch_size: 128
2024-03-05 16:26:08,771 [trainer.py] => num_workers: 8
2024-03-05 16:26:08,771 [trainer.py] => T: 5
2024-03-05 16:26:08,771 [trainer.py] => beta: 0.5
2024-03-05 16:26:08,771 [trainer.py] => alpha1: 1
2024-03-05 16:26:08,771 [trainer.py] => alpha2: 1
2024-03-05 16:26:08,771 [trainer.py] => ncm: False
2024-03-05 16:26:08,771 [trainer.py] => tukey: False
2024-03-05 16:26:08,771 [trainer.py] => diagonal: False
2024-03-05 16:26:08,771 [trainer.py] => per_class: True
2024-03-05 16:26:08,771 [trainer.py] => full_cov: True
2024-03-05 16:26:08,771 [trainer.py] => shrink: True
2024-03-05 16:26:08,771 [trainer.py] => norm_cov: False
2024-03-05 16:26:08,771 [trainer.py] => vecnorm: False
2024-03-05 16:26:08,771 [trainer.py] => ae_type: wae
2024-03-05 16:26:08,771 [trainer.py] => epochs: 1000
2024-03-05 16:26:08,771 [trainer.py] => ae_latent_dim: 32
2024-03-05 16:26:08,771 [trainer.py] => wae_sigma: 1
2024-03-05 16:26:08,771 [trainer.py] => wae_C: 5
Files already downloaded and verified
Files already downloaded and verified
2024-03-05 16:26:10,438 [data_manager.py] => [68, 56, 78, 8, 23, 84, 90, 65, 74, 76, 40, 89, 3, 92, 55, 9, 26, 80, 43, 38, 58, 70, 77, 1, 85, 19, 17, 50, 28, 53, 13, 81, 45, 82, 6, 59, 83, 16, 15, 44, 91, 41, 72, 60, 79, 52, 20, 10, 31, 54, 37, 95, 14, 71, 96, 98, 97, 2, 64, 66, 42, 22, 35, 86, 24, 34, 87, 21, 99, 0, 88, 27, 18, 94, 11, 12, 47, 25, 30, 46, 62, 69, 36, 61, 7, 63, 75, 5, 32, 4, 51, 48, 73, 93, 39, 67, 29, 49, 57, 33]
2024-03-05 16:26:10,723 [fecam.py] => Learning on 0-50
/home/z1165703/miniconda3/lib/python3.11/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
/home/z1165703/miniconda3/lib/python3.11/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
Processing class: 0
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 2.2161968708
Epoch:   200  |  train loss: 1.8514929056
Epoch:   300  |  train loss: 1.7915267944
Epoch:   400  |  train loss: 1.6375491142
Epoch:   500  |  train loss: 1.5695048571
Epoch:   600  |  train loss: 1.5400602818
Epoch:   700  |  train loss: 1.4923611879
Epoch:   800  |  train loss: 1.4752545595
Epoch:   900  |  train loss: 1.4372304201
Epoch:  1000  |  train loss: 1.4032553673
Processing class: 1
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 2.6221028805
Epoch:   200  |  train loss: 2.5636129856
Epoch:   300  |  train loss: 2.2301188469
Epoch:   400  |  train loss: 2.0721463919
Epoch:   500  |  train loss: 1.9283985138
Epoch:   600  |  train loss: 1.8701915503
Epoch:   700  |  train loss: 1.7958068609
Epoch:   800  |  train loss: 1.7204064846
Epoch:   900  |  train loss: 1.6709924221
Epoch:  1000  |  train loss: 1.6187756777
Processing class: 2
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 2.8617563248
Epoch:   200  |  train loss: 2.5339322567
Epoch:   300  |  train loss: 2.1929913044
Epoch:   400  |  train loss: 1.9977839231
Epoch:   500  |  train loss: 1.8444994211
Epoch:   600  |  train loss: 1.7006877899
Epoch:   700  |  train loss: 1.6050114870
Epoch:   800  |  train loss: 1.5342458487
Epoch:   900  |  train loss: 1.4629964828
Epoch:  1000  |  train loss: 1.3998083115
Processing class: 3
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 2.1582590342
Epoch:   200  |  train loss: 2.1597173691
Epoch:   300  |  train loss: 1.9541347027
Epoch:   400  |  train loss: 1.7044286728
Epoch:   500  |  train loss: 1.5890140295
Epoch:   600  |  train loss: 1.5320523739
Epoch:   700  |  train loss: 1.4120892048
Epoch:   800  |  train loss: 1.3588557482
Epoch:   900  |  train loss: 1.2801229000
Epoch:  1000  |  train loss: 1.2145678997
Processing class: 4
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 2.6226931095
Epoch:   200  |  train loss: 2.5460104942
Epoch:   300  |  train loss: 2.1598621845
Epoch:   400  |  train loss: 1.9822380543
Epoch:   500  |  train loss: 1.8392662048
Epoch:   600  |  train loss: 1.6997312546
Epoch:   700  |  train loss: 1.6422541857
Epoch:   800  |  train loss: 1.5939607859
Epoch:   900  |  train loss: 1.5375480890
Epoch:  1000  |  train loss: 1.4779921770
Processing class: 5
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 2.9252273083
Epoch:   200  |  train loss: 2.4467061043
Epoch:   300  |  train loss: 2.2534970284
Epoch:   400  |  train loss: 1.9681707621
Epoch:   500  |  train loss: 1.8830571413
Epoch:   600  |  train loss: 1.7759332657
Epoch:   700  |  train loss: 1.6381933928
Epoch:   800  |  train loss: 1.5582232237
Epoch:   900  |  train loss: 1.4901166677
Epoch:  1000  |  train loss: 1.4377565384
Processing class: 6
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 2.5738106728
Epoch:   200  |  train loss: 2.5581987858
Epoch:   300  |  train loss: 2.2171506405
Epoch:   400  |  train loss: 1.9718461275
Epoch:   500  |  train loss: 1.8598635912
Epoch:   600  |  train loss: 1.8064889669
Epoch:   700  |  train loss: 1.7511731386
Epoch:   800  |  train loss: 1.6654813051
Epoch:   900  |  train loss: 1.5726617098
Epoch:  1000  |  train loss: 1.5225781679
Processing class: 7
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 2.9033826351
Epoch:   200  |  train loss: 2.5425204754
Epoch:   300  |  train loss: 2.3702814579
Epoch:   400  |  train loss: 2.0948103428
Epoch:   500  |  train loss: 1.9156873703
Epoch:   600  |  train loss: 1.8203214645
Epoch:   700  |  train loss: 1.7253211737
Epoch:   800  |  train loss: 1.6574155807
Epoch:   900  |  train loss: 1.5814981222
Epoch:  1000  |  train loss: 1.5233486891
Processing class: 8
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 2.7414899349
Epoch:   200  |  train loss: 2.4842408657
Epoch:   300  |  train loss: 2.0787373543
Epoch:   400  |  train loss: 1.9138322592
Epoch:   500  |  train loss: 1.8058044672
Epoch:   600  |  train loss: 1.6745189667
Epoch:   700  |  train loss: 1.5951013803
Epoch:   800  |  train loss: 1.5328142405
Epoch:   900  |  train loss: 1.4761012793
Epoch:  1000  |  train loss: 1.4122848034
Processing class: 9
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 2.5435054779
Epoch:   200  |  train loss: 2.5142441750
Epoch:   300  |  train loss: 2.3216662884
Epoch:   400  |  train loss: 2.1393660069
Epoch:   500  |  train loss: 2.0415868998
Epoch:   600  |  train loss: 1.9853924751
Epoch:   700  |  train loss: 1.9324766874
Epoch:   800  |  train loss: 1.8799145222
Epoch:   900  |  train loss: 1.8311541557
Epoch:  1000  |  train loss: 1.7758891106
Processing class: 10
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 2.9135025978
Epoch:   200  |  train loss: 2.5750616550
Epoch:   300  |  train loss: 2.3963368416
Epoch:   400  |  train loss: 2.2133198261
Epoch:   500  |  train loss: 2.0611011267
Epoch:   600  |  train loss: 1.9526500463
Epoch:   700  |  train loss: 1.8549306870
Epoch:   800  |  train loss: 1.7883373260
Epoch:   900  |  train loss: 1.7297567129
Epoch:  1000  |  train loss: 1.6855852365
Processing class: 11
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 2.9419514179
Epoch:   200  |  train loss: 2.4507804871
Epoch:   300  |  train loss: 2.2946435928
Epoch:   400  |  train loss: 2.0811515331
Epoch:   500  |  train loss: 1.9128404617
Epoch:   600  |  train loss: 1.8249768972
Epoch:   700  |  train loss: 1.7370448112
Epoch:   800  |  train loss: 1.6611906290
Epoch:   900  |  train loss: 1.5948029995
Epoch:  1000  |  train loss: 1.5307977200
Processing class: 12
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 2.7491445065
Epoch:   200  |  train loss: 2.5205956459
Epoch:   300  |  train loss: 2.2635124207
Epoch:   400  |  train loss: 2.0155379295
Epoch:   500  |  train loss: 1.8714172363
Epoch:   600  |  train loss: 1.7366072416
Epoch:   700  |  train loss: 1.6471492290
Epoch:   800  |  train loss: 1.5950117111
Epoch:   900  |  train loss: 1.5349279404
Epoch:  1000  |  train loss: 1.4859673500
Processing class: 13
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 2.6274017811
Epoch:   200  |  train loss: 2.2740335464
Epoch:   300  |  train loss: 2.0118475199
Epoch:   400  |  train loss: 1.8146667004
Epoch:   500  |  train loss: 1.6851849794
Epoch:   600  |  train loss: 1.6250983953
Epoch:   700  |  train loss: 1.5531580210
Epoch:   800  |  train loss: 1.5050831079
Epoch:   900  |  train loss: 1.4618346691
Epoch:  1000  |  train loss: 1.4125533342
Processing class: 14
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 3.1261207104
Epoch:   200  |  train loss: 3.0820924759
Epoch:   300  |  train loss: 2.7347468853
Epoch:   400  |  train loss: 2.4299585342
Epoch:   500  |  train loss: 2.2922037601
Epoch:   600  |  train loss: 2.1588691711
Epoch:   700  |  train loss: 2.0703547239
Epoch:   800  |  train loss: 1.9959491253
Epoch:   900  |  train loss: 1.9432279110
Epoch:  1000  |  train loss: 1.8956682920
Processing class: 15
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 2.4874394417
Epoch:   200  |  train loss: 2.2993208408
Epoch:   300  |  train loss: 2.0102367163
Epoch:   400  |  train loss: 1.8695998192
Epoch:   500  |  train loss: 1.7545202494
Epoch:   600  |  train loss: 1.6926033974
Epoch:   700  |  train loss: 1.6451329470
Epoch:   800  |  train loss: 1.5830306768
Epoch:   900  |  train loss: 1.5582774639
Epoch:  1000  |  train loss: 1.5110571384
Processing class: 16
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 2.6533906460
Epoch:   200  |  train loss: 2.2002779961
Epoch:   300  |  train loss: 2.0217376947
Epoch:   400  |  train loss: 1.8957333803
Epoch:   500  |  train loss: 1.8475611448
Epoch:   600  |  train loss: 1.7637035131
Epoch:   700  |  train loss: 1.6785500288
Epoch:   800  |  train loss: 1.6178034067
Epoch:   900  |  train loss: 1.5645782232
Epoch:  1000  |  train loss: 1.5416510582
Processing class: 17
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 2.6447515488
Epoch:   200  |  train loss: 2.6010759830
Epoch:   300  |  train loss: 2.4845553875
Epoch:   400  |  train loss: 2.2293404579
Epoch:   500  |  train loss: 1.9904021263
Epoch:   600  |  train loss: 1.8803973436
Epoch:   700  |  train loss: 1.8075716972
Epoch:   800  |  train loss: 1.7421666622
Epoch:   900  |  train loss: 1.6827520847
Epoch:  1000  |  train loss: 1.6360445499
Processing class: 18
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 2.5918936729
Epoch:   200  |  train loss: 2.5457326412
Epoch:   300  |  train loss: 1.9611197948
Epoch:   400  |  train loss: 1.7841269970
Epoch:   500  |  train loss: 1.5950575590
Epoch:   600  |  train loss: 1.4773918629
Epoch:   700  |  train loss: 1.4143647194
Epoch:   800  |  train loss: 1.3455361128
Epoch:   900  |  train loss: 1.2953875780
Epoch:  1000  |  train loss: 1.2524303198
Processing class: 19
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 2.3787981033
Epoch:   200  |  train loss: 2.1426311493
Epoch:   300  |  train loss: 1.9094175577
Epoch:   400  |  train loss: 1.8556632757
Epoch:   500  |  train loss: 1.6866557598
Epoch:   600  |  train loss: 1.6106528521
Epoch:   700  |  train loss: 1.5486223698
Epoch:   800  |  train loss: 1.4761180639
Epoch:   900  |  train loss: 1.4149691820
Epoch:  1000  |  train loss: 1.3665296316
Processing class: 20
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 2.5838702679
Epoch:   200  |  train loss: 2.2438428402
Epoch:   300  |  train loss: 2.1619085312
Epoch:   400  |  train loss: 1.9176628113
Epoch:   500  |  train loss: 1.8614592314
Epoch:   600  |  train loss: 1.7584131956
Epoch:   700  |  train loss: 1.6649401903
Epoch:   800  |  train loss: 1.6301138163
Epoch:   900  |  train loss: 1.5787661791
Epoch:  1000  |  train loss: 1.5477610111
Processing class: 21
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 2.6487677574
Epoch:   200  |  train loss: 2.2757939816
Epoch:   300  |  train loss: 1.8154633045
Epoch:   400  |  train loss: 1.6334821939
Epoch:   500  |  train loss: 1.5211636305
Epoch:   600  |  train loss: 1.4533209085
Epoch:   700  |  train loss: 1.3988937855
Epoch:   800  |  train loss: 1.3494552851
Epoch:   900  |  train loss: 1.2955122471
Epoch:  1000  |  train loss: 1.2415497303
Processing class: 22
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 2.8659121037
Epoch:   200  |  train loss: 2.8147544861
Epoch:   300  |  train loss: 2.5857610703
Epoch:   400  |  train loss: 2.2116973877
Epoch:   500  |  train loss: 2.0790560007
Epoch:   600  |  train loss: 1.9367282629
Epoch:   700  |  train loss: 1.8156884193
Epoch:   800  |  train loss: 1.7275637388
Epoch:   900  |  train loss: 1.6628898859
Epoch:  1000  |  train loss: 1.6146837473
Processing class: 23
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 2.4660573959
Epoch:   200  |  train loss: 2.4186713696
Epoch:   300  |  train loss: 2.1415518284
Epoch:   400  |  train loss: 1.9559257507
Epoch:   500  |  train loss: 1.8628887177
Epoch:   600  |  train loss: 1.7160119772
Epoch:   700  |  train loss: 1.6232471466
Epoch:   800  |  train loss: 1.5336999178
Epoch:   900  |  train loss: 1.4697879553
Epoch:  1000  |  train loss: 1.4039987564
Processing class: 24
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 2.5986017227
Epoch:   200  |  train loss: 2.5852242947
Epoch:   300  |  train loss: 2.5531443119
Epoch:   400  |  train loss: 2.1973579884
Epoch:   500  |  train loss: 2.0736166716
Epoch:   600  |  train loss: 1.9667454004
Epoch:   700  |  train loss: 1.8750306606
Epoch:   800  |  train loss: 1.8081072092
Epoch:   900  |  train loss: 1.7343202114
Epoch:  1000  |  train loss: 1.6714534760
Processing class: 25
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 2.7398166656
Epoch:   200  |  train loss: 2.4677327156
Epoch:   300  |  train loss: 2.2848339558
Epoch:   400  |  train loss: 2.0122375727
Epoch:   500  |  train loss: 1.8489210844
Epoch:   600  |  train loss: 1.7581867456
Epoch:   700  |  train loss: 1.6863896608
Epoch:   800  |  train loss: 1.6165539026
Epoch:   900  |  train loss: 1.5517892838
Epoch:  1000  |  train loss: 1.5183560610
Processing class: 26
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 2.4404635429
Epoch:   200  |  train loss: 2.4208782673
Epoch:   300  |  train loss: 2.4529313564
Epoch:   400  |  train loss: 2.2183890343
Epoch:   500  |  train loss: 2.0281766415
Epoch:   600  |  train loss: 1.8245387316
Epoch:   700  |  train loss: 1.7590293407
Epoch:   800  |  train loss: 1.6924925804
Epoch:   900  |  train loss: 1.6177685261
Epoch:  1000  |  train loss: 1.5683362722
Processing class: 27
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 2.8610596180
Epoch:   200  |  train loss: 2.4966966629
Epoch:   300  |  train loss: 2.3687649250
Epoch:   400  |  train loss: 2.0984813929
Epoch:   500  |  train loss: 1.8978429556
Epoch:   600  |  train loss: 1.8245608807
Epoch:   700  |  train loss: 1.7755802393
Epoch:   800  |  train loss: 1.7090709925
Epoch:   900  |  train loss: 1.6241317749
Epoch:  1000  |  train loss: 1.5421026230
Processing class: 28
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 2.6388573170
Epoch:   200  |  train loss: 2.5818722248
Epoch:   300  |  train loss: 2.2663029194
Epoch:   400  |  train loss: 2.0511294365
Epoch:   500  |  train loss: 1.8839137316
Epoch:   600  |  train loss: 1.8265092134
Epoch:   700  |  train loss: 1.7707928896
Epoch:   800  |  train loss: 1.7058282852
Epoch:   900  |  train loss: 1.6667477608
Epoch:  1000  |  train loss: 1.6154260159
Processing class: 29
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 2.6641339302
Epoch:   200  |  train loss: 2.3000558376
Epoch:   300  |  train loss: 2.0577676773
Epoch:   400  |  train loss: 1.8389092207
Epoch:   500  |  train loss: 1.7309239626
Epoch:   600  |  train loss: 1.6570850611
Epoch:   700  |  train loss: 1.5559439421
Epoch:   800  |  train loss: 1.4672566414
Epoch:   900  |  train loss: 1.4150070906
Epoch:  1000  |  train loss: 1.3747999907
Processing class: 30
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 2.3990055084
Epoch:   200  |  train loss: 2.3429754257
Epoch:   300  |  train loss: 1.9185902834
Epoch:   400  |  train loss: 1.8211409092
Epoch:   500  |  train loss: 1.7934050560
Epoch:   600  |  train loss: 1.7099351645
Epoch:   700  |  train loss: 1.6114854574
Epoch:   800  |  train loss: 1.5477114439
Epoch:   900  |  train loss: 1.4959723949
Epoch:  1000  |  train loss: 1.4632135868
Processing class: 31
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 2.7026178360
Epoch:   200  |  train loss: 2.3608281136
Epoch:   300  |  train loss: 2.1168541908
Epoch:   400  |  train loss: 2.0633406878
Epoch:   500  |  train loss: 1.9114665031
Epoch:   600  |  train loss: 1.8369174242
Epoch:   700  |  train loss: 1.7644398451
Epoch:   800  |  train loss: 1.7154158592
Epoch:   900  |  train loss: 1.6410867453
Epoch:  1000  |  train loss: 1.5866727591
Processing class: 32
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 2.9990409851
Epoch:   200  |  train loss: 2.8407992363
Epoch:   300  |  train loss: 2.5864306450
Epoch:   400  |  train loss: 2.3479950905
Epoch:   500  |  train loss: 2.1200506687
Epoch:   600  |  train loss: 1.9553507566
Epoch:   700  |  train loss: 1.8351429701
Epoch:   800  |  train loss: 1.7670006990
Epoch:   900  |  train loss: 1.7028890610
Epoch:  1000  |  train loss: 1.6506650686
Processing class: 33
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 2.7524989128
Epoch:   200  |  train loss: 2.5076559544
Epoch:   300  |  train loss: 2.1572954178
Epoch:   400  |  train loss: 1.9453621149
Epoch:   500  |  train loss: 1.8338847876
Epoch:   600  |  train loss: 1.7077402592
Epoch:   700  |  train loss: 1.6404605865
Epoch:   800  |  train loss: 1.5812707186
Epoch:   900  |  train loss: 1.5222640991
Epoch:  1000  |  train loss: 1.4597878456
Processing class: 34
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 2.8217751980
Epoch:   200  |  train loss: 2.7151961327
Epoch:   300  |  train loss: 2.2649254799
Epoch:   400  |  train loss: 2.1254703522
Epoch:   500  |  train loss: 1.9473367214
Epoch:   600  |  train loss: 1.8085838079
Epoch:   700  |  train loss: 1.7176527739
Epoch:   800  |  train loss: 1.6434748173
Epoch:   900  |  train loss: 1.5779793739
Epoch:  1000  |  train loss: 1.5217251062
Processing class: 35
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 3.3312132835
Epoch:   200  |  train loss: 2.7210688114
Epoch:   300  |  train loss: 2.4796420097
Epoch:   400  |  train loss: 2.3614721298
Epoch:   500  |  train loss: 2.1921692371
Epoch:   600  |  train loss: 2.1307111740
Epoch:   700  |  train loss: 2.0311291456
Epoch:   800  |  train loss: 1.9501754522
Epoch:   900  |  train loss: 1.8803034306
Epoch:  1000  |  train loss: 1.8172205687
Processing class: 36
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 2.3538911819
Epoch:   200  |  train loss: 2.1564267635
Epoch:   300  |  train loss: 1.9565906525
Epoch:   400  |  train loss: 1.8818348646
Epoch:   500  |  train loss: 1.7558203220
Epoch:   600  |  train loss: 1.6295189857
Epoch:   700  |  train loss: 1.5373116493
Epoch:   800  |  train loss: 1.4694128275
Epoch:   900  |  train loss: 1.4073560715
Epoch:  1000  |  train loss: 1.3562426567
Processing class: 37
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 2.9497382164
Epoch:   200  |  train loss: 2.7653976917
Epoch:   300  |  train loss: 2.4520352840
Epoch:   400  |  train loss: 2.2099273682
Epoch:   500  |  train loss: 2.0676572561
Epoch:   600  |  train loss: 1.9653931618
Epoch:   700  |  train loss: 1.8824758291
Epoch:   800  |  train loss: 1.8210764408
Epoch:   900  |  train loss: 1.7394226074
Epoch:  1000  |  train loss: 1.6832934380
Processing class: 38
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 2.4647887707
Epoch:   200  |  train loss: 2.3972575665
Epoch:   300  |  train loss: 2.2401327133
Epoch:   400  |  train loss: 2.1219342947
Epoch:   500  |  train loss: 1.9449701309
Epoch:   600  |  train loss: 1.8153056383
Epoch:   700  |  train loss: 1.6901446581
Epoch:   800  |  train loss: 1.6434766531
Epoch:   900  |  train loss: 1.5987992048
Epoch:  1000  |  train loss: 1.5626419306
Processing class: 39
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 2.7092903614
Epoch:   200  |  train loss: 2.5480988979
Epoch:   300  |  train loss: 2.3416255474
Epoch:   400  |  train loss: 2.1556543827
Epoch:   500  |  train loss: 1.9690696716
Epoch:   600  |  train loss: 1.8744506121
Epoch:   700  |  train loss: 1.7809916019
Epoch:   800  |  train loss: 1.6995332003
Epoch:   900  |  train loss: 1.6234534502
Epoch:  1000  |  train loss: 1.5589684248
Processing class: 40
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 2.5816344261
Epoch:   200  |  train loss: 2.2691746235
Epoch:   300  |  train loss: 2.1552455425
Epoch:   400  |  train loss: 2.0107058048
Epoch:   500  |  train loss: 1.9176208735
Epoch:   600  |  train loss: 1.7551212788
Epoch:   700  |  train loss: 1.7002855301
Epoch:   800  |  train loss: 1.6362185478
Epoch:   900  |  train loss: 1.5878362656
Epoch:  1000  |  train loss: 1.5319143057
Processing class: 41
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 2.3251205444
Epoch:   200  |  train loss: 2.3383032322
Epoch:   300  |  train loss: 1.9623918533
Epoch:   400  |  train loss: 1.7811648607
Epoch:   500  |  train loss: 1.6886309385
Epoch:   600  |  train loss: 1.6276279211
Epoch:   700  |  train loss: 1.5642174959
Epoch:   800  |  train loss: 1.5148343086
Epoch:   900  |  train loss: 1.4696542501
Epoch:  1000  |  train loss: 1.4343592644
Processing class: 42
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 2.8370517731
Epoch:   200  |  train loss: 2.4964976311
Epoch:   300  |  train loss: 2.2283192635
Epoch:   400  |  train loss: 2.0256133795
Epoch:   500  |  train loss: 1.8702222347
Epoch:   600  |  train loss: 1.7258889198
Epoch:   700  |  train loss: 1.6407949448
Epoch:   800  |  train loss: 1.5594109774
Epoch:   900  |  train loss: 1.4886126995
Epoch:  1000  |  train loss: 1.4290266514
Processing class: 43
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 2.9575689316
Epoch:   200  |  train loss: 2.6591114998
Epoch:   300  |  train loss: 2.3011178493
Epoch:   400  |  train loss: 2.1994414806
Epoch:   500  |  train loss: 2.1142616749
Epoch:   600  |  train loss: 2.0805482388
Epoch:   700  |  train loss: 2.0424344778
Epoch:   800  |  train loss: 1.9993528605
Epoch:   900  |  train loss: 1.9441206694
Epoch:  1000  |  train loss: 1.8902285337
Processing class: 44
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 3.0172173023
Epoch:   200  |  train loss: 2.5711828709
Epoch:   300  |  train loss: 2.4345457077
Epoch:   400  |  train loss: 2.1951972961
Epoch:   500  |  train loss: 2.0886745214
Epoch:   600  |  train loss: 2.0030402422
Epoch:   700  |  train loss: 1.9254603148
Epoch:   800  |  train loss: 1.8512409449
Epoch:   900  |  train loss: 1.7693753481
Epoch:  1000  |  train loss: 1.7020577431
Processing class: 45
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 2.6034433365
Epoch:   200  |  train loss: 2.3018918991
Epoch:   300  |  train loss: 2.0824787855
Epoch:   400  |  train loss: 2.0969841480
Epoch:   500  |  train loss: 2.0579337120
Epoch:   600  |  train loss: 1.9510760307
Epoch:   700  |  train loss: 1.8655114651
Epoch:   800  |  train loss: 1.8228555441
Epoch:   900  |  train loss: 1.7974858999
Epoch:  1000  |  train loss: 1.7628258944
Processing class: 46
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 2.3962935925
Epoch:   200  |  train loss: 2.3084474564
Epoch:   300  |  train loss: 2.0425055981
Epoch:   400  |  train loss: 2.0095211506
Epoch:   500  |  train loss: 1.9416865349
Epoch:   600  |  train loss: 1.8914011955
Epoch:   700  |  train loss: 1.8144206047
Epoch:   800  |  train loss: 1.7596621752
Epoch:   900  |  train loss: 1.7014840364
Epoch:  1000  |  train loss: 1.6539460897
Processing class: 47
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 2.8902024746
Epoch:   200  |  train loss: 2.5834564686
Epoch:   300  |  train loss: 2.1933494091
Epoch:   400  |  train loss: 2.0234597683
Epoch:   500  |  train loss: 1.9069386482
Epoch:   600  |  train loss: 1.7820246696
Epoch:   700  |  train loss: 1.6817489147
Epoch:   800  |  train loss: 1.6122423172
Epoch:   900  |  train loss: 1.5580998659
Epoch:  1000  |  train loss: 1.5165985346
Processing class: 48
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 2.8100699425
Epoch:   200  |  train loss: 2.6926582813
Epoch:   300  |  train loss: 2.1629502296
Epoch:   400  |  train loss: 2.0047760725
Epoch:   500  |  train loss: 1.9652794600
Epoch:   600  |  train loss: 1.8370914221
Epoch:   700  |  train loss: 1.7808801413
Epoch:   800  |  train loss: 1.7395280600
Epoch:   900  |  train loss: 1.6945187807
Epoch:  1000  |  train loss: 1.6447042465
Processing class: 49
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 2.9097975731
Epoch:   200  |  train loss: 2.6276323795
Epoch:   300  |  train loss: 2.3107942104
Epoch:   400  |  train loss: 2.1538803577
Epoch:   500  |  train loss: 1.9671340942
Epoch:   600  |  train loss: 1.8761647701
Epoch:   700  |  train loss: 1.7716481686
Epoch:   800  |  train loss: 1.7190984011
Epoch:   900  |  train loss: 1.6622020245
Epoch:  1000  |  train loss: 1.6021209717
/home/z1165703/FeCAM/models/base.py:87: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  vec = torch.tensor(vec).cuda()
2024-03-05 16:43:35,263 [trainer.py] => CNN: {'total': 83.44, '00-09': 87.7, '10-19': 79.6, '20-29': 84.9, '30-39': 81.0, '40-49': 84.0, 'old': 0, 'new': 83.44}
2024-03-05 16:43:35,264 [trainer.py] => No NME accuracy
2024-03-05 16:43:35,264 [trainer.py] => FeCAM: {'total': 82.9, '00-09': 86.2, '10-19': 79.7, '20-29': 85.0, '30-39': 80.6, '40-49': 83.0, 'old': 0, 'new': 82.9}
2024-03-05 16:43:35,264 [trainer.py] => CNN top1 curve: [83.44]
2024-03-05 16:43:35,265 [trainer.py] => CNN top5 curve: [96.5]
2024-03-05 16:43:35,265 [trainer.py] => FeCAM top1 curve: [82.9]
2024-03-05 16:43:35,265 [trainer.py] => FeCAM top5 curve: [95.84]

2024-03-05 16:43:35,271 [fecam.py] => Learning on 50-60
Processing class: 50
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 3.8237466335
Epoch:   200  |  train loss: 3.2513173103
Epoch:   300  |  train loss: 2.9610545158
Epoch:   400  |  train loss: 2.6826898098
Epoch:   500  |  train loss: 2.4273481846
Epoch:   600  |  train loss: 2.2559938431
Epoch:   700  |  train loss: 2.1234175205
Epoch:   800  |  train loss: 2.0174373388
Epoch:   900  |  train loss: 1.9401814938
Epoch:  1000  |  train loss: 1.8750220537
Processing class: 51
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 3.8648899555
Epoch:   200  |  train loss: 3.3026566982
Epoch:   300  |  train loss: 2.9410999298
Epoch:   400  |  train loss: 2.7352659702
Epoch:   500  |  train loss: 2.4500975609
Epoch:   600  |  train loss: 2.2377586842
Epoch:   700  |  train loss: 2.1028385878
Epoch:   800  |  train loss: 2.0205400467
Epoch:   900  |  train loss: 1.9425372124
Epoch:  1000  |  train loss: 1.8592785358
Processing class: 52
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 4.4491670609
Epoch:   200  |  train loss: 4.1589317322
Epoch:   300  |  train loss: 3.6643565178
Epoch:   400  |  train loss: 3.3406363487
Epoch:   500  |  train loss: 3.0849230766
Epoch:   600  |  train loss: 2.8599164009
Epoch:   700  |  train loss: 2.6840887070
Epoch:   800  |  train loss: 2.5410066605
Epoch:   900  |  train loss: 2.4031325340
Epoch:  1000  |  train loss: 2.2858751297
Processing class: 53
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 3.2184413433
Epoch:   200  |  train loss: 2.7626974583
Epoch:   300  |  train loss: 2.5730831146
Epoch:   400  |  train loss: 2.3586063862
Epoch:   500  |  train loss: 2.2643913269
Epoch:   600  |  train loss: 2.1828216076
Epoch:   700  |  train loss: 2.0905991077
Epoch:   800  |  train loss: 2.0129023790
Epoch:   900  |  train loss: 1.9523984194
Epoch:  1000  |  train loss: 1.8960338116
Processing class: 54
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 2.9582095146
Epoch:   200  |  train loss: 2.6148494720
Epoch:   300  |  train loss: 2.2910708904
Epoch:   400  |  train loss: 2.1393504143
Epoch:   500  |  train loss: 2.0063498497
Epoch:   600  |  train loss: 1.9007722139
Epoch:   700  |  train loss: 1.8080348015
Epoch:   800  |  train loss: 1.7350827694
Epoch:   900  |  train loss: 1.6742433310
Epoch:  1000  |  train loss: 1.6094989538
Processing class: 55
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 4.8088750839
Epoch:   200  |  train loss: 4.4950343132
Epoch:   300  |  train loss: 4.1620903969
Epoch:   400  |  train loss: 3.8377345562
Epoch:   500  |  train loss: 3.5467898846
Epoch:   600  |  train loss: 3.2643493176
Epoch:   700  |  train loss: 3.0571234226
Epoch:   800  |  train loss: 2.8778447628
Epoch:   900  |  train loss: 2.7197566509
Epoch:  1000  |  train loss: 2.5913535118
Processing class: 56
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 3.6841636658
Epoch:   200  |  train loss: 2.9941679478
Epoch:   300  |  train loss: 2.4897569656
Epoch:   400  |  train loss: 2.2087186337
Epoch:   500  |  train loss: 2.0087657452
Epoch:   600  |  train loss: 1.8546977520
Epoch:   700  |  train loss: 1.7451217651
Epoch:   800  |  train loss: 1.6483702183
Epoch:   900  |  train loss: 1.5803174496
Epoch:  1000  |  train loss: 1.5194021940
Processing class: 57
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 4.7986522675
Epoch:   200  |  train loss: 4.4036579132
Epoch:   300  |  train loss: 4.0620359421
Epoch:   400  |  train loss: 3.7301310062
Epoch:   500  |  train loss: 3.4331091404
Epoch:   600  |  train loss: 3.2405849934
Epoch:   700  |  train loss: 3.0782494545
Epoch:   800  |  train loss: 2.9164961338
Epoch:   900  |  train loss: 2.7634747982
Epoch:  1000  |  train loss: 2.6123619080
Processing class: 58
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 3.9585968494
Epoch:   200  |  train loss: 2.9448937893
Epoch:   300  |  train loss: 2.5835701466
Epoch:   400  |  train loss: 2.2529487133
Epoch:   500  |  train loss: 2.1025496960
Epoch:   600  |  train loss: 1.9867651701
Epoch:   700  |  train loss: 1.8779488802
Epoch:   800  |  train loss: 1.7941578150
Epoch:   900  |  train loss: 1.7141808510
Epoch:  1000  |  train loss: 1.6436402321
Processing class: 59
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 4.3988603592
Epoch:   200  |  train loss: 3.8471621990
Epoch:   300  |  train loss: 3.3575278282
Epoch:   400  |  train loss: 2.9847201347
Epoch:   500  |  train loss: 2.7217572689
Epoch:   600  |  train loss: 2.5671632767
Epoch:   700  |  train loss: 2.4310838223
Epoch:   800  |  train loss: 2.3303635120
Epoch:   900  |  train loss: 2.2304752350
Epoch:  1000  |  train loss: 2.1609078407
2024-03-05 16:49:12,371 [trainer.py] => CNN: {'total': 71.75, '00-09': 82.2, '10-19': 72.8, '20-29': 78.5, '30-39': 74.9, '40-49': 68.4, '50-59': 53.7, 'old': 75.36, 'new': 53.7}
2024-03-05 16:49:12,372 [trainer.py] => No NME accuracy
2024-03-05 16:49:12,372 [trainer.py] => FeCAM: {'total': 72.78, '00-09': 82.6, '10-19': 74.0, '20-29': 81.0, '30-39': 76.4, '40-49': 76.8, '50-59': 45.9, 'old': 78.16, 'new': 45.9}
2024-03-05 16:49:12,372 [trainer.py] => CNN top1 curve: [83.44, 71.75]
2024-03-05 16:49:12,372 [trainer.py] => CNN top5 curve: [96.5, 89.6]
2024-03-05 16:49:12,372 [trainer.py] => FeCAM top1 curve: [82.9, 72.78]
2024-03-05 16:49:12,372 [trainer.py] => FeCAM top5 curve: [95.84, 91.18]

2024-03-05 16:49:12,378 [fecam.py] => Learning on 60-70
Processing class: 60
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 4.2831225395
Epoch:   200  |  train loss: 3.8111541271
Epoch:   300  |  train loss: 3.3935342312
Epoch:   400  |  train loss: 3.0384302139
Epoch:   500  |  train loss: 2.7542267799
Epoch:   600  |  train loss: 2.5340205669
Epoch:   700  |  train loss: 2.3561222553
Epoch:   800  |  train loss: 2.1973474979
Epoch:   900  |  train loss: 2.0951681376
Epoch:  1000  |  train loss: 1.9944880247
Processing class: 61
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 3.6600562096
Epoch:   200  |  train loss: 2.9837626934
Epoch:   300  |  train loss: 2.5412276745
Epoch:   400  |  train loss: 2.2391229153
Epoch:   500  |  train loss: 2.0631195068
Epoch:   600  |  train loss: 1.9282388210
Epoch:   700  |  train loss: 1.8133308411
Epoch:   800  |  train loss: 1.7122823954
Epoch:   900  |  train loss: 1.6438441753
Epoch:  1000  |  train loss: 1.5814040661
Processing class: 62
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 4.8075798035
Epoch:   200  |  train loss: 4.4303820610
Epoch:   300  |  train loss: 4.1359386444
Epoch:   400  |  train loss: 3.9031608105
Epoch:   500  |  train loss: 3.6967487812
Epoch:   600  |  train loss: 3.5107723713
Epoch:   700  |  train loss: 3.3245648861
Epoch:   800  |  train loss: 3.1553196430
Epoch:   900  |  train loss: 3.0074285507
Epoch:  1000  |  train loss: 2.8731659412
Processing class: 63
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 4.0730603218
Epoch:   200  |  train loss: 3.6830657959
Epoch:   300  |  train loss: 2.9522502899
Epoch:   400  |  train loss: 2.5812683105
Epoch:   500  |  train loss: 2.3392341137
Epoch:   600  |  train loss: 2.1778645992
Epoch:   700  |  train loss: 2.0587390661
Epoch:   800  |  train loss: 1.9708430529
Epoch:   900  |  train loss: 1.8803368330
Epoch:  1000  |  train loss: 1.8058976173
Processing class: 64
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 3.6775775433
Epoch:   200  |  train loss: 2.7133227825
Epoch:   300  |  train loss: 2.3434302330
Epoch:   400  |  train loss: 2.1235395908
Epoch:   500  |  train loss: 1.9122670412
Epoch:   600  |  train loss: 1.7662048101
Epoch:   700  |  train loss: 1.6659011602
Epoch:   800  |  train loss: 1.5872704744
Epoch:   900  |  train loss: 1.5213480949
Epoch:  1000  |  train loss: 1.4494183779
Processing class: 65
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 3.6827704430
Epoch:   200  |  train loss: 2.9700549126
Epoch:   300  |  train loss: 2.3759259701
Epoch:   400  |  train loss: 2.1281106472
Epoch:   500  |  train loss: 1.9654241323
Epoch:   600  |  train loss: 1.8239727497
Epoch:   700  |  train loss: 1.7098645449
Epoch:   800  |  train loss: 1.6249512672
Epoch:   900  |  train loss: 1.5398364544
Epoch:  1000  |  train loss: 1.4780956030
Processing class: 66
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 4.0698490143
Epoch:   200  |  train loss: 3.5523765564
Epoch:   300  |  train loss: 3.1168390751
Epoch:   400  |  train loss: 2.8498370647
Epoch:   500  |  train loss: 2.6955654621
Epoch:   600  |  train loss: 2.5584863663
Epoch:   700  |  train loss: 2.4600017548
Epoch:   800  |  train loss: 2.3828674316
Epoch:   900  |  train loss: 2.3038754463
Epoch:  1000  |  train loss: 2.2399872780
Processing class: 67
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 4.2607478142
Epoch:   200  |  train loss: 3.7255573273
Epoch:   300  |  train loss: 3.3522072315
Epoch:   400  |  train loss: 3.0908545494
Epoch:   500  |  train loss: 2.9025228977
Epoch:   600  |  train loss: 2.7398302555
Epoch:   700  |  train loss: 2.5897305012
Epoch:   800  |  train loss: 2.4752686501
Epoch:   900  |  train loss: 2.3691001415
Epoch:  1000  |  train loss: 2.2833479404
Processing class: 68
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 3.6028726101
Epoch:   200  |  train loss: 3.2437282562
Epoch:   300  |  train loss: 2.7069308758
Epoch:   400  |  train loss: 2.4955047131
Epoch:   500  |  train loss: 2.3418907166
Epoch:   600  |  train loss: 2.2062005997
Epoch:   700  |  train loss: 2.1030030966
Epoch:   800  |  train loss: 2.0100967169
Epoch:   900  |  train loss: 1.9309864998
Epoch:  1000  |  train loss: 1.8668184042
Processing class: 69
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 2.1147274017
Epoch:   200  |  train loss: 1.8357737064
Epoch:   300  |  train loss: 1.5943089247
Epoch:   400  |  train loss: 1.4508782148
Epoch:   500  |  train loss: 1.3642246723
Epoch:   600  |  train loss: 1.3164987803
Epoch:   700  |  train loss: 1.2835371971
Epoch:   800  |  train loss: 1.2454148293
Epoch:   900  |  train loss: 1.2257512093
Epoch:  1000  |  train loss: 1.1964241028
2024-03-05 16:55:40,682 [trainer.py] => CNN: {'total': 64.66, '00-09': 75.0, '10-19': 70.1, '20-29': 77.1, '30-39': 70.6, '40-49': 64.0, '50-59': 45.8, '60-69': 50.0, 'old': 67.1, 'new': 50.0}
2024-03-05 16:55:40,683 [trainer.py] => No NME accuracy
2024-03-05 16:55:40,683 [trainer.py] => FeCAM: {'total': 67.27, '00-09': 79.9, '10-19': 70.6, '20-29': 79.1, '30-39': 71.9, '40-49': 74.1, '50-59': 41.2, '60-69': 54.1, 'old': 69.47, 'new': 54.1}
2024-03-05 16:55:40,683 [trainer.py] => CNN top1 curve: [83.44, 71.75, 64.66]
2024-03-05 16:55:40,683 [trainer.py] => CNN top5 curve: [96.5, 89.6, 86.54]
2024-03-05 16:55:40,683 [trainer.py] => FeCAM top1 curve: [82.9, 72.78, 67.27]
2024-03-05 16:55:40,683 [trainer.py] => FeCAM top5 curve: [95.84, 91.18, 87.21]

2024-03-05 16:55:40,689 [fecam.py] => Learning on 70-80
Processing class: 70
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 3.8789900780
Epoch:   200  |  train loss: 3.4384567261
Epoch:   300  |  train loss: 2.8971400261
Epoch:   400  |  train loss: 2.6332964897
Epoch:   500  |  train loss: 2.4121942520
Epoch:   600  |  train loss: 2.2265001297
Epoch:   700  |  train loss: 2.0641499043
Epoch:   800  |  train loss: 1.9343461275
Epoch:   900  |  train loss: 1.8632206202
Epoch:  1000  |  train loss: 1.7990314484
Processing class: 71
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 4.3581090927
Epoch:   200  |  train loss: 3.8784876347
Epoch:   300  |  train loss: 3.2536808968
Epoch:   400  |  train loss: 2.9162876129
Epoch:   500  |  train loss: 2.6571752548
Epoch:   600  |  train loss: 2.4572197914
Epoch:   700  |  train loss: 2.2809358120
Epoch:   800  |  train loss: 2.1462865829
Epoch:   900  |  train loss: 2.0273478985
Epoch:  1000  |  train loss: 1.9250150681
Processing class: 72
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 4.1974488258
Epoch:   200  |  train loss: 3.7293871880
Epoch:   300  |  train loss: 3.2658475876
Epoch:   400  |  train loss: 2.9580735683
Epoch:   500  |  train loss: 2.7623857021
Epoch:   600  |  train loss: 2.5783799648
Epoch:   700  |  train loss: 2.4324889660
Epoch:   800  |  train loss: 2.2988465786
Epoch:   900  |  train loss: 2.1834128857
Epoch:  1000  |  train loss: 2.0928869486
Processing class: 73
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 4.0746757507
Epoch:   200  |  train loss: 3.5004335880
Epoch:   300  |  train loss: 2.8899236679
Epoch:   400  |  train loss: 2.6617095947
Epoch:   500  |  train loss: 2.5289266586
Epoch:   600  |  train loss: 2.4107993603
Epoch:   700  |  train loss: 2.3016452312
Epoch:   800  |  train loss: 2.2017035961
Epoch:   900  |  train loss: 2.1120314121
Epoch:  1000  |  train loss: 2.0394450903
Processing class: 74
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 4.6722662926
Epoch:   200  |  train loss: 4.4590710640
Epoch:   300  |  train loss: 3.9726292133
Epoch:   400  |  train loss: 3.5821037292
Epoch:   500  |  train loss: 3.2498632908
Epoch:   600  |  train loss: 3.0147352219
Epoch:   700  |  train loss: 2.8423099995
Epoch:   800  |  train loss: 2.6774591446
Epoch:   900  |  train loss: 2.5478327751
Epoch:  1000  |  train loss: 2.4403860569
Processing class: 75
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 4.1795510292
Epoch:   200  |  train loss: 3.7151122570
Epoch:   300  |  train loss: 3.2101146221
Epoch:   400  |  train loss: 2.9625879765
Epoch:   500  |  train loss: 2.7424911022
Epoch:   600  |  train loss: 2.5666429520
Epoch:   700  |  train loss: 2.4261803150
Epoch:   800  |  train loss: 2.3163625240
Epoch:   900  |  train loss: 2.2282278061
Epoch:  1000  |  train loss: 2.1422525883
Processing class: 76
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 2.3918060303
Epoch:   200  |  train loss: 1.9921800852
Epoch:   300  |  train loss: 1.7027929306
Epoch:   400  |  train loss: 1.5722806215
Epoch:   500  |  train loss: 1.5001527786
Epoch:   600  |  train loss: 1.4248680830
Epoch:   700  |  train loss: 1.3858406305
Epoch:   800  |  train loss: 1.3204368591
Epoch:   900  |  train loss: 1.2851854801
Epoch:  1000  |  train loss: 1.2514684916
Processing class: 77
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 3.6656954765
Epoch:   200  |  train loss: 3.2183800220
Epoch:   300  |  train loss: 2.7825205803
Epoch:   400  |  train loss: 2.5186685562
Epoch:   500  |  train loss: 2.2474558830
Epoch:   600  |  train loss: 2.0417416573
Epoch:   700  |  train loss: 1.9007552147
Epoch:   800  |  train loss: 1.7917446613
Epoch:   900  |  train loss: 1.7103365660
Epoch:  1000  |  train loss: 1.6243209600
Processing class: 78
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 3.8498399734
Epoch:   200  |  train loss: 3.0833977699
Epoch:   300  |  train loss: 2.8207708359
Epoch:   400  |  train loss: 2.5509231567
Epoch:   500  |  train loss: 2.3107860088
Epoch:   600  |  train loss: 2.1557702065
Epoch:   700  |  train loss: 2.0462876081
Epoch:   800  |  train loss: 1.9432223558
Epoch:   900  |  train loss: 1.8774458647
Epoch:  1000  |  train loss: 1.8125815153
Processing class: 79
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 4.8065814018
Epoch:   200  |  train loss: 4.2816929817
Epoch:   300  |  train loss: 3.9161673069
Epoch:   400  |  train loss: 3.5573424816
Epoch:   500  |  train loss: 3.3089150906
Epoch:   600  |  train loss: 3.0895613670
Epoch:   700  |  train loss: 2.9024253368
Epoch:   800  |  train loss: 2.7238979816
Epoch:   900  |  train loss: 2.5764288902
Epoch:  1000  |  train loss: 2.4468919277
2024-03-05 17:03:11,097 [trainer.py] => CNN: {'total': 59.18, '00-09': 73.6, '10-19': 68.4, '20-29': 76.9, '30-39': 69.1, '40-49': 60.6, '50-59': 37.2, '60-69': 45.7, '70-79': 41.9, 'old': 61.64, 'new': 41.9}
2024-03-05 17:03:11,100 [trainer.py] => No NME accuracy
2024-03-05 17:03:11,100 [trainer.py] => FeCAM: {'total': 61.72, '00-09': 78.1, '10-19': 69.3, '20-29': 78.0, '30-39': 69.5, '40-49': 72.1, '50-59': 35.7, '60-69': 49.1, '70-79': 42.0, 'old': 64.54, 'new': 42.0}
2024-03-05 17:03:11,100 [trainer.py] => CNN top1 curve: [83.44, 71.75, 64.66, 59.18]
2024-03-05 17:03:11,100 [trainer.py] => CNN top5 curve: [96.5, 89.6, 86.54, 84.09]
2024-03-05 17:03:11,100 [trainer.py] => FeCAM top1 curve: [82.9, 72.78, 67.27, 61.72]
2024-03-05 17:03:11,100 [trainer.py] => FeCAM top5 curve: [95.84, 91.18, 87.21, 84.09]

2024-03-05 17:03:11,107 [fecam.py] => Learning on 80-90
Processing class: 80
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 2.8204420567
Epoch:   200  |  train loss: 2.0470686913
Epoch:   300  |  train loss: 1.8224676609
Epoch:   400  |  train loss: 1.6828353405
Epoch:   500  |  train loss: 1.5695652485
Epoch:   600  |  train loss: 1.5087035894
Epoch:   700  |  train loss: 1.4446256399
Epoch:   800  |  train loss: 1.4119626045
Epoch:   900  |  train loss: 1.3704952002
Epoch:  1000  |  train loss: 1.3328954458
Processing class: 81
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 3.6635745049
Epoch:   200  |  train loss: 3.0178819180
Epoch:   300  |  train loss: 2.6975981236
Epoch:   400  |  train loss: 2.4832057476
Epoch:   500  |  train loss: 2.3042423725
Epoch:   600  |  train loss: 2.1756014824
Epoch:   700  |  train loss: 2.0673730373
Epoch:   800  |  train loss: 1.9909280539
Epoch:   900  |  train loss: 1.8980023623
Epoch:  1000  |  train loss: 1.8157225132
Processing class: 82
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 3.3208324432
Epoch:   200  |  train loss: 2.7195744991
Epoch:   300  |  train loss: 2.4274063587
Epoch:   400  |  train loss: 2.1917318821
Epoch:   500  |  train loss: 2.0080455542
Epoch:   600  |  train loss: 1.8929384708
Epoch:   700  |  train loss: 1.8069008589
Epoch:   800  |  train loss: 1.7296027422
Epoch:   900  |  train loss: 1.6643135786
Epoch:  1000  |  train loss: 1.6013262272
Processing class: 83
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 3.0489443779
Epoch:   200  |  train loss: 2.2193643570
Epoch:   300  |  train loss: 1.9023581743
Epoch:   400  |  train loss: 1.6916586876
Epoch:   500  |  train loss: 1.5675815582
Epoch:   600  |  train loss: 1.4862412930
Epoch:   700  |  train loss: 1.4339485407
Epoch:   800  |  train loss: 1.3799011469
Epoch:   900  |  train loss: 1.3310273886
Epoch:  1000  |  train loss: 1.2941691160
Processing class: 84
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 4.0022589684
Epoch:   200  |  train loss: 3.1100384712
Epoch:   300  |  train loss: 2.7254779816
Epoch:   400  |  train loss: 2.4106680870
Epoch:   500  |  train loss: 2.2206265926
Epoch:   600  |  train loss: 2.0812011003
Epoch:   700  |  train loss: 1.9725028992
Epoch:   800  |  train loss: 1.8899186134
Epoch:   900  |  train loss: 1.8020287991
Epoch:  1000  |  train loss: 1.7254669666
Processing class: 85
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 4.1234992027
Epoch:   200  |  train loss: 3.2579082966
Epoch:   300  |  train loss: 2.7803113461
Epoch:   400  |  train loss: 2.4967840672
Epoch:   500  |  train loss: 2.3259438992
Epoch:   600  |  train loss: 2.1761438847
Epoch:   700  |  train loss: 2.0743368387
Epoch:   800  |  train loss: 1.9810752869
Epoch:   900  |  train loss: 1.8954930782
Epoch:  1000  |  train loss: 1.8266083956
Processing class: 86
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 4.7513241768
Epoch:   200  |  train loss: 4.1380971909
Epoch:   300  |  train loss: 3.4520494461
Epoch:   400  |  train loss: 3.0745426655
Epoch:   500  |  train loss: 2.8228276253
Epoch:   600  |  train loss: 2.6209883213
Epoch:   700  |  train loss: 2.4591481209
Epoch:   800  |  train loss: 2.3294484615
Epoch:   900  |  train loss: 2.2301322937
Epoch:  1000  |  train loss: 2.1497324467
Processing class: 87
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 3.3197785854
Epoch:   200  |  train loss: 2.6353116989
Epoch:   300  |  train loss: 2.4202409267
Epoch:   400  |  train loss: 2.1596453667
Epoch:   500  |  train loss: 2.0124172926
Epoch:   600  |  train loss: 1.9335163593
Epoch:   700  |  train loss: 1.8559454441
Epoch:   800  |  train loss: 1.7807034969
Epoch:   900  |  train loss: 1.7153473377
Epoch:  1000  |  train loss: 1.6655032158
Processing class: 88
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 3.9826606274
Epoch:   200  |  train loss: 3.4662630558
Epoch:   300  |  train loss: 3.0546346664
Epoch:   400  |  train loss: 2.7424980164
Epoch:   500  |  train loss: 2.4703086376
Epoch:   600  |  train loss: 2.2681403160
Epoch:   700  |  train loss: 2.1409793854
Epoch:   800  |  train loss: 2.0343951225
Epoch:   900  |  train loss: 1.9527552366
Epoch:  1000  |  train loss: 1.8681013584
Processing class: 89
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 3.7431461811
Epoch:   200  |  train loss: 3.1442932606
Epoch:   300  |  train loss: 2.6514931679
Epoch:   400  |  train loss: 2.4288189888
Epoch:   500  |  train loss: 2.2176731825
Epoch:   600  |  train loss: 2.0454393387
Epoch:   700  |  train loss: 1.9199973822
Epoch:   800  |  train loss: 1.8249654531
Epoch:   900  |  train loss: 1.7331949472
Epoch:  1000  |  train loss: 1.6569832325
2024-03-05 17:11:55,297 [trainer.py] => CNN: {'total': 54.08, '00-09': 67.9, '10-19': 63.6, '20-29': 73.2, '30-39': 68.8, '40-49': 57.2, '50-59': 32.6, '60-69': 40.9, '70-79': 38.1, '80-89': 44.4, 'old': 55.29, 'new': 44.4}
2024-03-05 17:11:55,298 [trainer.py] => No NME accuracy
2024-03-05 17:11:55,298 [trainer.py] => FeCAM: {'total': 57.66, '00-09': 76.1, '10-19': 65.8, '20-29': 75.3, '30-39': 67.9, '40-49': 68.5, '50-59': 33.0, '60-69': 45.0, '70-79': 39.0, '80-89': 48.3, 'old': 58.82, 'new': 48.3}
2024-03-05 17:11:55,298 [trainer.py] => CNN top1 curve: [83.44, 71.75, 64.66, 59.18, 54.08]
2024-03-05 17:11:55,298 [trainer.py] => CNN top5 curve: [96.5, 89.6, 86.54, 84.09, 81.74]
2024-03-05 17:11:55,298 [trainer.py] => FeCAM top1 curve: [82.9, 72.78, 67.27, 61.72, 57.66]
2024-03-05 17:11:55,298 [trainer.py] => FeCAM top5 curve: [95.84, 91.18, 87.21, 84.09, 81.63]

2024-03-05 17:11:55,304 [fecam.py] => Learning on 90-100
Processing class: 90
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 4.1047986984
Epoch:   200  |  train loss: 3.6748829842
Epoch:   300  |  train loss: 3.1280376911
Epoch:   400  |  train loss: 2.7355709076
Epoch:   500  |  train loss: 2.4628869057
Epoch:   600  |  train loss: 2.2420610428
Epoch:   700  |  train loss: 2.0697253466
Epoch:   800  |  train loss: 1.9628193378
Epoch:   900  |  train loss: 1.8820380688
Epoch:  1000  |  train loss: 1.8046882153
Processing class: 91
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 2.3116016388
Epoch:   200  |  train loss: 2.0821400404
Epoch:   300  |  train loss: 1.8387489557
Epoch:   400  |  train loss: 1.7385944128
Epoch:   500  |  train loss: 1.6072328329
Epoch:   600  |  train loss: 1.5170409441
Epoch:   700  |  train loss: 1.4093149900
Epoch:   800  |  train loss: 1.3244915485
Epoch:   900  |  train loss: 1.2645598173
Epoch:  1000  |  train loss: 1.2274505615
Processing class: 92
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 3.6141979694
Epoch:   200  |  train loss: 2.7832108498
Epoch:   300  |  train loss: 2.5652258396
Epoch:   400  |  train loss: 2.3661763668
Epoch:   500  |  train loss: 2.1331072330
Epoch:   600  |  train loss: 1.9192841530
Epoch:   700  |  train loss: 1.7853053808
Epoch:   800  |  train loss: 1.6954360008
Epoch:   900  |  train loss: 1.6236714840
Epoch:  1000  |  train loss: 1.5597654343
Processing class: 93
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 3.8796705723
Epoch:   200  |  train loss: 3.0062578678
Epoch:   300  |  train loss: 2.5790745258
Epoch:   400  |  train loss: 2.3542273521
Epoch:   500  |  train loss: 2.1537006855
Epoch:   600  |  train loss: 1.9910610199
Epoch:   700  |  train loss: 1.8763360262
Epoch:   800  |  train loss: 1.7813342333
Epoch:   900  |  train loss: 1.7029734850
Epoch:  1000  |  train loss: 1.6193096876
Processing class: 94
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 3.4781792164
Epoch:   200  |  train loss: 2.9811674595
Epoch:   300  |  train loss: 2.4743026733
Epoch:   400  |  train loss: 2.2186796665
Epoch:   500  |  train loss: 2.0455804348
Epoch:   600  |  train loss: 1.9255978584
Epoch:   700  |  train loss: 1.8270113230
Epoch:   800  |  train loss: 1.7250230312
Epoch:   900  |  train loss: 1.6550028801
Epoch:  1000  |  train loss: 1.5872453451
Processing class: 95
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 4.3222226143
Epoch:   200  |  train loss: 3.9095311165
Epoch:   300  |  train loss: 3.4784569740
Epoch:   400  |  train loss: 3.0588527203
Epoch:   500  |  train loss: 2.7441829681
Epoch:   600  |  train loss: 2.5478769779
Epoch:   700  |  train loss: 2.3763673782
Epoch:   800  |  train loss: 2.2419576168
Epoch:   900  |  train loss: 2.1284572840
Epoch:  1000  |  train loss: 2.0350591421
Processing class: 96
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 4.4063341141
Epoch:   200  |  train loss: 3.8898460865
Epoch:   300  |  train loss: 3.3159763813
Epoch:   400  |  train loss: 3.0154335499
Epoch:   500  |  train loss: 2.7593185902
Epoch:   600  |  train loss: 2.5796797752
Epoch:   700  |  train loss: 2.4280729771
Epoch:   800  |  train loss: 2.2915020466
Epoch:   900  |  train loss: 2.1608519793
Epoch:  1000  |  train loss: 2.0663372755
Processing class: 97
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 3.8324699402
Epoch:   200  |  train loss: 3.0316487312
Epoch:   300  |  train loss: 2.5650909424
Epoch:   400  |  train loss: 2.3515461445
Epoch:   500  |  train loss: 2.1661971569
Epoch:   600  |  train loss: 2.0303168535
Epoch:   700  |  train loss: 1.9129417181
Epoch:   800  |  train loss: 1.8258710861
Epoch:   900  |  train loss: 1.7404436350
Epoch:  1000  |  train loss: 1.6617321491
Processing class: 98
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 3.2579437733
Epoch:   200  |  train loss: 2.7760216236
Epoch:   300  |  train loss: 2.3249956131
Epoch:   400  |  train loss: 2.0882302999
Epoch:   500  |  train loss: 1.9212260246
Epoch:   600  |  train loss: 1.7898407936
Epoch:   700  |  train loss: 1.6969603062
Epoch:   800  |  train loss: 1.6307914972
Epoch:   900  |  train loss: 1.5703372478
Epoch:  1000  |  train loss: 1.5333982468
Processing class: 99
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 4.0434371948
Epoch:   200  |  train loss: 3.4724778652
Epoch:   300  |  train loss: 2.8406270981
Epoch:   400  |  train loss: 2.5001824379
Epoch:   500  |  train loss: 2.2533173323
Epoch:   600  |  train loss: 2.1022313595
Epoch:   700  |  train loss: 1.9903154373
Epoch:   800  |  train loss: 1.9018117428
Epoch:   900  |  train loss: 1.8209657431
Epoch:  1000  |  train loss: 1.7475213766
2024-03-05 17:22:03,163 [trainer.py] => CNN: {'total': 50.36, '00-09': 57.7, '10-19': 63.6, '20-29': 71.4, '30-39': 68.5, '40-49': 56.8, '50-59': 29.6, '60-69': 38.9, '70-79': 36.6, '80-89': 42.7, '90-99': 37.8, 'old': 51.76, 'new': 37.8}
2024-03-05 17:22:03,164 [trainer.py] => No NME accuracy
2024-03-05 17:22:03,165 [trainer.py] => FeCAM: {'total': 54.74, '00-09': 73.3, '10-19': 65.1, '20-29': 74.1, '30-39': 67.2, '40-49': 67.1, '50-59': 30.8, '60-69': 43.3, '70-79': 37.2, '80-89': 46.9, '90-99': 42.4, 'old': 56.11, 'new': 42.4}
2024-03-05 17:22:03,165 [trainer.py] => CNN top1 curve: [83.44, 71.75, 64.66, 59.18, 54.08, 50.36]
2024-03-05 17:22:03,165 [trainer.py] => CNN top5 curve: [96.5, 89.6, 86.54, 84.09, 81.74, 79.63]
2024-03-05 17:22:03,165 [trainer.py] => FeCAM top1 curve: [82.9, 72.78, 67.27, 61.72, 57.66, 54.74]
2024-03-05 17:22:03,165 [trainer.py] => FeCAM top5 curve: [95.84, 91.18, 87.21, 84.09, 81.63, 79.21]

=========================================
2024-03-05 17:22:11,375 [trainer.py] => config: ./exps/FeCAM_cifar100.json
2024-03-05 17:22:11,375 [trainer.py] => prefix: train
2024-03-05 17:22:11,375 [trainer.py] => dataset: cifar100
2024-03-05 17:22:11,376 [trainer.py] => memory_size: 0
2024-03-05 17:22:11,376 [trainer.py] => shuffle: True
2024-03-05 17:22:11,376 [trainer.py] => init_cls: 50
2024-03-05 17:22:11,376 [trainer.py] => increment: 10
2024-03-05 17:22:11,376 [trainer.py] => model_name: fecam
2024-03-05 17:22:11,376 [trainer.py] => convnet_type: resnet18
2024-03-05 17:22:11,376 [trainer.py] => device: [device(type='cuda', index=0)]
2024-03-05 17:22:11,376 [trainer.py] => seed: 1993
2024-03-05 17:22:11,376 [trainer.py] => init_epochs: 200
2024-03-05 17:22:11,376 [trainer.py] => init_lr: 0.1
2024-03-05 17:22:11,376 [trainer.py] => init_weight_decay: 0.0005
2024-03-05 17:22:11,376 [trainer.py] => batch_size: 128
2024-03-05 17:22:11,376 [trainer.py] => num_workers: 8
2024-03-05 17:22:11,376 [trainer.py] => T: 5
2024-03-05 17:22:11,376 [trainer.py] => beta: 0.5
2024-03-05 17:22:11,376 [trainer.py] => alpha1: 1
2024-03-05 17:22:11,376 [trainer.py] => alpha2: 1
2024-03-05 17:22:11,376 [trainer.py] => ncm: False
2024-03-05 17:22:11,376 [trainer.py] => tukey: False
2024-03-05 17:22:11,376 [trainer.py] => diagonal: False
2024-03-05 17:22:11,376 [trainer.py] => per_class: True
2024-03-05 17:22:11,376 [trainer.py] => full_cov: True
2024-03-05 17:22:11,376 [trainer.py] => shrink: True
2024-03-05 17:22:11,376 [trainer.py] => norm_cov: False
2024-03-05 17:22:11,376 [trainer.py] => vecnorm: False
2024-03-05 17:22:11,376 [trainer.py] => ae_type: wae
2024-03-05 17:22:11,376 [trainer.py] => epochs: 1000
2024-03-05 17:22:11,376 [trainer.py] => ae_latent_dim: 32
2024-03-05 17:22:11,376 [trainer.py] => wae_sigma: 5
2024-03-05 17:22:11,376 [trainer.py] => wae_C: 5
Files already downloaded and verified
Files already downloaded and verified
2024-03-05 17:22:13,035 [data_manager.py] => [68, 56, 78, 8, 23, 84, 90, 65, 74, 76, 40, 89, 3, 92, 55, 9, 26, 80, 43, 38, 58, 70, 77, 1, 85, 19, 17, 50, 28, 53, 13, 81, 45, 82, 6, 59, 83, 16, 15, 44, 91, 41, 72, 60, 79, 52, 20, 10, 31, 54, 37, 95, 14, 71, 96, 98, 97, 2, 64, 66, 42, 22, 35, 86, 24, 34, 87, 21, 99, 0, 88, 27, 18, 94, 11, 12, 47, 25, 30, 46, 62, 69, 36, 61, 7, 63, 75, 5, 32, 4, 51, 48, 73, 93, 39, 67, 29, 49, 57, 33]
2024-03-05 17:22:13,306 [fecam.py] => Learning on 0-50
/home/z1165703/miniconda3/lib/python3.11/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
/home/z1165703/miniconda3/lib/python3.11/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
Processing class: 0
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 2.8211977482
Epoch:   200  |  train loss: 2.6078615665
Epoch:   300  |  train loss: 2.5977185726
Epoch:   400  |  train loss: 2.5147149086
Epoch:   500  |  train loss: 2.4652295113
Epoch:   600  |  train loss: 2.4498626232
Epoch:   700  |  train loss: 2.4227116108
Epoch:   800  |  train loss: 2.4115186691
Epoch:   900  |  train loss: 2.3778712749
Epoch:  1000  |  train loss: 2.3450695038
Processing class: 1
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 3.1063868999
Epoch:   200  |  train loss: 3.0652859211
Epoch:   300  |  train loss: 2.9437974930
Epoch:   400  |  train loss: 2.8623246193
Epoch:   500  |  train loss: 2.7602954865
Epoch:   600  |  train loss: 2.7444107533
Epoch:   700  |  train loss: 2.6881621361
Epoch:   800  |  train loss: 2.6478335857
Epoch:   900  |  train loss: 2.6226758957
Epoch:  1000  |  train loss: 2.5681683540
Processing class: 2
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 3.2657509804
Epoch:   200  |  train loss: 3.1035697937
Epoch:   300  |  train loss: 2.9530444622
Epoch:   400  |  train loss: 2.8153243065
Epoch:   500  |  train loss: 2.6855062008
Epoch:   600  |  train loss: 2.5958390236
Epoch:   700  |  train loss: 2.5124892235
Epoch:   800  |  train loss: 2.4666879654
Epoch:   900  |  train loss: 2.3936233997
Epoch:  1000  |  train loss: 2.3323499203
Processing class: 3
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 2.7887709618
Epoch:   200  |  train loss: 2.8004101276
Epoch:   300  |  train loss: 2.7168527603
Epoch:   400  |  train loss: 2.5739057064
Epoch:   500  |  train loss: 2.4885744572
Epoch:   600  |  train loss: 2.4643467426
Epoch:   700  |  train loss: 2.3828608990
Epoch:   800  |  train loss: 2.3309957027
Epoch:   900  |  train loss: 2.2679800034
Epoch:  1000  |  train loss: 2.2125600338
Processing class: 4
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 3.0999639511
Epoch:   200  |  train loss: 3.0423701763
Epoch:   300  |  train loss: 2.8686137676
Epoch:   400  |  train loss: 2.7561523438
Epoch:   500  |  train loss: 2.6725656033
Epoch:   600  |  train loss: 2.5673950195
Epoch:   700  |  train loss: 2.5396320820
Epoch:   800  |  train loss: 2.5045324326
Epoch:   900  |  train loss: 2.4639121532
Epoch:  1000  |  train loss: 2.4270237446
Processing class: 5
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 3.3065580368
Epoch:   200  |  train loss: 3.0664796352
Epoch:   300  |  train loss: 2.9512938499
Epoch:   400  |  train loss: 2.7983769894
Epoch:   500  |  train loss: 2.7407521725
Epoch:   600  |  train loss: 2.6887331963
Epoch:   700  |  train loss: 2.5952420712
Epoch:   800  |  train loss: 2.5175162315
Epoch:   900  |  train loss: 2.4531241417
Epoch:  1000  |  train loss: 2.4162331104
Processing class: 6
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 3.0720951557
Epoch:   200  |  train loss: 3.0711778641
Epoch:   300  |  train loss: 2.9184361935
Epoch:   400  |  train loss: 2.7796258450
Epoch:   500  |  train loss: 2.7199949741
Epoch:   600  |  train loss: 2.6779963493
Epoch:   700  |  train loss: 2.6336668968
Epoch:   800  |  train loss: 2.5789422989
Epoch:   900  |  train loss: 2.4977319241
Epoch:  1000  |  train loss: 2.4655708790
Processing class: 7
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 3.2788146496
Epoch:   200  |  train loss: 3.1174434662
Epoch:   300  |  train loss: 3.0265059471
Epoch:   400  |  train loss: 2.8736273289
Epoch:   500  |  train loss: 2.7607981682
Epoch:   600  |  train loss: 2.6913323402
Epoch:   700  |  train loss: 2.6334588528
Epoch:   800  |  train loss: 2.5948306561
Epoch:   900  |  train loss: 2.5203183651
Epoch:  1000  |  train loss: 2.4786372185
Processing class: 8
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 3.1764060974
Epoch:   200  |  train loss: 3.0726250648
Epoch:   300  |  train loss: 2.8564451694
Epoch:   400  |  train loss: 2.7735430717
Epoch:   500  |  train loss: 2.6932478428
Epoch:   600  |  train loss: 2.6051720619
Epoch:   700  |  train loss: 2.5598003864
Epoch:   800  |  train loss: 2.4949327946
Epoch:   900  |  train loss: 2.4551136494
Epoch:  1000  |  train loss: 2.3922636986
Processing class: 9
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 3.0417946339
Epoch:   200  |  train loss: 3.0350439072
Epoch:   300  |  train loss: 2.9788589954
Epoch:   400  |  train loss: 2.8614327431
Epoch:   500  |  train loss: 2.8235578060
Epoch:   600  |  train loss: 2.7786988258
Epoch:   700  |  train loss: 2.7588305473
Epoch:   800  |  train loss: 2.7318932056
Epoch:   900  |  train loss: 2.7015641212
Epoch:  1000  |  train loss: 2.6525239468
Processing class: 10
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 3.2684239388
Epoch:   200  |  train loss: 3.0977437973
Epoch:   300  |  train loss: 3.0200555325
Epoch:   400  |  train loss: 2.9412685871
Epoch:   500  |  train loss: 2.8185938835
Epoch:   600  |  train loss: 2.7555477142
Epoch:   700  |  train loss: 2.6990592957
Epoch:   800  |  train loss: 2.6412829876
Epoch:   900  |  train loss: 2.6054498672
Epoch:  1000  |  train loss: 2.5678010941
Processing class: 11
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 3.3132200241
Epoch:   200  |  train loss: 3.0827540398
Epoch:   300  |  train loss: 2.9809915543
Epoch:   400  |  train loss: 2.8773292065
Epoch:   500  |  train loss: 2.7629612446
Epoch:   600  |  train loss: 2.7132359982
Epoch:   700  |  train loss: 2.6694709301
Epoch:   800  |  train loss: 2.6148835659
Epoch:   900  |  train loss: 2.5617127419
Epoch:  1000  |  train loss: 2.5153573990
Processing class: 12
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 3.1752764702
Epoch:   200  |  train loss: 3.1019235611
Epoch:   300  |  train loss: 2.9675899029
Epoch:   400  |  train loss: 2.8234626770
Epoch:   500  |  train loss: 2.7324354649
Epoch:   600  |  train loss: 2.6333768845
Epoch:   700  |  train loss: 2.5740386486
Epoch:   800  |  train loss: 2.5393175125
Epoch:   900  |  train loss: 2.5007202625
Epoch:  1000  |  train loss: 2.4514207363
Processing class: 13
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 3.0898125172
Epoch:   200  |  train loss: 2.9239524841
Epoch:   300  |  train loss: 2.7853892326
Epoch:   400  |  train loss: 2.6565427780
Epoch:   500  |  train loss: 2.5749907494
Epoch:   600  |  train loss: 2.5217696667
Epoch:   700  |  train loss: 2.4727983952
Epoch:   800  |  train loss: 2.4455222130
Epoch:   900  |  train loss: 2.4184481621
Epoch:  1000  |  train loss: 2.3831627846
Processing class: 14
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 3.4229997635
Epoch:   200  |  train loss: 3.4113064289
Epoch:   300  |  train loss: 3.2720006943
Epoch:   400  |  train loss: 3.1360768318
Epoch:   500  |  train loss: 3.0490098953
Epoch:   600  |  train loss: 2.9709483624
Epoch:   700  |  train loss: 2.9298854351
Epoch:   800  |  train loss: 2.8999528885
Epoch:   900  |  train loss: 2.8615431309
Epoch:  1000  |  train loss: 2.8182210922
Processing class: 15
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 3.0194593430
Epoch:   200  |  train loss: 2.9446961403
Epoch:   300  |  train loss: 2.7604058266
Epoch:   400  |  train loss: 2.6802371502
Epoch:   500  |  train loss: 2.6139985085
Epoch:   600  |  train loss: 2.5571778774
Epoch:   700  |  train loss: 2.5376866341
Epoch:   800  |  train loss: 2.4880041599
Epoch:   900  |  train loss: 2.4826630592
Epoch:  1000  |  train loss: 2.4460326672
Processing class: 16
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 3.1166735649
Epoch:   200  |  train loss: 2.8966340065
Epoch:   300  |  train loss: 2.8079828739
Epoch:   400  |  train loss: 2.7300111294
Epoch:   500  |  train loss: 2.7106778145
Epoch:   600  |  train loss: 2.6545005798
Epoch:   700  |  train loss: 2.5957374096
Epoch:   800  |  train loss: 2.5671552658
Epoch:   900  |  train loss: 2.5086316586
Epoch:  1000  |  train loss: 2.5057140827
Processing class: 17
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 3.0865081310
Epoch:   200  |  train loss: 3.0749339104
Epoch:   300  |  train loss: 3.0478954315
Epoch:   400  |  train loss: 2.9341106892
Epoch:   500  |  train loss: 2.7665032387
Epoch:   600  |  train loss: 2.7067917824
Epoch:   700  |  train loss: 2.6532397747
Epoch:   800  |  train loss: 2.6055519104
Epoch:   900  |  train loss: 2.5644232273
Epoch:  1000  |  train loss: 2.5308405876
Processing class: 18
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 3.0727104187
Epoch:   200  |  train loss: 3.0596611977
Epoch:   300  |  train loss: 2.7619874954
Epoch:   400  |  train loss: 2.6521531105
Epoch:   500  |  train loss: 2.5270206928
Epoch:   600  |  train loss: 2.4221362591
Epoch:   700  |  train loss: 2.3900916576
Epoch:   800  |  train loss: 2.3277405739
Epoch:   900  |  train loss: 2.2889012337
Epoch:  1000  |  train loss: 2.2541520119
Processing class: 19
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 2.9152268410
Epoch:   200  |  train loss: 2.8190383434
Epoch:   300  |  train loss: 2.7004830360
Epoch:   400  |  train loss: 2.6616435528
Epoch:   500  |  train loss: 2.5456140041
Epoch:   600  |  train loss: 2.5028619289
Epoch:   700  |  train loss: 2.4619854927
Epoch:   800  |  train loss: 2.4021734715
Epoch:   900  |  train loss: 2.3719953060
Epoch:  1000  |  train loss: 2.3355579376
Processing class: 20
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 3.0825690746
Epoch:   200  |  train loss: 2.9266092300
Epoch:   300  |  train loss: 2.8859805107
Epoch:   400  |  train loss: 2.7574020386
Epoch:   500  |  train loss: 2.7390151024
Epoch:   600  |  train loss: 2.6659588337
Epoch:   700  |  train loss: 2.6038858891
Epoch:   800  |  train loss: 2.5806819916
Epoch:   900  |  train loss: 2.5419651031
Epoch:  1000  |  train loss: 2.5309570789
Processing class: 21
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 3.1221381187
Epoch:   200  |  train loss: 2.9622422218
Epoch:   300  |  train loss: 2.6968992233
Epoch:   400  |  train loss: 2.5650036812
Epoch:   500  |  train loss: 2.4799880505
Epoch:   600  |  train loss: 2.4304683208
Epoch:   700  |  train loss: 2.3861642361
Epoch:   800  |  train loss: 2.3332273483
Epoch:   900  |  train loss: 2.2949497700
Epoch:  1000  |  train loss: 2.2367728233
Processing class: 22
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 3.2662489414
Epoch:   200  |  train loss: 3.2395316124
Epoch:   300  |  train loss: 3.1709553242
Epoch:   400  |  train loss: 2.9894217968
Epoch:   500  |  train loss: 2.9050990582
Epoch:   600  |  train loss: 2.8088139057
Epoch:   700  |  train loss: 2.7360857010
Epoch:   800  |  train loss: 2.6631743431
Epoch:   900  |  train loss: 2.6109373093
Epoch:  1000  |  train loss: 2.5947364807
Processing class: 23
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 3.0127368450
Epoch:   200  |  train loss: 2.9990308285
Epoch:   300  |  train loss: 2.8887844086
Epoch:   400  |  train loss: 2.7726258278
Epoch:   500  |  train loss: 2.7065417767
Epoch:   600  |  train loss: 2.5953476429
Epoch:   700  |  train loss: 2.5304986000
Epoch:   800  |  train loss: 2.4594304085
Epoch:   900  |  train loss: 2.4179564476
Epoch:  1000  |  train loss: 2.3530964375
Processing class: 24
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 3.0951928139
Epoch:   200  |  train loss: 3.0805738449
Epoch:   300  |  train loss: 3.0758480549
Epoch:   400  |  train loss: 2.9284688950
Epoch:   500  |  train loss: 2.8491130829
Epoch:   600  |  train loss: 2.7952806950
Epoch:   700  |  train loss: 2.7177239418
Epoch:   800  |  train loss: 2.6836145401
Epoch:   900  |  train loss: 2.6450509071
Epoch:  1000  |  train loss: 2.5901434898
Processing class: 25
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 3.1767226696
Epoch:   200  |  train loss: 3.0524195671
Epoch:   300  |  train loss: 2.9635069370
Epoch:   400  |  train loss: 2.7936932564
Epoch:   500  |  train loss: 2.6964203358
Epoch:   600  |  train loss: 2.6231884956
Epoch:   700  |  train loss: 2.5902234554
Epoch:   800  |  train loss: 2.5305709362
Epoch:   900  |  train loss: 2.4757315636
Epoch:  1000  |  train loss: 2.4533248425
Processing class: 26
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 2.9964002132
Epoch:   200  |  train loss: 2.9956970692
Epoch:   300  |  train loss: 3.0154419899
Epoch:   400  |  train loss: 2.9286971092
Epoch:   500  |  train loss: 2.8442830086
Epoch:   600  |  train loss: 2.6995012283
Epoch:   700  |  train loss: 2.6585472107
Epoch:   800  |  train loss: 2.6242271423
Epoch:   900  |  train loss: 2.5659490585
Epoch:  1000  |  train loss: 2.5415880680
Processing class: 27
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 3.2705101490
Epoch:   200  |  train loss: 3.0820316792
Epoch:   300  |  train loss: 3.0291395664
Epoch:   400  |  train loss: 2.8819620609
Epoch:   500  |  train loss: 2.7662614822
Epoch:   600  |  train loss: 2.7211347103
Epoch:   700  |  train loss: 2.6883255482
Epoch:   800  |  train loss: 2.6430574894
Epoch:   900  |  train loss: 2.5915018559
Epoch:  1000  |  train loss: 2.5103466988
Processing class: 28
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 3.1424064159
Epoch:   200  |  train loss: 3.1186075687
Epoch:   300  |  train loss: 2.9645735264
Epoch:   400  |  train loss: 2.8491295815
Epoch:   500  |  train loss: 2.7597879410
Epoch:   600  |  train loss: 2.7219231129
Epoch:   700  |  train loss: 2.6961152077
Epoch:   800  |  train loss: 2.6617977619
Epoch:   900  |  train loss: 2.6363230228
Epoch:  1000  |  train loss: 2.5940119267
Processing class: 29
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 3.1428296089
Epoch:   200  |  train loss: 2.9752219200
Epoch:   300  |  train loss: 2.8316968918
Epoch:   400  |  train loss: 2.6839397430
Epoch:   500  |  train loss: 2.6170532703
Epoch:   600  |  train loss: 2.5614568710
Epoch:   700  |  train loss: 2.4852527618
Epoch:   800  |  train loss: 2.4072566032
Epoch:   900  |  train loss: 2.3646209240
Epoch:  1000  |  train loss: 2.3239518166
Processing class: 30
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 2.9785567760
Epoch:   200  |  train loss: 2.9615057468
Epoch:   300  |  train loss: 2.7660773277
Epoch:   400  |  train loss: 2.7217893124
Epoch:   500  |  train loss: 2.6994894505
Epoch:   600  |  train loss: 2.6439735889
Epoch:   700  |  train loss: 2.5682030201
Epoch:   800  |  train loss: 2.5343286514
Epoch:   900  |  train loss: 2.5027216911
Epoch:  1000  |  train loss: 2.4616940975
Processing class: 31
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 3.1704608440
Epoch:   200  |  train loss: 3.0189402103
Epoch:   300  |  train loss: 2.8863383770
Epoch:   400  |  train loss: 2.8639281273
Epoch:   500  |  train loss: 2.7847693443
Epoch:   600  |  train loss: 2.7472341061
Epoch:   700  |  train loss: 2.6914809704
Epoch:   800  |  train loss: 2.6610884190
Epoch:   900  |  train loss: 2.6197759151
Epoch:  1000  |  train loss: 2.5681090832
Processing class: 32
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 3.3425961018
Epoch:   200  |  train loss: 3.2974997997
Epoch:   300  |  train loss: 3.1799260139
Epoch:   400  |  train loss: 3.0419009209
Epoch:   500  |  train loss: 2.9340420723
Epoch:   600  |  train loss: 2.8095283031
Epoch:   700  |  train loss: 2.7431099415
Epoch:   800  |  train loss: 2.6959316730
Epoch:   900  |  train loss: 2.6436629772
Epoch:  1000  |  train loss: 2.5971466064
Processing class: 33
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 3.1742442608
Epoch:   200  |  train loss: 3.1295638561
Epoch:   300  |  train loss: 2.9162169933
Epoch:   400  |  train loss: 2.7835763454
Epoch:   500  |  train loss: 2.6999021053
Epoch:   600  |  train loss: 2.6300479889
Epoch:   700  |  train loss: 2.5676943779
Epoch:   800  |  train loss: 2.5243979454
Epoch:   900  |  train loss: 2.4738915920
Epoch:  1000  |  train loss: 2.4198204994
Processing class: 34
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 3.2343563080
Epoch:   200  |  train loss: 3.1772038937
Epoch:   300  |  train loss: 2.9842517376
Epoch:   400  |  train loss: 2.8965170860
Epoch:   500  |  train loss: 2.7882196426
Epoch:   600  |  train loss: 2.7115478039
Epoch:   700  |  train loss: 2.6325097561
Epoch:   800  |  train loss: 2.5912890434
Epoch:   900  |  train loss: 2.5461984158
Epoch:  1000  |  train loss: 2.5019695282
Processing class: 35
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 3.5194520950
Epoch:   200  |  train loss: 3.2576760769
Epoch:   300  |  train loss: 3.1538458824
Epoch:   400  |  train loss: 3.0869256973
Epoch:   500  |  train loss: 2.9845942020
Epoch:   600  |  train loss: 2.9580834389
Epoch:   700  |  train loss: 2.8987287998
Epoch:   800  |  train loss: 2.8436252117
Epoch:   900  |  train loss: 2.8009294987
Epoch:  1000  |  train loss: 2.7499320030
Processing class: 36
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 2.8910035610
Epoch:   200  |  train loss: 2.7998809338
Epoch:   300  |  train loss: 2.6947389126
Epoch:   400  |  train loss: 2.6471952915
Epoch:   500  |  train loss: 2.5800939083
Epoch:   600  |  train loss: 2.4903480530
Epoch:   700  |  train loss: 2.4141357422
Epoch:   800  |  train loss: 2.3767916679
Epoch:   900  |  train loss: 2.3106411457
Epoch:  1000  |  train loss: 2.2746836662
Processing class: 37
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 3.3372135162
Epoch:   200  |  train loss: 3.2565230370
Epoch:   300  |  train loss: 3.0965502739
Epoch:   400  |  train loss: 2.9755121708
Epoch:   500  |  train loss: 2.8953417778
Epoch:   600  |  train loss: 2.8171004295
Epoch:   700  |  train loss: 2.7579554558
Epoch:   800  |  train loss: 2.7211699009
Epoch:   900  |  train loss: 2.6505887508
Epoch:  1000  |  train loss: 2.6162700176
Processing class: 38
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 2.9974629879
Epoch:   200  |  train loss: 2.9728285313
Epoch:   300  |  train loss: 2.9075454235
Epoch:   400  |  train loss: 2.8635469913
Epoch:   500  |  train loss: 2.7590429783
Epoch:   600  |  train loss: 2.6653133869
Epoch:   700  |  train loss: 2.5789760113
Epoch:   800  |  train loss: 2.5579260826
Epoch:   900  |  train loss: 2.5085418224
Epoch:  1000  |  train loss: 2.4902889729
Processing class: 39
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 3.1482629776
Epoch:   200  |  train loss: 3.0694903851
Epoch:   300  |  train loss: 2.9792205334
Epoch:   400  |  train loss: 2.8868865967
Epoch:   500  |  train loss: 2.7775014877
Epoch:   600  |  train loss: 2.7221385479
Epoch:   700  |  train loss: 2.6616021633
Epoch:   800  |  train loss: 2.6101276398
Epoch:   900  |  train loss: 2.5518859386
Epoch:  1000  |  train loss: 2.5037838459
Processing class: 40
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 3.0750321388
Epoch:   200  |  train loss: 2.8918078899
Epoch:   300  |  train loss: 2.8547996044
Epoch:   400  |  train loss: 2.7842108250
Epoch:   500  |  train loss: 2.7287417412
Epoch:   600  |  train loss: 2.6137951374
Epoch:   700  |  train loss: 2.5856534958
Epoch:   800  |  train loss: 2.5422075272
Epoch:   900  |  train loss: 2.5027917385
Epoch:  1000  |  train loss: 2.4825636864
Processing class: 41
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 2.8978352070
Epoch:   200  |  train loss: 2.9112028122
Epoch:   300  |  train loss: 2.7202569008
Epoch:   400  |  train loss: 2.5907358170
Epoch:   500  |  train loss: 2.5242879391
Epoch:   600  |  train loss: 2.4773183346
Epoch:   700  |  train loss: 2.4415036201
Epoch:   800  |  train loss: 2.3996183395
Epoch:   900  |  train loss: 2.3631058693
Epoch:  1000  |  train loss: 2.3333241940
Processing class: 42
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 3.2420229435
Epoch:   200  |  train loss: 3.1066360950
Epoch:   300  |  train loss: 2.9718412399
Epoch:   400  |  train loss: 2.8459655762
Epoch:   500  |  train loss: 2.7486224651
Epoch:   600  |  train loss: 2.6497049332
Epoch:   700  |  train loss: 2.5882800579
Epoch:   800  |  train loss: 2.5224050522
Epoch:   900  |  train loss: 2.4562209129
Epoch:  1000  |  train loss: 2.4058132648
Processing class: 43
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 3.3244528294
Epoch:   200  |  train loss: 3.1945903778
Epoch:   300  |  train loss: 3.0030878544
Epoch:   400  |  train loss: 2.9589231491
Epoch:   500  |  train loss: 2.9143653870
Epoch:   600  |  train loss: 2.8919659615
Epoch:   700  |  train loss: 2.8793709755
Epoch:   800  |  train loss: 2.8493126392
Epoch:   900  |  train loss: 2.7980000973
Epoch:  1000  |  train loss: 2.7782044888
Processing class: 44
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 3.3401386738
Epoch:   200  |  train loss: 3.1481895924
Epoch:   300  |  train loss: 3.0724434376
Epoch:   400  |  train loss: 2.9471168995
Epoch:   500  |  train loss: 2.8897841454
Epoch:   600  |  train loss: 2.8366879940
Epoch:   700  |  train loss: 2.7979027748
Epoch:   800  |  train loss: 2.7439659595
Epoch:   900  |  train loss: 2.6830922127
Epoch:  1000  |  train loss: 2.6450492382
Processing class: 45
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 3.0956133842
Epoch:   200  |  train loss: 2.9832082748
Epoch:   300  |  train loss: 2.8318309307
Epoch:   400  |  train loss: 2.8587878704
Epoch:   500  |  train loss: 2.8324708939
Epoch:   600  |  train loss: 2.7906959057
Epoch:   700  |  train loss: 2.7388598442
Epoch:   800  |  train loss: 2.7081264496
Epoch:   900  |  train loss: 2.6984111309
Epoch:  1000  |  train loss: 2.6797700405
Processing class: 46
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 2.9587365627
Epoch:   200  |  train loss: 2.8928590775
Epoch:   300  |  train loss: 2.7603310585
Epoch:   400  |  train loss: 2.7500891685
Epoch:   500  |  train loss: 2.7178520203
Epoch:   600  |  train loss: 2.6965537071
Epoch:   700  |  train loss: 2.6630403042
Epoch:   800  |  train loss: 2.6370592594
Epoch:   900  |  train loss: 2.5886762619
Epoch:  1000  |  train loss: 2.5681231499
Processing class: 47
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 3.2775287628
Epoch:   200  |  train loss: 3.1530416489
Epoch:   300  |  train loss: 2.9376627445
Epoch:   400  |  train loss: 2.8390009403
Epoch:   500  |  train loss: 2.7569439888
Epoch:   600  |  train loss: 2.6655240059
Epoch:   700  |  train loss: 2.5951908112
Epoch:   800  |  train loss: 2.5311461926
Epoch:   900  |  train loss: 2.4875129700
Epoch:  1000  |  train loss: 2.4458622932
Processing class: 48
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 3.2287214279
Epoch:   200  |  train loss: 3.1779807091
Epoch:   300  |  train loss: 2.9118323326
Epoch:   400  |  train loss: 2.8047733784
Epoch:   500  |  train loss: 2.8045197487
Epoch:   600  |  train loss: 2.7284207344
Epoch:   700  |  train loss: 2.6778040886
Epoch:   800  |  train loss: 2.6508996010
Epoch:   900  |  train loss: 2.6409110546
Epoch:  1000  |  train loss: 2.6064360142
Processing class: 49
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 3.2883082390
Epoch:   200  |  train loss: 3.1731681347
Epoch:   300  |  train loss: 3.0302398682
Epoch:   400  |  train loss: 2.9360789299
Epoch:   500  |  train loss: 2.8325809002
Epoch:   600  |  train loss: 2.7782912254
Epoch:   700  |  train loss: 2.7095565796
Epoch:   800  |  train loss: 2.6645400047
Epoch:   900  |  train loss: 2.6217844009
Epoch:  1000  |  train loss: 2.5805853367
/home/z1165703/FeCAM/models/base.py:87: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  vec = torch.tensor(vec).cuda()
2024-03-05 17:39:43,515 [trainer.py] => CNN: {'total': 83.44, '00-09': 87.7, '10-19': 79.6, '20-29': 84.9, '30-39': 81.0, '40-49': 84.0, 'old': 0, 'new': 83.44}
2024-03-05 17:39:43,516 [trainer.py] => No NME accuracy
2024-03-05 17:39:43,516 [trainer.py] => FeCAM: {'total': 82.9, '00-09': 86.2, '10-19': 79.7, '20-29': 85.0, '30-39': 80.6, '40-49': 83.0, 'old': 0, 'new': 82.9}
2024-03-05 17:39:43,516 [trainer.py] => CNN top1 curve: [83.44]
2024-03-05 17:39:43,516 [trainer.py] => CNN top5 curve: [96.5]
2024-03-05 17:39:43,516 [trainer.py] => FeCAM top1 curve: [82.9]
2024-03-05 17:39:43,516 [trainer.py] => FeCAM top5 curve: [95.84]

2024-03-05 17:39:43,526 [fecam.py] => Learning on 50-60
Processing class: 50
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 3.7028900146
Epoch:   200  |  train loss: 3.4912862778
Epoch:   300  |  train loss: 3.3642478466
Epoch:   400  |  train loss: 3.2423252106
Epoch:   500  |  train loss: 3.1068406105
Epoch:   600  |  train loss: 2.9955232143
Epoch:   700  |  train loss: 2.9226696014
Epoch:   800  |  train loss: 2.8588420391
Epoch:   900  |  train loss: 2.8146632671
Epoch:  1000  |  train loss: 2.7692897320
Processing class: 51
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 3.7363771915
Epoch:   200  |  train loss: 3.5049521446
Epoch:   300  |  train loss: 3.3716999054
Epoch:   400  |  train loss: 3.2710383415
Epoch:   500  |  train loss: 3.1198404789
Epoch:   600  |  train loss: 2.9977184296
Epoch:   700  |  train loss: 2.8995959759
Epoch:   800  |  train loss: 2.8438736439
Epoch:   900  |  train loss: 2.8051826954
Epoch:  1000  |  train loss: 2.7316699982
Processing class: 52
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 3.9404382706
Epoch:   200  |  train loss: 3.8412996292
Epoch:   300  |  train loss: 3.6975137234
Epoch:   400  |  train loss: 3.5778930187
Epoch:   500  |  train loss: 3.4632896900
Epoch:   600  |  train loss: 3.3508512497
Epoch:   700  |  train loss: 3.2785345554
Epoch:   800  |  train loss: 3.2002295017
Epoch:   900  |  train loss: 3.1247459412
Epoch:  1000  |  train loss: 3.0434084415
Processing class: 53
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 3.4652284145
Epoch:   200  |  train loss: 3.2442387581
Epoch:   300  |  train loss: 3.1388457775
Epoch:   400  |  train loss: 3.0270427227
Epoch:   500  |  train loss: 2.9787172794
Epoch:   600  |  train loss: 2.9358128071
Epoch:   700  |  train loss: 2.8903388500
Epoch:   800  |  train loss: 2.8112255096
Epoch:   900  |  train loss: 2.7858078957
Epoch:  1000  |  train loss: 2.7342511177
Processing class: 54
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 3.2391755104
Epoch:   200  |  train loss: 3.0620023251
Epoch:   300  |  train loss: 2.8915509701
Epoch:   400  |  train loss: 2.8055643559
Epoch:   500  |  train loss: 2.7277452946
Epoch:   600  |  train loss: 2.6717064857
Epoch:   700  |  train loss: 2.6116041660
Epoch:   800  |  train loss: 2.5711907387
Epoch:   900  |  train loss: 2.5349552155
Epoch:  1000  |  train loss: 2.4778318405
Processing class: 55
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 4.0149741173
Epoch:   200  |  train loss: 3.9423465252
Epoch:   300  |  train loss: 3.8439279079
Epoch:   400  |  train loss: 3.7662114143
Epoch:   500  |  train loss: 3.6894988537
Epoch:   600  |  train loss: 3.5853104115
Epoch:   700  |  train loss: 3.5040357113
Epoch:   800  |  train loss: 3.4384936333
Epoch:   900  |  train loss: 3.3337579250
Epoch:  1000  |  train loss: 3.2779891968
Processing class: 56
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 3.6347863674
Epoch:   200  |  train loss: 3.3568006516
Epoch:   300  |  train loss: 3.0668240070
Epoch:   400  |  train loss: 2.9150096893
Epoch:   500  |  train loss: 2.7818551064
Epoch:   600  |  train loss: 2.6682199955
Epoch:   700  |  train loss: 2.5850790024
Epoch:   800  |  train loss: 2.5004904270
Epoch:   900  |  train loss: 2.4531560898
Epoch:  1000  |  train loss: 2.4073537350
Processing class: 57
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 3.9996989250
Epoch:   200  |  train loss: 3.9308523655
Epoch:   300  |  train loss: 3.8276525021
Epoch:   400  |  train loss: 3.7207620144
Epoch:   500  |  train loss: 3.6242445946
Epoch:   600  |  train loss: 3.5294223309
Epoch:   700  |  train loss: 3.4798712730
Epoch:   800  |  train loss: 3.4017404079
Epoch:   900  |  train loss: 3.3360812187
Epoch:  1000  |  train loss: 3.2367530346
Processing class: 58
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 3.7464112759
Epoch:   200  |  train loss: 3.3329605103
Epoch:   300  |  train loss: 3.1460349083
Epoch:   400  |  train loss: 2.9722034931
Epoch:   500  |  train loss: 2.8709763527
Epoch:   600  |  train loss: 2.8031238079
Epoch:   700  |  train loss: 2.7180775642
Epoch:   800  |  train loss: 2.6679672241
Epoch:   900  |  train loss: 2.6072850227
Epoch:  1000  |  train loss: 2.5496435642
Processing class: 59
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 3.9317805767
Epoch:   200  |  train loss: 3.7931955814
Epoch:   300  |  train loss: 3.5898453712
Epoch:   400  |  train loss: 3.4279539108
Epoch:   500  |  train loss: 3.3080142021
Epoch:   600  |  train loss: 3.2263116837
Epoch:   700  |  train loss: 3.1572023869
Epoch:   800  |  train loss: 3.0971415520
Epoch:   900  |  train loss: 3.0334553242
Epoch:  1000  |  train loss: 2.9879619122
2024-03-05 17:45:21,395 [trainer.py] => CNN: {'total': 71.75, '00-09': 82.2, '10-19': 72.8, '20-29': 78.5, '30-39': 74.9, '40-49': 68.4, '50-59': 53.7, 'old': 75.36, 'new': 53.7}
2024-03-05 17:45:21,395 [trainer.py] => No NME accuracy
2024-03-05 17:45:21,395 [trainer.py] => FeCAM: {'total': 72.78, '00-09': 82.6, '10-19': 74.0, '20-29': 81.0, '30-39': 76.4, '40-49': 76.8, '50-59': 45.9, 'old': 78.16, 'new': 45.9}
2024-03-05 17:45:21,395 [trainer.py] => CNN top1 curve: [83.44, 71.75]
2024-03-05 17:45:21,395 [trainer.py] => CNN top5 curve: [96.5, 89.6]
2024-03-05 17:45:21,395 [trainer.py] => FeCAM top1 curve: [82.9, 72.78]
2024-03-05 17:45:21,395 [trainer.py] => FeCAM top5 curve: [95.84, 91.18]

2024-03-05 17:45:21,403 [fecam.py] => Learning on 60-70
Processing class: 60
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 3.8966263771
Epoch:   200  |  train loss: 3.7444801807
Epoch:   300  |  train loss: 3.5616365910
Epoch:   400  |  train loss: 3.4016354561
Epoch:   500  |  train loss: 3.2750099659
Epoch:   600  |  train loss: 3.1506019592
Epoch:   700  |  train loss: 3.0746627331
Epoch:   800  |  train loss: 2.9507947445
Epoch:   900  |  train loss: 2.8858413219
Epoch:  1000  |  train loss: 2.8241456509
Processing class: 61
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 3.6101385117
Epoch:   200  |  train loss: 3.3147315979
Epoch:   300  |  train loss: 3.0986378193
Epoch:   400  |  train loss: 2.9137791157
Epoch:   500  |  train loss: 2.8142228603
Epoch:   600  |  train loss: 2.7085223675
Epoch:   700  |  train loss: 2.6372220516
Epoch:   800  |  train loss: 2.5736083508
Epoch:   900  |  train loss: 2.4976546288
Epoch:  1000  |  train loss: 2.4595578194
Processing class: 62
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 4.0305561066
Epoch:   200  |  train loss: 3.9310115337
Epoch:   300  |  train loss: 3.8633049011
Epoch:   400  |  train loss: 3.7899901390
Epoch:   500  |  train loss: 3.7072822094
Epoch:   600  |  train loss: 3.6552670956
Epoch:   700  |  train loss: 3.5907834053
Epoch:   800  |  train loss: 3.5184218884
Epoch:   900  |  train loss: 3.4406140327
Epoch:  1000  |  train loss: 3.3832700253
Processing class: 63
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 3.7841723442
Epoch:   200  |  train loss: 3.6766665459
Epoch:   300  |  train loss: 3.4108822346
Epoch:   400  |  train loss: 3.2216794968
Epoch:   500  |  train loss: 3.0823148251
Epoch:   600  |  train loss: 2.9990602493
Epoch:   700  |  train loss: 2.8951906681
Epoch:   800  |  train loss: 2.8625036716
Epoch:   900  |  train loss: 2.7779526234
Epoch:  1000  |  train loss: 2.7314483166
Processing class: 64
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 3.6747498989
Epoch:   200  |  train loss: 3.2285991192
Epoch:   300  |  train loss: 3.0387857437
Epoch:   400  |  train loss: 2.8997967720
Epoch:   500  |  train loss: 2.7504183292
Epoch:   600  |  train loss: 2.6392785072
Epoch:   700  |  train loss: 2.5465114594
Epoch:   800  |  train loss: 2.4936005116
Epoch:   900  |  train loss: 2.4548570156
Epoch:  1000  |  train loss: 2.3868065357
Processing class: 65
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 3.6241613865
Epoch:   200  |  train loss: 3.3169294357
Epoch:   300  |  train loss: 3.0463633060
Epoch:   400  |  train loss: 2.8970926762
Epoch:   500  |  train loss: 2.7798452377
Epoch:   600  |  train loss: 2.7007145882
Epoch:   700  |  train loss: 2.6187894821
Epoch:   800  |  train loss: 2.5683012486
Epoch:   900  |  train loss: 2.4905822277
Epoch:  1000  |  train loss: 2.4560893536
Processing class: 66
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 3.8243429661
Epoch:   200  |  train loss: 3.6435099602
Epoch:   300  |  train loss: 3.4663011551
Epoch:   400  |  train loss: 3.3363719940
Epoch:   500  |  train loss: 3.2706489563
Epoch:   600  |  train loss: 3.1956402779
Epoch:   700  |  train loss: 3.1525539398
Epoch:   800  |  train loss: 3.1088044643
Epoch:   900  |  train loss: 3.0584330559
Epoch:  1000  |  train loss: 3.0185040474
Processing class: 67
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 3.8474474430
Epoch:   200  |  train loss: 3.6953389645
Epoch:   300  |  train loss: 3.5487157822
Epoch:   400  |  train loss: 3.4426726341
Epoch:   500  |  train loss: 3.3530545235
Epoch:   600  |  train loss: 3.2748013973
Epoch:   700  |  train loss: 3.2040800571
Epoch:   800  |  train loss: 3.1508330345
Epoch:   900  |  train loss: 3.0869247437
Epoch:  1000  |  train loss: 3.0343807220
Processing class: 68
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 3.5531452179
Epoch:   200  |  train loss: 3.4307767391
Epoch:   300  |  train loss: 3.1583582401
Epoch:   400  |  train loss: 3.0395724297
Epoch:   500  |  train loss: 2.9560880661
Epoch:   600  |  train loss: 2.8727406025
Epoch:   700  |  train loss: 2.8140551567
Epoch:   800  |  train loss: 2.7488894939
Epoch:   900  |  train loss: 2.6970161915
Epoch:  1000  |  train loss: 2.6718019485
Processing class: 69
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 2.7766178608
Epoch:   200  |  train loss: 2.6078063011
Epoch:   300  |  train loss: 2.4424021244
Epoch:   400  |  train loss: 2.3323181152
Epoch:   500  |  train loss: 2.2586392403
Epoch:   600  |  train loss: 2.2032299042
Epoch:   700  |  train loss: 2.1833469868
Epoch:   800  |  train loss: 2.1475138903
Epoch:   900  |  train loss: 2.1498367310
Epoch:  1000  |  train loss: 2.1024067402
2024-03-05 17:51:54,153 [trainer.py] => CNN: {'total': 64.66, '00-09': 75.0, '10-19': 70.1, '20-29': 77.1, '30-39': 70.6, '40-49': 64.0, '50-59': 45.8, '60-69': 50.0, 'old': 67.1, 'new': 50.0}
2024-03-05 17:51:54,154 [trainer.py] => No NME accuracy
2024-03-05 17:51:54,154 [trainer.py] => FeCAM: {'total': 67.27, '00-09': 79.9, '10-19': 70.6, '20-29': 79.1, '30-39': 71.9, '40-49': 74.1, '50-59': 41.2, '60-69': 54.1, 'old': 69.47, 'new': 54.1}
2024-03-05 17:51:54,154 [trainer.py] => CNN top1 curve: [83.44, 71.75, 64.66]
2024-03-05 17:51:54,154 [trainer.py] => CNN top5 curve: [96.5, 89.6, 86.54]
2024-03-05 17:51:54,154 [trainer.py] => FeCAM top1 curve: [82.9, 72.78, 67.27]
2024-03-05 17:51:54,154 [trainer.py] => FeCAM top5 curve: [95.84, 91.18, 87.21]

2024-03-05 17:51:54,159 [fecam.py] => Learning on 70-80
Processing class: 70
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 3.7091493130
Epoch:   200  |  train loss: 3.5439586163
Epoch:   300  |  train loss: 3.3104156971
Epoch:   400  |  train loss: 3.1868088722
Epoch:   500  |  train loss: 3.0457638264
Epoch:   600  |  train loss: 2.9497260571
Epoch:   700  |  train loss: 2.8575901985
Epoch:   800  |  train loss: 2.7453687191
Epoch:   900  |  train loss: 2.7077872276
Epoch:  1000  |  train loss: 2.6533630848
Processing class: 71
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 3.8930564404
Epoch:   200  |  train loss: 3.7705135345
Epoch:   300  |  train loss: 3.5307545662
Epoch:   400  |  train loss: 3.3953333378
Epoch:   500  |  train loss: 3.2744397163
Epoch:   600  |  train loss: 3.1686077118
Epoch:   700  |  train loss: 3.0681353569
Epoch:   800  |  train loss: 2.9871685028
Epoch:   900  |  train loss: 2.9184576988
Epoch:  1000  |  train loss: 2.8411190987
Processing class: 72
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 3.8126018047
Epoch:   200  |  train loss: 3.6704650402
Epoch:   300  |  train loss: 3.5001021385
Epoch:   400  |  train loss: 3.3547637939
Epoch:   500  |  train loss: 3.2680763245
Epoch:   600  |  train loss: 3.1635286331
Epoch:   700  |  train loss: 3.0909235001
Epoch:   800  |  train loss: 3.0103903770
Epoch:   900  |  train loss: 2.9452042103
Epoch:  1000  |  train loss: 2.8991507530
Processing class: 73
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 3.8141457558
Epoch:   200  |  train loss: 3.6140611649
Epoch:   300  |  train loss: 3.3871105671
Epoch:   400  |  train loss: 3.2618341446
Epoch:   500  |  train loss: 3.2212331295
Epoch:   600  |  train loss: 3.1548192024
Epoch:   700  |  train loss: 3.0770598412
Epoch:   800  |  train loss: 3.0194070816
Epoch:   900  |  train loss: 2.9595560074
Epoch:  1000  |  train loss: 2.9181952477
Processing class: 74
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 4.0026105881
Epoch:   200  |  train loss: 3.9591225147
Epoch:   300  |  train loss: 3.8066959381
Epoch:   400  |  train loss: 3.6583121300
Epoch:   500  |  train loss: 3.5336678505
Epoch:   600  |  train loss: 3.4225467682
Epoch:   700  |  train loss: 3.3661233425
Epoch:   800  |  train loss: 3.2599191189
Epoch:   900  |  train loss: 3.1927203178
Epoch:  1000  |  train loss: 3.1474833012
Processing class: 75
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 3.8443533421
Epoch:   200  |  train loss: 3.6889560699
Epoch:   300  |  train loss: 3.5112478733
Epoch:   400  |  train loss: 3.3937749863
Epoch:   500  |  train loss: 3.3008684635
Epoch:   600  |  train loss: 3.2212083817
Epoch:   700  |  train loss: 3.1065823078
Epoch:   800  |  train loss: 3.0657464027
Epoch:   900  |  train loss: 3.0088255405
Epoch:  1000  |  train loss: 2.9655045033
Processing class: 76
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 2.8294047832
Epoch:   200  |  train loss: 2.5949215412
Epoch:   300  |  train loss: 2.4375916958
Epoch:   400  |  train loss: 2.3614959240
Epoch:   500  |  train loss: 2.3148614883
Epoch:   600  |  train loss: 2.2699517250
Epoch:   700  |  train loss: 2.2482663631
Epoch:   800  |  train loss: 2.1805984974
Epoch:   900  |  train loss: 2.1649327755
Epoch:  1000  |  train loss: 2.1465156555
Processing class: 77
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 3.6057452202
Epoch:   200  |  train loss: 3.4187271118
Epoch:   300  |  train loss: 3.2037033558
Epoch:   400  |  train loss: 3.0682141781
Epoch:   500  |  train loss: 2.9093043327
Epoch:   600  |  train loss: 2.7822263241
Epoch:   700  |  train loss: 2.6927872658
Epoch:   800  |  train loss: 2.6293663979
Epoch:   900  |  train loss: 2.5615439892
Epoch:  1000  |  train loss: 2.4961771011
Processing class: 78
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 3.7266538620
Epoch:   200  |  train loss: 3.4407561302
Epoch:   300  |  train loss: 3.3170341492
Epoch:   400  |  train loss: 3.1714222431
Epoch:   500  |  train loss: 3.0261208057
Epoch:   600  |  train loss: 2.9426843643
Epoch:   700  |  train loss: 2.8722826004
Epoch:   800  |  train loss: 2.8051945210
Epoch:   900  |  train loss: 2.7616434097
Epoch:  1000  |  train loss: 2.7135129452
Processing class: 79
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 4.0212171555
Epoch:   200  |  train loss: 3.8849573135
Epoch:   300  |  train loss: 3.8017523766
Epoch:   400  |  train loss: 3.6752107143
Epoch:   500  |  train loss: 3.5812303066
Epoch:   600  |  train loss: 3.5025013447
Epoch:   700  |  train loss: 3.4166131496
Epoch:   800  |  train loss: 3.3339392662
Epoch:   900  |  train loss: 3.2679110050
Epoch:  1000  |  train loss: 3.1812352657
2024-03-05 17:59:28,029 [trainer.py] => CNN: {'total': 59.18, '00-09': 73.6, '10-19': 68.4, '20-29': 76.9, '30-39': 69.1, '40-49': 60.6, '50-59': 37.2, '60-69': 45.7, '70-79': 41.9, 'old': 61.64, 'new': 41.9}
2024-03-05 17:59:28,030 [trainer.py] => No NME accuracy
2024-03-05 17:59:28,030 [trainer.py] => FeCAM: {'total': 61.72, '00-09': 78.1, '10-19': 69.3, '20-29': 78.0, '30-39': 69.5, '40-49': 72.1, '50-59': 35.7, '60-69': 49.1, '70-79': 42.0, 'old': 64.54, 'new': 42.0}
2024-03-05 17:59:28,030 [trainer.py] => CNN top1 curve: [83.44, 71.75, 64.66, 59.18]
2024-03-05 17:59:28,030 [trainer.py] => CNN top5 curve: [96.5, 89.6, 86.54, 84.09]
2024-03-05 17:59:28,030 [trainer.py] => FeCAM top1 curve: [82.9, 72.78, 67.27, 61.72]
2024-03-05 17:59:28,030 [trainer.py] => FeCAM top5 curve: [95.84, 91.18, 87.21, 84.09]

2024-03-05 17:59:28,038 [fecam.py] => Learning on 80-90
Processing class: 80
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 3.1986872196
Epoch:   200  |  train loss: 2.7724967480
Epoch:   300  |  train loss: 2.6254081249
Epoch:   400  |  train loss: 2.5352597237
Epoch:   500  |  train loss: 2.4486093998
Epoch:   600  |  train loss: 2.4014006615
Epoch:   700  |  train loss: 2.3346579075
Epoch:   800  |  train loss: 2.3257342815
Epoch:   900  |  train loss: 2.2897389412
Epoch:  1000  |  train loss: 2.2653814793
Processing class: 81
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 3.6284787655
Epoch:   200  |  train loss: 3.4042863369
Epoch:   300  |  train loss: 3.2302856922
Epoch:   400  |  train loss: 3.1360858440
Epoch:   500  |  train loss: 3.0291541576
Epoch:   600  |  train loss: 2.9636788845
Epoch:   700  |  train loss: 2.8855923176
Epoch:   800  |  train loss: 2.8455605507
Epoch:   900  |  train loss: 2.7771747589
Epoch:  1000  |  train loss: 2.7162951946
Processing class: 82
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 3.4470650673
Epoch:   200  |  train loss: 3.2005478382
Epoch:   300  |  train loss: 3.0452416420
Epoch:   400  |  train loss: 2.9040287495
Epoch:   500  |  train loss: 2.7908456802
Epoch:   600  |  train loss: 2.7086872578
Epoch:   700  |  train loss: 2.6471159458
Epoch:   800  |  train loss: 2.5984974384
Epoch:   900  |  train loss: 2.5453084946
Epoch:  1000  |  train loss: 2.4958538532
Processing class: 83
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 3.3469527721
Epoch:   200  |  train loss: 2.9404738426
Epoch:   300  |  train loss: 2.7231923580
Epoch:   400  |  train loss: 2.5726046562
Epoch:   500  |  train loss: 2.4869949818
Epoch:   600  |  train loss: 2.4092721462
Epoch:   700  |  train loss: 2.3794607639
Epoch:   800  |  train loss: 2.3503523350
Epoch:   900  |  train loss: 2.3000469685
Epoch:  1000  |  train loss: 2.2572009563
Processing class: 84
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 3.7880561352
Epoch:   200  |  train loss: 3.4401301384
Epoch:   300  |  train loss: 3.2679494858
Epoch:   400  |  train loss: 3.0960422993
Epoch:   500  |  train loss: 2.9881736279
Epoch:   600  |  train loss: 2.8891504288
Epoch:   700  |  train loss: 2.8285444260
Epoch:   800  |  train loss: 2.7831715107
Epoch:   900  |  train loss: 2.7037078381
Epoch:  1000  |  train loss: 2.6464637280
Processing class: 85
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 3.8180293560
Epoch:   200  |  train loss: 3.5140693665
Epoch:   300  |  train loss: 3.3054335117
Epoch:   400  |  train loss: 3.1462823391
Epoch:   500  |  train loss: 3.0527019501
Epoch:   600  |  train loss: 2.9675181389
Epoch:   700  |  train loss: 2.9184118748
Epoch:   800  |  train loss: 2.8414721966
Epoch:   900  |  train loss: 2.7864840984
Epoch:  1000  |  train loss: 2.7468132019
Processing class: 86
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 4.0210685253
Epoch:   200  |  train loss: 3.8564821243
Epoch:   300  |  train loss: 3.6547158718
Epoch:   400  |  train loss: 3.4852196217
Epoch:   500  |  train loss: 3.3717434883
Epoch:   600  |  train loss: 3.2734549046
Epoch:   700  |  train loss: 3.1855699062
Epoch:   800  |  train loss: 3.1075847149
Epoch:   900  |  train loss: 3.0463906765
Epoch:  1000  |  train loss: 3.0114778042
Processing class: 87
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 3.4877131462
Epoch:   200  |  train loss: 3.1471477509
Epoch:   300  |  train loss: 3.0120513916
Epoch:   400  |  train loss: 2.8508516788
Epoch:   500  |  train loss: 2.7471604347
Epoch:   600  |  train loss: 2.7048500538
Epoch:   700  |  train loss: 2.6532075405
Epoch:   800  |  train loss: 2.5976172447
Epoch:   900  |  train loss: 2.5382239819
Epoch:  1000  |  train loss: 2.5181739807
Processing class: 88
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 3.7315401077
Epoch:   200  |  train loss: 3.5427096367
Epoch:   300  |  train loss: 3.3482652664
Epoch:   400  |  train loss: 3.1942188740
Epoch:   500  |  train loss: 3.0408829689
Epoch:   600  |  train loss: 2.9098922253
Epoch:   700  |  train loss: 2.8406478882
Epoch:   800  |  train loss: 2.7682955265
Epoch:   900  |  train loss: 2.7145868778
Epoch:  1000  |  train loss: 2.6510765553
Processing class: 89
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 3.6767519951
Epoch:   200  |  train loss: 3.4400391579
Epoch:   300  |  train loss: 3.2099195957
Epoch:   400  |  train loss: 3.0952699661
Epoch:   500  |  train loss: 2.9847257614
Epoch:   600  |  train loss: 2.8718070507
Epoch:   700  |  train loss: 2.7779099941
Epoch:   800  |  train loss: 2.7331429482
Epoch:   900  |  train loss: 2.6589872837
Epoch:  1000  |  train loss: 2.6070663452
2024-03-05 18:08:13,704 [trainer.py] => CNN: {'total': 54.08, '00-09': 67.9, '10-19': 63.6, '20-29': 73.2, '30-39': 68.8, '40-49': 57.2, '50-59': 32.6, '60-69': 40.9, '70-79': 38.1, '80-89': 44.4, 'old': 55.29, 'new': 44.4}
2024-03-05 18:08:13,705 [trainer.py] => No NME accuracy
2024-03-05 18:08:13,705 [trainer.py] => FeCAM: {'total': 57.66, '00-09': 76.1, '10-19': 65.8, '20-29': 75.3, '30-39': 67.9, '40-49': 68.5, '50-59': 33.0, '60-69': 45.0, '70-79': 39.0, '80-89': 48.3, 'old': 58.82, 'new': 48.3}
2024-03-05 18:08:13,705 [trainer.py] => CNN top1 curve: [83.44, 71.75, 64.66, 59.18, 54.08]
2024-03-05 18:08:13,705 [trainer.py] => CNN top5 curve: [96.5, 89.6, 86.54, 84.09, 81.74]
2024-03-05 18:08:13,705 [trainer.py] => FeCAM top1 curve: [82.9, 72.78, 67.27, 61.72, 57.66]
2024-03-05 18:08:13,705 [trainer.py] => FeCAM top5 curve: [95.84, 91.18, 87.21, 84.09, 81.63]

2024-03-05 18:08:13,714 [fecam.py] => Learning on 90-100
Processing class: 90
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 3.8080011368
Epoch:   200  |  train loss: 3.6550087452
Epoch:   300  |  train loss: 3.3972122192
Epoch:   400  |  train loss: 3.2051970482
Epoch:   500  |  train loss: 3.0516589165
Epoch:   600  |  train loss: 2.9264510632
Epoch:   700  |  train loss: 2.8078743458
Epoch:   800  |  train loss: 2.7370165348
Epoch:   900  |  train loss: 2.6882364750
Epoch:  1000  |  train loss: 2.6443386555
Processing class: 91
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 2.8576840878
Epoch:   200  |  train loss: 2.7043410301
Epoch:   300  |  train loss: 2.5715581894
Epoch:   400  |  train loss: 2.5158136368
Epoch:   500  |  train loss: 2.4261742115
Epoch:   600  |  train loss: 2.3667816162
Epoch:   700  |  train loss: 2.2801126480
Epoch:   800  |  train loss: 2.2142217636
Epoch:   900  |  train loss: 2.1647732258
Epoch:  1000  |  train loss: 2.1329459906
Processing class: 92
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 3.6285002708
Epoch:   200  |  train loss: 3.2393013477
Epoch:   300  |  train loss: 3.0990468979
Epoch:   400  |  train loss: 2.9899833679
Epoch:   500  |  train loss: 2.8558598042
Epoch:   600  |  train loss: 2.7171587944
Epoch:   700  |  train loss: 2.6204976082
Epoch:   800  |  train loss: 2.5289252281
Epoch:   900  |  train loss: 2.4768073082
Epoch:  1000  |  train loss: 2.4267270565
Processing class: 93
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 3.7318249702
Epoch:   200  |  train loss: 3.3948257446
Epoch:   300  |  train loss: 3.1974307537
Epoch:   400  |  train loss: 3.0748538017
Epoch:   500  |  train loss: 2.9419254780
Epoch:   600  |  train loss: 2.8207033634
Epoch:   700  |  train loss: 2.7598283291
Epoch:   800  |  train loss: 2.6824778080
Epoch:   900  |  train loss: 2.6259374619
Epoch:  1000  |  train loss: 2.5552242756
Processing class: 94
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 3.3990598679
Epoch:   200  |  train loss: 3.1855224133
Epoch:   300  |  train loss: 2.9115839958
Epoch:   400  |  train loss: 2.7654358864
Epoch:   500  |  train loss: 2.6354272842
Epoch:   600  |  train loss: 2.5687613487
Epoch:   700  |  train loss: 2.5120452881
Epoch:   800  |  train loss: 2.4128262043
Epoch:   900  |  train loss: 2.3790945053
Epoch:  1000  |  train loss: 2.3227693558
Processing class: 95
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 3.8802498341
Epoch:   200  |  train loss: 3.7817704678
Epoch:   300  |  train loss: 3.6296826363
Epoch:   400  |  train loss: 3.4572181225
Epoch:   500  |  train loss: 3.3277498722
Epoch:   600  |  train loss: 3.2229716301
Epoch:   700  |  train loss: 3.1340272427
Epoch:   800  |  train loss: 3.0469211578
Epoch:   900  |  train loss: 2.9841365337
Epoch:  1000  |  train loss: 2.9312301159
Processing class: 96
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 3.9325983047
Epoch:   200  |  train loss: 3.7455865383
Epoch:   300  |  train loss: 3.5395798683
Epoch:   400  |  train loss: 3.4262400150
Epoch:   500  |  train loss: 3.2940693855
Epoch:   600  |  train loss: 3.1970412731
Epoch:   700  |  train loss: 3.1104597569
Epoch:   800  |  train loss: 3.0184852600
Epoch:   900  |  train loss: 2.9515630245
Epoch:  1000  |  train loss: 2.8815816879
Processing class: 97
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 3.7296934128
Epoch:   200  |  train loss: 3.3994146824
Epoch:   300  |  train loss: 3.1704648495
Epoch:   400  |  train loss: 3.0471935272
Epoch:   500  |  train loss: 2.9457787037
Epoch:   600  |  train loss: 2.8607604027
Epoch:   700  |  train loss: 2.7889848709
Epoch:   800  |  train loss: 2.7210307121
Epoch:   900  |  train loss: 2.6598256111
Epoch:  1000  |  train loss: 2.6011090755
Processing class: 98
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 3.4116944313
Epoch:   200  |  train loss: 3.1797030926
Epoch:   300  |  train loss: 2.9400651932
Epoch:   400  |  train loss: 2.7939985752
Epoch:   500  |  train loss: 2.6721756458
Epoch:   600  |  train loss: 2.5820657253
Epoch:   700  |  train loss: 2.4999773026
Epoch:   800  |  train loss: 2.4574548244
Epoch:   900  |  train loss: 2.4132527828
Epoch:  1000  |  train loss: 2.3736347675
Processing class: 99
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 3.8150171280
Epoch:   200  |  train loss: 3.5957458019
Epoch:   300  |  train loss: 3.3390146732
Epoch:   400  |  train loss: 3.1719069004
Epoch:   500  |  train loss: 3.0188573360
Epoch:   600  |  train loss: 2.9134653091
Epoch:   700  |  train loss: 2.8359842300
Epoch:   800  |  train loss: 2.7833142757
Epoch:   900  |  train loss: 2.7129221439
Epoch:  1000  |  train loss: 2.6602620602
2024-03-05 18:18:20,313 [trainer.py] => CNN: {'total': 50.36, '00-09': 57.7, '10-19': 63.6, '20-29': 71.4, '30-39': 68.5, '40-49': 56.8, '50-59': 29.6, '60-69': 38.9, '70-79': 36.6, '80-89': 42.7, '90-99': 37.8, 'old': 51.76, 'new': 37.8}
2024-03-05 18:18:20,314 [trainer.py] => No NME accuracy
2024-03-05 18:18:20,314 [trainer.py] => FeCAM: {'total': 54.74, '00-09': 73.3, '10-19': 65.1, '20-29': 74.1, '30-39': 67.2, '40-49': 67.1, '50-59': 30.8, '60-69': 43.3, '70-79': 37.2, '80-89': 46.9, '90-99': 42.4, 'old': 56.11, 'new': 42.4}
2024-03-05 18:18:20,316 [trainer.py] => CNN top1 curve: [83.44, 71.75, 64.66, 59.18, 54.08, 50.36]
2024-03-05 18:18:20,316 [trainer.py] => CNN top5 curve: [96.5, 89.6, 86.54, 84.09, 81.74, 79.63]
2024-03-05 18:18:20,316 [trainer.py] => FeCAM top1 curve: [82.9, 72.78, 67.27, 61.72, 57.66, 54.74]
2024-03-05 18:18:20,316 [trainer.py] => FeCAM top5 curve: [95.84, 91.18, 87.21, 84.09, 81.63, 79.21]

=========================================
2024-03-05 18:18:28,718 [trainer.py] => config: ./exps/FeCAM_cifar100.json
2024-03-05 18:18:28,718 [trainer.py] => prefix: train
2024-03-05 18:18:28,718 [trainer.py] => dataset: cifar100
2024-03-05 18:18:28,718 [trainer.py] => memory_size: 0
2024-03-05 18:18:28,718 [trainer.py] => shuffle: True
2024-03-05 18:18:28,718 [trainer.py] => init_cls: 50
2024-03-05 18:18:28,719 [trainer.py] => increment: 10
2024-03-05 18:18:28,719 [trainer.py] => model_name: fecam
2024-03-05 18:18:28,719 [trainer.py] => convnet_type: resnet18
2024-03-05 18:18:28,719 [trainer.py] => device: [device(type='cuda', index=0)]
2024-03-05 18:18:28,719 [trainer.py] => seed: 1993
2024-03-05 18:18:28,719 [trainer.py] => init_epochs: 200
2024-03-05 18:18:28,719 [trainer.py] => init_lr: 0.1
2024-03-05 18:18:28,719 [trainer.py] => init_weight_decay: 0.0005
2024-03-05 18:18:28,719 [trainer.py] => batch_size: 128
2024-03-05 18:18:28,719 [trainer.py] => num_workers: 8
2024-03-05 18:18:28,719 [trainer.py] => T: 5
2024-03-05 18:18:28,719 [trainer.py] => beta: 0.5
2024-03-05 18:18:28,719 [trainer.py] => alpha1: 1
2024-03-05 18:18:28,719 [trainer.py] => alpha2: 1
2024-03-05 18:18:28,719 [trainer.py] => ncm: False
2024-03-05 18:18:28,719 [trainer.py] => tukey: False
2024-03-05 18:18:28,719 [trainer.py] => diagonal: False
2024-03-05 18:18:28,719 [trainer.py] => per_class: True
2024-03-05 18:18:28,719 [trainer.py] => full_cov: True
2024-03-05 18:18:28,719 [trainer.py] => shrink: True
2024-03-05 18:18:28,719 [trainer.py] => norm_cov: False
2024-03-05 18:18:28,719 [trainer.py] => vecnorm: False
2024-03-05 18:18:28,719 [trainer.py] => ae_type: wae
2024-03-05 18:18:28,719 [trainer.py] => epochs: 1000
2024-03-05 18:18:28,719 [trainer.py] => ae_latent_dim: 32
2024-03-05 18:18:28,719 [trainer.py] => wae_sigma: 10
2024-03-05 18:18:28,719 [trainer.py] => wae_C: 5
Files already downloaded and verified
Files already downloaded and verified
2024-03-05 18:18:30,379 [data_manager.py] => [68, 56, 78, 8, 23, 84, 90, 65, 74, 76, 40, 89, 3, 92, 55, 9, 26, 80, 43, 38, 58, 70, 77, 1, 85, 19, 17, 50, 28, 53, 13, 81, 45, 82, 6, 59, 83, 16, 15, 44, 91, 41, 72, 60, 79, 52, 20, 10, 31, 54, 37, 95, 14, 71, 96, 98, 97, 2, 64, 66, 42, 22, 35, 86, 24, 34, 87, 21, 99, 0, 88, 27, 18, 94, 11, 12, 47, 25, 30, 46, 62, 69, 36, 61, 7, 63, 75, 5, 32, 4, 51, 48, 73, 93, 39, 67, 29, 49, 57, 33]
2024-03-05 18:18:30,652 [fecam.py] => Learning on 0-50
/home/z1165703/miniconda3/lib/python3.11/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
/home/z1165703/miniconda3/lib/python3.11/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
Processing class: 0
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 2.6615725994
Epoch:   200  |  train loss: 2.5405249119
Epoch:   300  |  train loss: 2.5504738331
Epoch:   400  |  train loss: 2.5081922054
Epoch:   500  |  train loss: 2.4718548298
Epoch:   600  |  train loss: 2.4653520107
Epoch:   700  |  train loss: 2.4531808853
Epoch:   800  |  train loss: 2.4481311798
Epoch:   900  |  train loss: 2.4212033272
Epoch:  1000  |  train loss: 2.3995073318
Processing class: 1
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 2.8279203892
Epoch:   200  |  train loss: 2.7996654510
Epoch:   300  |  train loss: 2.7639326096
Epoch:   400  |  train loss: 2.7273707390
Epoch:   500  |  train loss: 2.6492532730
Epoch:   600  |  train loss: 2.6636896610
Epoch:   700  |  train loss: 2.6199603558
Epoch:   800  |  train loss: 2.6069783688
Epoch:   900  |  train loss: 2.6078674316
Epoch:  1000  |  train loss: 2.5576397896
Processing class: 2
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 2.9203509331
Epoch:   200  |  train loss: 2.8312546730
Epoch:   300  |  train loss: 2.7708333969
Epoch:   400  |  train loss: 2.6835987091
Epoch:   500  |  train loss: 2.5752727032
Epoch:   600  |  train loss: 2.5441171169
Epoch:   700  |  train loss: 2.4871698856
Epoch:   800  |  train loss: 2.4734937191
Epoch:   900  |  train loss: 2.4145276070
Epoch:  1000  |  train loss: 2.3694804192
Processing class: 3
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 2.6670683384
Epoch:   200  |  train loss: 2.6898198605
Epoch:   300  |  train loss: 2.6521276951
Epoch:   400  |  train loss: 2.5729778767
Epoch:   500  |  train loss: 2.5130793095
Epoch:   600  |  train loss: 2.5139811516
Epoch:   700  |  train loss: 2.4739832878
Epoch:   800  |  train loss: 2.4289700508
Epoch:   900  |  train loss: 2.3959380627
Epoch:  1000  |  train loss: 2.3615752220
Processing class: 4
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 2.8202706814
Epoch:   200  |  train loss: 2.7774453163
Epoch:   300  |  train loss: 2.7057267666
Epoch:   400  |  train loss: 2.6376536846
Epoch:   500  |  train loss: 2.5971042156
Epoch:   600  |  train loss: 2.5262982845
Epoch:   700  |  train loss: 2.5207893848
Epoch:   800  |  train loss: 2.5015840530
Epoch:   900  |  train loss: 2.4803521156
Epoch:  1000  |  train loss: 2.4700082779
Processing class: 5
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 2.9494682789
Epoch:   200  |  train loss: 2.8124927044
Epoch:   300  |  train loss: 2.7508996487
Epoch:   400  |  train loss: 2.6665847778
Epoch:   500  |  train loss: 2.6309945107
Epoch:   600  |  train loss: 2.6257041454
Epoch:   700  |  train loss: 2.5740764141
Epoch:   800  |  train loss: 2.5122909069
Epoch:   900  |  train loss: 2.4665148735
Epoch:  1000  |  train loss: 2.4534491062
Processing class: 6
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 2.8094432831
Epoch:   200  |  train loss: 2.8176158905
Epoch:   300  |  train loss: 2.7562579155
Epoch:   400  |  train loss: 2.6744512081
Epoch:   500  |  train loss: 2.6556768894
Epoch:   600  |  train loss: 2.6225104809
Epoch:   700  |  train loss: 2.5893697739
Epoch:   800  |  train loss: 2.5628868103
Epoch:   900  |  train loss: 2.5006912231
Epoch:  1000  |  train loss: 2.4888659954
Processing class: 7
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 2.9188166618
Epoch:   200  |  train loss: 2.8367012978
Epoch:   300  |  train loss: 2.7930087566
Epoch:   400  |  train loss: 2.7120869637
Epoch:   500  |  train loss: 2.6524348736
Epoch:   600  |  train loss: 2.6054460049
Epoch:   700  |  train loss: 2.5802310944
Epoch:   800  |  train loss: 2.5721612930
Epoch:   900  |  train loss: 2.5097407341
Epoch:  1000  |  train loss: 2.4940846443
Processing class: 8
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 2.8629312038
Epoch:   200  |  train loss: 2.8209626198
Epoch:   300  |  train loss: 2.6956736088
Epoch:   400  |  train loss: 2.6702383995
Epoch:   500  |  train loss: 2.6131391525
Epoch:   600  |  train loss: 2.5645771027
Epoch:   700  |  train loss: 2.5532433033
Epoch:   800  |  train loss: 2.4978000164
Epoch:   900  |  train loss: 2.4826219559
Epoch:  1000  |  train loss: 2.4336367607
Processing class: 9
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 2.7716145515
Epoch:   200  |  train loss: 2.7805757046
Epoch:   300  |  train loss: 2.7737755775
Epoch:   400  |  train loss: 2.6891907215
Epoch:   500  |  train loss: 2.6869819164
Epoch:   600  |  train loss: 2.6478805065
Epoch:   700  |  train loss: 2.6448038101
Epoch:   800  |  train loss: 2.6346958160
Epoch:   900  |  train loss: 2.6172507286
Epoch:  1000  |  train loss: 2.5747956753
Processing class: 10
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 2.8954574585
Epoch:   200  |  train loss: 2.7977095127
Epoch:   300  |  train loss: 2.7618248463
Epoch:   400  |  train loss: 2.7428881168
Epoch:   500  |  train loss: 2.6437902927
Epoch:   600  |  train loss: 2.6152518749
Epoch:   700  |  train loss: 2.5900999546
Epoch:   800  |  train loss: 2.5475739956
Epoch:   900  |  train loss: 2.5330327988
Epoch:  1000  |  train loss: 2.5085623264
Processing class: 11
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 2.9524105549
Epoch:   200  |  train loss: 2.8345846176
Epoch:   300  |  train loss: 2.7660138607
Epoch:   400  |  train loss: 2.7193751335
Epoch:   500  |  train loss: 2.6501043797
Epoch:   600  |  train loss: 2.6323351860
Epoch:   700  |  train loss: 2.6256601810
Epoch:   800  |  train loss: 2.5909325123
Epoch:   900  |  train loss: 2.5576125145
Epoch:  1000  |  train loss: 2.5377785683
Processing class: 12
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 2.8453613281
Epoch:   200  |  train loss: 2.8406708241
Epoch:   300  |  train loss: 2.7653694630
Epoch:   400  |  train loss: 2.6938756943
Epoch:   500  |  train loss: 2.6439657211
Epoch:   600  |  train loss: 2.5756811619
Epoch:   700  |  train loss: 2.5441600323
Epoch:   800  |  train loss: 2.5281137466
Epoch:   900  |  train loss: 2.5151202202
Epoch:  1000  |  train loss: 2.4758186340
Processing class: 13
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 2.8015157700
Epoch:   200  |  train loss: 2.7320257664
Epoch:   300  |  train loss: 2.6687020302
Epoch:   400  |  train loss: 2.5942653656
Epoch:   500  |  train loss: 2.5556195259
Epoch:   600  |  train loss: 2.5120582581
Epoch:   700  |  train loss: 2.4881207943
Epoch:   800  |  train loss: 2.4808169365
Epoch:   900  |  train loss: 2.4699293613
Epoch:  1000  |  train loss: 2.4519195080
Processing class: 14
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 3.0100291729
Epoch:   200  |  train loss: 3.0148970127
Epoch:   300  |  train loss: 2.9497678757
Epoch:   400  |  train loss: 2.8927215099
Epoch:   500  |  train loss: 2.8360253334
Epoch:   600  |  train loss: 2.7905952454
Epoch:   700  |  train loss: 2.7803629398
Epoch:   800  |  train loss: 2.7827487946
Epoch:   900  |  train loss: 2.7539668083
Epoch:  1000  |  train loss: 2.7156109810
Processing class: 15
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 2.7740442276
Epoch:   200  |  train loss: 2.7437472820
Epoch:   300  |  train loss: 2.6496202469
Epoch:   400  |  train loss: 2.6073663235
Epoch:   500  |  train loss: 2.5804481030
Epoch:   600  |  train loss: 2.5319309235
Epoch:   700  |  train loss: 2.5361372471
Epoch:   800  |  train loss: 2.4924594402
Epoch:   900  |  train loss: 2.5058057785
Epoch:  1000  |  train loss: 2.4786730766
Processing class: 16
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 2.8313441277
Epoch:   200  |  train loss: 2.7447366714
Epoch:   300  |  train loss: 2.7090127468
Epoch:   400  |  train loss: 2.6548244953
Epoch:   500  |  train loss: 2.6565116882
Epoch:   600  |  train loss: 2.6154215336
Epoch:   700  |  train loss: 2.5754466057
Epoch:   800  |  train loss: 2.5768314362
Epoch:   900  |  train loss: 2.5221522808
Epoch:  1000  |  train loss: 2.5384333611
Processing class: 17
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 2.7846004486
Epoch:   200  |  train loss: 2.7922815323
Epoch:   300  |  train loss: 2.7980433464
Epoch:   400  |  train loss: 2.7409402370
Epoch:   500  |  train loss: 2.6188513756
Epoch:   600  |  train loss: 2.5963413715
Epoch:   700  |  train loss: 2.5599411488
Epoch:   800  |  train loss: 2.5317578793
Epoch:   900  |  train loss: 2.5105948448
Epoch:  1000  |  train loss: 2.4907579422
Processing class: 18
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 2.7945934772
Epoch:   200  |  train loss: 2.7921772957
Epoch:   300  |  train loss: 2.6419357777
Epoch:   400  |  train loss: 2.5770579338
Epoch:   500  |  train loss: 2.5176651955
Epoch:   600  |  train loss: 2.4428585529
Epoch:   700  |  train loss: 2.4479876995
Epoch:   800  |  train loss: 2.4049287319
Epoch:   900  |  train loss: 2.3838137627
Epoch:  1000  |  train loss: 2.3738316059
Processing class: 19
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 2.7030182362
Epoch:   200  |  train loss: 2.6803144455
Epoch:   300  |  train loss: 2.6151063919
Epoch:   400  |  train loss: 2.5912828445
Epoch:   500  |  train loss: 2.5114992142
Epoch:   600  |  train loss: 2.4907070160
Epoch:   700  |  train loss: 2.4716678143
Epoch:   800  |  train loss: 2.4297410965
Epoch:   900  |  train loss: 2.4355493546
Epoch:  1000  |  train loss: 2.4179386139
Processing class: 20
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 2.8125421047
Epoch:   200  |  train loss: 2.7383750916
Epoch:   300  |  train loss: 2.7256736755
Epoch:   400  |  train loss: 2.6588738918
Epoch:   500  |  train loss: 2.6655221462
Epoch:   600  |  train loss: 2.6127825260
Epoch:   700  |  train loss: 2.5791347980
Epoch:   800  |  train loss: 2.5660738945
Epoch:   900  |  train loss: 2.5476530075
Epoch:  1000  |  train loss: 2.5523013115
Processing class: 21
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 2.8378901958
Epoch:   200  |  train loss: 2.7678713322
Epoch:   300  |  train loss: 2.6202981472
Epoch:   400  |  train loss: 2.5436696053
Epoch:   500  |  train loss: 2.4975235462
Epoch:   600  |  train loss: 2.4743032932
Epoch:   700  |  train loss: 2.4450653076
Epoch:   800  |  train loss: 2.4043657303
Epoch:   900  |  train loss: 2.3924617767
Epoch:  1000  |  train loss: 2.3483716011
Processing class: 22
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 2.9161588192
Epoch:   200  |  train loss: 2.9035425186
Epoch:   300  |  train loss: 2.8885275364
Epoch:   400  |  train loss: 2.8074779034
Epoch:   500  |  train loss: 2.7579951286
Epoch:   600  |  train loss: 2.6937226295
Epoch:   700  |  train loss: 2.6578944206
Epoch:   800  |  train loss: 2.6065254211
Epoch:   900  |  train loss: 2.5697802067
Epoch:  1000  |  train loss: 2.5840485573
Processing class: 23
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 2.7827426434
Epoch:   200  |  train loss: 2.7895094395
Epoch:   300  |  train loss: 2.7499904633
Epoch:   400  |  train loss: 2.6672998905
Epoch:   500  |  train loss: 2.6255620003
Epoch:   600  |  train loss: 2.5489019394
Epoch:   700  |  train loss: 2.5159547329
Epoch:   800  |  train loss: 2.4664176941
Epoch:   900  |  train loss: 2.4500993252
Epoch:  1000  |  train loss: 2.3984431744
Processing class: 24
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 2.8269380569
Epoch:   200  |  train loss: 2.8154197216
Epoch:   300  |  train loss: 2.8004420280
Epoch:   400  |  train loss: 2.7413562298
Epoch:   500  |  train loss: 2.6872737408
Epoch:   600  |  train loss: 2.6670813084
Epoch:   700  |  train loss: 2.5996830940
Epoch:   800  |  train loss: 2.5875100613
Epoch:   900  |  train loss: 2.5778103828
Epoch:  1000  |  train loss: 2.5372066021
Processing class: 25
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 2.8601644516
Epoch:   200  |  train loss: 2.8054525852
Epoch:   300  |  train loss: 2.7710217476
Epoch:   400  |  train loss: 2.6598782539
Epoch:   500  |  train loss: 2.6172934055
Epoch:   600  |  train loss: 2.5648478985
Epoch:   700  |  train loss: 2.5620234966
Epoch:   800  |  train loss: 2.5185147762
Epoch:   900  |  train loss: 2.4776235580
Epoch:  1000  |  train loss: 2.4698554039
Processing class: 26
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 2.7799224377
Epoch:   200  |  train loss: 2.7969769955
Epoch:   300  |  train loss: 2.8001419544
Epoch:   400  |  train loss: 2.7702898026
Epoch:   500  |  train loss: 2.7382485390
Epoch:   600  |  train loss: 2.6308988571
Epoch:   700  |  train loss: 2.6130434036
Epoch:   800  |  train loss: 2.6021274090
Epoch:   900  |  train loss: 2.5616976261
Epoch:  1000  |  train loss: 2.5605940342
Processing class: 27
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 2.9240368366
Epoch:   200  |  train loss: 2.8103993893
Epoch:   300  |  train loss: 2.7907570839
Epoch:   400  |  train loss: 2.7128468037
Epoch:   500  |  train loss: 2.6558288097
Epoch:   600  |  train loss: 2.6316430092
Epoch:   700  |  train loss: 2.6145251274
Epoch:   800  |  train loss: 2.5870646000
Epoch:   900  |  train loss: 2.5623522758
Epoch:  1000  |  train loss: 2.4950195789
Processing class: 28
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 2.8648790359
Epoch:   200  |  train loss: 2.8665784836
Epoch:   300  |  train loss: 2.7946107864
Epoch:   400  |  train loss: 2.7361886978
Epoch:   500  |  train loss: 2.6913596630
Epoch:   600  |  train loss: 2.6672580719
Epoch:   700  |  train loss: 2.6577360630
Epoch:   800  |  train loss: 2.6439368725
Epoch:   900  |  train loss: 2.6319193363
Epoch:  1000  |  train loss: 2.5960316181
Processing class: 29
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 2.8493732452
Epoch:   200  |  train loss: 2.7651369572
Epoch:   300  |  train loss: 2.6820896149
Epoch:   400  |  train loss: 2.5880655289
Epoch:   500  |  train loss: 2.5625806332
Epoch:   600  |  train loss: 2.5328310966
Epoch:   700  |  train loss: 2.4854660988
Epoch:   800  |  train loss: 2.4270258427
Epoch:   900  |  train loss: 2.4021203995
Epoch:  1000  |  train loss: 2.3719659805
Processing class: 30
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 2.7725297451
Epoch:   200  |  train loss: 2.7780078411
Epoch:   300  |  train loss: 2.6956091881
Epoch:   400  |  train loss: 2.6891996861
Epoch:   500  |  train loss: 2.6676592827
Epoch:   600  |  train loss: 2.6301482677
Epoch:   700  |  train loss: 2.5798765182
Epoch:   800  |  train loss: 2.5741330624
Epoch:   900  |  train loss: 2.5646016598
Epoch:  1000  |  train loss: 2.5261401176
Processing class: 31
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 2.8786182880
Epoch:   200  |  train loss: 2.8168028831
Epoch:   300  |  train loss: 2.7410946846
Epoch:   400  |  train loss: 2.7462004185
Epoch:   500  |  train loss: 2.7090092659
Epoch:   600  |  train loss: 2.6988454342
Epoch:   700  |  train loss: 2.6614942074
Epoch:   800  |  train loss: 2.6452877522
Epoch:   900  |  train loss: 2.6330299377
Epoch:  1000  |  train loss: 2.5864978790
Processing class: 32
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 2.9644125938
Epoch:   200  |  train loss: 2.9588033199
Epoch:   300  |  train loss: 2.8921461105
Epoch:   400  |  train loss: 2.8004383564
Epoch:   500  |  train loss: 2.7696700096
Epoch:   600  |  train loss: 2.6854833603
Epoch:   700  |  train loss: 2.6676107883
Epoch:   800  |  train loss: 2.6445831776
Epoch:   900  |  train loss: 2.6079576969
Epoch:  1000  |  train loss: 2.5725698471
Processing class: 33
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 2.8383417130
Epoch:   200  |  train loss: 2.8673672199
Epoch:   300  |  train loss: 2.7180890083
Epoch:   400  |  train loss: 2.6442764282
Epoch:   500  |  train loss: 2.5947571278
Epoch:   600  |  train loss: 2.5767925739
Epoch:   700  |  train loss: 2.5262868881
Epoch:   800  |  train loss: 2.5045159340
Epoch:   900  |  train loss: 2.4706268787
Epoch:  1000  |  train loss: 2.4317900181
Processing class: 34
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 2.9059529781
Epoch:   200  |  train loss: 2.8684403896
Epoch:   300  |  train loss: 2.7814811707
Epoch:   400  |  train loss: 2.7216152191
Epoch:   500  |  train loss: 2.6577278614
Epoch:   600  |  train loss: 2.6284499168
Epoch:   700  |  train loss: 2.5666006088
Epoch:   800  |  train loss: 2.5558188915
Epoch:   900  |  train loss: 2.5331135273
Epoch:  1000  |  train loss: 2.5100946903
Processing class: 35
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 3.0496772289
Epoch:   200  |  train loss: 2.9320907593
Epoch:   300  |  train loss: 2.8910957813
Epoch:   400  |  train loss: 2.8577778339
Epoch:   500  |  train loss: 2.7891227245
Epoch:   600  |  train loss: 2.7857092381
Epoch:   700  |  train loss: 2.7542377949
Epoch:   800  |  train loss: 2.7210409641
Epoch:   900  |  train loss: 2.6999217987
Epoch:  1000  |  train loss: 2.6630159855
Processing class: 36
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 2.6930368423
Epoch:   200  |  train loss: 2.6450613499
Epoch:   300  |  train loss: 2.5946355343
Epoch:   400  |  train loss: 2.5662750244
Epoch:   500  |  train loss: 2.5431274891
Epoch:   600  |  train loss: 2.4800181389
Epoch:   700  |  train loss: 2.4230195999
Epoch:   800  |  train loss: 2.4185219288
Epoch:   900  |  train loss: 2.3590191364
Epoch:  1000  |  train loss: 2.3466209412
Processing class: 37
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 2.9752381325
Epoch:   200  |  train loss: 2.9281355381
Epoch:   300  |  train loss: 2.8393489361
Epoch:   400  |  train loss: 2.7834590435
Epoch:   500  |  train loss: 2.7458341122
Epoch:   600  |  train loss: 2.6884371281
Epoch:   700  |  train loss: 2.6542070389
Epoch:   800  |  train loss: 2.6384894371
Epoch:   900  |  train loss: 2.5854514122
Epoch:  1000  |  train loss: 2.5722888947
Processing class: 38
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 2.7608188629
Epoch:   200  |  train loss: 2.7535291672
Epoch:   300  |  train loss: 2.7309147835
Epoch:   400  |  train loss: 2.7177685261
Epoch:   500  |  train loss: 2.6594182491
Epoch:   600  |  train loss: 2.5974497318
Epoch:   700  |  train loss: 2.5473449707
Epoch:   800  |  train loss: 2.5485116005
Epoch:   900  |  train loss: 2.4998436451
Epoch:  1000  |  train loss: 2.4997319221
Processing class: 39
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 2.8508054733
Epoch:   200  |  train loss: 2.8014810085
Epoch:   300  |  train loss: 2.7629750252
Epoch:   400  |  train loss: 2.7191390038
Epoch:   500  |  train loss: 2.6521368504
Epoch:   600  |  train loss: 2.6251597404
Epoch:   700  |  train loss: 2.5939915180
Epoch:   800  |  train loss: 2.5714521408
Epoch:   900  |  train loss: 2.5384858131
Epoch:  1000  |  train loss: 2.5119408607
Processing class: 40
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 2.8130846024
Epoch:   200  |  train loss: 2.6948653221
Epoch:   300  |  train loss: 2.7019406319
Epoch:   400  |  train loss: 2.6703660488
Epoch:   500  |  train loss: 2.6448085785
Epoch:   600  |  train loss: 2.5653573036
Epoch:   700  |  train loss: 2.5566957951
Epoch:   800  |  train loss: 2.5297048092
Epoch:   900  |  train loss: 2.4986911297
Epoch:  1000  |  train loss: 2.5106193542
Processing class: 41
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 2.7016621113
Epoch:   200  |  train loss: 2.7131379128
Epoch:   300  |  train loss: 2.6182078362
Epoch:   400  |  train loss: 2.5313336372
Epoch:   500  |  train loss: 2.4890438557
Epoch:   600  |  train loss: 2.4607367992
Epoch:   700  |  train loss: 2.4506389618
Epoch:   800  |  train loss: 2.4183024406
Epoch:   900  |  train loss: 2.3914725780
Epoch:  1000  |  train loss: 2.3718958855
Processing class: 42
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 2.8959425926
Epoch:   200  |  train loss: 2.8447385311
Epoch:   300  |  train loss: 2.7814862251
Epoch:   400  |  train loss: 2.7114763737
Epoch:   500  |  train loss: 2.6611468315
Epoch:   600  |  train loss: 2.6075332642
Epoch:   700  |  train loss: 2.5762699127
Epoch:   800  |  train loss: 2.5374049664
Epoch:   900  |  train loss: 2.4873780251
Epoch:  1000  |  train loss: 2.4544385433
Processing class: 43
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 2.9518219948
Epoch:   200  |  train loss: 2.8804127216
Epoch:   300  |  train loss: 2.7999810696
Epoch:   400  |  train loss: 2.8015075207
Epoch:   500  |  train loss: 2.7827445984
Epoch:   600  |  train loss: 2.7637865067
Epoch:   700  |  train loss: 2.7669045925
Epoch:   800  |  train loss: 2.7444418430
Epoch:   900  |  train loss: 2.6979447365
Epoch:  1000  |  train loss: 2.7042640209
Processing class: 44
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 2.9491457939
Epoch:   200  |  train loss: 2.8699283123
Epoch:   300  |  train loss: 2.8208065033
Epoch:   400  |  train loss: 2.7547427654
Epoch:   500  |  train loss: 2.7307329655
Epoch:   600  |  train loss: 2.6944554329
Epoch:   700  |  train loss: 2.6866293907
Epoch:   800  |  train loss: 2.6503483295
Epoch:   900  |  train loss: 2.6081064701
Epoch:  1000  |  train loss: 2.5999605179
Processing class: 45
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 2.8269516468
Epoch:   200  |  train loss: 2.7957862854
Epoch:   300  |  train loss: 2.6972806931
Epoch:   400  |  train loss: 2.7319626331
Epoch:   500  |  train loss: 2.7132199764
Epoch:   600  |  train loss: 2.7111526012
Epoch:   700  |  train loss: 2.6849026680
Epoch:   800  |  train loss: 2.6610354424
Epoch:   900  |  train loss: 2.6585348129
Epoch:  1000  |  train loss: 2.6574958324
Processing class: 46
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 2.7530172825
Epoch:   200  |  train loss: 2.7124971390
Epoch:   300  |  train loss: 2.6447532177
Epoch:   400  |  train loss: 2.6461231709
Epoch:   500  |  train loss: 2.6297771931
Epoch:   600  |  train loss: 2.6200877190
Epoch:   700  |  train loss: 2.6042448521
Epoch:   800  |  train loss: 2.6040148735
Epoch:   900  |  train loss: 2.5640460014
Epoch:  1000  |  train loss: 2.5612952232
Processing class: 47
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 2.9170235157
Epoch:   200  |  train loss: 2.8688789845
Epoch:   300  |  train loss: 2.7390827656
Epoch:   400  |  train loss: 2.6987087727
Epoch:   500  |  train loss: 2.6498763084
Epoch:   600  |  train loss: 2.5935099125
Epoch:   700  |  train loss: 2.5544900894
Epoch:   800  |  train loss: 2.5048776627
Epoch:   900  |  train loss: 2.4815948486
Epoch:  1000  |  train loss: 2.4482439041
Processing class: 48
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 2.8983461857
Epoch:   200  |  train loss: 2.8882696629
Epoch:   300  |  train loss: 2.7391137123
Epoch:   400  |  train loss: 2.6609256744
Epoch:   500  |  train loss: 2.6852941513
Epoch:   600  |  train loss: 2.6405767441
Epoch:   700  |  train loss: 2.5970725060
Epoch:   800  |  train loss: 2.5816042900
Epoch:   900  |  train loss: 2.5993714333
Epoch:  1000  |  train loss: 2.5825433254
Processing class: 49
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 2.9284326077
Epoch:   200  |  train loss: 2.8741991043
Epoch:   300  |  train loss: 2.8239965916
Epoch:   400  |  train loss: 2.7742456436
Epoch:   500  |  train loss: 2.7212560177
Epoch:   600  |  train loss: 2.6917137623
Epoch:   700  |  train loss: 2.6506012440
Epoch:   800  |  train loss: 2.6171562195
Epoch:   900  |  train loss: 2.5889579296
Epoch:  1000  |  train loss: 2.5683906555
/home/z1165703/FeCAM/models/base.py:87: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  vec = torch.tensor(vec).cuda()
2024-03-05 18:36:04,540 [trainer.py] => CNN: {'total': 83.44, '00-09': 87.7, '10-19': 79.6, '20-29': 84.9, '30-39': 81.0, '40-49': 84.0, 'old': 0, 'new': 83.44}
2024-03-05 18:36:04,542 [trainer.py] => No NME accuracy
2024-03-05 18:36:04,542 [trainer.py] => FeCAM: {'total': 82.9, '00-09': 86.2, '10-19': 79.7, '20-29': 85.0, '30-39': 80.6, '40-49': 83.0, 'old': 0, 'new': 82.9}
2024-03-05 18:36:04,542 [trainer.py] => CNN top1 curve: [83.44]
2024-03-05 18:36:04,542 [trainer.py] => CNN top5 curve: [96.5]
2024-03-05 18:36:04,542 [trainer.py] => FeCAM top1 curve: [82.9]
2024-03-05 18:36:04,542 [trainer.py] => FeCAM top5 curve: [95.84]

2024-03-05 18:36:04,552 [fecam.py] => Learning on 50-60
Processing class: 50
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 3.1346239567
Epoch:   200  |  train loss: 3.0362472057
Epoch:   300  |  train loss: 2.9716787815
Epoch:   400  |  train loss: 2.9241116047
Epoch:   500  |  train loss: 2.8563502312
Epoch:   600  |  train loss: 2.7831801414
Epoch:   700  |  train loss: 2.7560634613
Epoch:   800  |  train loss: 2.7260334015
Epoch:   900  |  train loss: 2.7100808144
Epoch:  1000  |  train loss: 2.6852008820
Processing class: 51
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 3.1606783867
Epoch:   200  |  train loss: 3.0281925678
Epoch:   300  |  train loss: 2.9892874718
Epoch:   400  |  train loss: 2.9399688721
Epoch:   500  |  train loss: 2.8602737427
Epoch:   600  |  train loss: 2.7989706516
Epoch:   700  |  train loss: 2.7291248322
Epoch:   800  |  train loss: 2.6989455700
Epoch:   900  |  train loss: 2.6946797848
Epoch:  1000  |  train loss: 2.6395122051
Processing class: 52
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 3.2645516872
Epoch:   200  |  train loss: 3.2105044365
Epoch:   300  |  train loss: 3.1550291538
Epoch:   400  |  train loss: 3.1052208900
Epoch:   500  |  train loss: 3.0438095093
Epoch:   600  |  train loss: 2.9734678268
Epoch:   700  |  train loss: 2.9529666901
Epoch:   800  |  train loss: 2.9117461205
Epoch:   900  |  train loss: 2.8738815784
Epoch:  1000  |  train loss: 2.8174096584
Processing class: 53
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 3.0152865410
Epoch:   200  |  train loss: 2.9045719624
Epoch:   300  |  train loss: 2.8296324253
Epoch:   400  |  train loss: 2.7748634338
Epoch:   500  |  train loss: 2.7560862541
Epoch:   600  |  train loss: 2.7450903416
Epoch:   700  |  train loss: 2.7325238228
Epoch:   800  |  train loss: 2.6562302589
Epoch:   900  |  train loss: 2.6571120739
Epoch:  1000  |  train loss: 2.6136920452
Processing class: 54
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 2.8344967365
Epoch:   200  |  train loss: 2.7423480034
Epoch:   300  |  train loss: 2.6590084553
Epoch:   400  |  train loss: 2.6118769646
Epoch:   500  |  train loss: 2.5734465122
Epoch:   600  |  train loss: 2.5565976143
Epoch:   700  |  train loss: 2.5249133110
Epoch:   800  |  train loss: 2.5129706860
Epoch:   900  |  train loss: 2.4999487400
Epoch:  1000  |  train loss: 2.4577687263
Processing class: 55
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 3.2864014626
Epoch:   200  |  train loss: 3.2562263966
Epoch:   300  |  train loss: 3.2005190849
Epoch:   400  |  train loss: 3.1857388496
Epoch:   500  |  train loss: 3.1757255554
Epoch:   600  |  train loss: 3.1287970066
Epoch:   700  |  train loss: 3.0964612484
Epoch:   800  |  train loss: 3.0823299885
Epoch:   900  |  train loss: 3.0015561104
Epoch:  1000  |  train loss: 2.9860242844
Processing class: 56
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 3.0925073147
Epoch:   200  |  train loss: 2.9598119736
Epoch:   300  |  train loss: 2.7800867558
Epoch:   400  |  train loss: 2.7181485653
Epoch:   500  |  train loss: 2.6433611393
Epoch:   600  |  train loss: 2.5755949974
Epoch:   700  |  train loss: 2.5290598392
Epoch:   800  |  train loss: 2.4678808212
Epoch:   900  |  train loss: 2.4495726109
Epoch:  1000  |  train loss: 2.4273354053
Processing class: 57
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 3.2665402412
Epoch:   200  |  train loss: 3.2593374729
Epoch:   300  |  train loss: 3.2046262264
Epoch:   400  |  train loss: 3.1556147099
Epoch:   500  |  train loss: 3.1237158775
Epoch:   600  |  train loss: 3.0587674141
Epoch:   700  |  train loss: 3.0582720280
Epoch:   800  |  train loss: 3.0160279751
Epoch:   900  |  train loss: 2.9953410149
Epoch:  1000  |  train loss: 2.9205009937
Processing class: 58
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 3.1520102024
Epoch:   200  |  train loss: 2.9344103336
Epoch:   300  |  train loss: 2.8468943119
Epoch:   400  |  train loss: 2.7672496319
Epoch:   500  |  train loss: 2.7025924683
Epoch:   600  |  train loss: 2.6763836384
Epoch:   700  |  train loss: 2.6189824104
Epoch:   800  |  train loss: 2.6006692886
Epoch:   900  |  train loss: 2.5635772705
Epoch:  1000  |  train loss: 2.5266663074
Processing class: 59
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 3.2666420460
Epoch:   200  |  train loss: 3.2292002678
Epoch:   300  |  train loss: 3.1144318104
Epoch:   400  |  train loss: 3.0389263630
Epoch:   500  |  train loss: 2.9893649578
Epoch:   600  |  train loss: 2.9476068497
Epoch:   700  |  train loss: 2.9179165363
Epoch:   800  |  train loss: 2.8867614746
Epoch:   900  |  train loss: 2.8472936630
Epoch:  1000  |  train loss: 2.8189327240
2024-03-05 18:41:50,583 [trainer.py] => CNN: {'total': 71.75, '00-09': 82.2, '10-19': 72.8, '20-29': 78.5, '30-39': 74.9, '40-49': 68.4, '50-59': 53.7, 'old': 75.36, 'new': 53.7}
2024-03-05 18:41:50,584 [trainer.py] => No NME accuracy
2024-03-05 18:41:50,584 [trainer.py] => FeCAM: {'total': 72.78, '00-09': 82.6, '10-19': 74.0, '20-29': 81.0, '30-39': 76.4, '40-49': 76.8, '50-59': 45.9, 'old': 78.16, 'new': 45.9}
2024-03-05 18:41:50,584 [trainer.py] => CNN top1 curve: [83.44, 71.75]
2024-03-05 18:41:50,584 [trainer.py] => CNN top5 curve: [96.5, 89.6]
2024-03-05 18:41:50,584 [trainer.py] => FeCAM top1 curve: [82.9, 72.78]
2024-03-05 18:41:50,584 [trainer.py] => FeCAM top5 curve: [95.84, 91.18]

2024-03-05 18:41:50,589 [fecam.py] => Learning on 60-70
Processing class: 60
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 3.2567514896
Epoch:   200  |  train loss: 3.1849214077
Epoch:   300  |  train loss: 3.0808685780
Epoch:   400  |  train loss: 2.9939270973
Epoch:   500  |  train loss: 2.9450693607
Epoch:   600  |  train loss: 2.8765562057
Epoch:   700  |  train loss: 2.8729638100
Epoch:   800  |  train loss: 2.7767348766
Epoch:   900  |  train loss: 2.7415243626
Epoch:  1000  |  train loss: 2.7150189877
Processing class: 61
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 3.0785553455
Epoch:   200  |  train loss: 2.9123665333
Epoch:   300  |  train loss: 2.8122415543
Epoch:   400  |  train loss: 2.7077884197
Epoch:   500  |  train loss: 2.6717406750
Epoch:   600  |  train loss: 2.5919656277
Epoch:   700  |  train loss: 2.5652445316
Epoch:   800  |  train loss: 2.5385895252
Epoch:   900  |  train loss: 2.4706274509
Epoch:  1000  |  train loss: 2.4565148830
Processing class: 62
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 3.3101392746
Epoch:   200  |  train loss: 3.2535020828
Epoch:   300  |  train loss: 3.2394847393
Epoch:   400  |  train loss: 3.2120848179
Epoch:   500  |  train loss: 3.1596837521
Epoch:   600  |  train loss: 3.1562228680
Epoch:   700  |  train loss: 3.1334725380
Epoch:   800  |  train loss: 3.0997169971
Epoch:   900  |  train loss: 3.0515422821
Epoch:  1000  |  train loss: 3.0293185711
Processing class: 63
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 3.1595210075
Epoch:   200  |  train loss: 3.1175253391
Epoch:   300  |  train loss: 3.0178032875
Epoch:   400  |  train loss: 2.9182515621
Epoch:   500  |  train loss: 2.8473417759
Epoch:   600  |  train loss: 2.8210093021
Epoch:   700  |  train loss: 2.7372487545
Epoch:   800  |  train loss: 2.7494390965
Epoch:   900  |  train loss: 2.6790915012
Epoch:  1000  |  train loss: 2.6581995964
Processing class: 64
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 3.1489231110
Epoch:   200  |  train loss: 2.8983323574
Epoch:   300  |  train loss: 2.8237208843
Epoch:   400  |  train loss: 2.7458191872
Epoch:   500  |  train loss: 2.6555228710
Epoch:   600  |  train loss: 2.5868612289
Epoch:   700  |  train loss: 2.5138731003
Epoch:   800  |  train loss: 2.4902272224
Epoch:   900  |  train loss: 2.4810442448
Epoch:  1000  |  train loss: 2.4306532860
Processing class: 65
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 3.0939161301
Epoch:   200  |  train loss: 2.9270138264
Epoch:   300  |  train loss: 2.8127673149
Epoch:   400  |  train loss: 2.7383240223
Epoch:   500  |  train loss: 2.6623354435
Epoch:   600  |  train loss: 2.6387139320
Epoch:   700  |  train loss: 2.5901748657
Epoch:   800  |  train loss: 2.5799506187
Epoch:   900  |  train loss: 2.5186803818
Epoch:  1000  |  train loss: 2.5179461956
Processing class: 66
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 3.2181292534
Epoch:   200  |  train loss: 3.1422550201
Epoch:   300  |  train loss: 3.0645436764
Epoch:   400  |  train loss: 2.9943584919
Epoch:   500  |  train loss: 2.9763216496
Epoch:   600  |  train loss: 2.9358457088
Epoch:   700  |  train loss: 2.9285910130
Epoch:   800  |  train loss: 2.9061135292
Epoch:   900  |  train loss: 2.8745014191
Epoch:  1000  |  train loss: 2.8510037899
Processing class: 67
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 3.1924060345
Epoch:   200  |  train loss: 3.1364355087
Epoch:   300  |  train loss: 3.0628483772
Epoch:   400  |  train loss: 3.0129693508
Epoch:   500  |  train loss: 2.9651563644
Epoch:   600  |  train loss: 2.9248762131
Epoch:   700  |  train loss: 2.8927919865
Epoch:   800  |  train loss: 2.8747214317
Epoch:   900  |  train loss: 2.8366560459
Epoch:  1000  |  train loss: 2.8060079098
Processing class: 68
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 3.0381012440
Epoch:   200  |  train loss: 2.9784612656
Epoch:   300  |  train loss: 2.8221852779
Epoch:   400  |  train loss: 2.7607389927
Epoch:   500  |  train loss: 2.7164079666
Epoch:   600  |  train loss: 2.6642978191
Epoch:   700  |  train loss: 2.6385931492
Epoch:   800  |  train loss: 2.5916327000
Epoch:   900  |  train loss: 2.5640771866
Epoch:  1000  |  train loss: 2.5703004837
Processing class: 69
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 2.5907893181
Epoch:   200  |  train loss: 2.5039215565
Epoch:   300  |  train loss: 2.4249776363
Epoch:   400  |  train loss: 2.3651083946
Epoch:   500  |  train loss: 2.3178737640
Epoch:   600  |  train loss: 2.2659222603
Epoch:   700  |  train loss: 2.2651875019
Epoch:   800  |  train loss: 2.2405020714
Epoch:   900  |  train loss: 2.2647007942
Epoch:  1000  |  train loss: 2.2110260010
2024-03-05 18:48:20,794 [trainer.py] => CNN: {'total': 64.66, '00-09': 75.0, '10-19': 70.1, '20-29': 77.1, '30-39': 70.6, '40-49': 64.0, '50-59': 45.8, '60-69': 50.0, 'old': 67.1, 'new': 50.0}
2024-03-05 18:48:20,794 [trainer.py] => No NME accuracy
2024-03-05 18:48:20,794 [trainer.py] => FeCAM: {'total': 67.27, '00-09': 79.9, '10-19': 70.6, '20-29': 79.1, '30-39': 71.9, '40-49': 74.1, '50-59': 41.2, '60-69': 54.1, 'old': 69.47, 'new': 54.1}
2024-03-05 18:48:20,794 [trainer.py] => CNN top1 curve: [83.44, 71.75, 64.66]
2024-03-05 18:48:20,794 [trainer.py] => CNN top5 curve: [96.5, 89.6, 86.54]
2024-03-05 18:48:20,794 [trainer.py] => FeCAM top1 curve: [82.9, 72.78, 67.27]
2024-03-05 18:48:20,794 [trainer.py] => FeCAM top5 curve: [95.84, 91.18, 87.21]

2024-03-05 18:48:20,798 [fecam.py] => Learning on 70-80
Processing class: 70
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 3.1292549133
Epoch:   200  |  train loss: 3.0390767574
Epoch:   300  |  train loss: 2.9260576725
Epoch:   400  |  train loss: 2.8703208923
Epoch:   500  |  train loss: 2.7791839600
Epoch:   600  |  train loss: 2.7435323238
Epoch:   700  |  train loss: 2.7079338551
Epoch:   800  |  train loss: 2.6187354565
Epoch:   900  |  train loss: 2.6115904331
Epoch:  1000  |  train loss: 2.5722027779
Processing class: 71
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 3.2336440086
Epoch:   200  |  train loss: 3.1866013527
Epoch:   300  |  train loss: 3.0541166306
Epoch:   400  |  train loss: 2.9998018742
Epoch:   500  |  train loss: 2.9461904049
Epoch:   600  |  train loss: 2.8915718555
Epoch:   700  |  train loss: 2.8392391205
Epoch:   800  |  train loss: 2.7988980770
Epoch:   900  |  train loss: 2.7753812790
Epoch:  1000  |  train loss: 2.7264286518
Processing class: 72
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 3.1754180908
Epoch:   200  |  train loss: 3.1160310745
Epoch:   300  |  train loss: 3.0367471218
Epoch:   400  |  train loss: 2.9592998028
Epoch:   500  |  train loss: 2.9227050304
Epoch:   600  |  train loss: 2.8551348686
Epoch:   700  |  train loss: 2.8233425617
Epoch:   800  |  train loss: 2.7736366749
Epoch:   900  |  train loss: 2.7417048931
Epoch:  1000  |  train loss: 2.7289364338
Processing class: 73
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 3.2038719654
Epoch:   200  |  train loss: 3.1025725842
Epoch:   300  |  train loss: 3.0120283127
Epoch:   400  |  train loss: 2.9344386101
Epoch:   500  |  train loss: 2.9402811050
Epoch:   600  |  train loss: 2.9035603046
Epoch:   700  |  train loss: 2.8466161251
Epoch:   800  |  train loss: 2.8167862892
Epoch:   900  |  train loss: 2.7834325790
Epoch:  1000  |  train loss: 2.7675955296
Processing class: 74
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 3.3023910999
Epoch:   200  |  train loss: 3.2966228962
Epoch:   300  |  train loss: 3.2058454990
Epoch:   400  |  train loss: 3.1245332241
Epoch:   500  |  train loss: 3.0747802258
Epoch:   600  |  train loss: 3.0110131264
Epoch:   700  |  train loss: 3.0089982986
Epoch:   800  |  train loss: 2.9280372620
Epoch:   900  |  train loss: 2.8955786705
Epoch:  1000  |  train loss: 2.8893115520
Processing class: 75
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 3.2036629677
Epoch:   200  |  train loss: 3.1261314869
Epoch:   300  |  train loss: 3.0604940414
Epoch:   400  |  train loss: 2.9955075741
Epoch:   500  |  train loss: 2.9622088432
Epoch:   600  |  train loss: 2.9342689514
Epoch:   700  |  train loss: 2.8325484276
Epoch:   800  |  train loss: 2.8356621742
Epoch:   900  |  train loss: 2.8011239529
Epoch:  1000  |  train loss: 2.7918551445
Processing class: 76
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 2.5816734791
Epoch:   200  |  train loss: 2.4470730305
Epoch:   300  |  train loss: 2.3777154922
Epoch:   400  |  train loss: 2.3457737923
Epoch:   500  |  train loss: 2.3233249187
Epoch:   600  |  train loss: 2.3072464466
Epoch:   700  |  train loss: 2.2989731789
Epoch:   800  |  train loss: 2.2387278080
Epoch:   900  |  train loss: 2.2474402905
Epoch:  1000  |  train loss: 2.2453782082
Processing class: 77
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 3.0772490025
Epoch:   200  |  train loss: 2.9852379799
Epoch:   300  |  train loss: 2.8559182644
Epoch:   400  |  train loss: 2.7899033070
Epoch:   500  |  train loss: 2.7015484333
Epoch:   600  |  train loss: 2.6276801109
Epoch:   700  |  train loss: 2.5797580242
Epoch:   800  |  train loss: 2.5578026772
Epoch:   900  |  train loss: 2.5088456154
Epoch:  1000  |  train loss: 2.4672129154
Processing class: 78
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 3.1588605404
Epoch:   200  |  train loss: 3.0311608315
Epoch:   300  |  train loss: 2.9790222168
Epoch:   400  |  train loss: 2.8989118576
Epoch:   500  |  train loss: 2.8133329868
Epoch:   600  |  train loss: 2.7802573204
Epoch:   700  |  train loss: 2.7411531448
Epoch:   800  |  train loss: 2.7057579517
Epoch:   900  |  train loss: 2.6808320522
Epoch:  1000  |  train loss: 2.6495475292
Processing class: 79
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 3.2958795071
Epoch:   200  |  train loss: 3.2237947464
Epoch:   300  |  train loss: 3.2077599049
Epoch:   400  |  train loss: 3.1458665848
Epoch:   500  |  train loss: 3.1022696495
Epoch:   600  |  train loss: 3.0812867641
Epoch:   700  |  train loss: 3.0387444019
Epoch:   800  |  train loss: 3.0035095215
Epoch:   900  |  train loss: 2.9834280968
Epoch:  1000  |  train loss: 2.9229789257
2024-03-05 18:55:55,116 [trainer.py] => CNN: {'total': 59.18, '00-09': 73.6, '10-19': 68.4, '20-29': 76.9, '30-39': 69.1, '40-49': 60.6, '50-59': 37.2, '60-69': 45.7, '70-79': 41.9, 'old': 61.64, 'new': 41.9}
2024-03-05 18:55:55,117 [trainer.py] => No NME accuracy
2024-03-05 18:55:55,117 [trainer.py] => FeCAM: {'total': 61.72, '00-09': 78.1, '10-19': 69.3, '20-29': 78.0, '30-39': 69.5, '40-49': 72.1, '50-59': 35.7, '60-69': 49.1, '70-79': 42.0, 'old': 64.54, 'new': 42.0}
2024-03-05 18:55:55,117 [trainer.py] => CNN top1 curve: [83.44, 71.75, 64.66, 59.18]
2024-03-05 18:55:55,117 [trainer.py] => CNN top5 curve: [96.5, 89.6, 86.54, 84.09]
2024-03-05 18:55:55,117 [trainer.py] => FeCAM top1 curve: [82.9, 72.78, 67.27, 61.72]
2024-03-05 18:55:55,117 [trainer.py] => FeCAM top5 curve: [95.84, 91.18, 87.21, 84.09]

2024-03-05 18:55:55,123 [fecam.py] => Learning on 80-90
Processing class: 80
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 2.8436384201
Epoch:   200  |  train loss: 2.6289586067
Epoch:   300  |  train loss: 2.5499466896
Epoch:   400  |  train loss: 2.5023264408
Epoch:   500  |  train loss: 2.4485323906
Epoch:   600  |  train loss: 2.4194490433
Epoch:   700  |  train loss: 2.3625700951
Epoch:   800  |  train loss: 2.3785085201
Epoch:   900  |  train loss: 2.3512500763
Epoch:  1000  |  train loss: 2.3465974808
Processing class: 81
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 3.0926733494
Epoch:   200  |  train loss: 3.0104258060
Epoch:   300  |  train loss: 2.9013290882
Epoch:   400  |  train loss: 2.8727930069
Epoch:   500  |  train loss: 2.8079010487
Epoch:   600  |  train loss: 2.7876993656
Epoch:   700  |  train loss: 2.7385550976
Epoch:   800  |  train loss: 2.7272321224
Epoch:   900  |  train loss: 2.6864282131
Epoch:  1000  |  train loss: 2.6532598019
Processing class: 82
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 2.9829228401
Epoch:   200  |  train loss: 2.8713426113
Epoch:   300  |  train loss: 2.7892246246
Epoch:   400  |  train loss: 2.7075999737
Epoch:   500  |  train loss: 2.6468197346
Epoch:   600  |  train loss: 2.5949445248
Epoch:   700  |  train loss: 2.5581079006
Epoch:   800  |  train loss: 2.5359388828
Epoch:   900  |  train loss: 2.5033690453
Epoch:  1000  |  train loss: 2.4739590168
Processing class: 83
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 2.9459121704
Epoch:   200  |  train loss: 2.7472922325
Epoch:   300  |  train loss: 2.6129060268
Epoch:   400  |  train loss: 2.5297897339
Epoch:   500  |  train loss: 2.4933245659
Epoch:   600  |  train loss: 2.4326564312
Epoch:   700  |  train loss: 2.4292960644
Epoch:   800  |  train loss: 2.4305181980
Epoch:   900  |  train loss: 2.3912414551
Epoch:  1000  |  train loss: 2.3537065983
Processing class: 84
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 3.1885618687
Epoch:   200  |  train loss: 3.0142725468
Epoch:   300  |  train loss: 2.9406071663
Epoch:   400  |  train loss: 2.8491215229
Epoch:   500  |  train loss: 2.7998277187
Epoch:   600  |  train loss: 2.7310714245
Epoch:   700  |  train loss: 2.7104991436
Epoch:   800  |  train loss: 2.6971525192
Epoch:   900  |  train loss: 2.6325759888
Epoch:  1000  |  train loss: 2.5967913628
Processing class: 85
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 3.1953749180
Epoch:   200  |  train loss: 3.0452966690
Epoch:   300  |  train loss: 2.9558196068
Epoch:   400  |  train loss: 2.8623757839
Epoch:   500  |  train loss: 2.8198426723
Epoch:   600  |  train loss: 2.7838406563
Epoch:   700  |  train loss: 2.7753090858
Epoch:   800  |  train loss: 2.7157239914
Epoch:   900  |  train loss: 2.6867837429
Epoch:  1000  |  train loss: 2.6768270493
Processing class: 86
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 3.3088498592
Epoch:   200  |  train loss: 3.2194574356
Epoch:   300  |  train loss: 3.1534930706
Epoch:   400  |  train loss: 3.0593927860
Epoch:   500  |  train loss: 3.0040008545
Epoch:   600  |  train loss: 2.9597341537
Epoch:   700  |  train loss: 2.9148063183
Epoch:   800  |  train loss: 2.8699542522
Epoch:   900  |  train loss: 2.8364802361
Epoch:  1000  |  train loss: 2.8365461349
Processing class: 87
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 3.0123562336
Epoch:   200  |  train loss: 2.8415804386
Epoch:   300  |  train loss: 2.7645034313
Epoch:   400  |  train loss: 2.6804519176
Epoch:   500  |  train loss: 2.6169765472
Epoch:   600  |  train loss: 2.6016888142
Epoch:   700  |  train loss: 2.5745970726
Epoch:   800  |  train loss: 2.5406923771
Epoch:   900  |  train loss: 2.4936372280
Epoch:  1000  |  train loss: 2.5024665833
Processing class: 88
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 3.1392209530
Epoch:   200  |  train loss: 3.0413233280
Epoch:   300  |  train loss: 2.9381505489
Epoch:   400  |  train loss: 2.8611569881
Epoch:   500  |  train loss: 2.7750898361
Epoch:   600  |  train loss: 2.6918779373
Epoch:   700  |  train loss: 2.6650149822
Epoch:   800  |  train loss: 2.6212520599
Epoch:   900  |  train loss: 2.5931100845
Epoch:  1000  |  train loss: 2.5478162289
Processing class: 89
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 3.1257275105
Epoch:   200  |  train loss: 3.0197226524
Epoch:   300  |  train loss: 2.9052417755
Epoch:   400  |  train loss: 2.8480238914
Epoch:   500  |  train loss: 2.8026123524
Epoch:   600  |  train loss: 2.7376686573
Epoch:   700  |  train loss: 2.6725697994
Epoch:   800  |  train loss: 2.6722803593
Epoch:   900  |  train loss: 2.6218753815
Epoch:  1000  |  train loss: 2.5978188038
2024-03-05 19:04:48,328 [trainer.py] => CNN: {'total': 54.08, '00-09': 67.9, '10-19': 63.6, '20-29': 73.2, '30-39': 68.8, '40-49': 57.2, '50-59': 32.6, '60-69': 40.9, '70-79': 38.1, '80-89': 44.4, 'old': 55.29, 'new': 44.4}
2024-03-05 19:04:48,340 [trainer.py] => No NME accuracy
2024-03-05 19:04:48,340 [trainer.py] => FeCAM: {'total': 57.66, '00-09': 76.1, '10-19': 65.8, '20-29': 75.3, '30-39': 67.9, '40-49': 68.5, '50-59': 33.0, '60-69': 45.0, '70-79': 39.0, '80-89': 48.3, 'old': 58.82, 'new': 48.3}
2024-03-05 19:04:48,341 [trainer.py] => CNN top1 curve: [83.44, 71.75, 64.66, 59.18, 54.08]
2024-03-05 19:04:48,341 [trainer.py] => CNN top5 curve: [96.5, 89.6, 86.54, 84.09, 81.74]
2024-03-05 19:04:48,341 [trainer.py] => FeCAM top1 curve: [82.9, 72.78, 67.27, 61.72, 57.66]
2024-03-05 19:04:48,341 [trainer.py] => FeCAM top5 curve: [95.84, 91.18, 87.21, 84.09, 81.63]

2024-03-05 19:04:48,395 [fecam.py] => Learning on 90-100
Processing class: 90
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 3.1922523975
Epoch:   200  |  train loss: 3.1211687088
Epoch:   300  |  train loss: 2.9726648808
Epoch:   400  |  train loss: 2.8641511440
Epoch:   500  |  train loss: 2.7749969959
Epoch:   600  |  train loss: 2.7115521908
Epoch:   700  |  train loss: 2.6380638599
Epoch:   800  |  train loss: 2.6001208305
Epoch:   900  |  train loss: 2.5792840004
Epoch:  1000  |  train loss: 2.5675329208
Processing class: 91
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 2.6278453827
Epoch:   200  |  train loss: 2.5314404011
Epoch:   300  |  train loss: 2.4691636086
Epoch:   400  |  train loss: 2.4498060226
Epoch:   500  |  train loss: 2.4035448551
Epoch:   600  |  train loss: 2.3738817215
Epoch:   700  |  train loss: 2.3212050915
Epoch:   800  |  train loss: 2.2850651264
Epoch:   900  |  train loss: 2.2599417210
Epoch:  1000  |  train loss: 2.2419352531
Processing class: 92
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 3.1175729752
Epoch:   200  |  train loss: 2.8968650818
Epoch:   300  |  train loss: 2.8008573055
Epoch:   400  |  train loss: 2.7510165691
Epoch:   500  |  train loss: 2.6823142052
Epoch:   600  |  train loss: 2.6107800961
Epoch:   700  |  train loss: 2.5532531261
Epoch:   800  |  train loss: 2.4760457516
Epoch:   900  |  train loss: 2.4484157085
Epoch:  1000  |  train loss: 2.4196415901
Processing class: 93
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 3.1482026100
Epoch:   200  |  train loss: 2.9928367615
Epoch:   300  |  train loss: 2.9200163364
Epoch:   400  |  train loss: 2.8599438190
Epoch:   500  |  train loss: 2.7805611134
Epoch:   600  |  train loss: 2.6981748581
Epoch:   700  |  train loss: 2.6867878437
Epoch:   800  |  train loss: 2.6366227150
Epoch:   900  |  train loss: 2.6047427177
Epoch:  1000  |  train loss: 2.5575381279
Processing class: 94
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 2.8938316345
Epoch:   200  |  train loss: 2.7799215317
Epoch:   300  |  train loss: 2.6076847553
Epoch:   400  |  train loss: 2.5302351952
Epoch:   500  |  train loss: 2.4291553020
Epoch:   600  |  train loss: 2.4025991440
Epoch:   700  |  train loss: 2.3810333252
Epoch:   800  |  train loss: 2.2914604664
Epoch:   900  |  train loss: 2.2904553890
Epoch:  1000  |  train loss: 2.2504064083
Processing class: 95
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 3.2181022167
Epoch:   200  |  train loss: 3.1945191383
Epoch:   300  |  train loss: 3.1133395195
Epoch:   400  |  train loss: 3.0241400719
Epoch:   500  |  train loss: 2.9807972908
Epoch:   600  |  train loss: 2.9234786987
Epoch:   700  |  train loss: 2.8840600967
Epoch:   800  |  train loss: 2.8297698498
Epoch:   900  |  train loss: 2.8084319115
Epoch:  1000  |  train loss: 2.7903280258
Processing class: 96
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 3.2664229870
Epoch:   200  |  train loss: 3.1520562172
Epoch:   300  |  train loss: 3.0696261883
Epoch:   400  |  train loss: 3.0383850574
Epoch:   500  |  train loss: 2.9606604576
Epoch:   600  |  train loss: 2.9076665401
Epoch:   700  |  train loss: 2.8622741699
Epoch:   800  |  train loss: 2.8008471012
Epoch:   900  |  train loss: 2.7856204033
Epoch:  1000  |  train loss: 2.7431969643
Processing class: 97
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 3.1639626026
Epoch:   200  |  train loss: 2.9986211300
Epoch:   300  |  train loss: 2.8809694767
Epoch:   400  |  train loss: 2.8182780743
Epoch:   500  |  train loss: 2.7758476257
Epoch:   600  |  train loss: 2.7350688934
Epoch:   700  |  train loss: 2.7035311222
Epoch:   800  |  train loss: 2.6619297981
Epoch:   900  |  train loss: 2.6308740139
Epoch:  1000  |  train loss: 2.5952265739
Processing class: 98
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 2.9658771992
Epoch:   200  |  train loss: 2.8371896744
Epoch:   300  |  train loss: 2.7075671673
Epoch:   400  |  train loss: 2.6293620586
Epoch:   500  |  train loss: 2.5521409512
Epoch:   600  |  train loss: 2.5062105179
Epoch:   700  |  train loss: 2.4411303043
Epoch:   800  |  train loss: 2.4259255886
Epoch:   900  |  train loss: 2.3998934269
Epoch:  1000  |  train loss: 2.3677557945
Processing class: 99
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 3.2191982269
Epoch:   200  |  train loss: 3.0954317570
Epoch:   300  |  train loss: 2.9902831078
Epoch:   400  |  train loss: 2.9216852188
Epoch:   500  |  train loss: 2.8306253910
Epoch:   600  |  train loss: 2.7654141903
Epoch:   700  |  train loss: 2.7176109791
Epoch:   800  |  train loss: 2.7010661602
Epoch:   900  |  train loss: 2.6491405964
Epoch:  1000  |  train loss: 2.6217657089
2024-03-05 19:15:10,141 [trainer.py] => CNN: {'total': 50.36, '00-09': 57.7, '10-19': 63.6, '20-29': 71.4, '30-39': 68.5, '40-49': 56.8, '50-59': 29.6, '60-69': 38.9, '70-79': 36.6, '80-89': 42.7, '90-99': 37.8, 'old': 51.76, 'new': 37.8}
2024-03-05 19:15:10,143 [trainer.py] => No NME accuracy
2024-03-05 19:15:10,144 [trainer.py] => FeCAM: {'total': 54.74, '00-09': 73.3, '10-19': 65.1, '20-29': 74.1, '30-39': 67.2, '40-49': 67.1, '50-59': 30.8, '60-69': 43.3, '70-79': 37.2, '80-89': 46.9, '90-99': 42.4, 'old': 56.11, 'new': 42.4}
2024-03-05 19:15:10,144 [trainer.py] => CNN top1 curve: [83.44, 71.75, 64.66, 59.18, 54.08, 50.36]
2024-03-05 19:15:10,144 [trainer.py] => CNN top5 curve: [96.5, 89.6, 86.54, 84.09, 81.74, 79.63]
2024-03-05 19:15:10,144 [trainer.py] => FeCAM top1 curve: [82.9, 72.78, 67.27, 61.72, 57.66, 54.74]
2024-03-05 19:15:10,144 [trainer.py] => FeCAM top5 curve: [95.84, 91.18, 87.21, 84.09, 81.63, 79.21]

=========================================
2024-03-05 19:15:31,589 [trainer.py] => config: ./exps/FeCAM_cifar100.json
2024-03-05 19:15:31,589 [trainer.py] => prefix: train
2024-03-05 19:15:31,589 [trainer.py] => dataset: cifar100
2024-03-05 19:15:31,589 [trainer.py] => memory_size: 0
2024-03-05 19:15:31,589 [trainer.py] => shuffle: True
2024-03-05 19:15:31,589 [trainer.py] => init_cls: 50
2024-03-05 19:15:31,589 [trainer.py] => increment: 10
2024-03-05 19:15:31,589 [trainer.py] => model_name: fecam
2024-03-05 19:15:31,589 [trainer.py] => convnet_type: resnet18
2024-03-05 19:15:31,589 [trainer.py] => device: [device(type='cuda', index=0)]
2024-03-05 19:15:31,589 [trainer.py] => seed: 1993
2024-03-05 19:15:31,589 [trainer.py] => init_epochs: 200
2024-03-05 19:15:31,589 [trainer.py] => init_lr: 0.1
2024-03-05 19:15:31,589 [trainer.py] => init_weight_decay: 0.0005
2024-03-05 19:15:31,589 [trainer.py] => batch_size: 128
2024-03-05 19:15:31,589 [trainer.py] => num_workers: 8
2024-03-05 19:15:31,590 [trainer.py] => T: 5
2024-03-05 19:15:31,590 [trainer.py] => beta: 0.5
2024-03-05 19:15:31,590 [trainer.py] => alpha1: 1
2024-03-05 19:15:31,590 [trainer.py] => alpha2: 1
2024-03-05 19:15:31,590 [trainer.py] => ncm: False
2024-03-05 19:15:31,590 [trainer.py] => tukey: False
2024-03-05 19:15:31,590 [trainer.py] => diagonal: False
2024-03-05 19:15:31,590 [trainer.py] => per_class: True
2024-03-05 19:15:31,590 [trainer.py] => full_cov: True
2024-03-05 19:15:31,590 [trainer.py] => shrink: True
2024-03-05 19:15:31,590 [trainer.py] => norm_cov: False
2024-03-05 19:15:31,590 [trainer.py] => vecnorm: False
2024-03-05 19:15:31,590 [trainer.py] => ae_type: wae
2024-03-05 19:15:31,590 [trainer.py] => epochs: 1000
2024-03-05 19:15:31,590 [trainer.py] => ae_latent_dim: 32
2024-03-05 19:15:31,590 [trainer.py] => wae_sigma: 20
2024-03-05 19:15:31,590 [trainer.py] => wae_C: 5
Files already downloaded and verified
Files already downloaded and verified
2024-03-05 19:15:33,263 [data_manager.py] => [68, 56, 78, 8, 23, 84, 90, 65, 74, 76, 40, 89, 3, 92, 55, 9, 26, 80, 43, 38, 58, 70, 77, 1, 85, 19, 17, 50, 28, 53, 13, 81, 45, 82, 6, 59, 83, 16, 15, 44, 91, 41, 72, 60, 79, 52, 20, 10, 31, 54, 37, 95, 14, 71, 96, 98, 97, 2, 64, 66, 42, 22, 35, 86, 24, 34, 87, 21, 99, 0, 88, 27, 18, 94, 11, 12, 47, 25, 30, 46, 62, 69, 36, 61, 7, 63, 75, 5, 32, 4, 51, 48, 73, 93, 39, 67, 29, 49, 57, 33]
2024-03-05 19:15:33,531 [fecam.py] => Learning on 0-50
/home/z1165703/miniconda3/lib/python3.11/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
/home/z1165703/miniconda3/lib/python3.11/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
Processing class: 0
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 2.1675406456
Epoch:   200  |  train loss: 2.1363122463
Epoch:   300  |  train loss: 2.1614913940
Epoch:   400  |  train loss: 2.1545955658
Epoch:   500  |  train loss: 2.1306692600
Epoch:   600  |  train loss: 2.1298597097
Epoch:   700  |  train loss: 2.1328516006
Epoch:   800  |  train loss: 2.1337474585
Epoch:   900  |  train loss: 2.1141736031
Epoch:  1000  |  train loss: 2.1049231529
Processing class: 1
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 2.2157495975
Epoch:   200  |  train loss: 2.1982223988
Epoch:   300  |  train loss: 2.2318866253
Epoch:   400  |  train loss: 2.2314172268
Epoch:   500  |  train loss: 2.1769528389
Epoch:   600  |  train loss: 2.2167715073
Epoch:   700  |  train loss: 2.1857222080
Epoch:   800  |  train loss: 2.1981272221
Epoch:   900  |  train loss: 2.2242842674
Epoch:  1000  |  train loss: 2.1803142071
Processing class: 2
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 2.2426288128
Epoch:   200  |  train loss: 2.2113186359
Epoch:   300  |  train loss: 2.2232742786
Epoch:   400  |  train loss: 2.1854346752
Epoch:   500  |  train loss: 2.1000656128
Epoch:   600  |  train loss: 2.1256772995
Epoch:   700  |  train loss: 2.0985479832
Epoch:   800  |  train loss: 2.1195104837
Epoch:   900  |  train loss: 2.0800350666
Epoch:  1000  |  train loss: 2.0543764114
Processing class: 3
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 2.2170814991
Epoch:   200  |  train loss: 2.2526912689
Epoch:   300  |  train loss: 2.2526247501
Epoch:   400  |  train loss: 2.2302649498
Epoch:   500  |  train loss: 2.1961932182
Epoch:   600  |  train loss: 2.2233271122
Epoch:   700  |  train loss: 2.2259690762
Epoch:   800  |  train loss: 2.1891381502
Epoch:   900  |  train loss: 2.1902498245
Epoch:  1000  |  train loss: 2.1806827545
Processing class: 4
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 2.2055139065
Epoch:   200  |  train loss: 2.1769832134
Epoch:   300  |  train loss: 2.1904553413
Epoch:   400  |  train loss: 2.1663281441
Epoch:   500  |  train loss: 2.1664318562
Epoch:   600  |  train loss: 2.1325487614
Epoch:   700  |  train loss: 2.1469907284
Epoch:   800  |  train loss: 2.1442062855
Epoch:   900  |  train loss: 2.1434133053
Epoch:  1000  |  train loss: 2.1609182835
Processing class: 5
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 2.2600909710
Epoch:   200  |  train loss: 2.2058852196
Epoch:   300  |  train loss: 2.1947914600
Epoch:   400  |  train loss: 2.1668772697
Epoch:   500  |  train loss: 2.1514075756
Epoch:   600  |  train loss: 2.1898129940
Epoch:   700  |  train loss: 2.1786478996
Epoch:   800  |  train loss: 2.1362930775
Epoch:   900  |  train loss: 2.1136049747
Epoch:  1000  |  train loss: 2.1262749672
Processing class: 6
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 2.2127285957
Epoch:   200  |  train loss: 2.2294829845
Epoch:   300  |  train loss: 2.2465320587
Epoch:   400  |  train loss: 2.2135551453
Epoch:   500  |  train loss: 2.2354574203
Epoch:   600  |  train loss: 2.2110159397
Epoch:   700  |  train loss: 2.1890946865
Epoch:   800  |  train loss: 2.1891029358
Epoch:   900  |  train loss: 2.1509260654
Epoch:  1000  |  train loss: 2.1589524746
Processing class: 7
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 2.2290024757
Epoch:   200  |  train loss: 2.2088099480
Epoch:   300  |  train loss: 2.2053633213
Epoch:   400  |  train loss: 2.1863158226
Epoch:   500  |  train loss: 2.1787850857
Epoch:   600  |  train loss: 2.1521705627
Epoch:   700  |  train loss: 2.1585816860
Epoch:   800  |  train loss: 2.1819512844
Epoch:   900  |  train loss: 2.1345094681
Epoch:  1000  |  train loss: 2.1482059956
Processing class: 8
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 2.2171744823
Epoch:   200  |  train loss: 2.2238501072
Epoch:   300  |  train loss: 2.1724358559
Epoch:   400  |  train loss: 2.1987975597
Epoch:   500  |  train loss: 2.1632029057
Epoch:   600  |  train loss: 2.1537069798
Epoch:   700  |  train loss: 2.1747251511
Epoch:   800  |  train loss: 2.1319600582
Epoch:   900  |  train loss: 2.1415574074
Epoch:  1000  |  train loss: 2.1094634533
Processing class: 9
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 2.1645117283
Epoch:   200  |  train loss: 2.1875587940
Epoch:   300  |  train loss: 2.2164206505
Epoch:   400  |  train loss: 2.1601727962
Epoch:   500  |  train loss: 2.1883432388
Epoch:   600  |  train loss: 2.1557042599
Epoch:   700  |  train loss: 2.1658109665
Epoch:   800  |  train loss: 2.1703748226
Epoch:   900  |  train loss: 2.1648355484
Epoch:  1000  |  train loss: 2.1306345940
Processing class: 10
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 2.1936692238
Epoch:   200  |  train loss: 2.1574009895
Epoch:   300  |  train loss: 2.1552676201
Epoch:   400  |  train loss: 2.1837970257
Epoch:   500  |  train loss: 2.1091035843
Epoch:   600  |  train loss: 2.1133118629
Epoch:   700  |  train loss: 2.1166501522
Epoch:   800  |  train loss: 2.0916668892
Epoch:   900  |  train loss: 2.0964470387
Epoch:  1000  |  train loss: 2.0858335972
Processing class: 11
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 2.2619051933
Epoch:   200  |  train loss: 2.2337939739
Epoch:   300  |  train loss: 2.1951059818
Epoch:   400  |  train loss: 2.1952613831
Epoch:   500  |  train loss: 2.1704792500
Epoch:   600  |  train loss: 2.1838433266
Epoch:   700  |  train loss: 2.2119223118
Epoch:   800  |  train loss: 2.1948598862
Epoch:   900  |  train loss: 2.1832327366
Epoch:  1000  |  train loss: 2.1909390450
Processing class: 12
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 2.1815687180
Epoch:   200  |  train loss: 2.2299841404
Epoch:   300  |  train loss: 2.2051727295
Epoch:   400  |  train loss: 2.2021733761
Epoch:   500  |  train loss: 2.1891408443
Epoch:   600  |  train loss: 2.1507915974
Epoch:   700  |  train loss: 2.1471476078
Epoch:   800  |  train loss: 2.1492204666
Epoch:   900  |  train loss: 2.1606498241
Epoch:  1000  |  train loss: 2.1341102600
Processing class: 13
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 2.1822606564
Epoch:   200  |  train loss: 2.1952177525
Epoch:   300  |  train loss: 2.1997594833
Epoch:   400  |  train loss: 2.1794553757
Epoch:   500  |  train loss: 2.1861697674
Epoch:   600  |  train loss: 2.1519268036
Epoch:   700  |  train loss: 2.1553497314
Epoch:   800  |  train loss: 2.1670251369
Epoch:   900  |  train loss: 2.1715420246
Epoch:  1000  |  train loss: 2.1726276875
Processing class: 14
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 2.2742838860
Epoch:   200  |  train loss: 2.2913457394
Epoch:   300  |  train loss: 2.2802392483
Epoch:   400  |  train loss: 2.2863078594
Epoch:   500  |  train loss: 2.2585912228
Epoch:   600  |  train loss: 2.2418761253
Epoch:   700  |  train loss: 2.2550008297
Epoch:   800  |  train loss: 2.2869585514
Epoch:   900  |  train loss: 2.2660271168
Epoch:  1000  |  train loss: 2.2319895744
Processing class: 15
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 2.1885350704
Epoch:   200  |  train loss: 2.1937769413
Epoch:   300  |  train loss: 2.1894483566
Epoch:   400  |  train loss: 2.1853619099
Epoch:   500  |  train loss: 2.1936583519
Epoch:   600  |  train loss: 2.1574379921
Epoch:   700  |  train loss: 2.1850082397
Epoch:   800  |  train loss: 2.1455945492
Epoch:   900  |  train loss: 2.1765285969
Epoch:  1000  |  train loss: 2.1569474697
Processing class: 16
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 2.2153294086
Epoch:   200  |  train loss: 2.2462150097
Epoch:   300  |  train loss: 2.2589994907
Epoch:   400  |  train loss: 2.2252276897
Epoch:   500  |  train loss: 2.2477507591
Epoch:   600  |  train loss: 2.2164258957
Epoch:   700  |  train loss: 2.1946956158
Epoch:   800  |  train loss: 2.2261910439
Epoch:   900  |  train loss: 2.1767968655
Epoch:  1000  |  train loss: 2.2115530968
Processing class: 17
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 2.1527728558
Epoch:   200  |  train loss: 2.1747321606
Epoch:   300  |  train loss: 2.2049193859
Epoch:   400  |  train loss: 2.1912508011
Epoch:   500  |  train loss: 2.1141014099
Epoch:   600  |  train loss: 2.1259700775
Epoch:   700  |  train loss: 2.1066219807
Epoch:   800  |  train loss: 2.0981909275
Epoch:   900  |  train loss: 2.0976779461
Epoch:  1000  |  train loss: 2.0890893936
Processing class: 18
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 2.1823196888
Epoch:   200  |  train loss: 2.1889352798
Epoch:   300  |  train loss: 2.1612414837
Epoch:   400  |  train loss: 2.1359269142
Epoch:   500  |  train loss: 2.1421646118
Epoch:   600  |  train loss: 2.1023786545
Epoch:   700  |  train loss: 2.1449998379
Epoch:   800  |  train loss: 2.1255366325
Epoch:   900  |  train loss: 2.1245830059
Epoch:  1000  |  train loss: 2.1433472633
Processing class: 19
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 2.1594240665
Epoch:   200  |  train loss: 2.2025151730
Epoch:   300  |  train loss: 2.1806180954
Epoch:   400  |  train loss: 2.1753357887
Epoch:   500  |  train loss: 2.1295409203
Epoch:   600  |  train loss: 2.1271221638
Epoch:   700  |  train loss: 2.1311382771
Epoch:   800  |  train loss: 2.1076683283
Epoch:   900  |  train loss: 2.1527077675
Epoch:  1000  |  train loss: 2.1550654888
Processing class: 20
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 2.2055593014
Epoch:   200  |  train loss: 2.1978777409
Epoch:   300  |  train loss: 2.2109408379
Epoch:   400  |  train loss: 2.1988273144
Epoch:   500  |  train loss: 2.2268547058
Epoch:   600  |  train loss: 2.1927804947
Epoch:   700  |  train loss: 2.1861051083
Epoch:   800  |  train loss: 2.1820975780
Epoch:   900  |  train loss: 2.1852641582
Epoch:  1000  |  train loss: 2.2046091080
Processing class: 21
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 2.2183356762
Epoch:   200  |  train loss: 2.2179598331
Epoch:   300  |  train loss: 2.1765724659
Epoch:   400  |  train loss: 2.1559187412
Epoch:   500  |  train loss: 2.1489707947
Epoch:   600  |  train loss: 2.1542810440
Epoch:   700  |  train loss: 2.1396705627
Epoch:   800  |  train loss: 2.1164884806
Epoch:   900  |  train loss: 2.1330545425
Epoch:  1000  |  train loss: 2.1085833549
Processing class: 22
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 2.2330009937
Epoch:   200  |  train loss: 2.2350002289
Epoch:   300  |  train loss: 2.2559325218
Epoch:   400  |  train loss: 2.2566199780
Epoch:   500  |  train loss: 2.2424757957
Epoch:   600  |  train loss: 2.2069139004
Epoch:   700  |  train loss: 2.2048756123
Epoch:   800  |  train loss: 2.1773333549
Epoch:   900  |  train loss: 2.1545201778
Epoch:  1000  |  train loss: 2.1962647915
Processing class: 23
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 2.2146523952
Epoch:   200  |  train loss: 2.2400662899
Epoch:   300  |  train loss: 2.2529060364
Epoch:   400  |  train loss: 2.1987884045
Epoch:   500  |  train loss: 2.1818033695
Epoch:   600  |  train loss: 2.1401759148
Epoch:   700  |  train loss: 2.1417168140
Epoch:   800  |  train loss: 2.1129193783
Epoch:   900  |  train loss: 2.1222415924
Epoch:  1000  |  train loss: 2.0878643990
Processing class: 24
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 2.2193524837
Epoch:   200  |  train loss: 2.2139715672
Epoch:   300  |  train loss: 2.1866669655
Epoch:   400  |  train loss: 2.1939805508
Epoch:   500  |  train loss: 2.1630065441
Epoch:   600  |  train loss: 2.1713836670
Epoch:   700  |  train loss: 2.1148623466
Epoch:   800  |  train loss: 2.1216269493
Epoch:   900  |  train loss: 2.1376449108
Epoch:  1000  |  train loss: 2.1139598370
Processing class: 25
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 2.2112975597
Epoch:   200  |  train loss: 2.2123371124
Epoch:   300  |  train loss: 2.2264568329
Epoch:   400  |  train loss: 2.1687743664
Epoch:   500  |  train loss: 2.1804872990
Epoch:   600  |  train loss: 2.1483368874
Epoch:   700  |  train loss: 2.1738108635
Epoch:   800  |  train loss: 2.1474397659
Epoch:   900  |  train loss: 2.1233011246
Epoch:  1000  |  train loss: 2.1300185204
Processing class: 26
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 2.2260164738
Epoch:   200  |  train loss: 2.2616713047
Epoch:   300  |  train loss: 2.2463941097
Epoch:   400  |  train loss: 2.2610295296
Epoch:   500  |  train loss: 2.2733167648
Epoch:   600  |  train loss: 2.2005718231
Epoch:   700  |  train loss: 2.2062237263
Epoch:   800  |  train loss: 2.2147049904
Epoch:   900  |  train loss: 2.1937282085
Epoch:  1000  |  train loss: 2.2137904644
Processing class: 27
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 2.2419262886
Epoch:   200  |  train loss: 2.1913539410
Epoch:   300  |  train loss: 2.1965127468
Epoch:   400  |  train loss: 2.1778259754
Epoch:   500  |  train loss: 2.1732808113
Epoch:   600  |  train loss: 2.1663277626
Epoch:   700  |  train loss: 2.1670916557
Epoch:   800  |  train loss: 2.1563934803
Epoch:   900  |  train loss: 2.1559601784
Epoch:  1000  |  train loss: 2.1061319351
Processing class: 28
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 2.2469000816
Epoch:   200  |  train loss: 2.2727620125
Epoch:   300  |  train loss: 2.2735575676
Epoch:   400  |  train loss: 2.2658958912
Epoch:   500  |  train loss: 2.2588080406
Epoch:   600  |  train loss: 2.2497294903
Epoch:   700  |  train loss: 2.2529074192
Epoch:   800  |  train loss: 2.2565114498
Epoch:   900  |  train loss: 2.2572265148
Epoch:  1000  |  train loss: 2.2257512569
Processing class: 29
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 2.2168686867
Epoch:   200  |  train loss: 2.2041027546
Epoch:   300  |  train loss: 2.1738178730
Epoch:   400  |  train loss: 2.1300055027
Epoch:   500  |  train loss: 2.1430708408
Epoch:   600  |  train loss: 2.1414513588
Epoch:   700  |  train loss: 2.1232700825
Epoch:   800  |  train loss: 2.0867249489
Epoch:   900  |  train loss: 2.0812945366
Epoch:  1000  |  train loss: 2.0644054890
Processing class: 30
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 2.2237180233
Epoch:   200  |  train loss: 2.2517852306
Epoch:   300  |  train loss: 2.2636481285
Epoch:   400  |  train loss: 2.2938184738
Epoch:   500  |  train loss: 2.2714353561
Epoch:   600  |  train loss: 2.2534582138
Epoch:   700  |  train loss: 2.2294492245
Epoch:   800  |  train loss: 2.2501776695
Epoch:   900  |  train loss: 2.2615256310
Epoch:  1000  |  train loss: 2.2296954632
Processing class: 31
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 2.2523675442
Epoch:   200  |  train loss: 2.2641395092
Epoch:   300  |  train loss: 2.2394664764
Epoch:   400  |  train loss: 2.2708830833
Epoch:   500  |  train loss: 2.2689467907
Epoch:   600  |  train loss: 2.2869179249
Epoch:   700  |  train loss: 2.2676831722
Epoch:   800  |  train loss: 2.2641100407
Epoch:   900  |  train loss: 2.2784853935
Epoch:  1000  |  train loss: 2.2391510963
Processing class: 32
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 2.2589982986
Epoch:   200  |  train loss: 2.2834533691
Epoch:   300  |  train loss: 2.2558550358
Epoch:   400  |  train loss: 2.2021535873
Epoch:   500  |  train loss: 2.2358252048
Epoch:   600  |  train loss: 2.1927498341
Epoch:   700  |  train loss: 2.2192640781
Epoch:   800  |  train loss: 2.2215764046
Epoch:   900  |  train loss: 2.2012972832
Epoch:  1000  |  train loss: 2.1788413048
Processing class: 33
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 2.1717797279
Epoch:   200  |  train loss: 2.2479888439
Epoch:   300  |  train loss: 2.1532225609
Epoch:   400  |  train loss: 2.1330083847
Epoch:   500  |  train loss: 2.1190782070
Epoch:   600  |  train loss: 2.1475116730
Epoch:   700  |  train loss: 2.1109689474
Epoch:   800  |  train loss: 2.1108221531
Epoch:   900  |  train loss: 2.0951446772
Epoch:  1000  |  train loss: 2.0720204830
Processing class: 34
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 2.2454126358
Epoch:   200  |  train loss: 2.2255187035
Epoch:   300  |  train loss: 2.2189535618
Epoch:   400  |  train loss: 2.1853638649
Epoch:   500  |  train loss: 2.1612090111
Epoch:   600  |  train loss: 2.1725127220
Epoch:   700  |  train loss: 2.1308133602
Epoch:   800  |  train loss: 2.1496776581
Epoch:   900  |  train loss: 2.1488007545
Epoch:  1000  |  train loss: 2.1462370872
Processing class: 35
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 2.2695297718
Epoch:   200  |  train loss: 2.2592711449
Epoch:   300  |  train loss: 2.2667559147
Epoch:   400  |  train loss: 2.2635745049
Epoch:   500  |  train loss: 2.2231565475
Epoch:   600  |  train loss: 2.2375261784
Epoch:   700  |  train loss: 2.2330330849
Epoch:   800  |  train loss: 2.2190931320
Epoch:   900  |  train loss: 2.2163451672
Epoch:  1000  |  train loss: 2.1932514191
Processing class: 36
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 2.1698444843
Epoch:   200  |  train loss: 2.1596155643
Epoch:   300  |  train loss: 2.1591924191
Epoch:   400  |  train loss: 2.1493310452
Epoch:   500  |  train loss: 2.1694127083
Epoch:   600  |  train loss: 2.1324499130
Epoch:   700  |  train loss: 2.0967401028
Epoch:   800  |  train loss: 2.1267613411
Epoch:   900  |  train loss: 2.0771777630
Epoch:  1000  |  train loss: 2.0911500931
Processing class: 37
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 2.2796789646
Epoch:   200  |  train loss: 2.2589181423
Epoch:   300  |  train loss: 2.2267073631
Epoch:   400  |  train loss: 2.2229649544
Epoch:   500  |  train loss: 2.2227791309
Epoch:   600  |  train loss: 2.1862449169
Epoch:   700  |  train loss: 2.1769115925
Epoch:   800  |  train loss: 2.1800939560
Epoch:   900  |  train loss: 2.1464736938
Epoch:  1000  |  train loss: 2.1519362450
Processing class: 38
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 2.1863999844
Epoch:   200  |  train loss: 2.1944403172
Epoch:   300  |  train loss: 2.2097556591
Epoch:   400  |  train loss: 2.2170910835
Epoch:   500  |  train loss: 2.2026491165
Epoch:   600  |  train loss: 2.1729659081
Epoch:   700  |  train loss: 2.1583125114
Epoch:   800  |  train loss: 2.1802378654
Epoch:   900  |  train loss: 2.1358158112
Epoch:  1000  |  train loss: 2.1536053658
Processing class: 39
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 2.2248247623
Epoch:   200  |  train loss: 2.1997484207
Epoch:   300  |  train loss: 2.2037802696
Epoch:   400  |  train loss: 2.2016105175
Epoch:   500  |  train loss: 2.1707041740
Epoch:   600  |  train loss: 2.1692391396
Epoch:   700  |  train loss: 2.1672867298
Epoch:   800  |  train loss: 2.1735280514
Epoch:   900  |  train loss: 2.1680121899
Epoch:  1000  |  train loss: 2.1636281013
Processing class: 40
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 2.2165354729
Epoch:   200  |  train loss: 2.1584986210
Epoch:   300  |  train loss: 2.2042726994
Epoch:   400  |  train loss: 2.2039798260
Epoch:   500  |  train loss: 2.2068171501
Epoch:   600  |  train loss: 2.1644542217
Epoch:   700  |  train loss: 2.1730410099
Epoch:   800  |  train loss: 2.1618394852
Epoch:   900  |  train loss: 2.1395532846
Epoch:  1000  |  train loss: 2.1817421913
Processing class: 41
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 2.1669580936
Epoch:   200  |  train loss: 2.1758217335
Epoch:   300  |  train loss: 2.1637276649
Epoch:   400  |  train loss: 2.1226496220
Epoch:   500  |  train loss: 2.1068912745
Epoch:   600  |  train loss: 2.0984032154
Epoch:   700  |  train loss: 2.1129419327
Epoch:   800  |  train loss: 2.0909206390
Epoch:   900  |  train loss: 2.0755409241
Epoch:  1000  |  train loss: 2.0685005188
Processing class: 42
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 2.2183747768
Epoch:   200  |  train loss: 2.2316445351
Epoch:   300  |  train loss: 2.2293813229
Epoch:   400  |  train loss: 2.2112345219
Epoch:   500  |  train loss: 2.2047064781
Epoch:   600  |  train loss: 2.1937786102
Epoch:   700  |  train loss: 2.1938617706
Epoch:   800  |  train loss: 2.1860137463
Epoch:   900  |  train loss: 2.1557328701
Epoch:  1000  |  train loss: 2.1414976597
Processing class: 43
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 2.2494199276
Epoch:   200  |  train loss: 2.2229023933
Epoch:   300  |  train loss: 2.2420525551
Epoch:   400  |  train loss: 2.2850029469
Epoch:   500  |  train loss: 2.2889876366
Epoch:   600  |  train loss: 2.2723839760
Epoch:   700  |  train loss: 2.2897975922
Epoch:   800  |  train loss: 2.2743216038
Epoch:   900  |  train loss: 2.2342544079
Epoch:  1000  |  train loss: 2.2647123814
Processing class: 44
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 2.2350326538
Epoch:   200  |  train loss: 2.2436405182
Epoch:   300  |  train loss: 2.2160816669
Epoch:   400  |  train loss: 2.2002581596
Epoch:   500  |  train loss: 2.2056996822
Epoch:   600  |  train loss: 2.1833147049
Epoch:   700  |  train loss: 2.2028077126
Epoch:   800  |  train loss: 2.1833987713
Epoch:   900  |  train loss: 2.1591811180
Epoch:  1000  |  train loss: 2.1797130108
Processing class: 45
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 2.2225302696
Epoch:   200  |  train loss: 2.2555576801
Epoch:   300  |  train loss: 2.2141105652
Epoch:   400  |  train loss: 2.2535574436
Epoch:   500  |  train loss: 2.2419306278
Epoch:   600  |  train loss: 2.2750941753
Epoch:   700  |  train loss: 2.2734404087
Epoch:   800  |  train loss: 2.2559007645
Epoch:   900  |  train loss: 2.2591941357
Epoch:  1000  |  train loss: 2.2761115551
Processing class: 46
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 2.2086163521
Epoch:   200  |  train loss: 2.1967014313
Epoch:   300  |  train loss: 2.1879750252
Epoch:   400  |  train loss: 2.1991614342
Epoch:   500  |  train loss: 2.1961088181
Epoch:   600  |  train loss: 2.1944015980
Epoch:   700  |  train loss: 2.1926160812
Epoch:   800  |  train loss: 2.2186891079
Epoch:   900  |  train loss: 2.1880533695
Epoch:  1000  |  train loss: 2.2000361919
Processing class: 47
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 2.2251851559
Epoch:   200  |  train loss: 2.2358009815
Epoch:   300  |  train loss: 2.1787803173
Epoch:   400  |  train loss: 2.1912913799
Epoch:   500  |  train loss: 2.1745437622
Epoch:   600  |  train loss: 2.1536606789
Epoch:   700  |  train loss: 2.1467854977
Epoch:   800  |  train loss: 2.1146195412
Epoch:   900  |  train loss: 2.1136225700
Epoch:  1000  |  train loss: 2.0924791098
Processing class: 48
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 2.2341177464
Epoch:   200  |  train loss: 2.2611104488
Epoch:   300  |  train loss: 2.2064553738
Epoch:   400  |  train loss: 2.1555817127
Epoch:   500  |  train loss: 2.1987020016
Epoch:   600  |  train loss: 2.1794182777
Epoch:   700  |  train loss: 2.1466327667
Epoch:   800  |  train loss: 2.1412882805
Epoch:   900  |  train loss: 2.1820243835
Epoch:  1000  |  train loss: 2.1838776112
Processing class: 49
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 2.2391368389
Epoch:   200  |  train loss: 2.2308095932
Epoch:   300  |  train loss: 2.2571794033
Epoch:   400  |  train loss: 2.2473784447
Epoch:   500  |  train loss: 2.2390660763
Epoch:   600  |  train loss: 2.2304278851
Epoch:   700  |  train loss: 2.2154936790
Epoch:   800  |  train loss: 2.1955664158
Epoch:   900  |  train loss: 2.1811132431
Epoch:  1000  |  train loss: 2.1794606209
/home/z1165703/FeCAM/models/base.py:87: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  vec = torch.tensor(vec).cuda()
2024-03-05 19:33:18,981 [trainer.py] => CNN: {'total': 83.44, '00-09': 87.7, '10-19': 79.6, '20-29': 84.9, '30-39': 81.0, '40-49': 84.0, 'old': 0, 'new': 83.44}
2024-03-05 19:33:18,982 [trainer.py] => No NME accuracy
2024-03-05 19:33:18,982 [trainer.py] => FeCAM: {'total': 82.9, '00-09': 86.2, '10-19': 79.7, '20-29': 85.0, '30-39': 80.6, '40-49': 83.0, 'old': 0, 'new': 82.9}
2024-03-05 19:33:18,982 [trainer.py] => CNN top1 curve: [83.44]
2024-03-05 19:33:18,982 [trainer.py] => CNN top5 curve: [96.5]
2024-03-05 19:33:18,982 [trainer.py] => FeCAM top1 curve: [82.9]
2024-03-05 19:33:18,982 [trainer.py] => FeCAM top5 curve: [95.84]

2024-03-05 19:33:18,993 [fecam.py] => Learning on 50-60
Processing class: 50
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 2.2892720222
Epoch:   200  |  train loss: 2.2683875561
Epoch:   300  |  train loss: 2.2523344040
Epoch:   400  |  train loss: 2.2613592148
Epoch:   500  |  train loss: 2.2517511845
Epoch:   600  |  train loss: 2.2124767303
Epoch:   700  |  train loss: 2.2275797844
Epoch:   800  |  train loss: 2.2287857533
Epoch:   900  |  train loss: 2.2384193897
Epoch:  1000  |  train loss: 2.2346933365
Processing class: 51
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 2.3084957600
Epoch:   200  |  train loss: 2.2429163456
Epoch:   300  |  train loss: 2.2720331669
Epoch:   400  |  train loss: 2.2640615940
Epoch:   500  |  train loss: 2.2434765816
Epoch:   600  |  train loss: 2.2365530491
Epoch:   700  |  train loss: 2.1928834438
Epoch:   800  |  train loss: 2.1876667023
Epoch:   900  |  train loss: 2.2154039860
Epoch:  1000  |  train loss: 2.1825309753
Processing class: 52
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 2.3475264072
Epoch:   200  |  train loss: 2.3220513821
Epoch:   300  |  train loss: 2.3230607510
Epoch:   400  |  train loss: 2.3211108685
Epoch:   500  |  train loss: 2.2985097885
Epoch:   600  |  train loss: 2.2605936527
Epoch:   700  |  train loss: 2.2788115501
Epoch:   800  |  train loss: 2.2677412033
Epoch:   900  |  train loss: 2.2615506172
Epoch:  1000  |  train loss: 2.2264413834
Processing class: 53
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 2.2483512878
Epoch:   200  |  train loss: 2.2237846851
Epoch:   300  |  train loss: 2.1747383595
Epoch:   400  |  train loss: 2.1671695232
Epoch:   500  |  train loss: 2.1735540390
Epoch:   600  |  train loss: 2.1906355858
Epoch:   700  |  train loss: 2.2082915783
Epoch:   800  |  train loss: 2.1385810375
Epoch:   900  |  train loss: 2.1612002850
Epoch:  1000  |  train loss: 2.1294974327
Processing class: 54
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 2.1172783375
Epoch:   200  |  train loss: 2.0953374386
Epoch:   300  |  train loss: 2.0886578083
Epoch:   400  |  train loss: 2.0791658878
Epoch:   500  |  train loss: 2.0768994570
Epoch:   600  |  train loss: 2.0967207909
Epoch:   700  |  train loss: 2.0927459240
Epoch:   800  |  train loss: 2.1088217258
Epoch:   900  |  train loss: 2.1183768749
Epoch:  1000  |  train loss: 2.0952986717
Processing class: 55
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 2.3395972252
Epoch:   200  |  train loss: 2.3351325035
Epoch:   300  |  train loss: 2.3055012226
Epoch:   400  |  train loss: 2.3299881935
Epoch:   500  |  train loss: 2.3641517162
Epoch:   600  |  train loss: 2.3551728249
Epoch:   700  |  train loss: 2.3573640347
Epoch:   800  |  train loss: 2.3809241772
Epoch:   900  |  train loss: 2.3214108467
Epoch:  1000  |  train loss: 2.3369301796
Processing class: 56
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 2.2646209717
Epoch:   200  |  train loss: 2.2359778881
Epoch:   300  |  train loss: 2.1523256302
Epoch:   400  |  train loss: 2.1662069798
Epoch:   500  |  train loss: 2.1503905773
Epoch:   600  |  train loss: 2.1319190025
Epoch:   700  |  train loss: 2.1264386654
Epoch:   800  |  train loss: 2.0914222717
Epoch:   900  |  train loss: 2.1049526215
Epoch:  1000  |  train loss: 2.1087589741
Processing class: 57
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 2.3184149742
Epoch:   200  |  train loss: 2.3445143700
Epoch:   300  |  train loss: 2.3191057682
Epoch:   400  |  train loss: 2.3062418461
Epoch:   500  |  train loss: 2.3169317245
Epoch:   600  |  train loss: 2.2733492374
Epoch:   700  |  train loss: 2.3083949089
Epoch:   800  |  train loss: 2.2941310883
Epoch:   900  |  train loss: 2.3074396133
Epoch:  1000  |  train loss: 2.2542017937
Processing class: 58
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 2.2877932549
Epoch:   200  |  train loss: 2.2113146305
Epoch:   300  |  train loss: 2.2073197842
Epoch:   400  |  train loss: 2.2097121239
Epoch:   500  |  train loss: 2.1799135685
Epoch:   600  |  train loss: 2.1917315006
Epoch:   700  |  train loss: 2.1646626949
Epoch:   800  |  train loss: 2.1752977371
Epoch:   900  |  train loss: 2.1620293617
Epoch:  1000  |  train loss: 2.1488523483
Processing class: 59
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 2.3552057266
Epoch:   200  |  train loss: 2.3802775383
Epoch:   300  |  train loss: 2.3302429676
Epoch:   400  |  train loss: 2.3210444927
Epoch:   500  |  train loss: 2.3242774963
Epoch:   600  |  train loss: 2.3147436619
Epoch:   700  |  train loss: 2.3177624226
Epoch:   800  |  train loss: 2.3124705791
Epoch:   900  |  train loss: 2.2948336601
Epoch:  1000  |  train loss: 2.2797074795
2024-03-05 19:38:58,938 [trainer.py] => CNN: {'total': 71.75, '00-09': 82.2, '10-19': 72.8, '20-29': 78.5, '30-39': 74.9, '40-49': 68.4, '50-59': 53.7, 'old': 75.36, 'new': 53.7}
2024-03-05 19:38:58,939 [trainer.py] => No NME accuracy
2024-03-05 19:38:58,939 [trainer.py] => FeCAM: {'total': 72.78, '00-09': 82.6, '10-19': 74.0, '20-29': 81.0, '30-39': 76.4, '40-49': 76.8, '50-59': 45.9, 'old': 78.16, 'new': 45.9}
2024-03-05 19:38:58,939 [trainer.py] => CNN top1 curve: [83.44, 71.75]
2024-03-05 19:38:58,939 [trainer.py] => CNN top5 curve: [96.5, 89.6]
2024-03-05 19:38:58,939 [trainer.py] => FeCAM top1 curve: [82.9, 72.78]
2024-03-05 19:38:58,939 [trainer.py] => FeCAM top5 curve: [95.84, 91.18]

2024-03-05 19:38:58,944 [fecam.py] => Learning on 60-70
Processing class: 60
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 2.3633867741
Epoch:   200  |  train loss: 2.3403789997
Epoch:   300  |  train loss: 2.2931371689
Epoch:   400  |  train loss: 2.2607645988
Epoch:   500  |  train loss: 2.2754780293
Epoch:   600  |  train loss: 2.2545550823
Epoch:   700  |  train loss: 2.3168862820
Epoch:   800  |  train loss: 2.2490947247
Epoch:   900  |  train loss: 2.2423264503
Epoch:  1000  |  train loss: 2.2499583721
Processing class: 61
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 2.2598141670
Epoch:   200  |  train loss: 2.1876680374
Epoch:   300  |  train loss: 2.1845808029
Epoch:   400  |  train loss: 2.1536735058
Epoch:   500  |  train loss: 2.1768729687
Epoch:   600  |  train loss: 2.1247885227
Epoch:   700  |  train loss: 2.1446189404
Epoch:   800  |  train loss: 2.1541045189
Epoch:   900  |  train loss: 2.1006253719
Epoch:  1000  |  train loss: 2.1088534355
Processing class: 62
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 2.3677293301
Epoch:   200  |  train loss: 2.3356915951
Epoch:   300  |  train loss: 2.3545948505
Epoch:   400  |  train loss: 2.3571825504
Epoch:   500  |  train loss: 2.3253514290
Epoch:   600  |  train loss: 2.3570405960
Epoch:   700  |  train loss: 2.3614945889
Epoch:   800  |  train loss: 2.3575451374
Epoch:   900  |  train loss: 2.3333900928
Epoch:  1000  |  train loss: 2.3365629196
Processing class: 63
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 2.2744369030
Epoch:   200  |  train loss: 2.2721075535
Epoch:   300  |  train loss: 2.2881409168
Epoch:   400  |  train loss: 2.2596013069
Epoch:   500  |  train loss: 2.2496110916
Epoch:   600  |  train loss: 2.2713410378
Epoch:   700  |  train loss: 2.2092123508
Epoch:   800  |  train loss: 2.2618858814
Epoch:   900  |  train loss: 2.2071564674
Epoch:  1000  |  train loss: 2.2089643955
Processing class: 64
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 2.3300864697
Epoch:   200  |  train loss: 2.2280160904
Epoch:   300  |  train loss: 2.2529449463
Epoch:   400  |  train loss: 2.2306022167
Epoch:   500  |  train loss: 2.2011307240
Epoch:   600  |  train loss: 2.1758505821
Epoch:   700  |  train loss: 2.1271841049
Epoch:   800  |  train loss: 2.1325475216
Epoch:   900  |  train loss: 2.1529079437
Epoch:  1000  |  train loss: 2.1253467083
Processing class: 65
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 2.2798628807
Epoch:   200  |  train loss: 2.2211910248
Epoch:   300  |  train loss: 2.2335729599
Epoch:   400  |  train loss: 2.2277990341
Epoch:   500  |  train loss: 2.1934479713
Epoch:   600  |  train loss: 2.2241612434
Epoch:   700  |  train loss: 2.2082981110
Epoch:   800  |  train loss: 2.2400073528
Epoch:   900  |  train loss: 2.1964994431
Epoch:  1000  |  train loss: 2.2318567753
Processing class: 66
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 2.3451551914
Epoch:   200  |  train loss: 2.3419162273
Epoch:   300  |  train loss: 2.3396450043
Epoch:   400  |  train loss: 2.3176164627
Epoch:   500  |  train loss: 2.3376067638
Epoch:   600  |  train loss: 2.3273746967
Epoch:   700  |  train loss: 2.3518947124
Epoch:   800  |  train loss: 2.3479110241
Epoch:   900  |  train loss: 2.3351590157
Epoch:  1000  |  train loss: 2.3250078678
Processing class: 67
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 2.2906547070
Epoch:   200  |  train loss: 2.2922506809
Epoch:   300  |  train loss: 2.2700681210
Epoch:   400  |  train loss: 2.2610782623
Epoch:   500  |  train loss: 2.2466389656
Epoch:   600  |  train loss: 2.2341835499
Epoch:   700  |  train loss: 2.2312350750
Epoch:   800  |  train loss: 2.2422817707
Epoch:   900  |  train loss: 2.2261128426
Epoch:  1000  |  train loss: 2.2145689487
Processing class: 68
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 2.2355556011
Epoch:   200  |  train loss: 2.2182784557
Epoch:   300  |  train loss: 2.1588945389
Epoch:   400  |  train loss: 2.1485929489
Epoch:   500  |  train loss: 2.1373782158
Epoch:   600  |  train loss: 2.1149600029
Epoch:   700  |  train loss: 2.1189364433
Epoch:   800  |  train loss: 2.0904956818
Epoch:   900  |  train loss: 2.0883533478
Epoch:  1000  |  train loss: 2.1225667715
Processing class: 69
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 2.0570460558
Epoch:   200  |  train loss: 2.0483701706
Epoch:   300  |  train loss: 2.0614223480
Epoch:   400  |  train loss: 2.0575650692
Epoch:   500  |  train loss: 2.0420589447
Epoch:   600  |  train loss: 1.9961736917
Epoch:   700  |  train loss: 2.0190232038
Epoch:   800  |  train loss: 2.0078993320
Epoch:   900  |  train loss: 2.0530329704
Epoch:  1000  |  train loss: 1.9976625919
2024-03-05 19:45:28,501 [trainer.py] => CNN: {'total': 64.66, '00-09': 75.0, '10-19': 70.1, '20-29': 77.1, '30-39': 70.6, '40-49': 64.0, '50-59': 45.8, '60-69': 50.0, 'old': 67.1, 'new': 50.0}
2024-03-05 19:45:28,502 [trainer.py] => No NME accuracy
2024-03-05 19:45:28,502 [trainer.py] => FeCAM: {'total': 67.27, '00-09': 79.9, '10-19': 70.6, '20-29': 79.1, '30-39': 71.9, '40-49': 74.1, '50-59': 41.2, '60-69': 54.1, 'old': 69.47, 'new': 54.1}
2024-03-05 19:45:28,502 [trainer.py] => CNN top1 curve: [83.44, 71.75, 64.66]
2024-03-05 19:45:28,502 [trainer.py] => CNN top5 curve: [96.5, 89.6, 86.54]
2024-03-05 19:45:28,502 [trainer.py] => FeCAM top1 curve: [82.9, 72.78, 67.27]
2024-03-05 19:45:28,502 [trainer.py] => FeCAM top5 curve: [95.84, 91.18, 87.21]

2024-03-05 19:45:28,507 [fecam.py] => Learning on 70-80
Processing class: 70
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 2.2754899025
Epoch:   200  |  train loss: 2.2366157055
Epoch:   300  |  train loss: 2.2144083500
Epoch:   400  |  train loss: 2.2114284515
Epoch:   500  |  train loss: 2.1683819771
Epoch:   600  |  train loss: 2.1820163727
Epoch:   700  |  train loss: 2.1984369755
Epoch:   800  |  train loss: 2.1360252857
Epoch:   900  |  train loss: 2.1567048073
Epoch:  1000  |  train loss: 2.1341637611
Processing class: 71
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 2.3282618046
Epoch:   200  |  train loss: 2.3267404079
Epoch:   300  |  train loss: 2.2660784721
Epoch:   400  |  train loss: 2.2708120823
Epoch:   500  |  train loss: 2.2691241264
Epoch:   600  |  train loss: 2.2552863121
Epoch:   700  |  train loss: 2.2425737381
Epoch:   800  |  train loss: 2.2365528107
Epoch:   900  |  train loss: 2.2537838459
Epoch:  1000  |  train loss: 2.2319475174
Processing class: 72
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 2.2877125263
Epoch:   200  |  train loss: 2.2815933704
Epoch:   300  |  train loss: 2.2645956516
Epoch:   400  |  train loss: 2.2398911953
Epoch:   500  |  train loss: 2.2443325520
Epoch:   600  |  train loss: 2.2063079834
Epoch:   700  |  train loss: 2.2073771477
Epoch:   800  |  train loss: 2.1838631630
Epoch:   900  |  train loss: 2.1808279514
Epoch:  1000  |  train loss: 2.1982267380
Processing class: 73
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 2.3282961845
Epoch:   200  |  train loss: 2.2923095226
Epoch:   300  |  train loss: 2.2979803562
Epoch:   400  |  train loss: 2.2591908455
Epoch:   500  |  train loss: 2.2968446255
Epoch:   600  |  train loss: 2.2841302395
Epoch:   700  |  train loss: 2.2475854397
Epoch:   800  |  train loss: 2.2395209312
Epoch:   900  |  train loss: 2.2313237667
Epoch:  1000  |  train loss: 2.2391259193
Processing class: 74
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 2.3725872040
Epoch:   200  |  train loss: 2.3888169765
Epoch:   300  |  train loss: 2.3355024338
Epoch:   400  |  train loss: 2.2995487213
Epoch:   500  |  train loss: 2.3012269974
Epoch:   600  |  train loss: 2.2727408886
Epoch:   700  |  train loss: 2.3093644619
Epoch:   800  |  train loss: 2.2506942749
Epoch:   900  |  train loss: 2.2453754425
Epoch:  1000  |  train loss: 2.2715407372
Processing class: 75
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 2.3067117214
Epoch:   200  |  train loss: 2.2793322563
Epoch:   300  |  train loss: 2.2908013344
Epoch:   400  |  train loss: 2.2668775082
Epoch:   500  |  train loss: 2.2786302567
Epoch:   600  |  train loss: 2.2916260242
Epoch:   700  |  train loss: 2.2040421486
Epoch:   800  |  train loss: 2.2411491871
Epoch:   900  |  train loss: 2.2270630360
Epoch:  1000  |  train loss: 2.2477059364
Processing class: 76
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 2.0219596148
Epoch:   200  |  train loss: 1.9881348610
Epoch:   300  |  train loss: 2.0010581970
Epoch:   400  |  train loss: 2.0139382601
Epoch:   500  |  train loss: 2.0162328959
Epoch:   600  |  train loss: 2.0296139240
Epoch:   700  |  train loss: 2.0326458216
Epoch:   800  |  train loss: 1.9840106726
Epoch:   900  |  train loss: 2.0180691957
Epoch:  1000  |  train loss: 2.0317693472
Processing class: 77
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 2.2653731346
Epoch:   200  |  train loss: 2.2430621147
Epoch:   300  |  train loss: 2.1845036507
Epoch:   400  |  train loss: 2.1767918110
Epoch:   500  |  train loss: 2.1539277077
Epoch:   600  |  train loss: 2.1296632290
Epoch:   700  |  train loss: 2.1211509705
Epoch:   800  |  train loss: 2.1381021023
Epoch:   900  |  train loss: 2.1096494198
Epoch:  1000  |  train loss: 2.0925542831
Processing class: 78
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 2.3121335030
Epoch:   200  |  train loss: 2.2964337826
Epoch:   300  |  train loss: 2.3022058487
Epoch:   400  |  train loss: 2.2764418602
Epoch:   500  |  train loss: 2.2441045761
Epoch:   600  |  train loss: 2.2542391777
Epoch:   700  |  train loss: 2.2464594364
Epoch:   800  |  train loss: 2.2414813519
Epoch:   900  |  train loss: 2.2355665207
Epoch:  1000  |  train loss: 2.2197718143
Processing class: 79
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 2.3506154060
Epoch:   200  |  train loss: 2.3142333984
Epoch:   300  |  train loss: 2.3377584934
Epoch:   400  |  train loss: 2.3205142975
Epoch:   500  |  train loss: 2.3094270706
Epoch:   600  |  train loss: 2.3309778214
Epoch:   700  |  train loss: 2.3186810493
Epoch:   800  |  train loss: 2.3206601620
Epoch:   900  |  train loss: 2.3361349106
Epoch:  1000  |  train loss: 2.2990021229
2024-03-05 19:53:05,507 [trainer.py] => CNN: {'total': 59.18, '00-09': 73.6, '10-19': 68.4, '20-29': 76.9, '30-39': 69.1, '40-49': 60.6, '50-59': 37.2, '60-69': 45.7, '70-79': 41.9, 'old': 61.64, 'new': 41.9}
2024-03-05 19:53:05,508 [trainer.py] => No NME accuracy
2024-03-05 19:53:05,509 [trainer.py] => FeCAM: {'total': 61.72, '00-09': 78.1, '10-19': 69.3, '20-29': 78.0, '30-39': 69.5, '40-49': 72.1, '50-59': 35.7, '60-69': 49.1, '70-79': 42.0, 'old': 64.54, 'new': 42.0}
2024-03-05 19:53:05,509 [trainer.py] => CNN top1 curve: [83.44, 71.75, 64.66, 59.18]
2024-03-05 19:53:05,509 [trainer.py] => CNN top5 curve: [96.5, 89.6, 86.54, 84.09]
2024-03-05 19:53:05,509 [trainer.py] => FeCAM top1 curve: [82.9, 72.78, 67.27, 61.72]
2024-03-05 19:53:05,509 [trainer.py] => FeCAM top5 curve: [95.84, 91.18, 87.21, 84.09]

2024-03-05 19:53:05,519 [fecam.py] => Learning on 80-90
Processing class: 80
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 2.1656376839
Epoch:   200  |  train loss: 2.1409188271
Epoch:   300  |  train loss: 2.1312883377
Epoch:   400  |  train loss: 2.1260504723
Epoch:   500  |  train loss: 2.1067631721
Epoch:   600  |  train loss: 2.0975044727
Epoch:   700  |  train loss: 2.0554262877
Epoch:   800  |  train loss: 2.0962917805
Epoch:   900  |  train loss: 2.0788930416
Epoch:  1000  |  train loss: 2.0959559441
Processing class: 81
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 2.2679776192
Epoch:   200  |  train loss: 2.2836494923
Epoch:   300  |  train loss: 2.2286025524
Epoch:   400  |  train loss: 2.2549401283
Epoch:   500  |  train loss: 2.2267292500
Epoch:   600  |  train loss: 2.2455943584
Epoch:   700  |  train loss: 2.2258878231
Epoch:   800  |  train loss: 2.2392724991
Epoch:   900  |  train loss: 2.2274827003
Epoch:  1000  |  train loss: 2.2238398552
Processing class: 82
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 2.2165428162
Epoch:   200  |  train loss: 2.2057036877
Epoch:   300  |  train loss: 2.1850999355
Epoch:   400  |  train loss: 2.1569151402
Epoch:   500  |  train loss: 2.1453088760
Epoch:   600  |  train loss: 2.1228915215
Epoch:   700  |  train loss: 2.1115241528
Epoch:   800  |  train loss: 2.1122239590
Epoch:   900  |  train loss: 2.1038334131
Epoch:  1000  |  train loss: 2.0952228546
Processing class: 83
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 2.2228843212
Epoch:   200  |  train loss: 2.1951620102
Epoch:   300  |  train loss: 2.1444332123
Epoch:   400  |  train loss: 2.1295833588
Epoch:   500  |  train loss: 2.1452435970
Epoch:   600  |  train loss: 2.1068009377
Epoch:   700  |  train loss: 2.1324355364
Epoch:   800  |  train loss: 2.1643530369
Epoch:   900  |  train loss: 2.1388314247
Epoch:  1000  |  train loss: 2.1135943890
Processing class: 84
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 2.3212729454
Epoch:   200  |  train loss: 2.2678692818
Epoch:   300  |  train loss: 2.2716128826
Epoch:   400  |  train loss: 2.2463034630
Epoch:   500  |  train loss: 2.2501641750
Epoch:   600  |  train loss: 2.2090603828
Epoch:   700  |  train loss: 2.2251800537
Epoch:   800  |  train loss: 2.2407274723
Epoch:   900  |  train loss: 2.1933528900
Epoch:  1000  |  train loss: 2.1779508591
Processing class: 85
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 2.3123185158
Epoch:   200  |  train loss: 2.2648177624
Epoch:   300  |  train loss: 2.2646552086
Epoch:   400  |  train loss: 2.2262122631
Epoch:   500  |  train loss: 2.2291211605
Epoch:   600  |  train loss: 2.2373994827
Epoch:   700  |  train loss: 2.2641090393
Epoch:   800  |  train loss: 2.2216749668
Epoch:   900  |  train loss: 2.2164894104
Epoch:  1000  |  train loss: 2.2355788231
Processing class: 86
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 2.3725719929
Epoch:   200  |  train loss: 2.3256859779
Epoch:   300  |  train loss: 2.3436045647
Epoch:   400  |  train loss: 2.3050973892
Epoch:   500  |  train loss: 2.2910928249
Epoch:   600  |  train loss: 2.2901948929
Epoch:   700  |  train loss: 2.2793915272
Epoch:   800  |  train loss: 2.2628260612
Epoch:   900  |  train loss: 2.2528880596
Epoch:  1000  |  train loss: 2.2840372562
Processing class: 87
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 2.2297962189
Epoch:   200  |  train loss: 2.2001101017
Epoch:   300  |  train loss: 2.1772627831
Epoch:   400  |  train loss: 2.1660476208
Epoch:   500  |  train loss: 2.1431208611
Epoch:   600  |  train loss: 2.1527522564
Epoch:   700  |  train loss: 2.1511274338
Epoch:   800  |  train loss: 2.1403244495
Epoch:   900  |  train loss: 2.1097092152
Epoch:  1000  |  train loss: 2.1476300240
Processing class: 88
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 2.2830612659
Epoch:   200  |  train loss: 2.2462443829
Epoch:   300  |  train loss: 2.2157163620
Epoch:   400  |  train loss: 2.2006411076
Epoch:   500  |  train loss: 2.1726554394
Epoch:   600  |  train loss: 2.1360738754
Epoch:   700  |  train loss: 2.1478901863
Epoch:   800  |  train loss: 2.1342782974
Epoch:   900  |  train loss: 2.1313079357
Epoch:  1000  |  train loss: 2.1040887356
Processing class: 89
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 2.2935710430
Epoch:   200  |  train loss: 2.2855558395
Epoch:   300  |  train loss: 2.2632417202
Epoch:   400  |  train loss: 2.2529055119
Epoch:   500  |  train loss: 2.2623771191
Epoch:   600  |  train loss: 2.2434069633
Epoch:   700  |  train loss: 2.2037446022
Epoch:   800  |  train loss: 2.2461910725
Epoch:   900  |  train loss: 2.2195183277
Epoch:  1000  |  train loss: 2.2237957001
2024-03-05 20:02:01,445 [trainer.py] => CNN: {'total': 54.08, '00-09': 67.9, '10-19': 63.6, '20-29': 73.2, '30-39': 68.8, '40-49': 57.2, '50-59': 32.6, '60-69': 40.9, '70-79': 38.1, '80-89': 44.4, 'old': 55.29, 'new': 44.4}
2024-03-05 20:02:01,448 [trainer.py] => No NME accuracy
2024-03-05 20:02:01,448 [trainer.py] => FeCAM: {'total': 57.66, '00-09': 76.1, '10-19': 65.8, '20-29': 75.3, '30-39': 67.9, '40-49': 68.5, '50-59': 33.0, '60-69': 45.0, '70-79': 39.0, '80-89': 48.3, 'old': 58.82, 'new': 48.3}
2024-03-05 20:02:01,448 [trainer.py] => CNN top1 curve: [83.44, 71.75, 64.66, 59.18, 54.08]
2024-03-05 20:02:01,448 [trainer.py] => CNN top5 curve: [96.5, 89.6, 86.54, 84.09, 81.74]
2024-03-05 20:02:01,448 [trainer.py] => FeCAM top1 curve: [82.9, 72.78, 67.27, 61.72, 57.66]
2024-03-05 20:02:01,448 [trainer.py] => FeCAM top5 curve: [95.84, 91.18, 87.21, 84.09, 81.63]

2024-03-05 20:02:01,455 [fecam.py] => Learning on 90-100
Processing class: 90
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 2.3162796021
Epoch:   200  |  train loss: 2.3013792992
Epoch:   300  |  train loss: 2.2346555233
Epoch:   400  |  train loss: 2.1945369720
Epoch:   500  |  train loss: 2.1594896317
Epoch:   600  |  train loss: 2.1507908344
Epoch:   700  |  train loss: 2.1212890625
Epoch:   800  |  train loss: 2.1155366898
Epoch:   900  |  train loss: 2.1213305950
Epoch:  1000  |  train loss: 2.1407759190
Processing class: 91
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 2.0672611237
Epoch:   200  |  train loss: 2.0302461147
Epoch:   300  |  train loss: 2.0332560778
Epoch:   400  |  train loss: 2.0480561256
Epoch:   500  |  train loss: 2.0495759964
Epoch:   600  |  train loss: 2.0501755953
Epoch:   700  |  train loss: 2.0370915174
Epoch:   800  |  train loss: 2.0354032993
Epoch:   900  |  train loss: 2.0401227951
Epoch:  1000  |  train loss: 2.0382131100
Processing class: 92
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 2.3123212337
Epoch:   200  |  train loss: 2.2206346035
Epoch:   300  |  train loss: 2.1638280392
Epoch:   400  |  train loss: 2.1657950878
Epoch:   500  |  train loss: 2.1565338135
Epoch:   600  |  train loss: 2.1532222271
Epoch:   700  |  train loss: 2.1348333359
Epoch:   800  |  train loss: 2.0784807444
Epoch:   900  |  train loss: 2.0769885540
Epoch:  1000  |  train loss: 2.0718842983
Processing class: 93
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 2.2910145283
Epoch:   200  |  train loss: 2.2637866497
Epoch:   300  |  train loss: 2.2936471939
Epoch:   400  |  train loss: 2.2853665829
Epoch:   500  |  train loss: 2.2570243835
Epoch:   600  |  train loss: 2.2139997959
Epoch:   700  |  train loss: 2.2515312672
Epoch:   800  |  train loss: 2.2312048912
Epoch:   900  |  train loss: 2.2248341084
Epoch:  1000  |  train loss: 2.2041236877
Processing class: 94
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 2.1225609303
Epoch:   200  |  train loss: 2.0849358320
Epoch:   300  |  train loss: 2.0009142637
Epoch:   400  |  train loss: 1.9879842043
Epoch:   500  |  train loss: 1.9165605307
Epoch:   600  |  train loss: 1.9275091887
Epoch:   700  |  train loss: 1.9398835421
Epoch:   800  |  train loss: 1.8642315865
Epoch:   900  |  train loss: 1.8941177845
Epoch:  1000  |  train loss: 1.8717699766
Processing class: 95
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 2.3087481976
Epoch:   200  |  train loss: 2.3307877541
Epoch:   300  |  train loss: 2.2947334290
Epoch:   400  |  train loss: 2.2625684738
Epoch:   500  |  train loss: 2.2845620632
Epoch:   600  |  train loss: 2.2646091938
Epoch:   700  |  train loss: 2.2658694744
Epoch:   800  |  train loss: 2.2392592430
Epoch:   900  |  train loss: 2.2546371460
Epoch:  1000  |  train loss: 2.2684517384
Processing class: 96
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 2.3549530506
Epoch:   200  |  train loss: 2.2877193928
Epoch:   300  |  train loss: 2.2906737328
Epoch:   400  |  train loss: 2.3216474533
Epoch:   500  |  train loss: 2.2865416527
Epoch:   600  |  train loss: 2.2685145378
Epoch:   700  |  train loss: 2.2596055508
Epoch:   800  |  train loss: 2.2260348320
Epoch:   900  |  train loss: 2.2586688042
Epoch:  1000  |  train loss: 2.2441966057
Processing class: 97
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 2.3160590649
Epoch:   200  |  train loss: 2.2707976341
Epoch:   300  |  train loss: 2.2425307751
Epoch:   400  |  train loss: 2.2328822613
Epoch:   500  |  train loss: 2.2437214851
Epoch:   600  |  train loss: 2.2457698345
Epoch:   700  |  train loss: 2.2517553806
Epoch:   800  |  train loss: 2.2410031319
Epoch:   900  |  train loss: 2.2400279045
Epoch:  1000  |  train loss: 2.2267727375
Processing class: 98
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 2.2155652523
Epoch:   200  |  train loss: 2.1722247601
Epoch:   300  |  train loss: 2.1382159233
Epoch:   400  |  train loss: 2.1248659134
Epoch:   500  |  train loss: 2.0948046207
Epoch:   600  |  train loss: 2.0940119743
Epoch:   700  |  train loss: 2.0492719412
Epoch:   800  |  train loss: 2.0637312412
Epoch:   900  |  train loss: 2.0563470125
Epoch:  1000  |  train loss: 2.0348743677
Processing class: 99
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 2.3531581402
Epoch:   200  |  train loss: 2.2945187569
Epoch:   300  |  train loss: 2.3038805485
Epoch:   400  |  train loss: 2.3144529343
Epoch:   500  |  train loss: 2.2800086021
Epoch:   600  |  train loss: 2.2544628143
Epoch:   700  |  train loss: 2.2339325905
Epoch:   800  |  train loss: 2.2526305676
Epoch:   900  |  train loss: 2.2234752178
Epoch:  1000  |  train loss: 2.2203981400
2024-03-05 20:12:08,393 [trainer.py] => CNN: {'total': 50.36, '00-09': 57.7, '10-19': 63.6, '20-29': 71.4, '30-39': 68.5, '40-49': 56.8, '50-59': 29.6, '60-69': 38.9, '70-79': 36.6, '80-89': 42.7, '90-99': 37.8, 'old': 51.76, 'new': 37.8}
2024-03-05 20:12:08,394 [trainer.py] => No NME accuracy
2024-03-05 20:12:08,394 [trainer.py] => FeCAM: {'total': 54.74, '00-09': 73.3, '10-19': 65.1, '20-29': 74.1, '30-39': 67.2, '40-49': 67.1, '50-59': 30.8, '60-69': 43.3, '70-79': 37.2, '80-89': 46.9, '90-99': 42.4, 'old': 56.11, 'new': 42.4}
2024-03-05 20:12:08,394 [trainer.py] => CNN top1 curve: [83.44, 71.75, 64.66, 59.18, 54.08, 50.36]
2024-03-05 20:12:08,394 [trainer.py] => CNN top5 curve: [96.5, 89.6, 86.54, 84.09, 81.74, 79.63]
2024-03-05 20:12:08,394 [trainer.py] => FeCAM top1 curve: [82.9, 72.78, 67.27, 61.72, 57.66, 54.74]
2024-03-05 20:12:08,394 [trainer.py] => FeCAM top5 curve: [95.84, 91.18, 87.21, 84.09, 81.63, 79.21]

=========================================
2024-03-05 20:12:16,640 [trainer.py] => config: ./exps/FeCAM_cifar100.json
2024-03-05 20:12:16,640 [trainer.py] => prefix: train
2024-03-05 20:12:16,640 [trainer.py] => dataset: cifar100
2024-03-05 20:12:16,640 [trainer.py] => memory_size: 0
2024-03-05 20:12:16,640 [trainer.py] => shuffle: True
2024-03-05 20:12:16,640 [trainer.py] => init_cls: 50
2024-03-05 20:12:16,640 [trainer.py] => increment: 10
2024-03-05 20:12:16,640 [trainer.py] => model_name: fecam
2024-03-05 20:12:16,640 [trainer.py] => convnet_type: resnet18
2024-03-05 20:12:16,640 [trainer.py] => device: [device(type='cuda', index=0)]
2024-03-05 20:12:16,640 [trainer.py] => seed: 1993
2024-03-05 20:12:16,640 [trainer.py] => init_epochs: 200
2024-03-05 20:12:16,640 [trainer.py] => init_lr: 0.1
2024-03-05 20:12:16,640 [trainer.py] => init_weight_decay: 0.0005
2024-03-05 20:12:16,640 [trainer.py] => batch_size: 128
2024-03-05 20:12:16,640 [trainer.py] => num_workers: 8
2024-03-05 20:12:16,640 [trainer.py] => T: 5
2024-03-05 20:12:16,640 [trainer.py] => beta: 0.5
2024-03-05 20:12:16,640 [trainer.py] => alpha1: 1
2024-03-05 20:12:16,640 [trainer.py] => alpha2: 1
2024-03-05 20:12:16,640 [trainer.py] => ncm: False
2024-03-05 20:12:16,640 [trainer.py] => tukey: False
2024-03-05 20:12:16,640 [trainer.py] => diagonal: False
2024-03-05 20:12:16,640 [trainer.py] => per_class: True
2024-03-05 20:12:16,640 [trainer.py] => full_cov: True
2024-03-05 20:12:16,640 [trainer.py] => shrink: True
2024-03-05 20:12:16,640 [trainer.py] => norm_cov: False
2024-03-05 20:12:16,640 [trainer.py] => vecnorm: False
2024-03-05 20:12:16,641 [trainer.py] => ae_type: wae
2024-03-05 20:12:16,641 [trainer.py] => epochs: 1000
2024-03-05 20:12:16,641 [trainer.py] => ae_latent_dim: 32
2024-03-05 20:12:16,641 [trainer.py] => wae_sigma: 30
2024-03-05 20:12:16,641 [trainer.py] => wae_C: 5
Files already downloaded and verified
Files already downloaded and verified
2024-03-05 20:12:18,295 [data_manager.py] => [68, 56, 78, 8, 23, 84, 90, 65, 74, 76, 40, 89, 3, 92, 55, 9, 26, 80, 43, 38, 58, 70, 77, 1, 85, 19, 17, 50, 28, 53, 13, 81, 45, 82, 6, 59, 83, 16, 15, 44, 91, 41, 72, 60, 79, 52, 20, 10, 31, 54, 37, 95, 14, 71, 96, 98, 97, 2, 64, 66, 42, 22, 35, 86, 24, 34, 87, 21, 99, 0, 88, 27, 18, 94, 11, 12, 47, 25, 30, 46, 62, 69, 36, 61, 7, 63, 75, 5, 32, 4, 51, 48, 73, 93, 39, 67, 29, 49, 57, 33]
2024-03-05 20:12:18,564 [fecam.py] => Learning on 0-50
/home/z1165703/miniconda3/lib/python3.11/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
/home/z1165703/miniconda3/lib/python3.11/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
Processing class: 0
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 1.7739280939
Epoch:   200  |  train loss: 1.7846190691
Epoch:   300  |  train loss: 1.8151068211
Epoch:   400  |  train loss: 1.8232962608
Epoch:   500  |  train loss: 1.8060498714
Epoch:   600  |  train loss: 1.8062264681
Epoch:   700  |  train loss: 1.8166758299
Epoch:   800  |  train loss: 1.8203525066
Epoch:   900  |  train loss: 1.8047981262
Epoch:  1000  |  train loss: 1.8012968302
Processing class: 1
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 1.7687907219
Epoch:   200  |  train loss: 1.7552580833
Epoch:   300  |  train loss: 1.8169110775
Epoch:   400  |  train loss: 1.8293177128
Epoch:   500  |  train loss: 1.7888401508
Epoch:   600  |  train loss: 1.8365396261
Epoch:   700  |  train loss: 1.8129970789
Epoch:   800  |  train loss: 1.8360898018
Epoch:   900  |  train loss: 1.8719136715
Epoch:  1000  |  train loss: 1.8334441662
Processing class: 2
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 1.7655621052
Epoch:   200  |  train loss: 1.7571566105
Epoch:   300  |  train loss: 1.7972199917
Epoch:   400  |  train loss: 1.7826665163
Epoch:   500  |  train loss: 1.7119808674
Epoch:   600  |  train loss: 1.7603439569
Epoch:   700  |  train loss: 1.7483781099
Epoch:   800  |  train loss: 1.7851144314
Epoch:   900  |  train loss: 1.7576886654
Epoch:  1000  |  train loss: 1.7419118881
Processing class: 3
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 1.8436261654
Epoch:   200  |  train loss: 1.8848834515
Epoch:   300  |  train loss: 1.9010130167
Epoch:   400  |  train loss: 1.9035258770
Epoch:   500  |  train loss: 1.8823456764
Epoch:   600  |  train loss: 1.9224778175
Epoch:   700  |  train loss: 1.9448488474
Epoch:   800  |  train loss: 1.9133533478
Epoch:   900  |  train loss: 1.9311702251
Epoch:  1000  |  train loss: 1.9349137306
Processing class: 4
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 1.7564907789
Epoch:   200  |  train loss: 1.7353133202
Epoch:   300  |  train loss: 1.7827940464
Epoch:   400  |  train loss: 1.7800153971
Epoch:   500  |  train loss: 1.7978081942
Epoch:   600  |  train loss: 1.7832701445
Epoch:   700  |  train loss: 1.8053201675
Epoch:   800  |  train loss: 1.8099956512
Epoch:   900  |  train loss: 1.8188597441
Epoch:  1000  |  train loss: 1.8487656355
Processing class: 5
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 1.7762651920
Epoch:   200  |  train loss: 1.7565230608
Epoch:   300  |  train loss: 1.7685192108
Epoch:   400  |  train loss: 1.7641424894
Epoch:   500  |  train loss: 1.7575062752
Epoch:   600  |  train loss: 1.8133210182
Epoch:   700  |  train loss: 1.8201811790
Epoch:   800  |  train loss: 1.7891699314
Epoch:   900  |  train loss: 1.7792061806
Epoch:  1000  |  train loss: 1.8034487009
Processing class: 6
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 1.7721884727
Epoch:   200  |  train loss: 1.7923479795
Epoch:   300  |  train loss: 1.8414854765
Epoch:   400  |  train loss: 1.8294236422
Epoch:   500  |  train loss: 1.8698753357
Epoch:   600  |  train loss: 1.8505014896
Epoch:   700  |  train loss: 1.8343426228
Epoch:   800  |  train loss: 1.8451705933
Epoch:   900  |  train loss: 1.8217254162
Epoch:  1000  |  train loss: 1.8379380226
Processing class: 7
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 1.7470482826
Epoch:   200  |  train loss: 1.7519891024
Epoch:   300  |  train loss: 1.7649655342
Epoch:   400  |  train loss: 1.7714462519
Epoch:   500  |  train loss: 1.7880733728
Epoch:   600  |  train loss: 1.7704599619
Epoch:   700  |  train loss: 1.7906105757
Epoch:   800  |  train loss: 1.8277596712
Epoch:   900  |  train loss: 1.7897057772
Epoch:  1000  |  train loss: 1.8178393602
Processing class: 8
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 1.7554047108
Epoch:   200  |  train loss: 1.7807226419
Epoch:   300  |  train loss: 1.7602433443
Epoch:   400  |  train loss: 1.8077038527
Epoch:   500  |  train loss: 1.7819351912
Epoch:   600  |  train loss: 1.7909994841
Epoch:   700  |  train loss: 1.8249633789
Epoch:   800  |  train loss: 1.7906834126
Epoch:   900  |  train loss: 1.8103540897
Epoch:  1000  |  train loss: 1.7877586365
Processing class: 9
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 1.7201350689
Epoch:   200  |  train loss: 1.7482697725
Epoch:   300  |  train loss: 1.7886875629
Epoch:   400  |  train loss: 1.7460264921
Epoch:   500  |  train loss: 1.7853982449
Epoch:   600  |  train loss: 1.7576843977
Epoch:   700  |  train loss: 1.7722735405
Epoch:   800  |  train loss: 1.7827349424
Epoch:   900  |  train loss: 1.7825157642
Epoch:  1000  |  train loss: 1.7543936491
Processing class: 10
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 1.7075470209
Epoch:   200  |  train loss: 1.6978562593
Epoch:   300  |  train loss: 1.7100325584
Epoch:   400  |  train loss: 1.7545212030
Epoch:   500  |  train loss: 1.6940566063
Epoch:   600  |  train loss: 1.7126856327
Epoch:   700  |  train loss: 1.7277143240
Epoch:   800  |  train loss: 1.7126527309
Epoch:   900  |  train loss: 1.7247933626
Epoch:  1000  |  train loss: 1.7205237865
Processing class: 11
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 1.7783863544
Epoch:   200  |  train loss: 1.7866098404
Epoch:   300  |  train loss: 1.7626270056
Epoch:   400  |  train loss: 1.7809415340
Epoch:   500  |  train loss: 1.7774648190
Epoch:   600  |  train loss: 1.8046903849
Epoch:   700  |  train loss: 1.8465762615
Epoch:   800  |  train loss: 1.8365962744
Epoch:   900  |  train loss: 1.8359948397
Epoch:  1000  |  train loss: 1.8559803724
Processing class: 12
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 1.7122205019
Epoch:   200  |  train loss: 1.7784780979
Epoch:   300  |  train loss: 1.7764438629
Epoch:   400  |  train loss: 1.8039414883
Epoch:   500  |  train loss: 1.8058290243
Epoch:   600  |  train loss: 1.7820177317
Epoch:   700  |  train loss: 1.7917670250
Epoch:   800  |  train loss: 1.8015986443
Epoch:   900  |  train loss: 1.8226165533
Epoch:  1000  |  train loss: 1.8035762787
Processing class: 13
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 1.7340810061
Epoch:   200  |  train loss: 1.7801504374
Epoch:   300  |  train loss: 1.8131772518
Epoch:   400  |  train loss: 1.8180305481
Epoch:   500  |  train loss: 1.8469565868
Epoch:   600  |  train loss: 1.8174522161
Epoch:   700  |  train loss: 1.8346428394
Epoch:   800  |  train loss: 1.8537513256
Epoch:   900  |  train loss: 1.8646174908
Epoch:  1000  |  train loss: 1.8754515409
Processing class: 14
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 1.7726710558
Epoch:   200  |  train loss: 1.7929877281
Epoch:   300  |  train loss: 1.8020911694
Epoch:   400  |  train loss: 1.8328270674
Epoch:   500  |  train loss: 1.8199632168
Epoch:   600  |  train loss: 1.8164358854
Epoch:   700  |  train loss: 1.8363670349
Epoch:   800  |  train loss: 1.8802216053
Epoch:   900  |  train loss: 1.8630862951
Epoch:  1000  |  train loss: 1.8315731287
Processing class: 15
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 1.7513921738
Epoch:   200  |  train loss: 1.7711485386
Epoch:   300  |  train loss: 1.8073915243
Epoch:   400  |  train loss: 1.8221541166
Epoch:   500  |  train loss: 1.8436861992
Epoch:   600  |  train loss: 1.8164422512
Epoch:   700  |  train loss: 1.8537847996
Epoch:   800  |  train loss: 1.8170945406
Epoch:   900  |  train loss: 1.8542619705
Epoch:  1000  |  train loss: 1.8374031067
Processing class: 16
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 1.7674316645
Epoch:   200  |  train loss: 1.8456803322
Epoch:   300  |  train loss: 1.8793068647
Epoch:   400  |  train loss: 1.8555212498
Epoch:   500  |  train loss: 1.8872196913
Epoch:   600  |  train loss: 1.8589901447
Epoch:   700  |  train loss: 1.8467294455
Epoch:   800  |  train loss: 1.8913566828
Epoch:   900  |  train loss: 1.8460397005
Epoch:  1000  |  train loss: 1.8874811649
Processing class: 17
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 1.6995017529
Epoch:   200  |  train loss: 1.7250818014
Epoch:   300  |  train loss: 1.7632589340
Epoch:   400  |  train loss: 1.7662769794
Epoch:   500  |  train loss: 1.7137376785
Epoch:   600  |  train loss: 1.7397370100
Epoch:   700  |  train loss: 1.7290857792
Epoch:   800  |  train loss: 1.7299606562
Epoch:   900  |  train loss: 1.7390136242
Epoch:  1000  |  train loss: 1.7340584755
Processing class: 18
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 1.7357592821
Epoch:   200  |  train loss: 1.7461769104
Epoch:   300  |  train loss: 1.7685789824
Epoch:   400  |  train loss: 1.7603061199
Epoch:   500  |  train loss: 1.7958103180
Epoch:   600  |  train loss: 1.7746551991
Epoch:   700  |  train loss: 1.8328078508
Epoch:   800  |  train loss: 1.8261643410
Epoch:   900  |  train loss: 1.8356536865
Epoch:  1000  |  train loss: 1.8675534487
Processing class: 19
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 1.7444869041
Epoch:   200  |  train loss: 1.8142195225
Epoch:   300  |  train loss: 1.8104740143
Epoch:   400  |  train loss: 1.8156303883
Epoch:   500  |  train loss: 1.7865915537
Epoch:   600  |  train loss: 1.7912255287
Epoch:   700  |  train loss: 1.8062539577
Epoch:   800  |  train loss: 1.7920300961
Epoch:   900  |  train loss: 1.8550383329
Epoch:  1000  |  train loss: 1.8664190531
Processing class: 20
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 1.7599124432
Epoch:   200  |  train loss: 1.7789238691
Epoch:   300  |  train loss: 1.8024183273
Epoch:   400  |  train loss: 1.8149561405
Epoch:   500  |  train loss: 1.8505949259
Epoch:   600  |  train loss: 1.8257174015
Epoch:   700  |  train loss: 1.8309269428
Epoch:   800  |  train loss: 1.8306699276
Epoch:   900  |  train loss: 1.8437654734
Epoch:  1000  |  train loss: 1.8689944267
Processing class: 21
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 1.7670675039
Epoch:   200  |  train loss: 1.7925083160
Epoch:   300  |  train loss: 1.7988126755
Epoch:   400  |  train loss: 1.8046175003
Epoch:   500  |  train loss: 1.8152210474
Epoch:   600  |  train loss: 1.8340577364
Epoch:   700  |  train loss: 1.8257324696
Epoch:   800  |  train loss: 1.8130892515
Epoch:   900  |  train loss: 1.8424249172
Epoch:  1000  |  train loss: 1.8295579672
Processing class: 22
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 1.7533025742
Epoch:   200  |  train loss: 1.7631458282
Epoch:   300  |  train loss: 1.7944461584
Epoch:   400  |  train loss: 1.8264691591
Epoch:   500  |  train loss: 1.8305949688
Epoch:   600  |  train loss: 1.8081980467
Epoch:   700  |  train loss: 1.8210922003
Epoch:   800  |  train loss: 1.8062919140
Epoch:   900  |  train loss: 1.7894788980
Epoch:  1000  |  train loss: 1.8405810118
Processing class: 23
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 1.7859558344
Epoch:   200  |  train loss: 1.8184343815
Epoch:   300  |  train loss: 1.8488864422
Epoch:   400  |  train loss: 1.8088814735
Epoch:   500  |  train loss: 1.8043664694
Epoch:   600  |  train loss: 1.7799377680
Epoch:   700  |  train loss: 1.7980814457
Epoch:   800  |  train loss: 1.7786280870
Epoch:   900  |  train loss: 1.7991684198
Epoch:  1000  |  train loss: 1.7751580715
Processing class: 24
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 1.7709764242
Epoch:   200  |  train loss: 1.7701290607
Epoch:   300  |  train loss: 1.7389513731
Epoch:   400  |  train loss: 1.7698776007
Epoch:   500  |  train loss: 1.7503054619
Epoch:   600  |  train loss: 1.7697185755
Epoch:   700  |  train loss: 1.7210670948
Epoch:   800  |  train loss: 1.7353072166
Epoch:   900  |  train loss: 1.7611463547
Epoch:  1000  |  train loss: 1.7472724199
Processing class: 25
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 1.7485919714
Epoch:   200  |  train loss: 1.7706246614
Epoch:   300  |  train loss: 1.8044863939
Epoch:   400  |  train loss: 1.7719765186
Epoch:   500  |  train loss: 1.8086696148
Epoch:   600  |  train loss: 1.7858917236
Epoch:   700  |  train loss: 1.8228670597
Epoch:   800  |  train loss: 1.8050975323
Epoch:   900  |  train loss: 1.7910617352
Epoch:  1000  |  train loss: 1.8035285711
Processing class: 26
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 1.8033893108
Epoch:   200  |  train loss: 1.8466786623
Epoch:   300  |  train loss: 1.8227066755
Epoch:   400  |  train loss: 1.8544613123
Epoch:   500  |  train loss: 1.8848310471
Epoch:   600  |  train loss: 1.8301137209
Epoch:   700  |  train loss: 1.8465435743
Epoch:   800  |  train loss: 1.8616801977
Epoch:   900  |  train loss: 1.8510917425
Epoch:  1000  |  train loss: 1.8793280840
Processing class: 27
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 1.7612594366
Epoch:   200  |  train loss: 1.7395323992
Epoch:   300  |  train loss: 1.7531671762
Epoch:   400  |  train loss: 1.7589743614
Epoch:   500  |  train loss: 1.7766491175
Epoch:   600  |  train loss: 1.7762979746
Epoch:   700  |  train loss: 1.7867484808
Epoch:   800  |  train loss: 1.7835519552
Epoch:   900  |  train loss: 1.7934899807
Epoch:  1000  |  train loss: 1.7549988508
Processing class: 28
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 1.7942854643
Epoch:   200  |  train loss: 1.8294962168
Epoch:   300  |  train loss: 1.8618698597
Epoch:   400  |  train loss: 1.8760337830
Epoch:   500  |  train loss: 1.8840275526
Epoch:   600  |  train loss: 1.8835777521
Epoch:   700  |  train loss: 1.8911356926
Epoch:   800  |  train loss: 1.9016426086
Epoch:   900  |  train loss: 1.9076993465
Epoch:  1000  |  train loss: 1.8782593012
Processing class: 29
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 1.7583229780
Epoch:   200  |  train loss: 1.7772478104
Epoch:   300  |  train loss: 1.7697216988
Epoch:   400  |  train loss: 1.7489763737
Epoch:   500  |  train loss: 1.7770826340
Epoch:   600  |  train loss: 1.7890607119
Epoch:   700  |  train loss: 1.7848433018
Epoch:   800  |  train loss: 1.7600264311
Epoch:   900  |  train loss: 1.7639598608
Epoch:  1000  |  train loss: 1.7542207479
Processing class: 30
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 1.8008955240
Epoch:   200  |  train loss: 1.8390484810
Epoch:   300  |  train loss: 1.8887841463
Epoch:   400  |  train loss: 1.9347245932
Epoch:   500  |  train loss: 1.9120355844
Epoch:   600  |  train loss: 1.9058199883
Epoch:   700  |  train loss: 1.8947418451
Epoch:   800  |  train loss: 1.9258764505
Epoch:   900  |  train loss: 1.9454495430
Epoch:  1000  |  train loss: 1.9191998482
Processing class: 31
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 1.7976432800
Epoch:   200  |  train loss: 1.8377420425
Epoch:   300  |  train loss: 1.8369774342
Epoch:   400  |  train loss: 1.8787962675
Epoch:   500  |  train loss: 1.8907460451
Epoch:   600  |  train loss: 1.9224065065
Epoch:   700  |  train loss: 1.9115402460
Epoch:   800  |  train loss: 1.9128961086
Epoch:   900  |  train loss: 1.9381429672
Epoch:  1000  |  train loss: 1.9045310736
Processing class: 32
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 1.7696831465
Epoch:   200  |  train loss: 1.8048753262
Epoch:   300  |  train loss: 1.7941537857
Epoch:   400  |  train loss: 1.7582497358
Epoch:   500  |  train loss: 1.8150813341
Epoch:   600  |  train loss: 1.7930302143
Epoch:   700  |  train loss: 1.8363602638
Epoch:   800  |  train loss: 1.8508056164
Epoch:   900  |  train loss: 1.8385020733
Epoch:  1000  |  train loss: 1.8236268997
Processing class: 33
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 1.7039689541
Epoch:   200  |  train loss: 1.7897709846
Epoch:   300  |  train loss: 1.7223592520
Epoch:   400  |  train loss: 1.7257032156
Epoch:   500  |  train loss: 1.7285687685
Epoch:   600  |  train loss: 1.7741530180
Epoch:   700  |  train loss: 1.7460507393
Epoch:   800  |  train loss: 1.7554767370
Epoch:   900  |  train loss: 1.7488768339
Epoch:  1000  |  train loss: 1.7334056854
Processing class: 34
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 1.7749971628
Epoch:   200  |  train loss: 1.7638996124
Epoch:   300  |  train loss: 1.7873754978
Epoch:   400  |  train loss: 1.7672531128
Epoch:   500  |  train loss: 1.7609589338
Epoch:   600  |  train loss: 1.7873978376
Epoch:   700  |  train loss: 1.7580792189
Epoch:   800  |  train loss: 1.7897875786
Epoch:   900  |  train loss: 1.7985306740
Epoch:  1000  |  train loss: 1.8040395260
Processing class: 35
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 1.7514982939
Epoch:   200  |  train loss: 1.7808296919
Epoch:   300  |  train loss: 1.8061592102
Epoch:   400  |  train loss: 1.8161290884
Epoch:   500  |  train loss: 1.7892157078
Epoch:   600  |  train loss: 1.8086596727
Epoch:   700  |  train loss: 1.8171544790
Epoch:   800  |  train loss: 1.8113608122
Epoch:   900  |  train loss: 1.8154283047
Epoch:  1000  |  train loss: 1.7993316889
Processing class: 36
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 1.7648742914
Epoch:   200  |  train loss: 1.7724540472
Epoch:   300  |  train loss: 1.7937876940
Epoch:   400  |  train loss: 1.7928058386
Epoch:   500  |  train loss: 1.8320201397
Epoch:   600  |  train loss: 1.8089436054
Epoch:   700  |  train loss: 1.7847319841
Epoch:   800  |  train loss: 1.8303554535
Epoch:   900  |  train loss: 1.7879863739
Epoch:  1000  |  train loss: 1.8142530918
Processing class: 37
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 1.7928245783
Epoch:   200  |  train loss: 1.7837909460
Epoch:   300  |  train loss: 1.7746993542
Epoch:   400  |  train loss: 1.7909520626
Epoch:   500  |  train loss: 1.8062018394
Epoch:   600  |  train loss: 1.7806332588
Epoch:   700  |  train loss: 1.7829208612
Epoch:   800  |  train loss: 1.7933254004
Epoch:   900  |  train loss: 1.7705497265
Epoch:  1000  |  train loss: 1.7826282024
Processing class: 38
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 1.7551479578
Epoch:   200  |  train loss: 1.7699329376
Epoch:   300  |  train loss: 1.8016667128
Epoch:   400  |  train loss: 1.8140181065
Epoch:   500  |  train loss: 1.8204096556
Epoch:   600  |  train loss: 1.8067046165
Epoch:   700  |  train loss: 1.8080530882
Epoch:   800  |  train loss: 1.8378953695
Epoch:   900  |  train loss: 1.7986410141
Epoch:  1000  |  train loss: 1.8238138437
Processing class: 39
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 1.7722285271
Epoch:   200  |  train loss: 1.7584748030
Epoch:   300  |  train loss: 1.7797710896
Epoch:   400  |  train loss: 1.7949378014
Epoch:   500  |  train loss: 1.7798937559
Epoch:   600  |  train loss: 1.7890442371
Epoch:   700  |  train loss: 1.8006922007
Epoch:   800  |  train loss: 1.8195011854
Epoch:   900  |  train loss: 1.8276225328
Epoch:  1000  |  train loss: 1.8335733175
Processing class: 40
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 1.7752277136
Epoch:   200  |  train loss: 1.7460964918
Epoch:   300  |  train loss: 1.8065362453
Epoch:   400  |  train loss: 1.8178356409
Epoch:   500  |  train loss: 1.8330297470
Epoch:   600  |  train loss: 1.8104238510
Epoch:   700  |  train loss: 1.8254055977
Epoch:   800  |  train loss: 1.8219693899
Epoch:   900  |  train loss: 1.8045560837
Epoch:  1000  |  train loss: 1.8584227562
Processing class: 41
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 1.7534731388
Epoch:   200  |  train loss: 1.7605020761
Epoch:   300  |  train loss: 1.7831409454
Epoch:   400  |  train loss: 1.7658626080
Epoch:   500  |  train loss: 1.7638906479
Epoch:   600  |  train loss: 1.7644221544
Epoch:   700  |  train loss: 1.7889415264
Epoch:   800  |  train loss: 1.7722855568
Epoch:   900  |  train loss: 1.7637394905
Epoch:  1000  |  train loss: 1.7635853052
Processing class: 42
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 1.7426650524
Epoch:   200  |  train loss: 1.7800481319
Epoch:   300  |  train loss: 1.8027441502
Epoch:   400  |  train loss: 1.8080546141
Epoch:   500  |  train loss: 1.8202267408
Epoch:   600  |  train loss: 1.8273247480
Epoch:   700  |  train loss: 1.8419074297
Epoch:   800  |  train loss: 1.8502164841
Epoch:   900  |  train loss: 1.8308790922
Epoch:  1000  |  train loss: 1.8258539200
Processing class: 43
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 1.7619204283
Epoch:   200  |  train loss: 1.7546738148
Epoch:   300  |  train loss: 1.8150737524
Epoch:   400  |  train loss: 1.8726906061
Epoch:   500  |  train loss: 1.8860363483
Epoch:   600  |  train loss: 1.8709421873
Epoch:   700  |  train loss: 1.8942712069
Epoch:   800  |  train loss: 1.8823090553
Epoch:   900  |  train loss: 1.8475052595
Epoch:  1000  |  train loss: 1.8873697281
Processing class: 44
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 1.7444772959
Epoch:   200  |  train loss: 1.7861236572
Epoch:   300  |  train loss: 1.7680897713
Epoch:   400  |  train loss: 1.7737988472
Epoch:   500  |  train loss: 1.7909813881
Epoch:   600  |  train loss: 1.7754466534
Epoch:   700  |  train loss: 1.8051293612
Epoch:   800  |  train loss: 1.7938251019
Epoch:   900  |  train loss: 1.7785542965
Epoch:  1000  |  train loss: 1.8105520248
Processing class: 45
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 1.7773442030
Epoch:   200  |  train loss: 1.8340710163
Epoch:   300  |  train loss: 1.8229734421
Epoch:   400  |  train loss: 1.8613536835
Epoch:   500  |  train loss: 1.8536159992
Epoch:   600  |  train loss: 1.9005455971
Epoch:   700  |  train loss: 1.9097634792
Epoch:   800  |  train loss: 1.8959535122
Epoch:   900  |  train loss: 1.9013291597
Epoch:  1000  |  train loss: 1.9259365082
Processing class: 46
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 1.7889719963
Epoch:   200  |  train loss: 1.7919257641
Epoch:   300  |  train loss: 1.8100016117
Epoch:   400  |  train loss: 1.8247532368
Epoch:   500  |  train loss: 1.8277512550
Epoch:   600  |  train loss: 1.8285319090
Epoch:   700  |  train loss: 1.8328472376
Epoch:   800  |  train loss: 1.8701864481
Epoch:   900  |  train loss: 1.8456238270
Epoch:  1000  |  train loss: 1.8626740932
Processing class: 47
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 1.7424907207
Epoch:   200  |  train loss: 1.7750203133
Epoch:   300  |  train loss: 1.7508217335
Epoch:   400  |  train loss: 1.7843473673
Epoch:   500  |  train loss: 1.7819902182
Epoch:   600  |  train loss: 1.7777819157
Epoch:   700  |  train loss: 1.7863768578
Epoch:   800  |  train loss: 1.7641221285
Epoch:   900  |  train loss: 1.7735135794
Epoch:  1000  |  train loss: 1.7602460861
Processing class: 48
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 1.7626544714
Epoch:   200  |  train loss: 1.8044015884
Epoch:   300  |  train loss: 1.7893329144
Epoch:   400  |  train loss: 1.7531810045
Epoch:   500  |  train loss: 1.8007753849
Epoch:   600  |  train loss: 1.7918096304
Epoch:   700  |  train loss: 1.7674592733
Epoch:   800  |  train loss: 1.7660449982
Epoch:   900  |  train loss: 1.8133895636
Epoch:  1000  |  train loss: 1.8241991520
Processing class: 49
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 1.7577338457
Epoch:   200  |  train loss: 1.7670869827
Epoch:   300  |  train loss: 1.8228468895
Epoch:   400  |  train loss: 1.8298173189
Epoch:   500  |  train loss: 1.8409002542
Epoch:   600  |  train loss: 1.8407580853
Epoch:   700  |  train loss: 1.8378953934
Epoch:   800  |  train loss: 1.8259014845
Epoch:   900  |  train loss: 1.8180417061
Epoch:  1000  |  train loss: 1.8239097834
/home/z1165703/FeCAM/models/base.py:87: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  vec = torch.tensor(vec).cuda()
2024-03-05 20:30:09,351 [trainer.py] => CNN: {'total': 83.44, '00-09': 87.7, '10-19': 79.6, '20-29': 84.9, '30-39': 81.0, '40-49': 84.0, 'old': 0, 'new': 83.44}
2024-03-05 20:30:09,352 [trainer.py] => No NME accuracy
2024-03-05 20:30:09,352 [trainer.py] => FeCAM: {'total': 82.9, '00-09': 86.2, '10-19': 79.7, '20-29': 85.0, '30-39': 80.6, '40-49': 83.0, 'old': 0, 'new': 82.9}
2024-03-05 20:30:09,352 [trainer.py] => CNN top1 curve: [83.44]
2024-03-05 20:30:09,352 [trainer.py] => CNN top5 curve: [96.5]
2024-03-05 20:30:09,352 [trainer.py] => FeCAM top1 curve: [82.9]
2024-03-05 20:30:09,352 [trainer.py] => FeCAM top5 curve: [95.84]

2024-03-05 20:30:09,361 [fecam.py] => Learning on 50-60
Processing class: 50
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 1.7481394053
Epoch:   200  |  train loss: 1.7551193953
Epoch:   300  |  train loss: 1.7590944290
Epoch:   400  |  train loss: 1.7881841183
Epoch:   500  |  train loss: 1.8031694174
Epoch:   600  |  train loss: 1.7797407150
Epoch:   700  |  train loss: 1.8124833107
Epoch:   800  |  train loss: 1.8272057295
Epoch:   900  |  train loss: 1.8469411373
Epoch:  1000  |  train loss: 1.8534909964
Processing class: 51
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 1.7638246059
Epoch:   200  |  train loss: 1.7249506474
Epoch:   300  |  train loss: 1.7758643389
Epoch:   400  |  train loss: 1.7845148325
Epoch:   500  |  train loss: 1.7885000706
Epoch:   600  |  train loss: 1.8047181368
Epoch:   700  |  train loss: 1.7738173008
Epoch:   800  |  train loss: 1.7798895836
Epoch:   900  |  train loss: 1.8198664427
Epoch:  1000  |  train loss: 1.8003130198
Processing class: 52
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 1.7787287712
Epoch:   200  |  train loss: 1.7643339157
Epoch:   300  |  train loss: 1.7847974062
Epoch:   400  |  train loss: 1.7991657972
Epoch:   500  |  train loss: 1.7918506861
Epoch:   600  |  train loss: 1.7688272953
Epoch:   700  |  train loss: 1.7997359514
Epoch:   800  |  train loss: 1.8003847122
Epoch:   900  |  train loss: 1.8070666313
Epoch:  1000  |  train loss: 1.7817124367
Processing class: 53
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 1.7348304987
Epoch:   200  |  train loss: 1.7434866428
Epoch:   300  |  train loss: 1.7083697557
Epoch:   400  |  train loss: 1.7194840431
Epoch:   500  |  train loss: 1.7357521772
Epoch:   600  |  train loss: 1.7628143549
Epoch:   700  |  train loss: 1.7929311037
Epoch:   800  |  train loss: 1.7307386398
Epoch:   900  |  train loss: 1.7602659941
Epoch:  1000  |  train loss: 1.7370960474
Processing class: 54
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 1.6322700262
Epoch:   200  |  train loss: 1.6381600142
Epoch:   300  |  train loss: 1.6633950233
Epoch:   400  |  train loss: 1.6717791557
Epoch:   500  |  train loss: 1.6849684715
Epoch:   600  |  train loss: 1.7197911263
Epoch:   700  |  train loss: 1.7283061266
Epoch:   800  |  train loss: 1.7565523624
Epoch:   900  |  train loss: 1.7759330511
Epoch:  1000  |  train loss: 1.7641361237
Processing class: 55
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 1.7622063160
Epoch:   200  |  train loss: 1.7666729689
Epoch:   300  |  train loss: 1.7478615761
Epoch:   400  |  train loss: 1.7842191935
Epoch:   500  |  train loss: 1.8321039200
Epoch:   600  |  train loss: 1.8363676310
Epoch:   700  |  train loss: 1.8510680676
Epoch:   800  |  train loss: 1.8868913651
Epoch:   900  |  train loss: 1.8401633024
Epoch:  1000  |  train loss: 1.8663089514
Processing class: 56
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 1.7303082466
Epoch:   200  |  train loss: 1.7401139736
Epoch:   300  |  train loss: 1.7006998062
Epoch:   400  |  train loss: 1.7420409203
Epoch:   500  |  train loss: 1.7538736582
Epoch:   600  |  train loss: 1.7601074934
Epoch:   700  |  train loss: 1.7747388363
Epoch:   800  |  train loss: 1.7533142805
Epoch:   900  |  train loss: 1.7815548897
Epoch:  1000  |  train loss: 1.7976016998
Processing class: 57
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 1.7429412127
Epoch:   200  |  train loss: 1.7762962341
Epoch:   300  |  train loss: 1.7622174740
Epoch:   400  |  train loss: 1.7619477510
Epoch:   500  |  train loss: 1.7860969305
Epoch:   600  |  train loss: 1.7528549910
Epoch:   700  |  train loss: 1.7986778975
Epoch:   800  |  train loss: 1.7963591814
Epoch:   900  |  train loss: 1.8208815336
Epoch:  1000  |  train loss: 1.7799045324
Processing class: 58
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 1.7392073154
Epoch:   200  |  train loss: 1.7178494930
Epoch:   300  |  train loss: 1.7473479748
Epoch:   400  |  train loss: 1.7839760780
Epoch:   500  |  train loss: 1.7710921288
Epoch:   600  |  train loss: 1.7979104519
Epoch:   700  |  train loss: 1.7870401382
Epoch:   800  |  train loss: 1.8090458870
Epoch:   900  |  train loss: 1.8066081285
Epoch:  1000  |  train loss: 1.8059557199
Processing class: 59
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 1.7868478298
Epoch:   200  |  train loss: 1.8308374643
Epoch:   300  |  train loss: 1.8091238022
Epoch:   400  |  train loss: 1.8266017675
Epoch:   500  |  train loss: 1.8487050056
Epoch:   600  |  train loss: 1.8518526554
Epoch:   700  |  train loss: 1.8677020788
Epoch:   800  |  train loss: 1.8736688614
Epoch:   900  |  train loss: 1.8665277243
Epoch:  1000  |  train loss: 1.8561075926
2024-03-05 20:35:50,361 [trainer.py] => CNN: {'total': 71.75, '00-09': 82.2, '10-19': 72.8, '20-29': 78.5, '30-39': 74.9, '40-49': 68.4, '50-59': 53.7, 'old': 75.36, 'new': 53.7}
2024-03-05 20:35:50,361 [trainer.py] => No NME accuracy
2024-03-05 20:35:50,361 [trainer.py] => FeCAM: {'total': 72.78, '00-09': 82.6, '10-19': 74.0, '20-29': 81.0, '30-39': 76.4, '40-49': 76.8, '50-59': 45.9, 'old': 78.16, 'new': 45.9}
2024-03-05 20:35:50,361 [trainer.py] => CNN top1 curve: [83.44, 71.75]
2024-03-05 20:35:50,361 [trainer.py] => CNN top5 curve: [96.5, 89.6]
2024-03-05 20:35:50,361 [trainer.py] => FeCAM top1 curve: [82.9, 72.78]
2024-03-05 20:35:50,361 [trainer.py] => FeCAM top5 curve: [95.84, 91.18]

2024-03-05 20:35:50,366 [fecam.py] => Learning on 60-70
Processing class: 60
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 1.8020199299
Epoch:   200  |  train loss: 1.7951332569
Epoch:   300  |  train loss: 1.7710775375
Epoch:   400  |  train loss: 1.7606763840
Epoch:   500  |  train loss: 1.8001371861
Epoch:   600  |  train loss: 1.7987442017
Epoch:   700  |  train loss: 1.8868280411
Epoch:   800  |  train loss: 1.8349323273
Epoch:   900  |  train loss: 1.8412002325
Epoch:  1000  |  train loss: 1.8635437489
Processing class: 61
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 1.7288013935
Epoch:   200  |  train loss: 1.6943686962
Epoch:   300  |  train loss: 1.7302907705
Epoch:   400  |  train loss: 1.7316562891
Epoch:   500  |  train loss: 1.7784881592
Epoch:   600  |  train loss: 1.7423322439
Epoch:   700  |  train loss: 1.7833771706
Epoch:   800  |  train loss: 1.8082834482
Epoch:   900  |  train loss: 1.7653653860
Epoch:  1000  |  train loss: 1.7824912071
Processing class: 62
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 1.7893548965
Epoch:   200  |  train loss: 1.7675002575
Epoch:   300  |  train loss: 1.7962459803
Epoch:   400  |  train loss: 1.8085157394
Epoch:   500  |  train loss: 1.7865664959
Epoch:   600  |  train loss: 1.8295478821
Epoch:   700  |  train loss: 1.8424290180
Epoch:   800  |  train loss: 1.8506001711
Epoch:   900  |  train loss: 1.8375159025
Epoch:  1000  |  train loss: 1.8490450621
Processing class: 63
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 1.7200610876
Epoch:   200  |  train loss: 1.7305450916
Epoch:   300  |  train loss: 1.7861031055
Epoch:   400  |  train loss: 1.7867496252
Epoch:   500  |  train loss: 1.8033411503
Epoch:   600  |  train loss: 1.8418923140
Epoch:   700  |  train loss: 1.7925483465
Epoch:   800  |  train loss: 1.8602292538
Epoch:   900  |  train loss: 1.8148234844
Epoch:  1000  |  train loss: 1.8252790213
Processing class: 64
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 1.7949992657
Epoch:   200  |  train loss: 1.7546274662
Epoch:   300  |  train loss: 1.8187140942
Epoch:   400  |  train loss: 1.8200208902
Epoch:   500  |  train loss: 1.8206415653
Epoch:   600  |  train loss: 1.8154247999
Epoch:   700  |  train loss: 1.7810462236
Epoch:   800  |  train loss: 1.7982540607
Epoch:   900  |  train loss: 1.8311712027
Epoch:  1000  |  train loss: 1.8171388149
Processing class: 65
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 1.7503110409
Epoch:   200  |  train loss: 1.7373256922
Epoch:   300  |  train loss: 1.7995562077
Epoch:   400  |  train loss: 1.8231976748
Epoch:   500  |  train loss: 1.8096731663
Epoch:   600  |  train loss: 1.8641016960
Epoch:   700  |  train loss: 1.8633460760
Epoch:   800  |  train loss: 1.9135625839
Epoch:   900  |  train loss: 1.8799915552
Epoch:  1000  |  train loss: 1.9315939665
Processing class: 66
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 1.7910064936
Epoch:   200  |  train loss: 1.8136199951
Epoch:   300  |  train loss: 1.8401916027
Epoch:   400  |  train loss: 1.8390536785
Epoch:   500  |  train loss: 1.8726361036
Epoch:   600  |  train loss: 1.8756315470
Epoch:   700  |  train loss: 1.9127082586
Epoch:   800  |  train loss: 1.9164479256
Epoch:   900  |  train loss: 1.9138392448
Epoch:  1000  |  train loss: 1.9084856033
Processing class: 67
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 1.7314243317
Epoch:   200  |  train loss: 1.7496516705
Epoch:   300  |  train loss: 1.7469492197
Epoch:   400  |  train loss: 1.7537832022
Epoch:   500  |  train loss: 1.7537101030
Epoch:   600  |  train loss: 1.7513814688
Epoch:   700  |  train loss: 1.7589607954
Epoch:   800  |  train loss: 1.7812685013
Epoch:   900  |  train loss: 1.7746617556
Epoch:  1000  |  train loss: 1.7715161085
Processing class: 68
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 1.7116751194
Epoch:   200  |  train loss: 1.7099971294
Epoch:   300  |  train loss: 1.6930859327
Epoch:   400  |  train loss: 1.7040867567
Epoch:   500  |  train loss: 1.7064406633
Epoch:   600  |  train loss: 1.6984842300
Epoch:   700  |  train loss: 1.7141424417
Epoch:   800  |  train loss: 1.6963805437
Epoch:   900  |  train loss: 1.7065996408
Epoch:  1000  |  train loss: 1.7507985830
Processing class: 69
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 1.6470587015
Epoch:   200  |  train loss: 1.6723114252
Epoch:   300  |  train loss: 1.7279451847
Epoch:   400  |  train loss: 1.7510896921
Epoch:   500  |  train loss: 1.7521668673
Epoch:   600  |  train loss: 1.7110083580
Epoch:   700  |  train loss: 1.7459920406
Epoch:   800  |  train loss: 1.7420356035
Epoch:   900  |  train loss: 1.7943968058
Epoch:  1000  |  train loss: 1.7418840647
2024-03-05 20:42:27,994 [trainer.py] => CNN: {'total': 64.66, '00-09': 75.0, '10-19': 70.1, '20-29': 77.1, '30-39': 70.6, '40-49': 64.0, '50-59': 45.8, '60-69': 50.0, 'old': 67.1, 'new': 50.0}
2024-03-05 20:42:27,995 [trainer.py] => No NME accuracy
2024-03-05 20:42:27,995 [trainer.py] => FeCAM: {'total': 67.27, '00-09': 79.9, '10-19': 70.6, '20-29': 79.1, '30-39': 71.9, '40-49': 74.1, '50-59': 41.2, '60-69': 54.1, 'old': 69.47, 'new': 54.1}
2024-03-05 20:42:27,995 [trainer.py] => CNN top1 curve: [83.44, 71.75, 64.66]
2024-03-05 20:42:27,995 [trainer.py] => CNN top5 curve: [96.5, 89.6, 86.54]
2024-03-05 20:42:27,995 [trainer.py] => FeCAM top1 curve: [82.9, 72.78, 67.27]
2024-03-05 20:42:27,995 [trainer.py] => FeCAM top5 curve: [95.84, 91.18, 87.21]

2024-03-05 20:42:28,001 [fecam.py] => Learning on 70-80
Processing class: 70
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 1.7316564560
Epoch:   200  |  train loss: 1.7131092072
Epoch:   300  |  train loss: 1.7260306358
Epoch:   400  |  train loss: 1.7422767639
Epoch:   500  |  train loss: 1.7235275269
Epoch:   600  |  train loss: 1.7542932034
Epoch:   700  |  train loss: 1.7919146538
Epoch:   800  |  train loss: 1.7457890272
Epoch:   900  |  train loss: 1.7770091772
Epoch:  1000  |  train loss: 1.7636358023
Processing class: 71
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 1.7641514063
Epoch:   200  |  train loss: 1.7772394896
Epoch:   300  |  train loss: 1.7456367970
Epoch:   400  |  train loss: 1.7720277071
Epoch:   500  |  train loss: 1.7903556108
Epoch:   600  |  train loss: 1.7930543900
Epoch:   700  |  train loss: 1.7961878061
Epoch:   800  |  train loss: 1.8041036606
Epoch:   900  |  train loss: 1.8372729540
Epoch:  1000  |  train loss: 1.8280027151
Processing class: 72
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 1.7334463835
Epoch:   200  |  train loss: 1.7452779770
Epoch:   300  |  train loss: 1.7503062010
Epoch:   400  |  train loss: 1.7470252991
Epoch:   500  |  train loss: 1.7682185173
Epoch:   600  |  train loss: 1.7435321331
Epoch:   700  |  train loss: 1.7566551208
Epoch:   800  |  train loss: 1.7450026512
Epoch:   900  |  train loss: 1.7541272402
Epoch:  1000  |  train loss: 1.7837126255
Processing class: 73
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 1.7738356113
Epoch:   200  |  train loss: 1.7626499891
Epoch:   300  |  train loss: 1.8025646687
Epoch:   400  |  train loss: 1.7815878630
Epoch:   500  |  train loss: 1.8268640995
Epoch:   600  |  train loss: 1.8239110470
Epoch:   700  |  train loss: 1.7991493464
Epoch:   800  |  train loss: 1.7988917589
Epoch:   900  |  train loss: 1.8020456791
Epoch:  1000  |  train loss: 1.8199133635
Processing class: 74
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 1.7985356569
Epoch:   200  |  train loss: 1.8199712515
Epoch:   300  |  train loss: 1.7829533577
Epoch:   400  |  train loss: 1.7654010057
Epoch:   500  |  train loss: 1.7844503403
Epoch:   600  |  train loss: 1.7707434893
Epoch:   700  |  train loss: 1.8180663586
Epoch:   800  |  train loss: 1.7726455688
Epoch:   900  |  train loss: 1.7775116682
Epoch:  1000  |  train loss: 1.8151564837
Processing class: 75
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 1.7461577654
Epoch:   200  |  train loss: 1.7371281385
Epoch:   300  |  train loss: 1.7746359348
Epoch:   400  |  train loss: 1.7683990002
Epoch:   500  |  train loss: 1.7961776257
Epoch:   600  |  train loss: 1.8239653111
Epoch:   700  |  train loss: 1.7481352568
Epoch:   800  |  train loss: 1.7951957464
Epoch:   900  |  train loss: 1.7905332088
Epoch:  1000  |  train loss: 1.8224566460
Processing class: 76
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 1.6091234207
Epoch:   200  |  train loss: 1.6244361162
Epoch:   300  |  train loss: 1.6731607676
Epoch:   400  |  train loss: 1.7072811365
Epoch:   500  |  train loss: 1.7210026503
Epoch:   600  |  train loss: 1.7481544018
Epoch:   700  |  train loss: 1.7553900480
Epoch:   800  |  train loss: 1.7153647423
Epoch:   900  |  train loss: 1.7599505663
Epoch:  1000  |  train loss: 1.7800603151
Processing class: 77
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 1.7383336544
Epoch:   200  |  train loss: 1.7417654037
Epoch:   300  |  train loss: 1.7153332472
Epoch:   400  |  train loss: 1.7308034658
Epoch:   500  |  train loss: 1.7375377417
Epoch:   600  |  train loss: 1.7365502596
Epoch:   700  |  train loss: 1.7456318378
Epoch:   800  |  train loss: 1.7781316996
Epoch:   900  |  train loss: 1.7606586456
Epoch:  1000  |  train loss: 1.7556276321
Processing class: 78
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 1.7687276840
Epoch:   200  |  train loss: 1.7939405918
Epoch:   300  |  train loss: 1.8226731539
Epoch:   400  |  train loss: 1.8194005489
Epoch:   500  |  train loss: 1.8106396914
Epoch:   600  |  train loss: 1.8371958971
Epoch:   700  |  train loss: 1.8445028067
Epoch:   800  |  train loss: 1.8534393072
Epoch:   900  |  train loss: 1.8568607330
Epoch:  1000  |  train loss: 1.8479086876
Processing class: 79
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 1.7729196787
Epoch:   200  |  train loss: 1.7493381023
Epoch:   300  |  train loss: 1.7839481115
Epoch:   400  |  train loss: 1.7848352194
Epoch:   500  |  train loss: 1.7845814466
Epoch:   600  |  train loss: 1.8211392879
Epoch:   700  |  train loss: 1.8196682692
Epoch:   800  |  train loss: 1.8358240843
Epoch:   900  |  train loss: 1.8637545586
Epoch:  1000  |  train loss: 1.8382840157
2024-03-05 20:50:01,159 [trainer.py] => CNN: {'total': 59.18, '00-09': 73.6, '10-19': 68.4, '20-29': 76.9, '30-39': 69.1, '40-49': 60.6, '50-59': 37.2, '60-69': 45.7, '70-79': 41.9, 'old': 61.64, 'new': 41.9}
2024-03-05 20:50:01,160 [trainer.py] => No NME accuracy
2024-03-05 20:50:01,160 [trainer.py] => FeCAM: {'total': 61.72, '00-09': 78.1, '10-19': 69.3, '20-29': 78.0, '30-39': 69.5, '40-49': 72.1, '50-59': 35.7, '60-69': 49.1, '70-79': 42.0, 'old': 64.54, 'new': 42.0}
2024-03-05 20:50:01,160 [trainer.py] => CNN top1 curve: [83.44, 71.75, 64.66, 59.18]
2024-03-05 20:50:01,160 [trainer.py] => CNN top5 curve: [96.5, 89.6, 86.54, 84.09]
2024-03-05 20:50:01,160 [trainer.py] => FeCAM top1 curve: [82.9, 72.78, 67.27, 61.72]
2024-03-05 20:50:01,160 [trainer.py] => FeCAM top5 curve: [95.84, 91.18, 87.21, 84.09]

2024-03-05 20:50:01,168 [fecam.py] => Learning on 80-90
Processing class: 80
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 1.6927819014
Epoch:   200  |  train loss: 1.7490752220
Epoch:   300  |  train loss: 1.7725041151
Epoch:   400  |  train loss: 1.7869352818
Epoch:   500  |  train loss: 1.7844939709
Epoch:   600  |  train loss: 1.7851186514
Epoch:   700  |  train loss: 1.7532633781
Epoch:   800  |  train loss: 1.8039590597
Epoch:   900  |  train loss: 1.7922498465
Epoch:  1000  |  train loss: 1.8193852186
Processing class: 81
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 1.7336278677
Epoch:   200  |  train loss: 1.7818798780
Epoch:   300  |  train loss: 1.7521045446
Epoch:   400  |  train loss: 1.7997622728
Epoch:   500  |  train loss: 1.7882484436
Epoch:   600  |  train loss: 1.8217331648
Epoch:   700  |  train loss: 1.8165575743
Epoch:   800  |  train loss: 1.8389029264
Epoch:   900  |  train loss: 1.8414276600
Epoch:  1000  |  train loss: 1.8518666029
Processing class: 82
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 1.7076359749
Epoch:   200  |  train loss: 1.7341842651
Epoch:   300  |  train loss: 1.7391477346
Epoch:   400  |  train loss: 1.7343826771
Epoch:   500  |  train loss: 1.7451544762
Epoch:   600  |  train loss: 1.7363473177
Epoch:   700  |  train loss: 1.7377332449
Epoch:   800  |  train loss: 1.7464264631
Epoch:   900  |  train loss: 1.7508080482
Epoch:  1000  |  train loss: 1.7519152164
Processing class: 83
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 1.7282932043
Epoch:   200  |  train loss: 1.7688691139
Epoch:   300  |  train loss: 1.7594349146
Epoch:   400  |  train loss: 1.7756810188
Epoch:   500  |  train loss: 1.8151515722
Epoch:   600  |  train loss: 1.7901867628
Epoch:   700  |  train loss: 1.8292920351
Epoch:   800  |  train loss: 1.8738082409
Epoch:   900  |  train loss: 1.8559159756
Epoch:  1000  |  train loss: 1.8407814741
Processing class: 84
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 1.7710557461
Epoch:   200  |  train loss: 1.7631355286
Epoch:   300  |  train loss: 1.7968419313
Epoch:   400  |  train loss: 1.7985587120
Epoch:   500  |  train loss: 1.8249657393
Epoch:   600  |  train loss: 1.7968384027
Epoch:   700  |  train loss: 1.8277319431
Epoch:   800  |  train loss: 1.8547508240
Epoch:   900  |  train loss: 1.8178570509
Epoch:  1000  |  train loss: 1.8113369465
Processing class: 85
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 1.7564613819
Epoch:   200  |  train loss: 1.7472597837
Epoch:   300  |  train loss: 1.7797947645
Epoch:   400  |  train loss: 1.7652500629
Epoch:   500  |  train loss: 1.7872841358
Epoch:   600  |  train loss: 1.8138282537
Epoch:   700  |  train loss: 1.8536565065
Epoch:   800  |  train loss: 1.8206546783
Epoch:   900  |  train loss: 1.8252081156
Epoch:  1000  |  train loss: 1.8568730593
Processing class: 86
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 1.7967154264
Epoch:   200  |  train loss: 1.7663354635
Epoch:   300  |  train loss: 1.8098281622
Epoch:   400  |  train loss: 1.7947503805
Epoch:   500  |  train loss: 1.7953486919
Epoch:   600  |  train loss: 1.8120670557
Epoch:   700  |  train loss: 1.8143139839
Epoch:   800  |  train loss: 1.8102332830
Epoch:   900  |  train loss: 1.8100013256
Epoch:  1000  |  train loss: 1.8527810335
Processing class: 87
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 1.7124149084
Epoch:   200  |  train loss: 1.7407578468
Epoch:   300  |  train loss: 1.7423557758
Epoch:   400  |  train loss: 1.7628891706
Epoch:   500  |  train loss: 1.7589160442
Epoch:   600  |  train loss: 1.7786381006
Epoch:   700  |  train loss: 1.7892809153
Epoch:   800  |  train loss: 1.7895234585
Epoch:   900  |  train loss: 1.7692741394
Epoch:  1000  |  train loss: 1.8192082882
Processing class: 88
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 1.7393730640
Epoch:   200  |  train loss: 1.7256224632
Epoch:   300  |  train loss: 1.7247462988
Epoch:   400  |  train loss: 1.7338074923
Epoch:   500  |  train loss: 1.7304825068
Epoch:   600  |  train loss: 1.7170814037
Epoch:   700  |  train loss: 1.7447692156
Epoch:   800  |  train loss: 1.7466308832
Epoch:   900  |  train loss: 1.7549810648
Epoch:  1000  |  train loss: 1.7367977619
Processing class: 89
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 1.7568835258
Epoch:   200  |  train loss: 1.7868363380
Epoch:   300  |  train loss: 1.8022904158
Epoch:   400  |  train loss: 1.8110404968
Epoch:   500  |  train loss: 1.8417284966
Epoch:   600  |  train loss: 1.8446269989
Epoch:   700  |  train loss: 1.8161202669
Epoch:   800  |  train loss: 1.8761329651
Epoch:   900  |  train loss: 1.8608902693
Epoch:  1000  |  train loss: 1.8780606508
2024-03-05 20:58:47,989 [trainer.py] => CNN: {'total': 54.08, '00-09': 67.9, '10-19': 63.6, '20-29': 73.2, '30-39': 68.8, '40-49': 57.2, '50-59': 32.6, '60-69': 40.9, '70-79': 38.1, '80-89': 44.4, 'old': 55.29, 'new': 44.4}
2024-03-05 20:58:47,990 [trainer.py] => No NME accuracy
2024-03-05 20:58:47,990 [trainer.py] => FeCAM: {'total': 57.66, '00-09': 76.1, '10-19': 65.8, '20-29': 75.3, '30-39': 67.9, '40-49': 68.5, '50-59': 33.0, '60-69': 45.0, '70-79': 39.0, '80-89': 48.3, 'old': 58.82, 'new': 48.3}
2024-03-05 20:58:47,991 [trainer.py] => CNN top1 curve: [83.44, 71.75, 64.66, 59.18, 54.08]
2024-03-05 20:58:47,991 [trainer.py] => CNN top5 curve: [96.5, 89.6, 86.54, 84.09, 81.74]
2024-03-05 20:58:47,991 [trainer.py] => FeCAM top1 curve: [82.9, 72.78, 67.27, 61.72, 57.66]
2024-03-05 20:58:47,991 [trainer.py] => FeCAM top5 curve: [95.84, 91.18, 87.21, 84.09, 81.63]

2024-03-05 20:58:48,006 [fecam.py] => Learning on 90-100
Processing class: 90
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 1.7641692877
Epoch:   200  |  train loss: 1.7690027475
Epoch:   300  |  train loss: 1.7352019072
Epoch:   400  |  train loss: 1.7245957375
Epoch:   500  |  train loss: 1.7125174046
Epoch:   600  |  train loss: 1.7270491600
Epoch:   700  |  train loss: 1.7185228586
Epoch:   800  |  train loss: 1.7274112463
Epoch:   900  |  train loss: 1.7441950083
Epoch:  1000  |  train loss: 1.7765671968
Processing class: 91
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 1.6480739355
Epoch:   200  |  train loss: 1.6410584688
Epoch:   300  |  train loss: 1.6722027540
Epoch:   400  |  train loss: 1.7008338690
Epoch:   500  |  train loss: 1.7261438608
Epoch:   600  |  train loss: 1.7407719612
Epoch:   700  |  train loss: 1.7482553959
Epoch:   800  |  train loss: 1.7642137527
Epoch:   900  |  train loss: 1.7846898317
Epoch:  1000  |  train loss: 1.7905076027
Processing class: 92
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 1.7828809738
Epoch:   200  |  train loss: 1.7454802275
Epoch:   300  |  train loss: 1.7071159601
Epoch:   400  |  train loss: 1.7293635845
Epoch:   500  |  train loss: 1.7453842163
Epoch:   600  |  train loss: 1.7739407063
Epoch:   700  |  train loss: 1.7735028505
Epoch:   800  |  train loss: 1.7312291861
Epoch:   900  |  train loss: 1.7418717623
Epoch:  1000  |  train loss: 1.7484039307
Processing class: 93
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 1.7462321520
Epoch:   200  |  train loss: 1.7661426783
Epoch:   300  |  train loss: 1.8354539871
Epoch:   400  |  train loss: 1.8470860004
Epoch:   500  |  train loss: 1.8418619633
Epoch:   600  |  train loss: 1.8189193249
Epoch:   700  |  train loss: 1.8769374132
Epoch:   800  |  train loss: 1.8717635870
Epoch:   900  |  train loss: 1.8777521133
Epoch:  1000  |  train loss: 1.8702185869
Processing class: 94
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 1.6234214067
Epoch:   200  |  train loss: 1.6161225319
Epoch:   300  |  train loss: 1.5730736494
Epoch:   400  |  train loss: 1.5893998146
Epoch:   500  |  train loss: 1.5350012779
Epoch:   600  |  train loss: 1.5618638277
Epoch:   700  |  train loss: 1.5884495020
Epoch:   800  |  train loss: 1.5237920284
Epoch:   900  |  train loss: 1.5658418894
Epoch:  1000  |  train loss: 1.5524683952
Processing class: 95
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 1.7443415642
Epoch:   200  |  train loss: 1.7792851686
Epoch:   300  |  train loss: 1.7602364779
Epoch:   400  |  train loss: 1.7495483160
Epoch:   500  |  train loss: 1.7951928616
Epoch:   600  |  train loss: 1.7904235601
Epoch:   700  |  train loss: 1.8077315569
Epoch:   800  |  train loss: 1.7933552504
Epoch:   900  |  train loss: 1.8227938890
Epoch:  1000  |  train loss: 1.8500672340
Processing class: 96
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 1.7871202946
Epoch:   200  |  train loss: 1.7409393787
Epoch:   300  |  train loss: 1.7733169556
Epoch:   400  |  train loss: 1.8258871555
Epoch:   500  |  train loss: 1.8090219975
Epoch:   600  |  train loss: 1.8044837236
Epoch:   700  |  train loss: 1.8116318226
Epoch:   800  |  train loss: 1.7912731647
Epoch:   900  |  train loss: 1.8430924416
Epoch:  1000  |  train loss: 1.8417528868
Processing class: 97
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 1.7707584858
Epoch:   200  |  train loss: 1.7721873283
Epoch:   300  |  train loss: 1.7804901838
Epoch:   400  |  train loss: 1.7925507069
Epoch:   500  |  train loss: 1.8259330988
Epoch:   600  |  train loss: 1.8467563868
Epoch:   700  |  train loss: 1.8683694839
Epoch:   800  |  train loss: 1.8742126703
Epoch:   900  |  train loss: 1.8864831448
Epoch:  1000  |  train loss: 1.8828674316
Processing class: 98
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 1.7125944853
Epoch:   200  |  train loss: 1.7056323528
Epoch:   300  |  train loss: 1.7126800776
Epoch:   400  |  train loss: 1.7281969786
Epoch:   500  |  train loss: 1.7219361782
Epoch:   600  |  train loss: 1.7413615704
Epoch:   700  |  train loss: 1.7086931944
Epoch:   800  |  train loss: 1.7372860193
Epoch:   900  |  train loss: 1.7387295008
Epoch:  1000  |  train loss: 1.7238568306
Processing class: 99
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 1.8001840115
Epoch:   200  |  train loss: 1.7680806160
Epoch:   300  |  train loss: 1.8208436728
Epoch:   400  |  train loss: 1.8603069067
Epoch:   500  |  train loss: 1.8524255276
Epoch:   600  |  train loss: 1.8462183952
Epoch:   700  |  train loss: 1.8373984575
Epoch:   800  |  train loss: 1.8707913160
Epoch:   900  |  train loss: 1.8553912163
Epoch:  1000  |  train loss: 1.8624122381
2024-03-05 21:09:02,297 [trainer.py] => CNN: {'total': 50.36, '00-09': 57.7, '10-19': 63.6, '20-29': 71.4, '30-39': 68.5, '40-49': 56.8, '50-59': 29.6, '60-69': 38.9, '70-79': 36.6, '80-89': 42.7, '90-99': 37.8, 'old': 51.76, 'new': 37.8}
2024-03-05 21:09:02,297 [trainer.py] => No NME accuracy
2024-03-05 21:09:02,297 [trainer.py] => FeCAM: {'total': 54.74, '00-09': 73.3, '10-19': 65.1, '20-29': 74.1, '30-39': 67.2, '40-49': 67.1, '50-59': 30.8, '60-69': 43.3, '70-79': 37.2, '80-89': 46.9, '90-99': 42.4, 'old': 56.11, 'new': 42.4}
2024-03-05 21:09:02,298 [trainer.py] => CNN top1 curve: [83.44, 71.75, 64.66, 59.18, 54.08, 50.36]
2024-03-05 21:09:02,298 [trainer.py] => CNN top5 curve: [96.5, 89.6, 86.54, 84.09, 81.74, 79.63]
2024-03-05 21:09:02,298 [trainer.py] => FeCAM top1 curve: [82.9, 72.78, 67.27, 61.72, 57.66, 54.74]
2024-03-05 21:09:02,298 [trainer.py] => FeCAM top5 curve: [95.84, 91.18, 87.21, 84.09, 81.63, 79.21]

=========================================
2024-03-05 21:09:10,639 [trainer.py] => config: ./exps/FeCAM_cifar100.json
2024-03-05 21:09:10,639 [trainer.py] => prefix: train
2024-03-05 21:09:10,639 [trainer.py] => dataset: cifar100
2024-03-05 21:09:10,639 [trainer.py] => memory_size: 0
2024-03-05 21:09:10,639 [trainer.py] => shuffle: True
2024-03-05 21:09:10,639 [trainer.py] => init_cls: 50
2024-03-05 21:09:10,639 [trainer.py] => increment: 10
2024-03-05 21:09:10,639 [trainer.py] => model_name: fecam
2024-03-05 21:09:10,639 [trainer.py] => convnet_type: resnet18
2024-03-05 21:09:10,639 [trainer.py] => device: [device(type='cuda', index=0)]
2024-03-05 21:09:10,639 [trainer.py] => seed: 1993
2024-03-05 21:09:10,639 [trainer.py] => init_epochs: 200
2024-03-05 21:09:10,639 [trainer.py] => init_lr: 0.1
2024-03-05 21:09:10,639 [trainer.py] => init_weight_decay: 0.0005
2024-03-05 21:09:10,639 [trainer.py] => batch_size: 128
2024-03-05 21:09:10,639 [trainer.py] => num_workers: 8
2024-03-05 21:09:10,639 [trainer.py] => T: 5
2024-03-05 21:09:10,639 [trainer.py] => beta: 0.5
2024-03-05 21:09:10,639 [trainer.py] => alpha1: 1
2024-03-05 21:09:10,639 [trainer.py] => alpha2: 1
2024-03-05 21:09:10,639 [trainer.py] => ncm: False
2024-03-05 21:09:10,639 [trainer.py] => tukey: False
2024-03-05 21:09:10,639 [trainer.py] => diagonal: False
2024-03-05 21:09:10,639 [trainer.py] => per_class: True
2024-03-05 21:09:10,639 [trainer.py] => full_cov: True
2024-03-05 21:09:10,639 [trainer.py] => shrink: True
2024-03-05 21:09:10,640 [trainer.py] => norm_cov: False
2024-03-05 21:09:10,640 [trainer.py] => vecnorm: False
2024-03-05 21:09:10,640 [trainer.py] => ae_type: wae
2024-03-05 21:09:10,640 [trainer.py] => epochs: 1000
2024-03-05 21:09:10,640 [trainer.py] => ae_latent_dim: 32
2024-03-05 21:09:10,640 [trainer.py] => wae_sigma: 40
2024-03-05 21:09:10,640 [trainer.py] => wae_C: 5
Files already downloaded and verified
Files already downloaded and verified
2024-03-05 21:09:12,296 [data_manager.py] => [68, 56, 78, 8, 23, 84, 90, 65, 74, 76, 40, 89, 3, 92, 55, 9, 26, 80, 43, 38, 58, 70, 77, 1, 85, 19, 17, 50, 28, 53, 13, 81, 45, 82, 6, 59, 83, 16, 15, 44, 91, 41, 72, 60, 79, 52, 20, 10, 31, 54, 37, 95, 14, 71, 96, 98, 97, 2, 64, 66, 42, 22, 35, 86, 24, 34, 87, 21, 99, 0, 88, 27, 18, 94, 11, 12, 47, 25, 30, 46, 62, 69, 36, 61, 7, 63, 75, 5, 32, 4, 51, 48, 73, 93, 39, 67, 29, 49, 57, 33]
2024-03-05 21:09:12,590 [fecam.py] => Learning on 0-50
/home/z1165703/miniconda3/lib/python3.11/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
/home/z1165703/miniconda3/lib/python3.11/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
Processing class: 0
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 1.4818255901
Epoch:   200  |  train loss: 1.5145866871
Epoch:   300  |  train loss: 1.5468571663
Epoch:   400  |  train loss: 1.5624676704
Epoch:   500  |  train loss: 1.5496096611
Epoch:   600  |  train loss: 1.5495223761
Epoch:   700  |  train loss: 1.5639953613
Epoch:   800  |  train loss: 1.5692195177
Epoch:   900  |  train loss: 1.5562429667
Epoch:  1000  |  train loss: 1.5556500196
Processing class: 1
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 1.4494221926
Epoch:   200  |  train loss: 1.4377974272
Epoch:   300  |  train loss: 1.5121299267
Epoch:   400  |  train loss: 1.5294933558
Epoch:   500  |  train loss: 1.4987514257
Epoch:   600  |  train loss: 1.5480901480
Epoch:   700  |  train loss: 1.5297779083
Epoch:   800  |  train loss: 1.5577292204
Epoch:   900  |  train loss: 1.5969327927
Epoch:  1000  |  train loss: 1.5633937120
Processing class: 2
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 1.4305709600
Epoch:   200  |  train loss: 1.4330537796
Epoch:   300  |  train loss: 1.4855395794
Epoch:   400  |  train loss: 1.4836705446
Epoch:   500  |  train loss: 1.4239963531
Epoch:   600  |  train loss: 1.4816687107
Epoch:   700  |  train loss: 1.4783289433
Epoch:   800  |  train loss: 1.5225732803
Epoch:   900  |  train loss: 1.5031870842
Epoch:  1000  |  train loss: 1.4929315805
Processing class: 3
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 1.5605491877
Epoch:   200  |  train loss: 1.6039868832
Epoch:   300  |  train loss: 1.6281898260
Epoch:   400  |  train loss: 1.6434807539
Epoch:   500  |  train loss: 1.6297860146
Epoch:   600  |  train loss: 1.6767870665
Epoch:   700  |  train loss: 1.7091847181
Epoch:   800  |  train loss: 1.6816871643
Epoch:   900  |  train loss: 1.7082135439
Epoch:  1000  |  train loss: 1.7197945595
Processing class: 4
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 1.4357857227
Epoch:   200  |  train loss: 1.4193099260
Epoch:   300  |  train loss: 1.4820415497
Epoch:   400  |  train loss: 1.4911238432
Epoch:   500  |  train loss: 1.5174529076
Epoch:   600  |  train loss: 1.5144167662
Epoch:   700  |  train loss: 1.5393425941
Epoch:   800  |  train loss: 1.5477752209
Epoch:   900  |  train loss: 1.5616178513
Epoch:  1000  |  train loss: 1.5972692966
Processing class: 5
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 1.4367814302
Epoch:   200  |  train loss: 1.4344382286
Epoch:   300  |  train loss: 1.4585588932
Epoch:   400  |  train loss: 1.4659878254
Epoch:   500  |  train loss: 1.4639644623
Epoch:   600  |  train loss: 1.5268741846
Epoch:   700  |  train loss: 1.5427615881
Epoch:   800  |  train loss: 1.5192262173
Epoch:   900  |  train loss: 1.5169146776
Epoch:  1000  |  train loss: 1.5465359926
Processing class: 6
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 1.4557891607
Epoch:   200  |  train loss: 1.4772627831
Epoch:   300  |  train loss: 1.5410393238
Epoch:   400  |  train loss: 1.5396241903
Epoch:   500  |  train loss: 1.5892324209
Epoch:   600  |  train loss: 1.5734293222
Epoch:   700  |  train loss: 1.5608028173
Epoch:   800  |  train loss: 1.5763682127
Epoch:   900  |  train loss: 1.5627135038
Epoch:  1000  |  train loss: 1.5825165987
Processing class: 7
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 1.4098613739
Epoch:   200  |  train loss: 1.4271225929
Epoch:   300  |  train loss: 1.4477401257
Epoch:   400  |  train loss: 1.4664619923
Epoch:   500  |  train loss: 1.4957300663
Epoch:   600  |  train loss: 1.4829558134
Epoch:   700  |  train loss: 1.5096009731
Epoch:   800  |  train loss: 1.5529758215
Epoch:   900  |  train loss: 1.5215393543
Epoch:  1000  |  train loss: 1.5572477341
Processing class: 8
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 1.4288914919
Epoch:   200  |  train loss: 1.4623049021
Epoch:   300  |  train loss: 1.4576287031
Epoch:   400  |  train loss: 1.5143969059
Epoch:   500  |  train loss: 1.4940516472
Epoch:   600  |  train loss: 1.5131332636
Epoch:   700  |  train loss: 1.5523343563
Epoch:   800  |  train loss: 1.5243399620
Epoch:   900  |  train loss: 1.5481463909
Epoch:  1000  |  train loss: 1.5315815449
Processing class: 9
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 1.4031979084
Epoch:   200  |  train loss: 1.4329354525
Epoch:   300  |  train loss: 1.4769302368
Epoch:   400  |  train loss: 1.4425490618
Epoch:   500  |  train loss: 1.4860575676
Epoch:   600  |  train loss: 1.4623998642
Epoch:   700  |  train loss: 1.4784709215
Epoch:   800  |  train loss: 1.4915126801
Epoch:   900  |  train loss: 1.4940310240
Epoch:  1000  |  train loss: 1.4706729174
Processing class: 10
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 1.3694077730
Epoch:   200  |  train loss: 1.3733777285
Epoch:   300  |  train loss: 1.3927388430
Epoch:   400  |  train loss: 1.4425587893
Epoch:   500  |  train loss: 1.3919074297
Epoch:   600  |  train loss: 1.4175652027
Epoch:   700  |  train loss: 1.4378120184
Epoch:   800  |  train loss: 1.4291260481
Epoch:   900  |  train loss: 1.4441866159
Epoch:  1000  |  train loss: 1.4432008028
Processing class: 11
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 1.4392851591
Epoch:   200  |  train loss: 1.4650577307
Epoch:   300  |  train loss: 1.4499181747
Epoch:   400  |  train loss: 1.4761639357
Epoch:   500  |  train loss: 1.4844354391
Epoch:   600  |  train loss: 1.5183909178
Epoch:   700  |  train loss: 1.5658254385
Epoch:   800  |  train loss: 1.5591645956
Epoch:   900  |  train loss: 1.5648051739
Epoch:  1000  |  train loss: 1.5906907082
Processing class: 12
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 1.3827745438
Epoch:   200  |  train loss: 1.4547390223
Epoch:   300  |  train loss: 1.4650787830
Epoch:   400  |  train loss: 1.5078352451
Epoch:   500  |  train loss: 1.5164973497
Epoch:   600  |  train loss: 1.5010711670
Epoch:   700  |  train loss: 1.5180605173
Epoch:   800  |  train loss: 1.5315553427
Epoch:   900  |  train loss: 1.5564185858
Epoch:  1000  |  train loss: 1.5422942162
Processing class: 13
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 1.4156490564
Epoch:   200  |  train loss: 1.4764848232
Epoch:   300  |  train loss: 1.5228908539
Epoch:   400  |  train loss: 1.5408620358
Epoch:   500  |  train loss: 1.5816246033
Epoch:   600  |  train loss: 1.5552589893
Epoch:   700  |  train loss: 1.5799526453
Epoch:   800  |  train loss: 1.6019826889
Epoch:   900  |  train loss: 1.6156766176
Epoch:  1000  |  train loss: 1.6319063187
Processing class: 14
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 1.4251682758
Epoch:   200  |  train loss: 1.4459136248
Epoch:   300  |  train loss: 1.4641786337
Epoch:   400  |  train loss: 1.5060308218
Epoch:   500  |  train loss: 1.5020843983
Epoch:   600  |  train loss: 1.5057868481
Epoch:   700  |  train loss: 1.5272796154
Epoch:   800  |  train loss: 1.5760114193
Epoch:   900  |  train loss: 1.5613835096
Epoch:  1000  |  train loss: 1.5318568468
Processing class: 15
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 1.4364212036
Epoch:   200  |  train loss: 1.4631573439
Epoch:   300  |  train loss: 1.5195847750
Epoch:   400  |  train loss: 1.5447388649
Epoch:   500  |  train loss: 1.5714307785
Epoch:   600  |  train loss: 1.5508877039
Epoch:   700  |  train loss: 1.5923325539
Epoch:   800  |  train loss: 1.5581074715
Epoch:   900  |  train loss: 1.5970201015
Epoch:  1000  |  train loss: 1.5814657927
Processing class: 16
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 1.4479914665
Epoch:   200  |  train loss: 1.5470754862
Epoch:   300  |  train loss: 1.5905941010
Epoch:   400  |  train loss: 1.5729083538
Epoch:   500  |  train loss: 1.6087364435
Epoch:   600  |  train loss: 1.5820383787
Epoch:   700  |  train loss: 1.5755491257
Epoch:   800  |  train loss: 1.6260996103
Epoch:   900  |  train loss: 1.5843349695
Epoch:  1000  |  train loss: 1.6277060270
Processing class: 17
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 1.3789835215
Epoch:   200  |  train loss: 1.4048123360
Epoch:   300  |  train loss: 1.4454360723
Epoch:   400  |  train loss: 1.4559908152
Epoch:   500  |  train loss: 1.4194037437
Epoch:   600  |  train loss: 1.4516079187
Epoch:   700  |  train loss: 1.4461701393
Epoch:   800  |  train loss: 1.4520311594
Epoch:   900  |  train loss: 1.4658760309
Epoch:  1000  |  train loss: 1.4621646404
Processing class: 18
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 1.4173369884
Epoch:   200  |  train loss: 1.4295011282
Epoch:   300  |  train loss: 1.4758759260
Epoch:   400  |  train loss: 1.4762750626
Epoch:   500  |  train loss: 1.5261110067
Epoch:   600  |  train loss: 1.5160207510
Epoch:   700  |  train loss: 1.5806114912
Epoch:   800  |  train loss: 1.5815336466
Epoch:   900  |  train loss: 1.5969140053
Epoch:  1000  |  train loss: 1.6349034309
Processing class: 19
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 1.4421625614
Epoch:   200  |  train loss: 1.5233321190
Epoch:   300  |  train loss: 1.5287871838
Epoch:   400  |  train loss: 1.5401388884
Epoch:   500  |  train loss: 1.5210060596
Epoch:   600  |  train loss: 1.5287220716
Epoch:   700  |  train loss: 1.5494097471
Epoch:   800  |  train loss: 1.5406882524
Epoch:   900  |  train loss: 1.6120254993
Epoch:  1000  |  train loss: 1.6277565956
Processing class: 20
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 1.4410163403
Epoch:   200  |  train loss: 1.4725629807
Epoch:   300  |  train loss: 1.5004737616
Epoch:   400  |  train loss: 1.5259740353
Epoch:   500  |  train loss: 1.5641134739
Epoch:   600  |  train loss: 1.5447857618
Epoch:   700  |  train loss: 1.5559799194
Epoch:   800  |  train loss: 1.5574921846
Epoch:   900  |  train loss: 1.5755022287
Epoch:  1000  |  train loss: 1.6030712605
Processing class: 21
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 1.4451582909
Epoch:   200  |  train loss: 1.4814896822
Epoch:   300  |  train loss: 1.5127246141
Epoch:   400  |  train loss: 1.5326942205
Epoch:   500  |  train loss: 1.5520800352
Epoch:   600  |  train loss: 1.5778093576
Epoch:   700  |  train loss: 1.5725981236
Epoch:   800  |  train loss: 1.5666249514
Epoch:   900  |  train loss: 1.6019586086
Epoch:  1000  |  train loss: 1.5964537144
Processing class: 22
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 1.4169974804
Epoch:   200  |  train loss: 1.4314945698
Epoch:   300  |  train loss: 1.4653724432
Epoch:   400  |  train loss: 1.5109692335
Epoch:   500  |  train loss: 1.5258136749
Epoch:   600  |  train loss: 1.5107606173
Epoch:   700  |  train loss: 1.5313202381
Epoch:   800  |  train loss: 1.5241678715
Epoch:   900  |  train loss: 1.5105999470
Epoch:  1000  |  train loss: 1.5642418623
Processing class: 23
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 1.4750883341
Epoch:   200  |  train loss: 1.5101208448
Epoch:   300  |  train loss: 1.5468418360
Epoch:   400  |  train loss: 1.5155853271
Epoch:   500  |  train loss: 1.5183276415
Epoch:   600  |  train loss: 1.5037801504
Epoch:   700  |  train loss: 1.5305264235
Epoch:   800  |  train loss: 1.5161672831
Epoch:   900  |  train loss: 1.5418422937
Epoch:  1000  |  train loss: 1.5246376753
Processing class: 24
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 1.4492526054
Epoch:   200  |  train loss: 1.4515771866
Epoch:   300  |  train loss: 1.4199254751
Epoch:   400  |  train loss: 1.4602252722
Epoch:   500  |  train loss: 1.4474723577
Epoch:   600  |  train loss: 1.4715446949
Epoch:   700  |  train loss: 1.4292954683
Epoch:   800  |  train loss: 1.4468026400
Epoch:   900  |  train loss: 1.4764614582
Epoch:  1000  |  train loss: 1.4689213753
Processing class: 25
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 1.4219115257
Epoch:   200  |  train loss: 1.4529536486
Epoch:   300  |  train loss: 1.4954779863
Epoch:   400  |  train loss: 1.4771723032
Epoch:   500  |  train loss: 1.5264787197
Epoch:   600  |  train loss: 1.5089283466
Epoch:   700  |  train loss: 1.5506199121
Epoch:   800  |  train loss: 1.5378835201
Epoch:   900  |  train loss: 1.5305673361
Epoch:  1000  |  train loss: 1.5452968597
Processing class: 26
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 1.4952455997
Epoch:   200  |  train loss: 1.5413801193
Epoch:   300  |  train loss: 1.5131526947
Epoch:   400  |  train loss: 1.5522531748
Epoch:   500  |  train loss: 1.5909349442
Epoch:   600  |  train loss: 1.5477610826
Epoch:   700  |  train loss: 1.5695595264
Epoch:   800  |  train loss: 1.5869647264
Epoch:   900  |  train loss: 1.5825597286
Epoch:  1000  |  train loss: 1.6140848398
Processing class: 27
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 1.4236891270
Epoch:   200  |  train loss: 1.4180267334
Epoch:   300  |  train loss: 1.4348561764
Epoch:   400  |  train loss: 1.4523765802
Epoch:   500  |  train loss: 1.4808140993
Epoch:   600  |  train loss: 1.4834876299
Epoch:   700  |  train loss: 1.4995334387
Epoch:   800  |  train loss: 1.5002132416
Epoch:   900  |  train loss: 1.5152165174
Epoch:  1000  |  train loss: 1.4848487854
Processing class: 28
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 1.4705846786
Epoch:   200  |  train loss: 1.5092537403
Epoch:   300  |  train loss: 1.5572388411
Epoch:   400  |  train loss: 1.5822093487
Epoch:   500  |  train loss: 1.5970370531
Epoch:   600  |  train loss: 1.6020854473
Epoch:   700  |  train loss: 1.6112046003
Epoch:   800  |  train loss: 1.6247704268
Epoch:   900  |  train loss: 1.6333091736
Epoch:  1000  |  train loss: 1.6053939819
Processing class: 29
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 1.4322047949
Epoch:   200  |  train loss: 1.4674857616
Epoch:   300  |  train loss: 1.4716914415
Epoch:   400  |  train loss: 1.4634238243
Epoch:   500  |  train loss: 1.4976427794
Epoch:   600  |  train loss: 1.5167646408
Epoch:   700  |  train loss: 1.5202615499
Epoch:   800  |  train loss: 1.5026215553
Epoch:   900  |  train loss: 1.5115038157
Epoch:  1000  |  train loss: 1.5060046434
Processing class: 30
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 1.4914865732
Epoch:   200  |  train loss: 1.5342558861
Epoch:   300  |  train loss: 1.6012920856
Epoch:   400  |  train loss: 1.6542954445
Epoch:   500  |  train loss: 1.6319031239
Epoch:   600  |  train loss: 1.6335646868
Epoch:   700  |  train loss: 1.6297801495
Epoch:   800  |  train loss: 1.6650476217
Epoch:   900  |  train loss: 1.6879849672
Epoch:  1000  |  train loss: 1.6661075115
Processing class: 31
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 1.4732738018
Epoch:   200  |  train loss: 1.5254997253
Epoch:   300  |  train loss: 1.5379601717
Epoch:   400  |  train loss: 1.5837751865
Epoch:   500  |  train loss: 1.6019174814
Epoch:   600  |  train loss: 1.6407779217
Epoch:   700  |  train loss: 1.6343924522
Epoch:   800  |  train loss: 1.6378885984
Epoch:   900  |  train loss: 1.6679041386
Epoch:  1000  |  train loss: 1.6389538527
Processing class: 32
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 1.4281450987
Epoch:   200  |  train loss: 1.4672402859
Epoch:   300  |  train loss: 1.4657465219
Epoch:   400  |  train loss: 1.4403970957
Epoch:   500  |  train loss: 1.5056164026
Epoch:   600  |  train loss: 1.4961067677
Epoch:   700  |  train loss: 1.5458091021
Epoch:   800  |  train loss: 1.5666371346
Epoch:   900  |  train loss: 1.5587555408
Epoch:  1000  |  train loss: 1.5489672184
Processing class: 33
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 1.3768131495
Epoch:   200  |  train loss: 1.4612781048
Epoch:   300  |  train loss: 1.4110611916
Epoch:   400  |  train loss: 1.4264200449
Epoch:   500  |  train loss: 1.4381194115
Epoch:   600  |  train loss: 1.4900768042
Epoch:   700  |  train loss: 1.4678215027
Epoch:   800  |  train loss: 1.4819303274
Epoch:   900  |  train loss: 1.4804491043
Epoch:  1000  |  train loss: 1.4693514109
Processing class: 34
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 1.4424664736
Epoch:   200  |  train loss: 1.4367154837
Epoch:   300  |  train loss: 1.4732832432
Epoch:   400  |  train loss: 1.4612867117
Epoch:   500  |  train loss: 1.4645000935
Epoch:   600  |  train loss: 1.4969552040
Epoch:   700  |  train loss: 1.4761525631
Epoch:   800  |  train loss: 1.5137305498
Epoch:   900  |  train loss: 1.5272109985
Epoch:  1000  |  train loss: 1.5361140251
Processing class: 35
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 1.3971440554
Epoch:   200  |  train loss: 1.4437246084
Epoch:   300  |  train loss: 1.4765094519
Epoch:   400  |  train loss: 1.4931197882
Epoch:   500  |  train loss: 1.4743129253
Epoch:   600  |  train loss: 1.4946957827
Epoch:   700  |  train loss: 1.5102511644
Epoch:   800  |  train loss: 1.5086284637
Epoch:   900  |  train loss: 1.5154372931
Epoch:  1000  |  train loss: 1.5036087751
Processing class: 36
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 1.4671860933
Epoch:   200  |  train loss: 1.4846320868
Epoch:   300  |  train loss: 1.5166182041
Epoch:   400  |  train loss: 1.5206175804
Epoch:   500  |  train loss: 1.5688953876
Epoch:   600  |  train loss: 1.5546058655
Epoch:   700  |  train loss: 1.5374711752
Epoch:   800  |  train loss: 1.5903907061
Epoch:   900  |  train loss: 1.5536556244
Epoch:  1000  |  train loss: 1.5858744144
Processing class: 37
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 1.4515959740
Epoch:   200  |  train loss: 1.4490010023
Epoch:   300  |  train loss: 1.4514240503
Epoch:   400  |  train loss: 1.4765825987
Epoch:   500  |  train loss: 1.4990471363
Epoch:   600  |  train loss: 1.4803385258
Epoch:   700  |  train loss: 1.4887510300
Epoch:   800  |  train loss: 1.5020656586
Epoch:   900  |  train loss: 1.4862552166
Epoch:  1000  |  train loss: 1.5006609678
Processing class: 38
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 1.4435362577
Epoch:   200  |  train loss: 1.4616207123
Epoch:   300  |  train loss: 1.5011980772
Epoch:   400  |  train loss: 1.5141065598
Epoch:   500  |  train loss: 1.5318809032
Epoch:   600  |  train loss: 1.5272965670
Epoch:   700  |  train loss: 1.5368004799
Epoch:   800  |  train loss: 1.5695163965
Epoch:   900  |  train loss: 1.5351092100
Epoch:  1000  |  train loss: 1.5632896662
Processing class: 39
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 1.4499323130
Epoch:   200  |  train loss: 1.4427156210
Epoch:   300  |  train loss: 1.4720237494
Epoch:   400  |  train loss: 1.4954388857
Epoch:   500  |  train loss: 1.4888880968
Epoch:   600  |  train loss: 1.5030142307
Epoch:   700  |  train loss: 1.5217133999
Epoch:   800  |  train loss: 1.5463785172
Epoch:   900  |  train loss: 1.5619886398
Epoch:  1000  |  train loss: 1.5733831882
Processing class: 40
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 1.4579424381
Epoch:   200  |  train loss: 1.4452346325
Epoch:   300  |  train loss: 1.5111401796
Epoch:   400  |  train loss: 1.5272077084
Epoch:   500  |  train loss: 1.5483792782
Epoch:   600  |  train loss: 1.5378141165
Epoch:   700  |  train loss: 1.5551158190
Epoch:   800  |  train loss: 1.5562197685
Epoch:   900  |  train loss: 1.5419917822
Epoch:  1000  |  train loss: 1.6001516342
Processing class: 41
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 1.4509389400
Epoch:   200  |  train loss: 1.4565927029
Epoch:   300  |  train loss: 1.4959966660
Epoch:   400  |  train loss: 1.4926188946
Epoch:   500  |  train loss: 1.4986032009
Epoch:   600  |  train loss: 1.5035941601
Epoch:   700  |  train loss: 1.5323405743
Epoch:   800  |  train loss: 1.5189760447
Epoch:   900  |  train loss: 1.5148773432
Epoch:  1000  |  train loss: 1.5188022375
Processing class: 42
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 1.4092401028
Epoch:   200  |  train loss: 1.4569669962
Epoch:   300  |  train loss: 1.4913375616
Epoch:   400  |  train loss: 1.5089885473
Epoch:   500  |  train loss: 1.5300555229
Epoch:   600  |  train loss: 1.5457582951
Epoch:   700  |  train loss: 1.5676805258
Epoch:   800  |  train loss: 1.5851703405
Epoch:   900  |  train loss: 1.5725630045
Epoch:  1000  |  train loss: 1.5726912022
Processing class: 43
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 1.4218197107
Epoch:   200  |  train loss: 1.4249188662
Epoch:   300  |  train loss: 1.5043473244
Epoch:   400  |  train loss: 1.5664736271
Epoch:   500  |  train loss: 1.5841672421
Epoch:   600  |  train loss: 1.5703459978
Epoch:   700  |  train loss: 1.5962883711
Epoch:   800  |  train loss: 1.5864223003
Epoch:   900  |  train loss: 1.5560678482
Epoch:  1000  |  train loss: 1.5995543718
Processing class: 44
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 1.4038816690
Epoch:   200  |  train loss: 1.4597436428
Epoch:   300  |  train loss: 1.4470016479
Epoch:   400  |  train loss: 1.4636199474
Epoch:   500  |  train loss: 1.4858369827
Epoch:   600  |  train loss: 1.4746797323
Epoch:   700  |  train loss: 1.5081734180
Epoch:   800  |  train loss: 1.5015162230
Epoch:   900  |  train loss: 1.4915379524
Epoch:  1000  |  train loss: 1.5281643867
Processing class: 45
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 1.4579218864
Epoch:   200  |  train loss: 1.5241486788
Epoch:   300  |  train loss: 1.5311344624
Epoch:   400  |  train loss: 1.5667368650
Epoch:   500  |  train loss: 1.5615927458
Epoch:   600  |  train loss: 1.6139496326
Epoch:   700  |  train loss: 1.6284893274
Epoch:   800  |  train loss: 1.6173611641
Epoch:   900  |  train loss: 1.6234842777
Epoch:  1000  |  train loss: 1.6514602900
Processing class: 46
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 1.4816537380
Epoch:   200  |  train loss: 1.4931784630
Epoch:   300  |  train loss: 1.5251538277
Epoch:   400  |  train loss: 1.5411670685
Epoch:   500  |  train loss: 1.5475045443
Epoch:   600  |  train loss: 1.5490908861
Epoch:   700  |  train loss: 1.5565711021
Epoch:   800  |  train loss: 1.5987822771
Epoch:   900  |  train loss: 1.5786578894
Epoch:  1000  |  train loss: 1.5973819256
Processing class: 47
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 1.4051342010
Epoch:   200  |  train loss: 1.4470165730
Epoch:   300  |  train loss: 1.4407569647
Epoch:   400  |  train loss: 1.4833294630
Epoch:   500  |  train loss: 1.4883627892
Epoch:   600  |  train loss: 1.4930892944
Epoch:   700  |  train loss: 1.5100936890
Epoch:   800  |  train loss: 1.4940714836
Epoch:   900  |  train loss: 1.5087346077
Epoch:  1000  |  train loss: 1.5007265806
Processing class: 48
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 1.4304078102
Epoch:   200  |  train loss: 1.4781528950
Epoch:   300  |  train loss: 1.4832513571
Epoch:   400  |  train loss: 1.4566322088
Epoch:   500  |  train loss: 1.5040083170
Epoch:   600  |  train loss: 1.5001642942
Epoch:   700  |  train loss: 1.4823644638
Epoch:   800  |  train loss: 1.4825953007
Epoch:   900  |  train loss: 1.5308105946
Epoch:  1000  |  train loss: 1.5463249922
Processing class: 49
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 1.4208786249
Epoch:   200  |  train loss: 1.4383460283
Epoch:   300  |  train loss: 1.5065383196
Epoch:   400  |  train loss: 1.5215215683
Epoch:   500  |  train loss: 1.5423393011
Epoch:   600  |  train loss: 1.5463507414
Epoch:   700  |  train loss: 1.5498827457
Epoch:   800  |  train loss: 1.5430491924
Epoch:   900  |  train loss: 1.5390485525
Epoch:  1000  |  train loss: 1.5481628180
/home/z1165703/FeCAM/models/base.py:87: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  vec = torch.tensor(vec).cuda()
2024-03-05 21:26:51,974 [trainer.py] => CNN: {'total': 83.44, '00-09': 87.7, '10-19': 79.6, '20-29': 84.9, '30-39': 81.0, '40-49': 84.0, 'old': 0, 'new': 83.44}
2024-03-05 21:26:51,976 [trainer.py] => No NME accuracy
2024-03-05 21:26:51,976 [trainer.py] => FeCAM: {'total': 82.9, '00-09': 86.2, '10-19': 79.7, '20-29': 85.0, '30-39': 80.6, '40-49': 83.0, 'old': 0, 'new': 82.9}
2024-03-05 21:26:51,976 [trainer.py] => CNN top1 curve: [83.44]
2024-03-05 21:26:51,976 [trainer.py] => CNN top5 curve: [96.5]
2024-03-05 21:26:51,976 [trainer.py] => FeCAM top1 curve: [82.9]
2024-03-05 21:26:51,976 [trainer.py] => FeCAM top5 curve: [95.84]

2024-03-05 21:26:51,987 [fecam.py] => Learning on 50-60
Processing class: 50
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 1.3834910870
Epoch:   200  |  train loss: 1.4028238535
Epoch:   300  |  train loss: 1.4168128490
Epoch:   400  |  train loss: 1.4537961483
Epoch:   500  |  train loss: 1.4808734417
Epoch:   600  |  train loss: 1.4665305853
Epoch:   700  |  train loss: 1.5072594166
Epoch:   800  |  train loss: 1.5286186934
Epoch:   900  |  train loss: 1.5525459528
Epoch:  1000  |  train loss: 1.5646582365
Processing class: 51
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 1.3967465162
Epoch:   200  |  train loss: 1.3722755671
Epoch:   300  |  train loss: 1.4302237511
Epoch:   400  |  train loss: 1.4467482090
Epoch:   500  |  train loss: 1.4629836798
Epoch:   600  |  train loss: 1.4904090643
Epoch:   700  |  train loss: 1.4671486139
Epoch:   800  |  train loss: 1.4789573908
Epoch:   900  |  train loss: 1.5234432697
Epoch:  1000  |  train loss: 1.5126688242
Processing class: 52
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 1.3999095678
Epoch:   200  |  train loss: 1.3913633585
Epoch:   300  |  train loss: 1.4200156689
Epoch:   400  |  train loss: 1.4409038067
Epoch:   500  |  train loss: 1.4411953211
Epoch:   600  |  train loss: 1.4269677877
Epoch:   700  |  train loss: 1.4620684385
Epoch:   800  |  train loss: 1.4679660320
Epoch:   900  |  train loss: 1.4806917429
Epoch:  1000  |  train loss: 1.4609950542
Processing class: 53
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 1.3825481176
Epoch:   200  |  train loss: 1.4064179659
Epoch:   300  |  train loss: 1.3805357933
Epoch:   400  |  train loss: 1.4002017736
Epoch:   500  |  train loss: 1.4208080292
Epoch:   600  |  train loss: 1.4512615204
Epoch:   700  |  train loss: 1.4868458748
Epoch:   800  |  train loss: 1.4317895889
Epoch:   900  |  train loss: 1.4630569935
Epoch:  1000  |  train loss: 1.4462684870
Processing class: 54
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 1.2991173506
Epoch:   200  |  train loss: 1.3178150654
Epoch:   300  |  train loss: 1.3581943512
Epoch:   400  |  train loss: 1.3761363745
Epoch:   500  |  train loss: 1.3967972755
Epoch:   600  |  train loss: 1.4380777121
Epoch:   700  |  train loss: 1.4529057026
Epoch:   800  |  train loss: 1.4867555618
Epoch:   900  |  train loss: 1.5107685089
Epoch:  1000  |  train loss: 1.5061138391
Processing class: 55
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 1.3807184696
Epoch:   200  |  train loss: 1.3890374899
Epoch:   300  |  train loss: 1.3764261723
Epoch:   400  |  train loss: 1.4164772511
Epoch:   500  |  train loss: 1.4686630964
Epoch:   600  |  train loss: 1.4785871267
Epoch:   700  |  train loss: 1.4988257885
Epoch:   800  |  train loss: 1.5385634422
Epoch:   900  |  train loss: 1.5011046171
Epoch:  1000  |  train loss: 1.5310508490
Processing class: 56
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 1.3693704844
Epoch:   200  |  train loss: 1.3964472294
Epoch:   300  |  train loss: 1.3816894054
Epoch:   400  |  train loss: 1.4334759712
Epoch:   500  |  train loss: 1.4597776890
Epoch:   600  |  train loss: 1.4798198700
Epoch:   700  |  train loss: 1.5050258636
Epoch:   800  |  train loss: 1.4916595936
Epoch:   900  |  train loss: 1.5270126104
Epoch:  1000  |  train loss: 1.5491857290
Processing class: 57
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 1.3639351368
Epoch:   200  |  train loss: 1.3976110935
Epoch:   300  |  train loss: 1.3896043301
Epoch:   400  |  train loss: 1.3950821638
Epoch:   500  |  train loss: 1.4239003897
Epoch:   600  |  train loss: 1.3973786116
Epoch:   700  |  train loss: 1.4458619118
Epoch:   800  |  train loss: 1.4499116659
Epoch:   900  |  train loss: 1.4780634880
Epoch:  1000  |  train loss: 1.4456181765
Processing class: 58
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 1.3709943295
Epoch:   200  |  train loss: 1.3766149998
Epoch:   300  |  train loss: 1.4215044022
Epoch:   400  |  train loss: 1.4743683100
Epoch:   500  |  train loss: 1.4711371422
Epoch:   600  |  train loss: 1.5042442560
Epoch:   700  |  train loss: 1.5030650854
Epoch:   800  |  train loss: 1.5298984766
Epoch:   900  |  train loss: 1.5329934835
Epoch:  1000  |  train loss: 1.5396239281
Processing class: 59
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 1.4072415829
Epoch:   200  |  train loss: 1.4573630333
Epoch:   300  |  train loss: 1.4514297724
Epoch:   400  |  train loss: 1.4818478107
Epoch:   500  |  train loss: 1.5117536068
Epoch:   600  |  train loss: 1.5207403183
Epoch:   700  |  train loss: 1.5422756910
Epoch:   800  |  train loss: 1.5539030790
Epoch:   900  |  train loss: 1.5528908491
Epoch:  1000  |  train loss: 1.5445295334
2024-03-05 21:32:27,477 [trainer.py] => CNN: {'total': 71.75, '00-09': 82.2, '10-19': 72.8, '20-29': 78.5, '30-39': 74.9, '40-49': 68.4, '50-59': 53.7, 'old': 75.36, 'new': 53.7}
2024-03-05 21:32:27,480 [trainer.py] => No NME accuracy
2024-03-05 21:32:27,480 [trainer.py] => FeCAM: {'total': 72.78, '00-09': 82.6, '10-19': 74.0, '20-29': 81.0, '30-39': 76.4, '40-49': 76.8, '50-59': 45.9, 'old': 78.16, 'new': 45.9}
2024-03-05 21:32:27,480 [trainer.py] => CNN top1 curve: [83.44, 71.75]
2024-03-05 21:32:27,480 [trainer.py] => CNN top5 curve: [96.5, 89.6]
2024-03-05 21:32:27,480 [trainer.py] => FeCAM top1 curve: [82.9, 72.78]
2024-03-05 21:32:27,480 [trainer.py] => FeCAM top5 curve: [95.84, 91.18]

2024-03-05 21:32:27,484 [fecam.py] => Learning on 60-70
Processing class: 60
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 1.4255830050
Epoch:   200  |  train loss: 1.4256387949
Epoch:   300  |  train loss: 1.4137980700
Epoch:   400  |  train loss: 1.4145511389
Epoch:   500  |  train loss: 1.4648256302
Epoch:   600  |  train loss: 1.4727833986
Epoch:   700  |  train loss: 1.5709743977
Epoch:   800  |  train loss: 1.5295717716
Epoch:   900  |  train loss: 1.5426828146
Epoch:  1000  |  train loss: 1.5718420029
Processing class: 61
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 1.3691629410
Epoch:   200  |  train loss: 1.3540417194
Epoch:   300  |  train loss: 1.4079346180
Epoch:   400  |  train loss: 1.4256443977
Epoch:   500  |  train loss: 1.4823045492
Epoch:   600  |  train loss: 1.4567894697
Epoch:   700  |  train loss: 1.5080560923
Epoch:   800  |  train loss: 1.5401020765
Epoch:   900  |  train loss: 1.5049043417
Epoch:  1000  |  train loss: 1.5258210659
Processing class: 62
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 1.4054965734
Epoch:   200  |  train loss: 1.3896039486
Epoch:   300  |  train loss: 1.4214090109
Epoch:   400  |  train loss: 1.4371738434
Epoch:   500  |  train loss: 1.4216626406
Epoch:   600  |  train loss: 1.4679807901
Epoch:   700  |  train loss: 1.4838076830
Epoch:   800  |  train loss: 1.4979091167
Epoch:   900  |  train loss: 1.4912061453
Epoch:  1000  |  train loss: 1.5057249784
Processing class: 63
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 1.3506646633
Epoch:   200  |  train loss: 1.3661854506
Epoch:   300  |  train loss: 1.4372163296
Epoch:   400  |  train loss: 1.4525087833
Epoch:   500  |  train loss: 1.4825346947
Epoch:   600  |  train loss: 1.5268078089
Epoch:   700  |  train loss: 1.4863345623
Epoch:   800  |  train loss: 1.5592803955
Epoch:   900  |  train loss: 1.5204275131
Epoch:  1000  |  train loss: 1.5344681025
Processing class: 64
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 1.4305116177
Epoch:   200  |  train loss: 1.4222814798
Epoch:   300  |  train loss: 1.5032310247
Epoch:   400  |  train loss: 1.5160993814
Epoch:   500  |  train loss: 1.5336740971
Epoch:   600  |  train loss: 1.5389316797
Epoch:   700  |  train loss: 1.5139307737
Epoch:   800  |  train loss: 1.5361342907
Epoch:   900  |  train loss: 1.5745975256
Epoch:  1000  |  train loss: 1.5692481756
Processing class: 65
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 1.3904457331
Epoch:   200  |  train loss: 1.4012056589
Epoch:   300  |  train loss: 1.4859071016
Epoch:   400  |  train loss: 1.5237482309
Epoch:   500  |  train loss: 1.5222324133
Epoch:   600  |  train loss: 1.5877721071
Epoch:   700  |  train loss: 1.5950873852
Epoch:   800  |  train loss: 1.6537172318
Epoch:   900  |  train loss: 1.6268515110
Epoch:  1000  |  train loss: 1.6859034061
Processing class: 66
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 1.4181542873
Epoch:   200  |  train loss: 1.4517604589
Epoch:   300  |  train loss: 1.4913021803
Epoch:   400  |  train loss: 1.5012237549
Epoch:   500  |  train loss: 1.5399564028
Epoch:   600  |  train loss: 1.5496789455
Epoch:   700  |  train loss: 1.5919075966
Epoch:   800  |  train loss: 1.5993811846
Epoch:   900  |  train loss: 1.6030533552
Epoch:  1000  |  train loss: 1.5996379375
Processing class: 67
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 1.3599888563
Epoch:   200  |  train loss: 1.3835120916
Epoch:   300  |  train loss: 1.3900519609
Epoch:   400  |  train loss: 1.4044072390
Epoch:   500  |  train loss: 1.4118526220
Epoch:   600  |  train loss: 1.4140362501
Epoch:   700  |  train loss: 1.4260432243
Epoch:   800  |  train loss: 1.4531609058
Epoch:   900  |  train loss: 1.4516182423
Epoch:  1000  |  train loss: 1.4528387785
Processing class: 68
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 1.3559150219
Epoch:   200  |  train loss: 1.3613136053
Epoch:   300  |  train loss: 1.3668929577
Epoch:   400  |  train loss: 1.3879127979
Epoch:   500  |  train loss: 1.3967895508
Epoch:   600  |  train loss: 1.3969995260
Epoch:   700  |  train loss: 1.4176041126
Epoch:   800  |  train loss: 1.4070122480
Epoch:   900  |  train loss: 1.4237857342
Epoch:  1000  |  train loss: 1.4712526560
Processing class: 69
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 1.3500926018
Epoch:   200  |  train loss: 1.3916869402
Epoch:   300  |  train loss: 1.4684831381
Epoch:   400  |  train loss: 1.5057010889
Epoch:   500  |  train loss: 1.5161243677
Epoch:   600  |  train loss: 1.4788635731
Epoch:   700  |  train loss: 1.5201777935
Epoch:   800  |  train loss: 1.5203885317
Epoch:   900  |  train loss: 1.5744739056
Epoch:  1000  |  train loss: 1.5259455681
2024-03-05 21:39:01,543 [trainer.py] => CNN: {'total': 64.66, '00-09': 75.0, '10-19': 70.1, '20-29': 77.1, '30-39': 70.6, '40-49': 64.0, '50-59': 45.8, '60-69': 50.0, 'old': 67.1, 'new': 50.0}
2024-03-05 21:39:01,543 [trainer.py] => No NME accuracy
2024-03-05 21:39:01,543 [trainer.py] => FeCAM: {'total': 67.27, '00-09': 79.9, '10-19': 70.6, '20-29': 79.1, '30-39': 71.9, '40-49': 74.1, '50-59': 41.2, '60-69': 54.1, 'old': 69.47, 'new': 54.1}
2024-03-05 21:39:01,543 [trainer.py] => CNN top1 curve: [83.44, 71.75, 64.66]
2024-03-05 21:39:01,543 [trainer.py] => CNN top5 curve: [96.5, 89.6, 86.54]
2024-03-05 21:39:01,543 [trainer.py] => FeCAM top1 curve: [82.9, 72.78, 67.27]
2024-03-05 21:39:01,543 [trainer.py] => FeCAM top5 curve: [95.84, 91.18, 87.21]

2024-03-05 21:39:01,548 [fecam.py] => Learning on 70-80
Processing class: 70
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 1.3662901163
Epoch:   200  |  train loss: 1.3582049131
Epoch:   300  |  train loss: 1.3875503540
Epoch:   400  |  train loss: 1.4117343903
Epoch:   500  |  train loss: 1.4072468042
Epoch:   600  |  train loss: 1.4441497326
Epoch:   700  |  train loss: 1.4910676479
Epoch:   800  |  train loss: 1.4561053753
Epoch:   900  |  train loss: 1.4913482904
Epoch:  1000  |  train loss: 1.4836278200
Processing class: 71
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 1.3879483461
Epoch:   200  |  train loss: 1.4066251993
Epoch:   300  |  train loss: 1.3906727076
Epoch:   400  |  train loss: 1.4263653755
Epoch:   500  |  train loss: 1.4537616730
Epoch:   600  |  train loss: 1.4645887613
Epoch:   700  |  train loss: 1.4751737356
Epoch:   800  |  train loss: 1.4898348331
Epoch:   900  |  train loss: 1.5295826197
Epoch:  1000  |  train loss: 1.5272145987
Processing class: 72
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 1.3639451504
Epoch:   200  |  train loss: 1.3831434488
Epoch:   300  |  train loss: 1.3977371454
Epoch:   400  |  train loss: 1.4051019430
Epoch:   500  |  train loss: 1.4340918064
Epoch:   600  |  train loss: 1.4170848846
Epoch:   700  |  train loss: 1.4350121260
Epoch:   800  |  train loss: 1.4299551249
Epoch:   900  |  train loss: 1.4450349092
Epoch:  1000  |  train loss: 1.4798330307
Processing class: 73
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 1.4014446735
Epoch:   200  |  train loss: 1.4024502993
Epoch:   300  |  train loss: 1.4567833900
Epoch:   400  |  train loss: 1.4460051775
Epoch:   500  |  train loss: 1.4916406393
Epoch:   600  |  train loss: 1.4934731722
Epoch:   700  |  train loss: 1.4767135143
Epoch:   800  |  train loss: 1.4797229052
Epoch:   900  |  train loss: 1.4886800051
Epoch:  1000  |  train loss: 1.5112504721
Processing class: 74
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 1.4164001703
Epoch:   200  |  train loss: 1.4384312630
Epoch:   300  |  train loss: 1.4113636494
Epoch:   400  |  train loss: 1.4035901308
Epoch:   500  |  train loss: 1.4294366837
Epoch:   600  |  train loss: 1.4236380816
Epoch:   700  |  train loss: 1.4728779793
Epoch:   800  |  train loss: 1.4370338917
Epoch:   900  |  train loss: 1.4464476824
Epoch:  1000  |  train loss: 1.4880102158
Processing class: 75
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 1.3725084066
Epoch:   200  |  train loss: 1.3722208977
Epoch:   300  |  train loss: 1.4197601795
Epoch:   400  |  train loss: 1.4229398966
Epoch:   500  |  train loss: 1.4570769310
Epoch:   600  |  train loss: 1.4905814409
Epoch:   700  |  train loss: 1.4249339104
Epoch:   800  |  train loss: 1.4740015030
Epoch:   900  |  train loss: 1.4744667530
Epoch:  1000  |  train loss: 1.5107374430
Processing class: 76
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 1.3135158777
Epoch:   200  |  train loss: 1.3555809975
Epoch:   300  |  train loss: 1.4216271639
Epoch:   400  |  train loss: 1.4668183804
Epoch:   500  |  train loss: 1.4862512589
Epoch:   600  |  train loss: 1.5204128981
Epoch:   700  |  train loss: 1.5293689728
Epoch:   800  |  train loss: 1.4959343672
Epoch:   900  |  train loss: 1.5447116613
Epoch:  1000  |  train loss: 1.5675396919
Processing class: 77
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 1.3809489489
Epoch:   200  |  train loss: 1.3957878351
Epoch:   300  |  train loss: 1.3870727062
Epoch:   400  |  train loss: 1.4131094456
Epoch:   500  |  train loss: 1.4351155043
Epoch:   600  |  train loss: 1.4469387054
Epoch:   700  |  train loss: 1.4649554491
Epoch:   800  |  train loss: 1.5039067268
Epoch:   900  |  train loss: 1.4932000399
Epoch:  1000  |  train loss: 1.4949576139
Processing class: 78
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 1.4018695831
Epoch:   200  |  train loss: 1.4449261904
Epoch:   300  |  train loss: 1.4842906475
Epoch:   400  |  train loss: 1.4920953035
Epoch:   500  |  train loss: 1.4956557751
Epoch:   600  |  train loss: 1.5290157557
Epoch:   700  |  train loss: 1.5444610357
Epoch:   800  |  train loss: 1.5606607199
Epoch:   900  |  train loss: 1.5692263603
Epoch:  1000  |  train loss: 1.5639506340
Processing class: 79
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 1.3907279491
Epoch:   200  |  train loss: 1.3735805988
Epoch:   300  |  train loss: 1.4113206148
Epoch:   400  |  train loss: 1.4215120316
Epoch:   500  |  train loss: 1.4257837534
Epoch:   600  |  train loss: 1.4681019068
Epoch:   700  |  train loss: 1.4716285944
Epoch:   800  |  train loss: 1.4939146042
Epoch:   900  |  train loss: 1.5265997171
Epoch:  1000  |  train loss: 1.5083065510
2024-03-05 21:46:35,226 [trainer.py] => CNN: {'total': 59.18, '00-09': 73.6, '10-19': 68.4, '20-29': 76.9, '30-39': 69.1, '40-49': 60.6, '50-59': 37.2, '60-69': 45.7, '70-79': 41.9, 'old': 61.64, 'new': 41.9}
2024-03-05 21:46:35,227 [trainer.py] => No NME accuracy
2024-03-05 21:46:35,227 [trainer.py] => FeCAM: {'total': 61.72, '00-09': 78.1, '10-19': 69.3, '20-29': 78.0, '30-39': 69.5, '40-49': 72.1, '50-59': 35.7, '60-69': 49.1, '70-79': 42.0, 'old': 64.54, 'new': 42.0}
2024-03-05 21:46:35,227 [trainer.py] => CNN top1 curve: [83.44, 71.75, 64.66, 59.18]
2024-03-05 21:46:35,227 [trainer.py] => CNN top5 curve: [96.5, 89.6, 86.54, 84.09]
2024-03-05 21:46:35,227 [trainer.py] => FeCAM top1 curve: [82.9, 72.78, 67.27, 61.72]
2024-03-05 21:46:35,227 [trainer.py] => FeCAM top5 curve: [95.84, 91.18, 87.21, 84.09]

2024-03-05 21:46:35,231 [fecam.py] => Learning on 80-90
Processing class: 80
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 1.3625665665
Epoch:   200  |  train loss: 1.4580000401
Epoch:   300  |  train loss: 1.4990384817
Epoch:   400  |  train loss: 1.5236057043
Epoch:   500  |  train loss: 1.5304651976
Epoch:   600  |  train loss: 1.5365948677
Epoch:   700  |  train loss: 1.5121251345
Epoch:   800  |  train loss: 1.5663497448
Epoch:   900  |  train loss: 1.5583975554
Epoch:  1000  |  train loss: 1.5903735161
Processing class: 81
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 1.3719349384
Epoch:   200  |  train loss: 1.4323055744
Epoch:   300  |  train loss: 1.4172486782
Epoch:   400  |  train loss: 1.4739512444
Epoch:   500  |  train loss: 1.4715792656
Epoch:   600  |  train loss: 1.5107267380
Epoch:   700  |  train loss: 1.5137332916
Epoch:   800  |  train loss: 1.5394310713
Epoch:   900  |  train loss: 1.5498887539
Epoch:  1000  |  train loss: 1.5676395893
Processing class: 82
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 1.3596834421
Epoch:   200  |  train loss: 1.4025143385
Epoch:   300  |  train loss: 1.4200386047
Epoch:   400  |  train loss: 1.4271727562
Epoch:   500  |  train loss: 1.4497123003
Epoch:   600  |  train loss: 1.4481926203
Epoch:   700  |  train loss: 1.4568364859
Epoch:   800  |  train loss: 1.4683123112
Epoch:   900  |  train loss: 1.4800258398
Epoch:  1000  |  train loss: 1.4862260342
Processing class: 83
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 1.3857120275
Epoch:   200  |  train loss: 1.4579570532
Epoch:   300  |  train loss: 1.4718208551
Epoch:   400  |  train loss: 1.5038165808
Epoch:   500  |  train loss: 1.5548660278
Epoch:   600  |  train loss: 1.5388100863
Epoch:   700  |  train loss: 1.5843563557
Epoch:   800  |  train loss: 1.6339897871
Epoch:   900  |  train loss: 1.6208703518
Epoch:  1000  |  train loss: 1.6133787870
Processing class: 84
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 1.4012835741
Epoch:   200  |  train loss: 1.4148785353
Epoch:   300  |  train loss: 1.4620988607
Epoch:   400  |  train loss: 1.4770130873
Epoch:   500  |  train loss: 1.5141654968
Epoch:   600  |  train loss: 1.4934233904
Epoch:   700  |  train loss: 1.5306790829
Epoch:   800  |  train loss: 1.5626009464
Epoch:   900  |  train loss: 1.5329931736
Epoch:  1000  |  train loss: 1.5309004784
Processing class: 85
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 1.3844162226
Epoch:   200  |  train loss: 1.3935324430
Epoch:   300  |  train loss: 1.4400667667
Epoch:   400  |  train loss: 1.4381198406
Epoch:   500  |  train loss: 1.4691814423
Epoch:   600  |  train loss: 1.5040443420
Epoch:   700  |  train loss: 1.5488353252
Epoch:   800  |  train loss: 1.5222001791
Epoch:   900  |  train loss: 1.5311640739
Epoch:  1000  |  train loss: 1.5686711311
Processing class: 86
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 1.4139436483
Epoch:   200  |  train loss: 1.3927435637
Epoch:   300  |  train loss: 1.4450454712
Epoch:   400  |  train loss: 1.4425592422
Epoch:   500  |  train loss: 1.4493746758
Epoch:   600  |  train loss: 1.4745748520
Epoch:   700  |  train loss: 1.4826595306
Epoch:   800  |  train loss: 1.4852720976
Epoch:   900  |  train loss: 1.4898465872
Epoch:  1000  |  train loss: 1.5368059158
Processing class: 87
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 1.3596849203
Epoch:   200  |  train loss: 1.4159153938
Epoch:   300  |  train loss: 1.4303055525
Epoch:   400  |  train loss: 1.4664153337
Epoch:   500  |  train loss: 1.4725233316
Epoch:   600  |  train loss: 1.4965928316
Epoch:   700  |  train loss: 1.5137831450
Epoch:   800  |  train loss: 1.5198571205
Epoch:   900  |  train loss: 1.5066306353
Epoch:  1000  |  train loss: 1.5614034891
Processing class: 88
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 1.3739690781
Epoch:   200  |  train loss: 1.3715230942
Epoch:   300  |  train loss: 1.3851207972
Epoch:   400  |  train loss: 1.4052905560
Epoch:   500  |  train loss: 1.4141317606
Epoch:   600  |  train loss: 1.4140950680
Epoch:   700  |  train loss: 1.4488968372
Epoch:   800  |  train loss: 1.4596312523
Epoch:   900  |  train loss: 1.4736515045
Epoch:  1000  |  train loss: 1.4608518839
Processing class: 89
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 1.3936747074
Epoch:   200  |  train loss: 1.4412319422
Epoch:   300  |  train loss: 1.4750989199
Epoch:   400  |  train loss: 1.4930005074
Epoch:   500  |  train loss: 1.5328143358
Epoch:   600  |  train loss: 1.5477723598
Epoch:   700  |  train loss: 1.5253694057
Epoch:   800  |  train loss: 1.5927539587
Epoch:   900  |  train loss: 1.5840167761
Epoch:  1000  |  train loss: 1.6076298714
2024-03-05 21:55:27,771 [trainer.py] => CNN: {'total': 54.08, '00-09': 67.9, '10-19': 63.6, '20-29': 73.2, '30-39': 68.8, '40-49': 57.2, '50-59': 32.6, '60-69': 40.9, '70-79': 38.1, '80-89': 44.4, 'old': 55.29, 'new': 44.4}
2024-03-05 21:55:27,771 [trainer.py] => No NME accuracy
2024-03-05 21:55:27,772 [trainer.py] => FeCAM: {'total': 57.66, '00-09': 76.1, '10-19': 65.8, '20-29': 75.3, '30-39': 67.9, '40-49': 68.5, '50-59': 33.0, '60-69': 45.0, '70-79': 39.0, '80-89': 48.3, 'old': 58.82, 'new': 48.3}
2024-03-05 21:55:27,772 [trainer.py] => CNN top1 curve: [83.44, 71.75, 64.66, 59.18, 54.08]
2024-03-05 21:55:27,772 [trainer.py] => CNN top5 curve: [96.5, 89.6, 86.54, 84.09, 81.74]
2024-03-05 21:55:27,772 [trainer.py] => FeCAM top1 curve: [82.9, 72.78, 67.27, 61.72, 57.66]
2024-03-05 21:55:27,772 [trainer.py] => FeCAM top5 curve: [95.84, 91.18, 87.21, 84.09, 81.63]

2024-03-05 21:55:27,778 [fecam.py] => Learning on 90-100
Processing class: 90
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 1.3942163229
Epoch:   200  |  train loss: 1.4073411942
Epoch:   300  |  train loss: 1.3904250622
Epoch:   400  |  train loss: 1.3951796055
Epoch:   500  |  train loss: 1.3947956800
Epoch:   600  |  train loss: 1.4205925226
Epoch:   700  |  train loss: 1.4237177849
Epoch:   800  |  train loss: 1.4401096582
Epoch:   900  |  train loss: 1.4617206097
Epoch:  1000  |  train loss: 1.4996915340
Processing class: 91
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 1.3466438770
Epoch:   200  |  train loss: 1.3566093683
Epoch:   300  |  train loss: 1.4012105227
Epoch:   400  |  train loss: 1.4357505560
Epoch:   500  |  train loss: 1.4739677906
Epoch:   600  |  train loss: 1.4957376957
Epoch:   700  |  train loss: 1.5148349524
Epoch:   800  |  train loss: 1.5405502319
Epoch:   900  |  train loss: 1.5698216915
Epoch:  1000  |  train loss: 1.5796082735
Processing class: 92
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 1.4210847139
Epoch:   200  |  train loss: 1.4124163389
Epoch:   300  |  train loss: 1.3847164154
Epoch:   400  |  train loss: 1.4156215906
Epoch:   500  |  train loss: 1.4438129425
Epoch:   600  |  train loss: 1.4888639212
Epoch:   700  |  train loss: 1.4977273226
Epoch:   800  |  train loss: 1.4654923916
Epoch:   900  |  train loss: 1.4821992874
Epoch:  1000  |  train loss: 1.4950300455
Processing class: 93
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 1.3805442095
Epoch:   200  |  train loss: 1.4215731144
Epoch:   300  |  train loss: 1.5077207804
Epoch:   400  |  train loss: 1.5281429768
Epoch:   500  |  train loss: 1.5349233627
Epoch:   600  |  train loss: 1.5239849567
Epoch:   700  |  train loss: 1.5906836510
Epoch:   800  |  train loss: 1.5941658258
Epoch:   900  |  train loss: 1.6069401979
Epoch:  1000  |  train loss: 1.6065817356
Processing class: 94
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 1.2857076883
Epoch:   200  |  train loss: 1.2930826187
Epoch:   300  |  train loss: 1.2729205132
Epoch:   400  |  train loss: 1.3043040991
Epoch:   500  |  train loss: 1.2613671303
Epoch:   600  |  train loss: 1.2956559181
Epoch:   700  |  train loss: 1.3285088062
Epoch:   800  |  train loss: 1.2727723837
Epoch:   900  |  train loss: 1.3196374416
Epoch:  1000  |  train loss: 1.3113799572
Processing class: 95
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 1.3692302704
Epoch:   200  |  train loss: 1.4075620413
Epoch:   300  |  train loss: 1.3971354485
Epoch:   400  |  train loss: 1.3968857288
Epoch:   500  |  train loss: 1.4520331144
Epoch:   600  |  train loss: 1.4548550129
Epoch:   700  |  train loss: 1.4794406414
Epoch:   800  |  train loss: 1.4718340874
Epoch:   900  |  train loss: 1.5068702698
Epoch:  1000  |  train loss: 1.5404329777
Processing class: 96
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 1.4080379248
Epoch:   200  |  train loss: 1.3747790813
Epoch:   300  |  train loss: 1.4189844847
Epoch:   400  |  train loss: 1.4794061899
Epoch:   500  |  train loss: 1.4724074841
Epoch:   600  |  train loss: 1.4740626097
Epoch:   700  |  train loss: 1.4894933462
Epoch:   800  |  train loss: 1.4767284393
Epoch:   900  |  train loss: 1.5364841700
Epoch:  1000  |  train loss: 1.5421679258
Processing class: 97
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 1.4022593737
Epoch:   200  |  train loss: 1.4261775494
Epoch:   300  |  train loss: 1.4524297476
Epoch:   400  |  train loss: 1.4745975971
Epoch:   500  |  train loss: 1.5187027454
Epoch:   600  |  train loss: 1.5485739708
Epoch:   700  |  train loss: 1.5773524761
Epoch:   800  |  train loss: 1.5928706884
Epoch:   900  |  train loss: 1.6115868330
Epoch:  1000  |  train loss: 1.6126692533
Processing class: 98
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 1.3669898987
Epoch:   200  |  train loss: 1.3785677671
Epoch:   300  |  train loss: 1.4061279297
Epoch:   400  |  train loss: 1.4360977411
Epoch:   500  |  train loss: 1.4433969975
Epoch:   600  |  train loss: 1.4726249218
Epoch:   700  |  train loss: 1.4480346441
Epoch:   800  |  train loss: 1.4837327480
Epoch:   900  |  train loss: 1.4899355650
Epoch:  1000  |  train loss: 1.4794239998
Processing class: 99
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 1.4267731190
Epoch:   200  |  train loss: 1.4090425968
Epoch:   300  |  train loss: 1.4810513496
Epoch:   400  |  train loss: 1.5319968224
Epoch:   500  |  train loss: 1.5389481306
Epoch:   600  |  train loss: 1.5436910629
Epoch:   700  |  train loss: 1.5407001495
Epoch:   800  |  train loss: 1.5804800034
Epoch:   900  |  train loss: 1.5741854906
Epoch:  1000  |  train loss: 1.5858203888
2024-03-05 22:05:39,042 [trainer.py] => CNN: {'total': 50.36, '00-09': 57.7, '10-19': 63.6, '20-29': 71.4, '30-39': 68.5, '40-49': 56.8, '50-59': 29.6, '60-69': 38.9, '70-79': 36.6, '80-89': 42.7, '90-99': 37.8, 'old': 51.76, 'new': 37.8}
2024-03-05 22:05:39,044 [trainer.py] => No NME accuracy
2024-03-05 22:05:39,044 [trainer.py] => FeCAM: {'total': 54.74, '00-09': 73.3, '10-19': 65.1, '20-29': 74.1, '30-39': 67.2, '40-49': 67.1, '50-59': 30.8, '60-69': 43.3, '70-79': 37.2, '80-89': 46.9, '90-99': 42.4, 'old': 56.11, 'new': 42.4}
2024-03-05 22:05:39,044 [trainer.py] => CNN top1 curve: [83.44, 71.75, 64.66, 59.18, 54.08, 50.36]
2024-03-05 22:05:39,044 [trainer.py] => CNN top5 curve: [96.5, 89.6, 86.54, 84.09, 81.74, 79.63]
2024-03-05 22:05:39,044 [trainer.py] => FeCAM top1 curve: [82.9, 72.78, 67.27, 61.72, 57.66, 54.74]
2024-03-05 22:05:39,044 [trainer.py] => FeCAM top5 curve: [95.84, 91.18, 87.21, 84.09, 81.63, 79.21]

=========================================
2024-03-05 22:05:47,404 [trainer.py] => config: ./exps/FeCAM_cifar100.json
2024-03-05 22:05:47,405 [trainer.py] => prefix: train
2024-03-05 22:05:47,405 [trainer.py] => dataset: cifar100
2024-03-05 22:05:47,405 [trainer.py] => memory_size: 0
2024-03-05 22:05:47,405 [trainer.py] => shuffle: True
2024-03-05 22:05:47,405 [trainer.py] => init_cls: 50
2024-03-05 22:05:47,405 [trainer.py] => increment: 10
2024-03-05 22:05:47,405 [trainer.py] => model_name: fecam
2024-03-05 22:05:47,405 [trainer.py] => convnet_type: resnet18
2024-03-05 22:05:47,405 [trainer.py] => device: [device(type='cuda', index=0)]
2024-03-05 22:05:47,405 [trainer.py] => seed: 1993
2024-03-05 22:05:47,405 [trainer.py] => init_epochs: 200
2024-03-05 22:05:47,405 [trainer.py] => init_lr: 0.1
2024-03-05 22:05:47,405 [trainer.py] => init_weight_decay: 0.0005
2024-03-05 22:05:47,405 [trainer.py] => batch_size: 128
2024-03-05 22:05:47,405 [trainer.py] => num_workers: 8
2024-03-05 22:05:47,405 [trainer.py] => T: 5
2024-03-05 22:05:47,405 [trainer.py] => beta: 0.5
2024-03-05 22:05:47,405 [trainer.py] => alpha1: 1
2024-03-05 22:05:47,405 [trainer.py] => alpha2: 1
2024-03-05 22:05:47,405 [trainer.py] => ncm: False
2024-03-05 22:05:47,405 [trainer.py] => tukey: False
2024-03-05 22:05:47,405 [trainer.py] => diagonal: False
2024-03-05 22:05:47,405 [trainer.py] => per_class: True
2024-03-05 22:05:47,406 [trainer.py] => full_cov: True
2024-03-05 22:05:47,406 [trainer.py] => shrink: True
2024-03-05 22:05:47,406 [trainer.py] => norm_cov: False
2024-03-05 22:05:47,406 [trainer.py] => vecnorm: False
2024-03-05 22:05:47,406 [trainer.py] => ae_type: wae
2024-03-05 22:05:47,406 [trainer.py] => epochs: 1000
2024-03-05 22:05:47,406 [trainer.py] => ae_latent_dim: 32
2024-03-05 22:05:47,406 [trainer.py] => wae_sigma: 50
2024-03-05 22:05:47,406 [trainer.py] => wae_C: 5
Files already downloaded and verified
Files already downloaded and verified
2024-03-05 22:05:49,078 [data_manager.py] => [68, 56, 78, 8, 23, 84, 90, 65, 74, 76, 40, 89, 3, 92, 55, 9, 26, 80, 43, 38, 58, 70, 77, 1, 85, 19, 17, 50, 28, 53, 13, 81, 45, 82, 6, 59, 83, 16, 15, 44, 91, 41, 72, 60, 79, 52, 20, 10, 31, 54, 37, 95, 14, 71, 96, 98, 97, 2, 64, 66, 42, 22, 35, 86, 24, 34, 87, 21, 99, 0, 88, 27, 18, 94, 11, 12, 47, 25, 30, 46, 62, 69, 36, 61, 7, 63, 75, 5, 32, 4, 51, 48, 73, 93, 39, 67, 29, 49, 57, 33]
2024-03-05 22:05:49,357 [fecam.py] => Learning on 0-50
/home/z1165703/miniconda3/lib/python3.11/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
/home/z1165703/miniconda3/lib/python3.11/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
Processing class: 0
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 1.2619186640
Epoch:   200  |  train loss: 1.3069031239
Epoch:   300  |  train loss: 1.3394016504
Epoch:   400  |  train loss: 1.3588355541
Epoch:   500  |  train loss: 1.3491027355
Epoch:   600  |  train loss: 1.3484244108
Epoch:   700  |  train loss: 1.3651095629
Epoch:   800  |  train loss: 1.3712654114
Epoch:   900  |  train loss: 1.3600970268
Epoch:  1000  |  train loss: 1.3610574245
Processing class: 1
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 1.2147409439
Epoch:   200  |  train loss: 1.2042356730
Epoch:   300  |  train loss: 1.2841360807
Epoch:   400  |  train loss: 1.3031943083
Epoch:   500  |  train loss: 1.2796129227
Epoch:   600  |  train loss: 1.3281010866
Epoch:   700  |  train loss: 1.3136474133
Epoch:   800  |  train loss: 1.3437108755
Epoch:   900  |  train loss: 1.3834884405
Epoch:  1000  |  train loss: 1.3542408943
Processing class: 2
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 1.1873499155
Epoch:   200  |  train loss: 1.1954153061
Epoch:   300  |  train loss: 1.2533500433
Epoch:   400  |  train loss: 1.2589179993
Epoch:   500  |  train loss: 1.2078873158
Epoch:   600  |  train loss: 1.2688354254
Epoch:   700  |  train loss: 1.2707329988
Epoch:   800  |  train loss: 1.3184188366
Epoch:   900  |  train loss: 1.3046688795
Epoch:  1000  |  train loss: 1.2977088928
Processing class: 3
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 1.3441305637
Epoch:   200  |  train loss: 1.3880047560
Epoch:   300  |  train loss: 1.4163637161
Epoch:   400  |  train loss: 1.4386994123
Epoch:   500  |  train loss: 1.4297366858
Epoch:   600  |  train loss: 1.4802995205
Epoch:   700  |  train loss: 1.5179276705
Epoch:   800  |  train loss: 1.4935838938
Epoch:   900  |  train loss: 1.5247186899
Epoch:  1000  |  train loss: 1.5411326647
Processing class: 4
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 1.2003278255
Epoch:   200  |  train loss: 1.1871768951
Epoch:   300  |  train loss: 1.2566756725
Epoch:   400  |  train loss: 1.2727715254
Epoch:   500  |  train loss: 1.3032943726
Epoch:   600  |  train loss: 1.3075503826
Epoch:   700  |  train loss: 1.3332525015
Epoch:   800  |  train loss: 1.3436643600
Epoch:   900  |  train loss: 1.3601356030
Epoch:  1000  |  train loss: 1.3982962132
Processing class: 5
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 1.1904297113
Epoch:   200  |  train loss: 1.1976649761
Epoch:   300  |  train loss: 1.2285212040
Epoch:   400  |  train loss: 1.2423585892
Epoch:   500  |  train loss: 1.2429821491
Epoch:   600  |  train loss: 1.3082927465
Epoch:   700  |  train loss: 1.3288979769
Epoch:   800  |  train loss: 1.3106216431
Epoch:   900  |  train loss: 1.3131608486
Epoch:  1000  |  train loss: 1.3451469660
Processing class: 6
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 1.2225356817
Epoch:   200  |  train loss: 1.2443469048
Epoch:   300  |  train loss: 1.3147190571
Epoch:   400  |  train loss: 1.3191880941
Epoch:   500  |  train loss: 1.3732746601
Epoch:   600  |  train loss: 1.3601654530
Epoch:   700  |  train loss: 1.3499180555
Epoch:   800  |  train loss: 1.3674749136
Epoch:   900  |  train loss: 1.3606020689
Epoch:  1000  |  train loss: 1.3819027185
Processing class: 7
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 1.1657175303
Epoch:   200  |  train loss: 1.1894692183
Epoch:   300  |  train loss: 1.2137247801
Epoch:   400  |  train loss: 1.2386525631
Epoch:   500  |  train loss: 1.2748502731
Epoch:   600  |  train loss: 1.2649962902
Epoch:   700  |  train loss: 1.2946899891
Epoch:   800  |  train loss: 1.3406409502
Epoch:   900  |  train loss: 1.3141269922
Epoch:  1000  |  train loss: 1.3538388491
Processing class: 8
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 1.1907362461
Epoch:   200  |  train loss: 1.2275957584
Epoch:   300  |  train loss: 1.2317943335
Epoch:   400  |  train loss: 1.2924544811
Epoch:   500  |  train loss: 1.2754613876
Epoch:   600  |  train loss: 1.3002358437
Epoch:   700  |  train loss: 1.3411643267
Epoch:   800  |  train loss: 1.3180208683
Epoch:   900  |  train loss: 1.3432772398
Epoch:  1000  |  train loss: 1.3308269739
Processing class: 9
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 1.1709490061
Epoch:   200  |  train loss: 1.2007523537
Epoch:   300  |  train loss: 1.2450901031
Epoch:   400  |  train loss: 1.2163779259
Epoch:   500  |  train loss: 1.2609429836
Epoch:   600  |  train loss: 1.2406246901
Epoch:   700  |  train loss: 1.2569409609
Epoch:   800  |  train loss: 1.2710483313
Epoch:   900  |  train loss: 1.2750608444
Epoch:  1000  |  train loss: 1.2554722548
Processing class: 10
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 1.1257542372
Epoch:   200  |  train loss: 1.1373072147
Epoch:   300  |  train loss: 1.1604632378
Epoch:   400  |  train loss: 1.2113366365
Epoch:   500  |  train loss: 1.1679377317
Epoch:   600  |  train loss: 1.1970647335
Epoch:   700  |  train loss: 1.2195626974
Epoch:   800  |  train loss: 1.2152433634
Epoch:   900  |  train loss: 1.2313098192
Epoch:  1000  |  train loss: 1.2321587563
Processing class: 11
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 1.1932151318
Epoch:   200  |  train loss: 1.2280286551
Epoch:   300  |  train loss: 1.2187083006
Epoch:   400  |  train loss: 1.2483746529
Epoch:   500  |  train loss: 1.2635768652
Epoch:   600  |  train loss: 1.3007784128
Epoch:   700  |  train loss: 1.3500092983
Epoch:   800  |  train loss: 1.3450576067
Epoch:   900  |  train loss: 1.3544193506
Epoch:  1000  |  train loss: 1.3831333876
Processing class: 12
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 1.1439309835
Epoch:   200  |  train loss: 1.2165909290
Epoch:   300  |  train loss: 1.2342038870
Epoch:   400  |  train loss: 1.2848553419
Epoch:   500  |  train loss: 1.2966742277
Epoch:   600  |  train loss: 1.2865520239
Epoch:   700  |  train loss: 1.3076288223
Epoch:   800  |  train loss: 1.3228773355
Epoch:   900  |  train loss: 1.3490376472
Epoch:  1000  |  train loss: 1.3383502960
Processing class: 13
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 1.1827123165
Epoch:   200  |  train loss: 1.2500063658
Epoch:   300  |  train loss: 1.3027887821
Epoch:   400  |  train loss: 1.3279389858
Epoch:   500  |  train loss: 1.3751448154
Epoch:   600  |  train loss: 1.3511557579
Epoch:   700  |  train loss: 1.3800295353
Epoch:   800  |  train loss: 1.4029762983
Epoch:   900  |  train loss: 1.4178980827
Epoch:  1000  |  train loss: 1.4371849060
Processing class: 14
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 1.1749632359
Epoch:   200  |  train loss: 1.1951221943
Epoch:   300  |  train loss: 1.2177903175
Epoch:   400  |  train loss: 1.2647183180
Epoch:   500  |  train loss: 1.2664956093
Epoch:   600  |  train loss: 1.2744764805
Epoch:   700  |  train loss: 1.2956146002
Epoch:   800  |  train loss: 1.3459597826
Epoch:   900  |  train loss: 1.3332141876
Epoch:  1000  |  train loss: 1.3053913832
Processing class: 15
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 1.2039765596
Epoch:   200  |  train loss: 1.2342058897
Epoch:   300  |  train loss: 1.3009158373
Epoch:   400  |  train loss: 1.3320277691
Epoch:   500  |  train loss: 1.3604340076
Epoch:   600  |  train loss: 1.3449555159
Epoch:   700  |  train loss: 1.3878119707
Epoch:   800  |  train loss: 1.3558691502
Epoch:   900  |  train loss: 1.3945755005
Epoch:  1000  |  train loss: 1.3798270226
Processing class: 16
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 1.2135616302
Epoch:   200  |  train loss: 1.3214376926
Epoch:   300  |  train loss: 1.3696406364
Epoch:   400  |  train loss: 1.3560684919
Epoch:   500  |  train loss: 1.3935319662
Epoch:   600  |  train loss: 1.3679114819
Epoch:   700  |  train loss: 1.3651985168
Epoch:   800  |  train loss: 1.4181603670
Epoch:   900  |  train loss: 1.3795411587
Epoch:  1000  |  train loss: 1.4227446556
Processing class: 17
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 1.1454048395
Epoch:   200  |  train loss: 1.1702895164
Epoch:   300  |  train loss: 1.2110889673
Epoch:   400  |  train loss: 1.2251848221
Epoch:   500  |  train loss: 1.1996924400
Epoch:   600  |  train loss: 1.2344610453
Epoch:   700  |  train loss: 1.2323984385
Epoch:   800  |  train loss: 1.2411029577
Epoch:   900  |  train loss: 1.2573733807
Epoch:  1000  |  train loss: 1.2540489435
Processing class: 18
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 1.1838838339
Epoch:   200  |  train loss: 1.1968168736
Epoch:   300  |  train loss: 1.2552779436
Epoch:   400  |  train loss: 1.2604529619
Epoch:   500  |  train loss: 1.3174291134
Epoch:   600  |  train loss: 1.3143515348
Epoch:   700  |  train loss: 1.3811201096
Epoch:   800  |  train loss: 1.3867290974
Epoch:   900  |  train loss: 1.4055459261
Epoch:  1000  |  train loss: 1.4461743832
Processing class: 19
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 1.2172797203
Epoch:   200  |  train loss: 1.3028886318
Epoch:   300  |  train loss: 1.3134137154
Epoch:   400  |  train loss: 1.3285349369
Epoch:   500  |  train loss: 1.3157946587
Epoch:   600  |  train loss: 1.3248842001
Epoch:   700  |  train loss: 1.3485016108
Epoch:   800  |  train loss: 1.3433774948
Epoch:   900  |  train loss: 1.4181951761
Epoch:  1000  |  train loss: 1.4360159636
Processing class: 20
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 1.2066049814
Epoch:   200  |  train loss: 1.2443100929
Epoch:   300  |  train loss: 1.2738830090
Epoch:   400  |  train loss: 1.3066033125
Epoch:   500  |  train loss: 1.3450405359
Epoch:   600  |  train loss: 1.3294129610
Epoch:   700  |  train loss: 1.3437944889
Epoch:   800  |  train loss: 1.3461767912
Epoch:   900  |  train loss: 1.3666321754
Epoch:  1000  |  train loss: 1.3948957205
Processing class: 21
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 1.2089691639
Epoch:   200  |  train loss: 1.2498307467
Epoch:   300  |  train loss: 1.2949410439
Epoch:   400  |  train loss: 1.3229314566
Epoch:   500  |  train loss: 1.3469042778
Epoch:   600  |  train loss: 1.3761774063
Epoch:   700  |  train loss: 1.3726540327
Epoch:   800  |  train loss: 1.3710740328
Epoch:   900  |  train loss: 1.4091223717
Epoch:  1000  |  train loss: 1.4085156202
Processing class: 22
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 1.1731570244
Epoch:   200  |  train loss: 1.1904946327
Epoch:   300  |  train loss: 1.2240601778
Epoch:   400  |  train loss: 1.2754767656
Epoch:   500  |  train loss: 1.2969290972
Epoch:   600  |  train loss: 1.2864239931
Epoch:   700  |  train loss: 1.3111574888
Epoch:   800  |  train loss: 1.3088756561
Epoch:   900  |  train loss: 1.2973302364
Epoch:  1000  |  train loss: 1.3506600142
Processing class: 23
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 1.2444425583
Epoch:   200  |  train loss: 1.2800141335
Epoch:   300  |  train loss: 1.3184373140
Epoch:   400  |  train loss: 1.2931715488
Epoch:   500  |  train loss: 1.3004346848
Epoch:   600  |  train loss: 1.2920890093
Epoch:   700  |  train loss: 1.3233591557
Epoch:   800  |  train loss: 1.3121363401
Epoch:   900  |  train loss: 1.3400301933
Epoch:  1000  |  train loss: 1.3275484800
Processing class: 24
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 1.2124019861
Epoch:   200  |  train loss: 1.2169342041
Epoch:   300  |  train loss: 1.1861620188
Epoch:   400  |  train loss: 1.2298893213
Epoch:   500  |  train loss: 1.2216041327
Epoch:   600  |  train loss: 1.2475178957
Epoch:   700  |  train loss: 1.2105452299
Epoch:   800  |  train loss: 1.2294342756
Epoch:   900  |  train loss: 1.2602571726
Epoch:  1000  |  train loss: 1.2570304394
Processing class: 25
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 1.1838818073
Epoch:   200  |  train loss: 1.2187902451
Epoch:   300  |  train loss: 1.2649074316
Epoch:   400  |  train loss: 1.2553885460
Epoch:   500  |  train loss: 1.3111404181
Epoch:   600  |  train loss: 1.2968734264
Epoch:   700  |  train loss: 1.3401288986
Epoch:   800  |  train loss: 1.3306154728
Epoch:   900  |  train loss: 1.3279735804
Epoch:  1000  |  train loss: 1.3434140205
Processing class: 26
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 1.2657591343
Epoch:   200  |  train loss: 1.3124631166
Epoch:   300  |  train loss: 1.2822360754
Epoch:   400  |  train loss: 1.3243070126
Epoch:   500  |  train loss: 1.3667945385
Epoch:   600  |  train loss: 1.3317151070
Epoch:   700  |  train loss: 1.3562045336
Epoch:   800  |  train loss: 1.3741145849
Epoch:   900  |  train loss: 1.3735882044
Epoch:  1000  |  train loss: 1.4062325239
Processing class: 27
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 1.1786729097
Epoch:   200  |  train loss: 1.1827763796
Epoch:   300  |  train loss: 1.2006702423
Epoch:   400  |  train loss: 1.2240640879
Epoch:   500  |  train loss: 1.2579365969
Epoch:   600  |  train loss: 1.2621188402
Epoch:   700  |  train loss: 1.2814446926
Epoch:   800  |  train loss: 1.2842746735
Epoch:   900  |  train loss: 1.3018557310
Epoch:  1000  |  train loss: 1.2775537252
Processing class: 28
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 1.2326483011
Epoch:   200  |  train loss: 1.2721300125
Epoch:   300  |  train loss: 1.3280861616
Epoch:   400  |  train loss: 1.3586782217
Epoch:   500  |  train loss: 1.3766637087
Epoch:   600  |  train loss: 1.3853490591
Epoch:   700  |  train loss: 1.3948753834
Epoch:   800  |  train loss: 1.4097692013
Epoch:   900  |  train loss: 1.4194760799
Epoch:  1000  |  train loss: 1.3929006338
Processing class: 29
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 1.1935499430
Epoch:   200  |  train loss: 1.2377137899
Epoch:   300  |  train loss: 1.2485175848
Epoch:   400  |  train loss: 1.2475876570
Epoch:   500  |  train loss: 1.2839979172
Epoch:   600  |  train loss: 1.3069728136
Epoch:   700  |  train loss: 1.3150642633
Epoch:   800  |  train loss: 1.3021577358
Epoch:   900  |  train loss: 1.3137758970
Epoch:  1000  |  train loss: 1.3109653711
Processing class: 30
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 1.2606532812
Epoch:   200  |  train loss: 1.3053089619
Epoch:   300  |  train loss: 1.3803148985
Epoch:   400  |  train loss: 1.4361655712
Epoch:   500  |  train loss: 1.4144074917
Epoch:   600  |  train loss: 1.4215282202
Epoch:   700  |  train loss: 1.4221618652
Epoch:   800  |  train loss: 1.4587421656
Epoch:   900  |  train loss: 1.4828877449
Epoch:  1000  |  train loss: 1.4644168377
Processing class: 31
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 1.2350961447
Epoch:   200  |  train loss: 1.2923329115
Epoch:   300  |  train loss: 1.3126367331
Epoch:   400  |  train loss: 1.3595174789
Epoch:   500  |  train loss: 1.3804470062
Epoch:   600  |  train loss: 1.4230916977
Epoch:   700  |  train loss: 1.4193137407
Epoch:   800  |  train loss: 1.4237587452
Epoch:   900  |  train loss: 1.4557268381
Epoch:  1000  |  train loss: 1.4305698156
Processing class: 32
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 1.1811291456
Epoch:   200  |  train loss: 1.2211625099
Epoch:   300  |  train loss: 1.2252564430
Epoch:   400  |  train loss: 1.2068632364
Epoch:   500  |  train loss: 1.2742223501
Epoch:   600  |  train loss: 1.2727742434
Epoch:   700  |  train loss: 1.3243039370
Epoch:   800  |  train loss: 1.3485811234
Epoch:   900  |  train loss: 1.3433432817
Epoch:  1000  |  train loss: 1.3371824265
Processing class: 33
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 1.1400998592
Epoch:   200  |  train loss: 1.2197221756
Epoch:   300  |  train loss: 1.1813575268
Epoch:   400  |  train loss: 1.2032430887
Epoch:   500  |  train loss: 1.2198243141
Epoch:   600  |  train loss: 1.2735339642
Epoch:   700  |  train loss: 1.2555854559
Epoch:   800  |  train loss: 1.2720446110
Epoch:   900  |  train loss: 1.2736618757
Epoch:  1000  |  train loss: 1.2653104067
Processing class: 34
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 1.1998890400
Epoch:   200  |  train loss: 1.1976863146
Epoch:   300  |  train loss: 1.2400540829
Epoch:   400  |  train loss: 1.2333326817
Epoch:   500  |  train loss: 1.2420262337
Epoch:   600  |  train loss: 1.2765956402
Epoch:   700  |  train loss: 1.2619309664
Epoch:   800  |  train loss: 1.3020111561
Epoch:   900  |  train loss: 1.3179040432
Epoch:  1000  |  train loss: 1.3281742334
Processing class: 35
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 1.1441943407
Epoch:   200  |  train loss: 1.1984951258
Epoch:   300  |  train loss: 1.2342933416
Epoch:   400  |  train loss: 1.2544314384
Epoch:   500  |  train loss: 1.2410041332
Epoch:   600  |  train loss: 1.2608042955
Epoch:   700  |  train loss: 1.2803548574
Epoch:   800  |  train loss: 1.2811068058
Epoch:   900  |  train loss: 1.2889460802
Epoch:  1000  |  train loss: 1.2799715519
Processing class: 36
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 1.2442656755
Epoch:   200  |  train loss: 1.2675036907
Epoch:   300  |  train loss: 1.3048594713
Epoch:   400  |  train loss: 1.3118635893
Epoch:   500  |  train loss: 1.3643212557
Epoch:   600  |  train loss: 1.3560325861
Epoch:   700  |  train loss: 1.3435844183
Epoch:   800  |  train loss: 1.3996791601
Epoch:   900  |  train loss: 1.3674735069
Epoch:  1000  |  train loss: 1.4024815321
Processing class: 37
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 1.2040845871
Epoch:   200  |  train loss: 1.2053487778
Epoch:   300  |  train loss: 1.2140381813
Epoch:   400  |  train loss: 1.2432729721
Epoch:   500  |  train loss: 1.2691820621
Epoch:   600  |  train loss: 1.2551409721
Epoch:   700  |  train loss: 1.2669543982
Epoch:   800  |  train loss: 1.2813172340
Epoch:   900  |  train loss: 1.2702960253
Epoch:  1000  |  train loss: 1.2852696896
Processing class: 38
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 1.2130799055
Epoch:   200  |  train loss: 1.2327713013
Epoch:   300  |  train loss: 1.2760473728
Epoch:   400  |  train loss: 1.2880542755
Epoch:   500  |  train loss: 1.3124272108
Epoch:   600  |  train loss: 1.3134485722
Epoch:   700  |  train loss: 1.3273381948
Epoch:   800  |  train loss: 1.3606755495
Epoch:   900  |  train loss: 1.3304926634
Epoch:  1000  |  train loss: 1.3596299410
Processing class: 39
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 1.2135795116
Epoch:   200  |  train loss: 1.2105061769
Epoch:   300  |  train loss: 1.2435728550
Epoch:   400  |  train loss: 1.2710426569
Epoch:   500  |  train loss: 1.2695134401
Epoch:   600  |  train loss: 1.2860280514
Epoch:   700  |  train loss: 1.3085097313
Epoch:   800  |  train loss: 1.3358308315
Epoch:   900  |  train loss: 1.3557351589
Epoch:  1000  |  train loss: 1.3701663971
Processing class: 40
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 1.2239005089
Epoch:   200  |  train loss: 1.2213219166
Epoch:   300  |  train loss: 1.2885779858
Epoch:   400  |  train loss: 1.3065097094
Epoch:   500  |  train loss: 1.3306281328
Epoch:   600  |  train loss: 1.3278921604
Epoch:   700  |  train loss: 1.3457583666
Epoch:   800  |  train loss: 1.3497357607
Epoch:   900  |  train loss: 1.3377529621
Epoch:  1000  |  train loss: 1.3968590498
Processing class: 41
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 1.2254391909
Epoch:   200  |  train loss: 1.2300304413
Epoch:   300  |  train loss: 1.2778812170
Epoch:   400  |  train loss: 1.2831229925
Epoch:   500  |  train loss: 1.2939458847
Epoch:   600  |  train loss: 1.3012123108
Epoch:   700  |  train loss: 1.3315763950
Epoch:   800  |  train loss: 1.3204427958
Epoch:   900  |  train loss: 1.3193514585
Epoch:  1000  |  train loss: 1.3258011818
Processing class: 42
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 1.1674988031
Epoch:   200  |  train loss: 1.2195875645
Epoch:   300  |  train loss: 1.2595688343
Epoch:   400  |  train loss: 1.2841525555
Epoch:   500  |  train loss: 1.3095386028
Epoch:   600  |  train loss: 1.3295407772
Epoch:   700  |  train loss: 1.3552659035
Epoch:   800  |  train loss: 1.3781771660
Epoch:   900  |  train loss: 1.3699901104
Epoch:  1000  |  train loss: 1.3732084513
Processing class: 43
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 1.1759295702
Epoch:   200  |  train loss: 1.1851366520
Epoch:   300  |  train loss: 1.2733417749
Epoch:   400  |  train loss: 1.3356693029
Epoch:   500  |  train loss: 1.3554071188
Epoch:   600  |  train loss: 1.3426891565
Epoch:   700  |  train loss: 1.3696829557
Epoch:   800  |  train loss: 1.3611737728
Epoch:   900  |  train loss: 1.3345446825
Epoch:  1000  |  train loss: 1.3790251732
Processing class: 44
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 1.1584433317
Epoch:   200  |  train loss: 1.2203498125
Epoch:   300  |  train loss: 1.2109078884
Epoch:   400  |  train loss: 1.2334228754
Epoch:   500  |  train loss: 1.2576489449
Epoch:   600  |  train loss: 1.2495897055
Epoch:   700  |  train loss: 1.2841187239
Epoch:   800  |  train loss: 1.2803810835
Epoch:   900  |  train loss: 1.2738186359
Epoch:  1000  |  train loss: 1.3119746447
Processing class: 45
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 1.2226124048
Epoch:   200  |  train loss: 1.2921886206
Epoch:   300  |  train loss: 1.3105379105
Epoch:   400  |  train loss: 1.3430306196
Epoch:   500  |  train loss: 1.3397502184
Epoch:   600  |  train loss: 1.3937350750
Epoch:   700  |  train loss: 1.4109385014
Epoch:   800  |  train loss: 1.4018540144
Epoch:   900  |  train loss: 1.4081566811
Epoch:  1000  |  train loss: 1.4374346018
Processing class: 46
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 1.2522546530
Epoch:   200  |  train loss: 1.2689848900
Epoch:   300  |  train loss: 1.3085028648
Epoch:   400  |  train loss: 1.3247744322
Epoch:   500  |  train loss: 1.3330991268
Epoch:   600  |  train loss: 1.3348583698
Epoch:   700  |  train loss: 1.3440682650
Epoch:   800  |  train loss: 1.3880945206
Epoch:   900  |  train loss: 1.3713381767
Epoch:  1000  |  train loss: 1.3903367281
Processing class: 47
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 1.1610393524
Epoch:   200  |  train loss: 1.2068378687
Epoch:   300  |  train loss: 1.2112799168
Epoch:   400  |  train loss: 1.2575794935
Epoch:   500  |  train loss: 1.2666556835
Epoch:   600  |  train loss: 1.2765047312
Epoch:   700  |  train loss: 1.2983341217
Epoch:   800  |  train loss: 1.2864757776
Epoch:   900  |  train loss: 1.3038999081
Epoch:  1000  |  train loss: 1.2995092869
Processing class: 48
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 1.1886511326
Epoch:   200  |  train loss: 1.2383998871
Epoch:   300  |  train loss: 1.2547242880
Epoch:   400  |  train loss: 1.2347659588
Epoch:   500  |  train loss: 1.2802529573
Epoch:   600  |  train loss: 1.2792206049
Epoch:   700  |  train loss: 1.2664626122
Epoch:   800  |  train loss: 1.2673500538
Epoch:   900  |  train loss: 1.3142300844
Epoch:  1000  |  train loss: 1.3322854280
Processing class: 49
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 1.1768663883
Epoch:   200  |  train loss: 1.1982383728
Epoch:   300  |  train loss: 1.2713711739
Epoch:   400  |  train loss: 1.2903422117
Epoch:   500  |  train loss: 1.3162969112
Epoch:   600  |  train loss: 1.3225319624
Epoch:   700  |  train loss: 1.3296718359
Epoch:   800  |  train loss: 1.3263548374
Epoch:   900  |  train loss: 1.3248506784
Epoch:  1000  |  train loss: 1.3353230953
/home/z1165703/FeCAM/models/base.py:87: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  vec = torch.tensor(vec).cuda()
2024-03-05 22:23:38,376 [trainer.py] => CNN: {'total': 83.44, '00-09': 87.7, '10-19': 79.6, '20-29': 84.9, '30-39': 81.0, '40-49': 84.0, 'old': 0, 'new': 83.44}
2024-03-05 22:23:38,377 [trainer.py] => No NME accuracy
2024-03-05 22:23:38,377 [trainer.py] => FeCAM: {'total': 82.9, '00-09': 86.2, '10-19': 79.7, '20-29': 85.0, '30-39': 80.6, '40-49': 83.0, 'old': 0, 'new': 82.9}
2024-03-05 22:23:38,377 [trainer.py] => CNN top1 curve: [83.44]
2024-03-05 22:23:38,377 [trainer.py] => CNN top5 curve: [96.5]
2024-03-05 22:23:38,377 [trainer.py] => FeCAM top1 curve: [82.9]
2024-03-05 22:23:38,377 [trainer.py] => FeCAM top5 curve: [95.84]

2024-03-05 22:23:38,385 [fecam.py] => Learning on 50-60
Processing class: 50
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 1.1255791664
Epoch:   200  |  train loss: 1.1507647514
Epoch:   300  |  train loss: 1.1701330662
Epoch:   400  |  train loss: 1.2099428177
Epoch:   500  |  train loss: 1.2432675123
Epoch:   600  |  train loss: 1.2346884251
Epoch:   700  |  train loss: 1.2788993597
Epoch:   800  |  train loss: 1.3036364317
Epoch:   900  |  train loss: 1.3291070938
Epoch:  1000  |  train loss: 1.3443557739
Processing class: 51
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 1.1369818449
Epoch:   200  |  train loss: 1.1214115381
Epoch:   300  |  train loss: 1.1806020021
Epoch:   400  |  train loss: 1.2010478497
Epoch:   500  |  train loss: 1.2238794088
Epoch:   600  |  train loss: 1.2569278955
Epoch:   700  |  train loss: 1.2387327671
Epoch:   800  |  train loss: 1.2535804272
Epoch:   900  |  train loss: 1.2991456270
Epoch:  1000  |  train loss: 1.2944298744
Processing class: 52
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 1.1337438345
Epoch:   200  |  train loss: 1.1287266970
Epoch:   300  |  train loss: 1.1608866215
Epoch:   400  |  train loss: 1.1843198776
Epoch:   500  |  train loss: 1.1888118505
Epoch:   600  |  train loss: 1.1803496361
Epoch:   700  |  train loss: 1.2162548304
Epoch:   800  |  train loss: 1.2245791674
Epoch:   900  |  train loss: 1.2402526855
Epoch:  1000  |  train loss: 1.2242721081
Processing class: 53
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 1.1307579517
Epoch:   200  |  train loss: 1.1619964838
Epoch:   300  |  train loss: 1.1426894188
Epoch:   400  |  train loss: 1.1664150715
Epoch:   500  |  train loss: 1.1888904095
Epoch:   600  |  train loss: 1.2199784517
Epoch:   700  |  train loss: 1.2577833176
Epoch:   800  |  train loss: 1.2090227127
Epoch:   900  |  train loss: 1.2399785042
Epoch:  1000  |  train loss: 1.2280099869
Processing class: 54
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 1.0611456871
Epoch:   200  |  train loss: 1.0860178471
Epoch:   300  |  train loss: 1.1338398457
Epoch:   400  |  train loss: 1.1571262598
Epoch:   500  |  train loss: 1.1815322399
Epoch:   600  |  train loss: 1.2253419399
Epoch:   700  |  train loss: 1.2434913874
Epoch:   800  |  train loss: 1.2797952414
Epoch:   900  |  train loss: 1.3059485912
Epoch:  1000  |  train loss: 1.3060077429
Processing class: 55
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 1.1141012907
Epoch:   200  |  train loss: 1.1241481304
Epoch:   300  |  train loss: 1.1155489206
Epoch:   400  |  train loss: 1.1561184406
Epoch:   500  |  train loss: 1.2088886499
Epoch:   600  |  train loss: 1.2213276386
Epoch:   700  |  train loss: 1.2441426516
Epoch:   800  |  train loss: 1.2844382048
Epoch:   900  |  train loss: 1.2540209532
Epoch:  1000  |  train loss: 1.2849747419
Processing class: 56
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 1.1138415575
Epoch:   200  |  train loss: 1.1491520882
Epoch:   300  |  train loss: 1.1494282246
Epoch:   400  |  train loss: 1.2045646906
Epoch:   500  |  train loss: 1.2387841702
Epoch:   600  |  train loss: 1.2669481993
Epoch:   700  |  train loss: 1.2978507280
Epoch:   800  |  train loss: 1.2896486044
Epoch:   900  |  train loss: 1.3283557653
Epoch:  1000  |  train loss: 1.3536174297
Processing class: 57
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 1.0996755600
Epoch:   200  |  train loss: 1.1314246178
Epoch:   300  |  train loss: 1.1271389723
Epoch:   400  |  train loss: 1.1355493546
Epoch:   500  |  train loss: 1.1656090260
Epoch:   600  |  train loss: 1.1439207077
Epoch:   700  |  train loss: 1.1918905735
Epoch:   800  |  train loss: 1.1996749163
Epoch:   900  |  train loss: 1.2284136057
Epoch:  1000  |  train loss: 1.2022967577
Processing class: 58
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 1.1112980843
Epoch:   200  |  train loss: 1.1314705133
Epoch:   300  |  train loss: 1.1836165428
Epoch:   400  |  train loss: 1.2443589926
Epoch:   500  |  train loss: 1.2470760107
Epoch:   600  |  train loss: 1.2826093912
Epoch:   700  |  train loss: 1.2875575781
Epoch:   800  |  train loss: 1.3163305044
Epoch:   900  |  train loss: 1.3224173069
Epoch:  1000  |  train loss: 1.3334987640
Processing class: 59
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 1.1400316954
Epoch:   200  |  train loss: 1.1913705111
Epoch:   300  |  train loss: 1.1951112032
Epoch:   400  |  train loss: 1.2322072268
Epoch:   500  |  train loss: 1.2653005600
Epoch:   600  |  train loss: 1.2771052599
Epoch:   700  |  train loss: 1.3011414051
Epoch:   800  |  train loss: 1.3157601118
Epoch:   900  |  train loss: 1.3186496019
Epoch:  1000  |  train loss: 1.3113204002
2024-03-05 22:29:18,664 [trainer.py] => CNN: {'total': 71.75, '00-09': 82.2, '10-19': 72.8, '20-29': 78.5, '30-39': 74.9, '40-49': 68.4, '50-59': 53.7, 'old': 75.36, 'new': 53.7}
2024-03-05 22:29:18,666 [trainer.py] => No NME accuracy
2024-03-05 22:29:18,666 [trainer.py] => FeCAM: {'total': 72.78, '00-09': 82.6, '10-19': 74.0, '20-29': 81.0, '30-39': 76.4, '40-49': 76.8, '50-59': 45.9, 'old': 78.16, 'new': 45.9}
2024-03-05 22:29:18,666 [trainer.py] => CNN top1 curve: [83.44, 71.75]
2024-03-05 22:29:18,666 [trainer.py] => CNN top5 curve: [96.5, 89.6]
2024-03-05 22:29:18,666 [trainer.py] => FeCAM top1 curve: [82.9, 72.78]
2024-03-05 22:29:18,666 [trainer.py] => FeCAM top5 curve: [95.84, 91.18]

2024-03-05 22:29:18,674 [fecam.py] => Learning on 60-70
Processing class: 60
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 1.1597841978
Epoch:   200  |  train loss: 1.1631836891
Epoch:   300  |  train loss: 1.1585680008
Epoch:   400  |  train loss: 1.1656057835
Epoch:   500  |  train loss: 1.2204817533
Epoch:   600  |  train loss: 1.2332022905
Epoch:   700  |  train loss: 1.3344316959
Epoch:   800  |  train loss: 1.3005051613
Epoch:   900  |  train loss: 1.3174232721
Epoch:  1000  |  train loss: 1.3496621132
Processing class: 61
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 1.1141180038
Epoch:   200  |  train loss: 1.1100769281
Epoch:   300  |  train loss: 1.1724593163
Epoch:   400  |  train loss: 1.1989694834
Epoch:   500  |  train loss: 1.2592683792
Epoch:   600  |  train loss: 1.2413226128
Epoch:   700  |  train loss: 1.2973833561
Epoch:   800  |  train loss: 1.3327043056
Epoch:   900  |  train loss: 1.3032782316
Epoch:  1000  |  train loss: 1.3257090807
Processing class: 62
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 1.1363132954
Epoch:   200  |  train loss: 1.1243679285
Epoch:   300  |  train loss: 1.1566277742
Epoch:   400  |  train loss: 1.1734364033
Epoch:   500  |  train loss: 1.1625300169
Epoch:   600  |  train loss: 1.2089682102
Epoch:   700  |  train loss: 1.2256491184
Epoch:   800  |  train loss: 1.2428319931
Epoch:   900  |  train loss: 1.2401846409
Epoch:  1000  |  train loss: 1.2555796623
Processing class: 63
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 1.0914512157
Epoch:   200  |  train loss: 1.1089214325
Epoch:   300  |  train loss: 1.1856065273
Epoch:   400  |  train loss: 1.2088794470
Epoch:   500  |  train loss: 1.2460523129
Epoch:   600  |  train loss: 1.2914729357
Epoch:   700  |  train loss: 1.2575631142
Epoch:   800  |  train loss: 1.3314845562
Epoch:   900  |  train loss: 1.2975634813
Epoch:  1000  |  train loss: 1.3130267143
Processing class: 64
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 1.1707741261
Epoch:   200  |  train loss: 1.1810725689
Epoch:   300  |  train loss: 1.2689324379
Epoch:   400  |  train loss: 1.2877792597
Epoch:   500  |  train loss: 1.3155092955
Epoch:   600  |  train loss: 1.3265841246
Epoch:   700  |  train loss: 1.3081230879
Epoch:   800  |  train loss: 1.3322448015
Epoch:   900  |  train loss: 1.3729426384
Epoch:  1000  |  train loss: 1.3734233618
Processing class: 65
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 1.1344923019
Epoch:   200  |  train loss: 1.1587202549
Epoch:   300  |  train loss: 1.2536428213
Epoch:   400  |  train loss: 1.2985414982
Epoch:   500  |  train loss: 1.3044453859
Epoch:   600  |  train loss: 1.3750254393
Epoch:   700  |  train loss: 1.3869718790
Epoch:   800  |  train loss: 1.4491091490
Epoch:   900  |  train loss: 1.4271782637
Epoch:  1000  |  train loss: 1.4893238544
Processing class: 66
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 1.1544640541
Epoch:   200  |  train loss: 1.1928301334
Epoch:   300  |  train loss: 1.2383687973
Epoch:   400  |  train loss: 1.2546365976
Epoch:   500  |  train loss: 1.2949649811
Epoch:   600  |  train loss: 1.3083073139
Epoch:   700  |  train loss: 1.3523127794
Epoch:   800  |  train loss: 1.3616858482
Epoch:   900  |  train loss: 1.3694675207
Epoch:  1000  |  train loss: 1.3668456793
Processing class: 67
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 1.0996907234
Epoch:   200  |  train loss: 1.1243921041
Epoch:   300  |  train loss: 1.1357392073
Epoch:   400  |  train loss: 1.1539299250
Epoch:   500  |  train loss: 1.1656292915
Epoch:   600  |  train loss: 1.1700036287
Epoch:   700  |  train loss: 1.1838920832
Epoch:   800  |  train loss: 1.2129423857
Epoch:   900  |  train loss: 1.2143399000
Epoch:  1000  |  train loss: 1.2179992199
Processing class: 68
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 1.1032980204
Epoch:   200  |  train loss: 1.1121093273
Epoch:   300  |  train loss: 1.1306650877
Epoch:   400  |  train loss: 1.1565384150
Epoch:   500  |  train loss: 1.1687635183
Epoch:   600  |  train loss: 1.1739054203
Epoch:   700  |  train loss: 1.1964901924
Epoch:   800  |  train loss: 1.1909871340
Epoch:   900  |  train loss: 1.2113173485
Epoch:  1000  |  train loss: 1.2591926813
Processing class: 69
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 1.1307396412
Epoch:   200  |  train loss: 1.1804032564
Epoch:   300  |  train loss: 1.2677910805
Epoch:   400  |  train loss: 1.3125612020
Epoch:   500  |  train loss: 1.3284810781
Epoch:   600  |  train loss: 1.2945006371
Epoch:   700  |  train loss: 1.3390036583
Epoch:   800  |  train loss: 1.3417892218
Epoch:   900  |  train loss: 1.3952158689
Epoch:  1000  |  train loss: 1.3507298470
2024-03-05 22:35:47,606 [trainer.py] => CNN: {'total': 64.66, '00-09': 75.0, '10-19': 70.1, '20-29': 77.1, '30-39': 70.6, '40-49': 64.0, '50-59': 45.8, '60-69': 50.0, 'old': 67.1, 'new': 50.0}
2024-03-05 22:35:47,608 [trainer.py] => No NME accuracy
2024-03-05 22:35:47,608 [trainer.py] => FeCAM: {'total': 67.27, '00-09': 79.9, '10-19': 70.6, '20-29': 79.1, '30-39': 71.9, '40-49': 74.1, '50-59': 41.2, '60-69': 54.1, 'old': 69.47, 'new': 54.1}
2024-03-05 22:35:47,608 [trainer.py] => CNN top1 curve: [83.44, 71.75, 64.66]
2024-03-05 22:35:47,608 [trainer.py] => CNN top5 curve: [96.5, 89.6, 86.54]
2024-03-05 22:35:47,608 [trainer.py] => FeCAM top1 curve: [82.9, 72.78, 67.27]
2024-03-05 22:35:47,608 [trainer.py] => FeCAM top5 curve: [95.84, 91.18, 87.21]

2024-03-05 22:35:47,613 [fecam.py] => Learning on 70-80
Processing class: 70
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 1.1084834576
Epoch:   200  |  train loss: 1.1064395428
Epoch:   300  |  train loss: 1.1439803839
Epoch:   400  |  train loss: 1.1713697195
Epoch:   500  |  train loss: 1.1758323669
Epoch:   600  |  train loss: 1.2143964767
Epoch:   700  |  train loss: 1.2651814222
Epoch:   800  |  train loss: 1.2382886648
Epoch:   900  |  train loss: 1.2747127056
Epoch:  1000  |  train loss: 1.2707151413
Processing class: 71
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 1.1235055447
Epoch:   200  |  train loss: 1.1442068100
Epoch:   300  |  train loss: 1.1377737761
Epoch:   400  |  train loss: 1.1774506569
Epoch:   500  |  train loss: 1.2090742350
Epoch:   600  |  train loss: 1.2242163420
Epoch:   700  |  train loss: 1.2385082483
Epoch:   800  |  train loss: 1.2566791058
Epoch:   900  |  train loss: 1.2988346815
Epoch:  1000  |  train loss: 1.3006029844
Processing class: 72
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 1.1043328285
Epoch:   200  |  train loss: 1.1266352177
Epoch:   300  |  train loss: 1.1456612349
Epoch:   400  |  train loss: 1.1588007689
Epoch:   500  |  train loss: 1.1914421320
Epoch:   600  |  train loss: 1.1793792963
Epoch:   700  |  train loss: 1.1990518808
Epoch:   800  |  train loss: 1.1980264187
Epoch:   900  |  train loss: 1.2161745310
Epoch:  1000  |  train loss: 1.2529949188
Processing class: 73
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 1.1384913921
Epoch:   200  |  train loss: 1.1461547375
Epoch:   300  |  train loss: 1.2065843105
Epoch:   400  |  train loss: 1.2022345781
Epoch:   500  |  train loss: 1.2457652807
Epoch:   600  |  train loss: 1.2501373291
Epoch:   700  |  train loss: 1.2391174793
Epoch:   800  |  train loss: 1.2435497522
Epoch:   900  |  train loss: 1.2556065321
Epoch:  1000  |  train loss: 1.2803438663
Processing class: 74
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 1.1478351116
Epoch:   200  |  train loss: 1.1689317226
Epoch:   300  |  train loss: 1.1486946821
Epoch:   400  |  train loss: 1.1467326164
Epoch:   500  |  train loss: 1.1751031876
Epoch:   600  |  train loss: 1.1740234852
Epoch:   700  |  train loss: 1.2219597101
Epoch:   800  |  train loss: 1.1933959723
Epoch:   900  |  train loss: 1.2049617290
Epoch:  1000  |  train loss: 1.2472355843
Processing class: 75
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 1.1101166010
Epoch:   200  |  train loss: 1.1144794464
Epoch:   300  |  train loss: 1.1655483961
Epoch:   400  |  train loss: 1.1742084026
Epoch:   500  |  train loss: 1.2106492758
Epoch:   600  |  train loss: 1.2460152388
Epoch:   700  |  train loss: 1.1889733076
Epoch:   800  |  train loss: 1.2370438337
Epoch:   900  |  train loss: 1.2405220985
Epoch:  1000  |  train loss: 1.2780983686
Processing class: 76
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 1.0964364767
Epoch:   200  |  train loss: 1.1537534714
Epoch:   300  |  train loss: 1.2283191681
Epoch:   400  |  train loss: 1.2794088602
Epoch:   500  |  train loss: 1.3017181396
Epoch:   600  |  train loss: 1.3394698381
Epoch:   700  |  train loss: 1.3490907192
Epoch:   800  |  train loss: 1.3207892179
Epoch:   900  |  train loss: 1.3707722425
Epoch:  1000  |  train loss: 1.3946264744
Processing class: 77
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 1.1271955252
Epoch:   200  |  train loss: 1.1473840475
Epoch:   300  |  train loss: 1.1493076324
Epoch:   400  |  train loss: 1.1803100348
Epoch:   500  |  train loss: 1.2106080532
Epoch:   600  |  train loss: 1.2300056934
Epoch:   700  |  train loss: 1.2528027296
Epoch:   800  |  train loss: 1.2941073656
Epoch:   900  |  train loss: 1.2878840446
Epoch:  1000  |  train loss: 1.2937378883
Processing class: 78
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 1.1420313597
Epoch:   200  |  train loss: 1.1930884600
Epoch:   300  |  train loss: 1.2375386238
Epoch:   400  |  train loss: 1.2512170792
Epoch:   500  |  train loss: 1.2617391348
Epoch:   600  |  train loss: 1.2977444887
Epoch:   700  |  train loss: 1.3177684307
Epoch:   800  |  train loss: 1.3379604816
Epoch:   900  |  train loss: 1.3495012283
Epoch:  1000  |  train loss: 1.3464394569
Processing class: 79
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 1.1233667850
Epoch:   200  |  train loss: 1.1099657059
Epoch:   300  |  train loss: 1.1479161263
Epoch:   400  |  train loss: 1.1632907629
Epoch:   500  |  train loss: 1.1695983410
Epoch:   600  |  train loss: 1.2137446880
Epoch:   700  |  train loss: 1.2198888540
Epoch:   800  |  train loss: 1.2447627783
Epoch:   900  |  train loss: 1.2790110826
Epoch:  1000  |  train loss: 1.2655513763
2024-03-05 22:43:21,639 [trainer.py] => CNN: {'total': 59.18, '00-09': 73.6, '10-19': 68.4, '20-29': 76.9, '30-39': 69.1, '40-49': 60.6, '50-59': 37.2, '60-69': 45.7, '70-79': 41.9, 'old': 61.64, 'new': 41.9}
2024-03-05 22:43:21,639 [trainer.py] => No NME accuracy
2024-03-05 22:43:21,639 [trainer.py] => FeCAM: {'total': 61.72, '00-09': 78.1, '10-19': 69.3, '20-29': 78.0, '30-39': 69.5, '40-49': 72.1, '50-59': 35.7, '60-69': 49.1, '70-79': 42.0, 'old': 64.54, 'new': 42.0}
2024-03-05 22:43:21,639 [trainer.py] => CNN top1 curve: [83.44, 71.75, 64.66, 59.18]
2024-03-05 22:43:21,639 [trainer.py] => CNN top5 curve: [96.5, 89.6, 86.54, 84.09]
2024-03-05 22:43:21,639 [trainer.py] => FeCAM top1 curve: [82.9, 72.78, 67.27, 61.72]
2024-03-05 22:43:21,639 [trainer.py] => FeCAM top5 curve: [95.84, 91.18, 87.21, 84.09]

2024-03-05 22:43:21,646 [fecam.py] => Learning on 80-90
Processing class: 80
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 1.1239223242
Epoch:   200  |  train loss: 1.2389012337
Epoch:   300  |  train loss: 1.2897494078
Epoch:   400  |  train loss: 1.3197241545
Epoch:   500  |  train loss: 1.3320489168
Epoch:   600  |  train loss: 1.3414304495
Epoch:   700  |  train loss: 1.3224444866
Epoch:   800  |  train loss: 1.3772985935
Epoch:   900  |  train loss: 1.3719867229
Epoch:  1000  |  train loss: 1.4061777115
Processing class: 81
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 1.1155434132
Epoch:   200  |  train loss: 1.1798129082
Epoch:   300  |  train loss: 1.1741944075
Epoch:   400  |  train loss: 1.2344449997
Epoch:   500  |  train loss: 1.2375235081
Epoch:   600  |  train loss: 1.2784443378
Epoch:   700  |  train loss: 1.2863587379
Epoch:   800  |  train loss: 1.3130556107
Epoch:   900  |  train loss: 1.3281523228
Epoch:  1000  |  train loss: 1.3497974873
Processing class: 82
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 1.1115054369
Epoch:   200  |  train loss: 1.1615890265
Epoch:   300  |  train loss: 1.1857089043
Epoch:   400  |  train loss: 1.1992846489
Epoch:   500  |  train loss: 1.2284027576
Epoch:   600  |  train loss: 1.2310970068
Epoch:   700  |  train loss: 1.2441152573
Epoch:   800  |  train loss: 1.2562795162
Epoch:   900  |  train loss: 1.2723785162
Epoch:  1000  |  train loss: 1.2813918114
Processing class: 83
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 1.1393442154
Epoch:   200  |  train loss: 1.2268557787
Epoch:   300  |  train loss: 1.2547201157
Epoch:   400  |  train loss: 1.2950172186
Epoch:   500  |  train loss: 1.3516420841
Epoch:   600  |  train loss: 1.3417734146
Epoch:   700  |  train loss: 1.3902157307
Epoch:   800  |  train loss: 1.4414999008
Epoch:   900  |  train loss: 1.4316058159
Epoch:  1000  |  train loss: 1.4298095942
Processing class: 84
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 1.1400371313
Epoch:   200  |  train loss: 1.1647902012
Epoch:   300  |  train loss: 1.2182442188
Epoch:   400  |  train loss: 1.2401029825
Epoch:   500  |  train loss: 1.2824725866
Epoch:   600  |  train loss: 1.2663907290
Epoch:   700  |  train loss: 1.3061964273
Epoch:   800  |  train loss: 1.3400637388
Epoch:   900  |  train loss: 1.3158136368
Epoch:  1000  |  train loss: 1.3160788536
Processing class: 85
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 1.1223620176
Epoch:   200  |  train loss: 1.1411673307
Epoch:   300  |  train loss: 1.1937655926
Epoch:   400  |  train loss: 1.1990857840
Epoch:   500  |  train loss: 1.2344590664
Epoch:   600  |  train loss: 1.2730600119
Epoch:   700  |  train loss: 1.3192562342
Epoch:   800  |  train loss: 1.2972977877
Epoch:   900  |  train loss: 1.3082649946
Epoch:  1000  |  train loss: 1.3483782053
Processing class: 86
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 1.1451470852
Epoch:   200  |  train loss: 1.1298221111
Epoch:   300  |  train loss: 1.1845407486
Epoch:   400  |  train loss: 1.1895353794
Epoch:   500  |  train loss: 1.1991553307
Epoch:   600  |  train loss: 1.2286376238
Epoch:   700  |  train loss: 1.2394436121
Epoch:   800  |  train loss: 1.2459735155
Epoch:   900  |  train loss: 1.2531467676
Epoch:  1000  |  train loss: 1.3009818077
Processing class: 87
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 1.1086576939
Epoch:   200  |  train loss: 1.1789750099
Epoch:   300  |  train loss: 1.2005327463
Epoch:   400  |  train loss: 1.2445767403
Epoch:   500  |  train loss: 1.2564159632
Epoch:   600  |  train loss: 1.2822263956
Epoch:   700  |  train loss: 1.3030421495
Epoch:   800  |  train loss: 1.3123449802
Epoch:   900  |  train loss: 1.3040757418
Epoch:  1000  |  train loss: 1.3603138208
Processing class: 88
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 1.1159125566
Epoch:   200  |  train loss: 1.1196906090
Epoch:   300  |  train loss: 1.1408690691
Epoch:   400  |  train loss: 1.1664647579
Epoch:   500  |  train loss: 1.1816638708
Epoch:   600  |  train loss: 1.1898806334
Epoch:   700  |  train loss: 1.2277876854
Epoch:   800  |  train loss: 1.2438660145
Epoch:   900  |  train loss: 1.2608253717
Epoch:  1000  |  train loss: 1.2515519619
Processing class: 89
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 1.1359873772
Epoch:   200  |  train loss: 1.1921204090
Epoch:   300  |  train loss: 1.2355135202
Epoch:   400  |  train loss: 1.2580454350
Epoch:   500  |  train loss: 1.3016726494
Epoch:   600  |  train loss: 1.3237215996
Epoch:   700  |  train loss: 1.3051185608
Epoch:   800  |  train loss: 1.3751153469
Epoch:   900  |  train loss: 1.3704344988
Epoch:  1000  |  train loss: 1.3973263264
2024-03-05 22:52:09,457 [trainer.py] => CNN: {'total': 54.08, '00-09': 67.9, '10-19': 63.6, '20-29': 73.2, '30-39': 68.8, '40-49': 57.2, '50-59': 32.6, '60-69': 40.9, '70-79': 38.1, '80-89': 44.4, 'old': 55.29, 'new': 44.4}
2024-03-05 22:52:09,457 [trainer.py] => No NME accuracy
2024-03-05 22:52:09,457 [trainer.py] => FeCAM: {'total': 57.66, '00-09': 76.1, '10-19': 65.8, '20-29': 75.3, '30-39': 67.9, '40-49': 68.5, '50-59': 33.0, '60-69': 45.0, '70-79': 39.0, '80-89': 48.3, 'old': 58.82, 'new': 48.3}
2024-03-05 22:52:09,457 [trainer.py] => CNN top1 curve: [83.44, 71.75, 64.66, 59.18, 54.08]
2024-03-05 22:52:09,457 [trainer.py] => CNN top5 curve: [96.5, 89.6, 86.54, 84.09, 81.74]
2024-03-05 22:52:09,457 [trainer.py] => FeCAM top1 curve: [82.9, 72.78, 67.27, 61.72, 57.66]
2024-03-05 22:52:09,457 [trainer.py] => FeCAM top5 curve: [95.84, 91.18, 87.21, 84.09, 81.63]

2024-03-05 22:52:09,463 [fecam.py] => Learning on 90-100
Processing class: 90
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 1.1333120108
Epoch:   200  |  train loss: 1.1500186443
Epoch:   300  |  train loss: 1.1428593159
Epoch:   400  |  train loss: 1.1563004017
Epoch:   500  |  train loss: 1.1623999119
Epoch:   600  |  train loss: 1.1938779116
Epoch:   700  |  train loss: 1.2039882421
Epoch:   800  |  train loss: 1.2244012833
Epoch:   900  |  train loss: 1.2480262280
Epoch:  1000  |  train loss: 1.2882092476
Processing class: 91
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 1.1247812033
Epoch:   200  |  train loss: 1.1448881865
Epoch:   300  |  train loss: 1.1960099697
Epoch:   400  |  train loss: 1.2328693390
Epoch:   500  |  train loss: 1.2783251524
Epoch:   600  |  train loss: 1.3038218260
Epoch:   700  |  train loss: 1.3297582626
Epoch:   800  |  train loss: 1.3610500097
Epoch:   900  |  train loss: 1.3953298807
Epoch:  1000  |  train loss: 1.4072210312
Processing class: 92
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 1.1627356529
Epoch:   200  |  train loss: 1.1709303856
Epoch:   300  |  train loss: 1.1500755787
Epoch:   400  |  train loss: 1.1845241308
Epoch:   500  |  train loss: 1.2187936068
Epoch:   600  |  train loss: 1.2726797819
Epoch:   700  |  train loss: 1.2866477251
Epoch:   800  |  train loss: 1.2618036509
Epoch:   900  |  train loss: 1.2815872192
Epoch:  1000  |  train loss: 1.2979935408
Processing class: 93
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 1.1225520611
Epoch:   200  |  train loss: 1.1735794067
Epoch:   300  |  train loss: 1.2665907860
Epoch:   400  |  train loss: 1.2910004139
Epoch:   500  |  train loss: 1.3044268370
Epoch:   600  |  train loss: 1.3012527704
Epoch:   700  |  train loss: 1.3710805416
Epoch:   800  |  train loss: 1.3798269272
Epoch:   900  |  train loss: 1.3965531826
Epoch:  1000  |  train loss: 1.4003144979
Processing class: 94
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 1.0463731050
Epoch:   200  |  train loss: 1.0614954472
Epoch:   300  |  train loss: 1.0552934647
Epoch:   400  |  train loss: 1.0948165417
Epoch:   500  |  train loss: 1.0601404190
Epoch:   600  |  train loss: 1.0978874683
Epoch:   700  |  train loss: 1.1333424568
Epoch:   800  |  train loss: 1.0848611116
Epoch:   900  |  train loss: 1.1332338572
Epoch:  1000  |  train loss: 1.1281084061
Processing class: 95
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 1.1062909365
Epoch:   200  |  train loss: 1.1444845438
Epoch:   300  |  train loss: 1.1390599728
Epoch:   400  |  train loss: 1.1444940329
Epoch:   500  |  train loss: 1.2030893087
Epoch:   600  |  train loss: 1.2100811481
Epoch:   700  |  train loss: 1.2380203724
Epoch:   800  |  train loss: 1.2345987797
Epoch:   900  |  train loss: 1.2715123892
Epoch:  1000  |  train loss: 1.3079883337
Processing class: 96
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 1.1412477732
Epoch:   200  |  train loss: 1.1168575048
Epoch:   300  |  train loss: 1.1656752348
Epoch:   400  |  train loss: 1.2282194614
Epoch:   500  |  train loss: 1.2271691799
Epoch:   600  |  train loss: 1.2318774939
Epoch:   700  |  train loss: 1.2518619061
Epoch:   800  |  train loss: 1.2438970804
Epoch:   900  |  train loss: 1.3064523220
Epoch:  1000  |  train loss: 1.3161443710
Processing class: 97
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 1.1411268950
Epoch:   200  |  train loss: 1.1768218756
Epoch:   300  |  train loss: 1.2125316620
Epoch:   400  |  train loss: 1.2395526409
Epoch:   500  |  train loss: 1.2888299227
Epoch:   600  |  train loss: 1.3231095076
Epoch:   700  |  train loss: 1.3551609755
Epoch:   800  |  train loss: 1.3765333652
Epoch:   900  |  train loss: 1.3984648705
Epoch:  1000  |  train loss: 1.4019863844
Processing class: 98
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 1.1196716309
Epoch:   200  |  train loss: 1.1414572716
Epoch:   300  |  train loss: 1.1798925400
Epoch:   400  |  train loss: 1.2174142599
Epoch:   500  |  train loss: 1.2329213142
Epoch:   600  |  train loss: 1.2669489622
Epoch:   700  |  train loss: 1.2481140614
Epoch:   800  |  train loss: 1.2873262644
Epoch:   900  |  train loss: 1.2962764978
Epoch:  1000  |  train loss: 1.2887872219
Processing class: 99
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 1.1620033264
Epoch:   200  |  train loss: 1.1530920267
Epoch:   300  |  train loss: 1.2337477446
Epoch:   400  |  train loss: 1.2888173819
Epoch:   500  |  train loss: 1.3047348976
Epoch:   600  |  train loss: 1.3161281586
Epoch:   700  |  train loss: 1.3162703037
Epoch:   800  |  train loss: 1.3585725546
Epoch:   900  |  train loss: 1.3585658073
Epoch:  1000  |  train loss: 1.3723340273
2024-03-05 23:02:18,981 [trainer.py] => CNN: {'total': 50.36, '00-09': 57.7, '10-19': 63.6, '20-29': 71.4, '30-39': 68.5, '40-49': 56.8, '50-59': 29.6, '60-69': 38.9, '70-79': 36.6, '80-89': 42.7, '90-99': 37.8, 'old': 51.76, 'new': 37.8}
2024-03-05 23:02:18,983 [trainer.py] => No NME accuracy
2024-03-05 23:02:18,983 [trainer.py] => FeCAM: {'total': 54.74, '00-09': 73.3, '10-19': 65.1, '20-29': 74.1, '30-39': 67.2, '40-49': 67.1, '50-59': 30.8, '60-69': 43.3, '70-79': 37.2, '80-89': 46.9, '90-99': 42.4, 'old': 56.11, 'new': 42.4}
2024-03-05 23:02:18,983 [trainer.py] => CNN top1 curve: [83.44, 71.75, 64.66, 59.18, 54.08, 50.36]
2024-03-05 23:02:18,983 [trainer.py] => CNN top5 curve: [96.5, 89.6, 86.54, 84.09, 81.74, 79.63]
2024-03-05 23:02:18,983 [trainer.py] => FeCAM top1 curve: [82.9, 72.78, 67.27, 61.72, 57.66, 54.74]
2024-03-05 23:02:18,983 [trainer.py] => FeCAM top5 curve: [95.84, 91.18, 87.21, 84.09, 81.63, 79.21]

=========================================
2024-03-05 23:02:27,462 [trainer.py] => config: ./exps/FeCAM_cifar100.json
2024-03-05 23:02:27,462 [trainer.py] => prefix: train
2024-03-05 23:02:27,462 [trainer.py] => dataset: cifar100
2024-03-05 23:02:27,462 [trainer.py] => memory_size: 0
2024-03-05 23:02:27,463 [trainer.py] => shuffle: True
2024-03-05 23:02:27,463 [trainer.py] => init_cls: 50
2024-03-05 23:02:27,463 [trainer.py] => increment: 10
2024-03-05 23:02:27,463 [trainer.py] => model_name: fecam
2024-03-05 23:02:27,463 [trainer.py] => convnet_type: resnet18
2024-03-05 23:02:27,463 [trainer.py] => device: [device(type='cuda', index=0)]
2024-03-05 23:02:27,463 [trainer.py] => seed: 1993
2024-03-05 23:02:27,463 [trainer.py] => init_epochs: 200
2024-03-05 23:02:27,463 [trainer.py] => init_lr: 0.1
2024-03-05 23:02:27,463 [trainer.py] => init_weight_decay: 0.0005
2024-03-05 23:02:27,463 [trainer.py] => batch_size: 128
2024-03-05 23:02:27,463 [trainer.py] => num_workers: 8
2024-03-05 23:02:27,463 [trainer.py] => T: 5
2024-03-05 23:02:27,463 [trainer.py] => beta: 0.5
2024-03-05 23:02:27,463 [trainer.py] => alpha1: 1
2024-03-05 23:02:27,463 [trainer.py] => alpha2: 1
2024-03-05 23:02:27,463 [trainer.py] => ncm: False
2024-03-05 23:02:27,463 [trainer.py] => tukey: False
2024-03-05 23:02:27,463 [trainer.py] => diagonal: False
2024-03-05 23:02:27,463 [trainer.py] => per_class: True
2024-03-05 23:02:27,463 [trainer.py] => full_cov: True
2024-03-05 23:02:27,463 [trainer.py] => shrink: True
2024-03-05 23:02:27,463 [trainer.py] => norm_cov: False
2024-03-05 23:02:27,463 [trainer.py] => vecnorm: False
2024-03-05 23:02:27,463 [trainer.py] => ae_type: wae
2024-03-05 23:02:27,463 [trainer.py] => epochs: 1000
2024-03-05 23:02:27,464 [trainer.py] => ae_latent_dim: 32
2024-03-05 23:02:27,464 [trainer.py] => wae_sigma: 1
2024-03-05 23:02:27,464 [trainer.py] => wae_C: 10
Files already downloaded and verified
Files already downloaded and verified
2024-03-05 23:02:29,119 [data_manager.py] => [68, 56, 78, 8, 23, 84, 90, 65, 74, 76, 40, 89, 3, 92, 55, 9, 26, 80, 43, 38, 58, 70, 77, 1, 85, 19, 17, 50, 28, 53, 13, 81, 45, 82, 6, 59, 83, 16, 15, 44, 91, 41, 72, 60, 79, 52, 20, 10, 31, 54, 37, 95, 14, 71, 96, 98, 97, 2, 64, 66, 42, 22, 35, 86, 24, 34, 87, 21, 99, 0, 88, 27, 18, 94, 11, 12, 47, 25, 30, 46, 62, 69, 36, 61, 7, 63, 75, 5, 32, 4, 51, 48, 73, 93, 39, 67, 29, 49, 57, 33]
2024-03-05 23:02:29,403 [fecam.py] => Learning on 0-50
/home/z1165703/miniconda3/lib/python3.11/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
/home/z1165703/miniconda3/lib/python3.11/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
Processing class: 0
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 4.4320830345
Epoch:   200  |  train loss: 3.7027341843
Epoch:   300  |  train loss: 3.5828209877
Epoch:   400  |  train loss: 3.2748935699
Epoch:   500  |  train loss: 3.1388166904
Epoch:   600  |  train loss: 3.0799358368
Epoch:   700  |  train loss: 2.9845492840
Epoch:   800  |  train loss: 2.9503428936
Epoch:   900  |  train loss: 2.8743020535
Epoch:  1000  |  train loss: 2.8063605309
Processing class: 1
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 5.2438733101
Epoch:   200  |  train loss: 5.1269046783
Epoch:   300  |  train loss: 4.4599705696
Epoch:   400  |  train loss: 4.1440565109
Epoch:   500  |  train loss: 3.8565827847
Epoch:   600  |  train loss: 3.7401826382
Epoch:   700  |  train loss: 3.5914281368
Epoch:   800  |  train loss: 3.4406440258
Epoch:   900  |  train loss: 3.3418246269
Epoch:  1000  |  train loss: 3.2374001980
Processing class: 2
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 5.7230956078
Epoch:   200  |  train loss: 5.0675019264
Epoch:   300  |  train loss: 4.3856784821
Epoch:   400  |  train loss: 3.9952898979
Epoch:   500  |  train loss: 3.6887434006
Epoch:   600  |  train loss: 3.4011417866
Epoch:   700  |  train loss: 3.2098055363
Epoch:   800  |  train loss: 3.0682912827
Epoch:   900  |  train loss: 2.9258071423
Epoch:  1000  |  train loss: 2.7994408131
Processing class: 3
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 4.3161243439
Epoch:   200  |  train loss: 4.3190487862
Epoch:   300  |  train loss: 3.9079114914
Epoch:   400  |  train loss: 3.4085356236
Epoch:   500  |  train loss: 3.1777324677
Epoch:   600  |  train loss: 3.0638240814
Epoch:   700  |  train loss: 2.8239233971
Epoch:   800  |  train loss: 2.7174728394
Epoch:   900  |  train loss: 2.5600230694
Epoch:  1000  |  train loss: 2.4289295197
Processing class: 4
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 5.2450665474
Epoch:   200  |  train loss: 5.0917082787
Epoch:   300  |  train loss: 4.3194539070
Epoch:   400  |  train loss: 3.9642322540
Epoch:   500  |  train loss: 3.6783138275
Epoch:   600  |  train loss: 3.3992671967
Epoch:   700  |  train loss: 3.2843250751
Epoch:   800  |  train loss: 3.1877466202
Epoch:   900  |  train loss: 3.0749304771
Epoch:  1000  |  train loss: 2.9558275700
Processing class: 5
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 5.8501029015
Epoch:   200  |  train loss: 4.8931110382
Epoch:   300  |  train loss: 4.5067125320
Epoch:   400  |  train loss: 3.9360986710
Epoch:   500  |  train loss: 3.7658886909
Epoch:   600  |  train loss: 3.5516588211
Epoch:   700  |  train loss: 3.2761995316
Epoch:   800  |  train loss: 3.1162709236
Epoch:   900  |  train loss: 2.9800676823
Epoch:  1000  |  train loss: 2.8753571987
Processing class: 6
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 5.1472914696
Epoch:   200  |  train loss: 5.1160737991
Epoch:   300  |  train loss: 4.4340273857
Epoch:   400  |  train loss: 3.9434557438
Epoch:   500  |  train loss: 3.7195164204
Epoch:   600  |  train loss: 3.6127763748
Epoch:   700  |  train loss: 3.5021521091
Epoch:   800  |  train loss: 3.3307810307
Epoch:   900  |  train loss: 3.1451573849
Epoch:  1000  |  train loss: 3.0450001717
Processing class: 7
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 5.8063831329
Epoch:   200  |  train loss: 5.0847122192
Epoch:   300  |  train loss: 4.7402544975
Epoch:   400  |  train loss: 4.1893482208
Epoch:   500  |  train loss: 3.8311312199
Epoch:   600  |  train loss: 3.6404136181
Epoch:   700  |  train loss: 3.4504322052
Epoch:   800  |  train loss: 3.3146336079
Epoch:   900  |  train loss: 3.1628087044
Epoch:  1000  |  train loss: 3.0465173721
Processing class: 8
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 5.4825709343
Epoch:   200  |  train loss: 4.9681147575
Epoch:   300  |  train loss: 4.1571778297
Epoch:   400  |  train loss: 3.8273973465
Epoch:   500  |  train loss: 3.6113582134
Epoch:   600  |  train loss: 3.3488153934
Epoch:   700  |  train loss: 3.1899951458
Epoch:   800  |  train loss: 3.0654312611
Epoch:   900  |  train loss: 2.9520161152
Epoch:  1000  |  train loss: 2.8243933678
Processing class: 9
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 5.0867442131
Epoch:   200  |  train loss: 5.0282271385
Epoch:   300  |  train loss: 4.6430975914
Epoch:   400  |  train loss: 4.2785377502
Epoch:   500  |  train loss: 4.0830011368
Epoch:   600  |  train loss: 3.9706233978
Epoch:   700  |  train loss: 3.8648056030
Epoch:   800  |  train loss: 3.7596908569
Epoch:   900  |  train loss: 3.6621797085
Epoch:  1000  |  train loss: 3.5516587257
Processing class: 10
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 5.8266787529
Epoch:   200  |  train loss: 5.1498559952
Epoch:   300  |  train loss: 4.7924312592
Epoch:   400  |  train loss: 4.4264213562
Epoch:   500  |  train loss: 4.1220061302
Epoch:   600  |  train loss: 3.9051223278
Epoch:   700  |  train loss: 3.7097017288
Epoch:   800  |  train loss: 3.5765245438
Epoch:   900  |  train loss: 3.4593698502
Epoch:  1000  |  train loss: 3.3710327625
Processing class: 11
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 5.8834778786
Epoch:   200  |  train loss: 4.9012084007
Epoch:   300  |  train loss: 4.5889467239
Epoch:   400  |  train loss: 4.1619988441
Epoch:   500  |  train loss: 3.8254085541
Epoch:   600  |  train loss: 3.6497019768
Epoch:   700  |  train loss: 3.4738615990
Epoch:   800  |  train loss: 3.3221691132
Epoch:   900  |  train loss: 3.1894047737
Epoch:  1000  |  train loss: 3.0614053726
Processing class: 12
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 5.4979320526
Epoch:   200  |  train loss: 5.0408717155
Epoch:   300  |  train loss: 4.5267421722
Epoch:   400  |  train loss: 4.0308242321
Epoch:   500  |  train loss: 3.7426069260
Epoch:   600  |  train loss: 3.4730110168
Epoch:   700  |  train loss: 3.2941114902
Epoch:   800  |  train loss: 3.1898459911
Epoch:   900  |  train loss: 3.0696866035
Epoch:  1000  |  train loss: 2.9717728615
Processing class: 13
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 5.2543048859
Epoch:   200  |  train loss: 4.5476376534
Epoch:   300  |  train loss: 4.0233294487
Epoch:   400  |  train loss: 3.6290264130
Epoch:   500  |  train loss: 3.3700944424
Epoch:   600  |  train loss: 3.2499357224
Epoch:   700  |  train loss: 3.1060729504
Epoch:   800  |  train loss: 3.0099392891
Epoch:   900  |  train loss: 2.9234584332
Epoch:  1000  |  train loss: 2.8249085426
Processing class: 14
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 6.2519679070
Epoch:   200  |  train loss: 6.1639146805
Epoch:   300  |  train loss: 5.4692560196
Epoch:   400  |  train loss: 4.8597135544
Epoch:   500  |  train loss: 4.5842244148
Epoch:   600  |  train loss: 4.3175709724
Epoch:   700  |  train loss: 4.1405530453
Epoch:   800  |  train loss: 3.9917507172
Epoch:   900  |  train loss: 3.8863142014
Epoch:  1000  |  train loss: 3.7912011623
Processing class: 15
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 4.9745074272
Epoch:   200  |  train loss: 4.5983169556
Epoch:   300  |  train loss: 4.0201950550
Epoch:   400  |  train loss: 3.7389551163
Epoch:   500  |  train loss: 3.5088207722
Epoch:   600  |  train loss: 3.3850026131
Epoch:   700  |  train loss: 3.2900763035
Epoch:   800  |  train loss: 3.1658855438
Epoch:   900  |  train loss: 3.1163885593
Epoch:  1000  |  train loss: 3.0219569206
Processing class: 16
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 5.3064126968
Epoch:   200  |  train loss: 4.4002372742
Epoch:   300  |  train loss: 4.0431939125
Epoch:   400  |  train loss: 3.7912103176
Epoch:   500  |  train loss: 3.6948797226
Epoch:   600  |  train loss: 3.5271784306
Epoch:   700  |  train loss: 3.3568901539
Epoch:   800  |  train loss: 3.2354081631
Epoch:   900  |  train loss: 3.1289646626
Epoch:  1000  |  train loss: 3.0831178188
Processing class: 17
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 5.2891975403
Epoch:   200  |  train loss: 5.2018509865
Epoch:   300  |  train loss: 4.9688335419
Epoch:   400  |  train loss: 4.4584370613
Epoch:   500  |  train loss: 3.9805907726
Epoch:   600  |  train loss: 3.7606000423
Epoch:   700  |  train loss: 3.6149579525
Epoch:   800  |  train loss: 3.4841584206
Epoch:   900  |  train loss: 3.3653405666
Epoch:  1000  |  train loss: 3.2719331741
Processing class: 18
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 5.1833469391
Epoch:   200  |  train loss: 5.0910402298
Epoch:   300  |  train loss: 3.9219083786
Epoch:   400  |  train loss: 3.5679521561
Epoch:   500  |  train loss: 3.1898492336
Epoch:   600  |  train loss: 2.9545421124
Epoch:   700  |  train loss: 2.8284992695
Epoch:   800  |  train loss: 2.6908586025
Epoch:   900  |  train loss: 2.5905759811
Epoch:  1000  |  train loss: 2.5046705246
Processing class: 19
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 4.7572249413
Epoch:   200  |  train loss: 4.2849236488
Epoch:   300  |  train loss: 3.8185375690
Epoch:   400  |  train loss: 3.7110447884
Epoch:   500  |  train loss: 3.3730594635
Epoch:   600  |  train loss: 3.2210718632
Epoch:   700  |  train loss: 3.0970248699
Epoch:   800  |  train loss: 2.9520315647
Epoch:   900  |  train loss: 2.8297502995
Epoch:  1000  |  train loss: 2.7328811169
Processing class: 20
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 5.1673340797
Epoch:   200  |  train loss: 4.4873468399
Epoch:   300  |  train loss: 4.3234955788
Epoch:   400  |  train loss: 3.8350495338
Epoch:   500  |  train loss: 3.7226558208
Epoch:   600  |  train loss: 3.5165880680
Epoch:   700  |  train loss: 3.3296685219
Epoch:   800  |  train loss: 3.2600243568
Epoch:   900  |  train loss: 3.1573376179
Epoch:  1000  |  train loss: 3.0953369141
Processing class: 21
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 5.2970954895
Epoch:   200  |  train loss: 4.5511971474
Epoch:   300  |  train loss: 3.6306236267
Epoch:   400  |  train loss: 3.2667015076
Epoch:   500  |  train loss: 3.0420848846
Epoch:   600  |  train loss: 2.9064207554
Epoch:   700  |  train loss: 2.7975822926
Epoch:   800  |  train loss: 2.6987174034
Epoch:   900  |  train loss: 2.5908421516
Epoch:  1000  |  train loss: 2.4829272270
Processing class: 22
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 5.7315372467
Epoch:   200  |  train loss: 5.6292281151
Epoch:   300  |  train loss: 5.1712740898
Epoch:   400  |  train loss: 4.4231897354
Epoch:   500  |  train loss: 4.1579246998
Epoch:   600  |  train loss: 3.8732894897
Epoch:   700  |  train loss: 3.6312283993
Epoch:   800  |  train loss: 3.4549907684
Epoch:   900  |  train loss: 3.3256520271
Epoch:  1000  |  train loss: 3.2292471409
Processing class: 23
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 4.9317180634
Epoch:   200  |  train loss: 4.8369582176
Epoch:   300  |  train loss: 4.2827613831
Epoch:   400  |  train loss: 3.9115377426
Epoch:   500  |  train loss: 3.7254779816
Epoch:   600  |  train loss: 3.4317484379
Epoch:   700  |  train loss: 3.2462409496
Epoch:   800  |  train loss: 3.0671672344
Epoch:   900  |  train loss: 2.9393582821
Epoch:  1000  |  train loss: 2.8077933311
Processing class: 24
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 5.1969045639
Epoch:   200  |  train loss: 5.1701558113
Epoch:   300  |  train loss: 5.1060038567
Epoch:   400  |  train loss: 4.3944737434
Epoch:   500  |  train loss: 4.1470151901
Epoch:   600  |  train loss: 3.9332884312
Epoch:   700  |  train loss: 3.7498729229
Epoch:   800  |  train loss: 3.6160426617
Epoch:   900  |  train loss: 3.4684800625
Epoch:  1000  |  train loss: 3.3427556992
Processing class: 25
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 5.4792709351
Epoch:   200  |  train loss: 4.9351354599
Epoch:   300  |  train loss: 4.5693700790
Epoch:   400  |  train loss: 4.0242285252
Epoch:   500  |  train loss: 3.6976249695
Epoch:   600  |  train loss: 3.5161722660
Epoch:   700  |  train loss: 3.3725941181
Epoch:   800  |  train loss: 3.2329370499
Epoch:   900  |  train loss: 3.1034177780
Epoch:  1000  |  train loss: 3.0365586281
Processing class: 26
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 4.8806522369
Epoch:   200  |  train loss: 4.8414864540
Epoch:   300  |  train loss: 4.9055950165
Epoch:   400  |  train loss: 4.4365315437
Epoch:   500  |  train loss: 4.0561361313
Epoch:   600  |  train loss: 3.6488890648
Epoch:   700  |  train loss: 3.5178836346
Epoch:   800  |  train loss: 3.3848235130
Epoch:   900  |  train loss: 3.2353876591
Epoch:  1000  |  train loss: 3.1365334988
Processing class: 27
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 5.7217283249
Epoch:   200  |  train loss: 4.9930472374
Epoch:   300  |  train loss: 4.7372015953
Epoch:   400  |  train loss: 4.1966775417
Epoch:   500  |  train loss: 3.7954462051
Epoch:   600  |  train loss: 3.6488966465
Epoch:   700  |  train loss: 3.5509459019
Epoch:   800  |  train loss: 3.4179422379
Epoch:   900  |  train loss: 3.2480826378
Epoch:  1000  |  train loss: 3.0840368748
Processing class: 28
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 5.2773998260
Epoch:   200  |  train loss: 5.1634372711
Epoch:   300  |  train loss: 4.5323394775
Epoch:   400  |  train loss: 4.1020231724
Epoch:   500  |  train loss: 3.7676225185
Epoch:   600  |  train loss: 3.6528261185
Epoch:   700  |  train loss: 3.5414081097
Epoch:   800  |  train loss: 3.4114886761
Epoch:   900  |  train loss: 3.3333354473
Epoch:  1000  |  train loss: 3.2307020664
Processing class: 29
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 5.3277466774
Epoch:   200  |  train loss: 4.5997051239
Epoch:   300  |  train loss: 4.1151962280
Epoch:   400  |  train loss: 3.6775256634
Epoch:   500  |  train loss: 3.4615754604
Epoch:   600  |  train loss: 3.3139141083
Epoch:   700  |  train loss: 3.1116538048
Epoch:   800  |  train loss: 2.9343005180
Epoch:   900  |  train loss: 2.8298144817
Epoch:  1000  |  train loss: 2.7494101524
Processing class: 30
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 4.7976521492
Epoch:   200  |  train loss: 4.6856032372
Epoch:   300  |  train loss: 3.8369133949
Epoch:   400  |  train loss: 3.6420490742
Epoch:   500  |  train loss: 3.5865843296
Epoch:   600  |  train loss: 3.4196629047
Epoch:   700  |  train loss: 3.2227824211
Epoch:   800  |  train loss: 3.0952504635
Epoch:   900  |  train loss: 2.9917824745
Epoch:  1000  |  train loss: 2.9262725830
Processing class: 31
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 5.4049391747
Epoch:   200  |  train loss: 4.7214011192
Epoch:   300  |  train loss: 4.2334879875
Epoch:   400  |  train loss: 4.1264692307
Epoch:   500  |  train loss: 3.8227421761
Epoch:   600  |  train loss: 3.6736587048
Epoch:   700  |  train loss: 3.5287145615
Epoch:   800  |  train loss: 3.4306756973
Epoch:   900  |  train loss: 3.2820274830
Epoch:  1000  |  train loss: 3.1732079983
Processing class: 32
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 5.9977498055
Epoch:   200  |  train loss: 5.6812911987
Epoch:   300  |  train loss: 5.1725894928
Epoch:   400  |  train loss: 4.6957427979
Epoch:   500  |  train loss: 4.2398791313
Epoch:   600  |  train loss: 3.9104969501
Epoch:   700  |  train loss: 3.6700964451
Epoch:   800  |  train loss: 3.5338214874
Epoch:   900  |  train loss: 3.4056053162
Epoch:  1000  |  train loss: 3.3011654854
Processing class: 33
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 5.5045135498
Epoch:   200  |  train loss: 5.0148863792
Epoch:   300  |  train loss: 4.3142521858
Epoch:   400  |  train loss: 3.8904290199
Epoch:   500  |  train loss: 3.6674988747
Epoch:   600  |  train loss: 3.4152348518
Epoch:   700  |  train loss: 3.2806878567
Epoch:   800  |  train loss: 3.1623198032
Epoch:   900  |  train loss: 3.0443199158
Epoch:  1000  |  train loss: 2.9193826199
Processing class: 34
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 5.6431383133
Epoch:   200  |  train loss: 5.4299923897
Epoch:   300  |  train loss: 4.5295147896
Epoch:   400  |  train loss: 4.2506288528
Epoch:   500  |  train loss: 3.8943922997
Epoch:   600  |  train loss: 3.6169118881
Epoch:   700  |  train loss: 3.4350642204
Epoch:   800  |  train loss: 3.2867239475
Epoch:   900  |  train loss: 3.1557469368
Epoch:  1000  |  train loss: 3.0432520866
Processing class: 35
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 6.6621582985
Epoch:   200  |  train loss: 5.4419201851
Epoch:   300  |  train loss: 4.9590945244
Epoch:   400  |  train loss: 4.7227672577
Epoch:   500  |  train loss: 4.3841814995
Epoch:   600  |  train loss: 4.2612740040
Epoch:   700  |  train loss: 4.0621240616
Epoch:   800  |  train loss: 3.9002252579
Epoch:   900  |  train loss: 3.7604890823
Epoch:  1000  |  train loss: 3.6343288898
Processing class: 36
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 4.7073904991
Epoch:   200  |  train loss: 4.3124994278
Epoch:   300  |  train loss: 3.9128679752
Epoch:   400  |  train loss: 3.7633695126
Epoch:   500  |  train loss: 3.5113630295
Epoch:   600  |  train loss: 3.2587880135
Epoch:   700  |  train loss: 3.0743960857
Epoch:   800  |  train loss: 2.9386142731
Epoch:   900  |  train loss: 2.8145102978
Epoch:  1000  |  train loss: 2.7122946262
Processing class: 37
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 5.8991154671
Epoch:   200  |  train loss: 5.5304646492
Epoch:   300  |  train loss: 4.9037834167
Epoch:   400  |  train loss: 4.4196065903
Epoch:   500  |  train loss: 4.1350965500
Epoch:   600  |  train loss: 3.9305880547
Epoch:   700  |  train loss: 3.7647644043
Epoch:   800  |  train loss: 3.6419762135
Epoch:   900  |  train loss: 3.4786797523
Epoch:  1000  |  train loss: 3.3664301872
Processing class: 38
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 4.9292463303
Epoch:   200  |  train loss: 4.7942000389
Epoch:   300  |  train loss: 4.4799791336
Epoch:   400  |  train loss: 4.2436009407
Epoch:   500  |  train loss: 3.8897015572
Epoch:   600  |  train loss: 3.6303928852
Epoch:   700  |  train loss: 3.3800907135
Epoch:   800  |  train loss: 3.2867668629
Epoch:   900  |  train loss: 3.1974225521
Epoch:  1000  |  train loss: 3.1251161575
Processing class: 39
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 5.4182684898
Epoch:   200  |  train loss: 5.0959025383
Epoch:   300  |  train loss: 4.6829874992
Epoch:   400  |  train loss: 4.3110678673
Epoch:   500  |  train loss: 3.9379292011
Epoch:   600  |  train loss: 3.7487069130
Epoch:   700  |  train loss: 3.5618011951
Epoch:   800  |  train loss: 3.3988960743
Epoch:   900  |  train loss: 3.2467454910
Epoch:  1000  |  train loss: 3.1177824020
Processing class: 40
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 5.1629350662
Epoch:   200  |  train loss: 4.5380619049
Epoch:   300  |  train loss: 4.3102179527
Epoch:   400  |  train loss: 4.0211622715
Epoch:   500  |  train loss: 3.8350011826
Epoch:   600  |  train loss: 3.5100243092
Epoch:   700  |  train loss: 3.4003657341
Epoch:   800  |  train loss: 3.2722413063
Epoch:   900  |  train loss: 3.1754848480
Epoch:  1000  |  train loss: 3.0636511326
Processing class: 41
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 4.6498584747
Epoch:   200  |  train loss: 4.6762324333
Epoch:   300  |  train loss: 3.9244870663
Epoch:   400  |  train loss: 3.5620729923
Epoch:   500  |  train loss: 3.3770259380
Epoch:   600  |  train loss: 3.2550337315
Epoch:   700  |  train loss: 3.1282297134
Epoch:   800  |  train loss: 3.0294786453
Epoch:   900  |  train loss: 2.9391313553
Epoch:  1000  |  train loss: 2.8685504913
Processing class: 42
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 5.6737702370
Epoch:   200  |  train loss: 4.9926930428
Epoch:   300  |  train loss: 4.4563746452
Epoch:   400  |  train loss: 4.0509923935
Epoch:   500  |  train loss: 3.7402284622
Epoch:   600  |  train loss: 3.4515828133
Epoch:   700  |  train loss: 3.2814066410
Epoch:   800  |  train loss: 3.1186501503
Epoch:   900  |  train loss: 2.9770639896
Epoch:  1000  |  train loss: 2.8579008102
Processing class: 43
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 5.9148878098
Epoch:   200  |  train loss: 5.3180042267
Epoch:   300  |  train loss: 4.6020555496
Epoch:   400  |  train loss: 4.3987217903
Epoch:   500  |  train loss: 4.2283765793
Epoch:   600  |  train loss: 4.1609573364
Epoch:   700  |  train loss: 4.0847383022
Epoch:   800  |  train loss: 3.9985799789
Epoch:   900  |  train loss: 3.8881206036
Epoch:  1000  |  train loss: 3.7803439140
Processing class: 44
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 6.0340808868
Epoch:   200  |  train loss: 5.1420681953
Epoch:   300  |  train loss: 4.8688127518
Epoch:   400  |  train loss: 4.3901533127
Epoch:   500  |  train loss: 4.1771209240
Epoch:   600  |  train loss: 4.0058687210
Epoch:   700  |  train loss: 3.8507227898
Epoch:   800  |  train loss: 3.7022980213
Epoch:   900  |  train loss: 3.5385806084
Epoch:  1000  |  train loss: 3.4039566517
Processing class: 45
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 5.2065962791
Epoch:   200  |  train loss: 4.6035428047
Epoch:   300  |  train loss: 4.1647470951
Epoch:   400  |  train loss: 4.1937654972
Epoch:   500  |  train loss: 4.1156694889
Epoch:   600  |  train loss: 3.9019712925
Epoch:   700  |  train loss: 3.7308580399
Epoch:   800  |  train loss: 3.6455518246
Epoch:   900  |  train loss: 3.5948238373
Epoch:  1000  |  train loss: 3.5255114079
Processing class: 46
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 4.7923067093
Epoch:   200  |  train loss: 4.6166226387
Epoch:   300  |  train loss: 4.0847781181
Epoch:   400  |  train loss: 4.0188204765
Epoch:   500  |  train loss: 3.8831660748
Epoch:   600  |  train loss: 3.7826056480
Epoch:   700  |  train loss: 3.6286655903
Epoch:   800  |  train loss: 3.5191637039
Epoch:   900  |  train loss: 3.4028207302
Epoch:  1000  |  train loss: 3.3077544212
Processing class: 47
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 5.7800120354
Epoch:   200  |  train loss: 5.1665677071
Epoch:   300  |  train loss: 4.3864182472
Epoch:   400  |  train loss: 4.0466641903
Epoch:   500  |  train loss: 3.8136418819
Epoch:   600  |  train loss: 3.5638360500
Epoch:   700  |  train loss: 3.3633027077
Epoch:   800  |  train loss: 3.2243020058
Epoch:   900  |  train loss: 3.1160265446
Epoch:  1000  |  train loss: 3.0330307961
Processing class: 48
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 5.6197727203
Epoch:   200  |  train loss: 5.3849599838
Epoch:   300  |  train loss: 4.3256204605
Epoch:   400  |  train loss: 4.0093010426
Epoch:   500  |  train loss: 3.9303192139
Epoch:   600  |  train loss: 3.6739662170
Epoch:   700  |  train loss: 3.5615563869
Epoch:   800  |  train loss: 3.4788618565
Epoch:   900  |  train loss: 3.3888537407
Epoch:  1000  |  train loss: 3.2892351627
Processing class: 49
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 5.8191978455
Epoch:   200  |  train loss: 5.2549057007
Epoch:   300  |  train loss: 4.6212817192
Epoch:   400  |  train loss: 4.3074794769
Epoch:   500  |  train loss: 3.9340240002
Epoch:   600  |  train loss: 3.7521031857
Epoch:   700  |  train loss: 3.5430918217
Epoch:   800  |  train loss: 3.4380019665
Epoch:   900  |  train loss: 3.3242167950
Epoch:  1000  |  train loss: 3.2040633678
/home/z1165703/FeCAM/models/base.py:87: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  vec = torch.tensor(vec).cuda()
2024-03-05 23:20:12,016 [trainer.py] => CNN: {'total': 83.44, '00-09': 87.7, '10-19': 79.6, '20-29': 84.9, '30-39': 81.0, '40-49': 84.0, 'old': 0, 'new': 83.44}
2024-03-05 23:20:12,017 [trainer.py] => No NME accuracy
2024-03-05 23:20:12,018 [trainer.py] => FeCAM: {'total': 82.9, '00-09': 86.2, '10-19': 79.7, '20-29': 85.0, '30-39': 80.6, '40-49': 83.0, 'old': 0, 'new': 82.9}
2024-03-05 23:20:12,018 [trainer.py] => CNN top1 curve: [83.44]
2024-03-05 23:20:12,018 [trainer.py] => CNN top5 curve: [96.5]
2024-03-05 23:20:12,018 [trainer.py] => FeCAM top1 curve: [82.9]
2024-03-05 23:20:12,018 [trainer.py] => FeCAM top5 curve: [95.84]

2024-03-05 23:20:12,030 [fecam.py] => Learning on 50-60
Processing class: 50
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 7.6472207069
Epoch:   200  |  train loss: 6.5024131775
Epoch:   300  |  train loss: 5.9219153404
Epoch:   400  |  train loss: 5.3652053833
Epoch:   500  |  train loss: 4.8545426369
Epoch:   600  |  train loss: 4.5118473053
Epoch:   700  |  train loss: 4.2467041016
Epoch:   800  |  train loss: 4.0347513199
Epoch:   900  |  train loss: 3.8802458286
Epoch:  1000  |  train loss: 3.7499324799
Processing class: 51
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 7.7294751167
Epoch:   200  |  train loss: 6.6050675392
Epoch:   300  |  train loss: 5.8819897652
Epoch:   400  |  train loss: 5.4703389168
Epoch:   500  |  train loss: 4.9000227928
Epoch:   600  |  train loss: 4.4753640175
Epoch:   700  |  train loss: 4.2055358887
Epoch:   800  |  train loss: 4.0409468651
Epoch:   900  |  train loss: 3.8849483013
Epoch:  1000  |  train loss: 3.7184376717
Processing class: 52
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 8.8980377197
Epoch:   200  |  train loss: 8.3175970078
Epoch:   300  |  train loss: 7.3284958839
Epoch:   400  |  train loss: 6.6810770988
Epoch:   500  |  train loss: 6.1696648598
Epoch:   600  |  train loss: 5.7196680069
Epoch:   700  |  train loss: 5.3680239677
Epoch:   800  |  train loss: 5.0818682671
Epoch:   900  |  train loss: 4.8061278343
Epoch:  1000  |  train loss: 4.5716202736
Processing class: 53
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 6.4366615295
Epoch:   200  |  train loss: 5.5252264977
Epoch:   300  |  train loss: 5.1460139275
Epoch:   400  |  train loss: 4.7170804024
Epoch:   500  |  train loss: 4.5286595345
Epoch:   600  |  train loss: 4.3655292988
Epoch:   700  |  train loss: 4.1810915947
Epoch:   800  |  train loss: 4.0257051945
Epoch:   900  |  train loss: 3.9047023773
Epoch:  1000  |  train loss: 3.7919770241
Processing class: 54
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 5.9161344528
Epoch:   200  |  train loss: 5.2294581413
Epoch:   300  |  train loss: 4.5819397926
Epoch:   400  |  train loss: 4.2785224438
Epoch:   500  |  train loss: 4.0125361443
Epoch:   600  |  train loss: 3.8013918400
Epoch:   700  |  train loss: 3.6159270763
Epoch:   800  |  train loss: 3.4700312614
Epoch:   900  |  train loss: 3.3483599186
Epoch:  1000  |  train loss: 3.2188773155
Processing class: 55
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 9.6175251007
Epoch:   200  |  train loss: 8.9898635864
Epoch:   300  |  train loss: 8.3239984512
Epoch:   400  |  train loss: 7.6753061295
Epoch:   500  |  train loss: 7.0934306145
Epoch:   600  |  train loss: 6.5285622597
Epoch:   700  |  train loss: 6.1141199112
Epoch:   800  |  train loss: 5.7555714607
Epoch:   900  |  train loss: 5.4394020081
Epoch:  1000  |  train loss: 5.1826014519
Processing class: 56
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 7.3679640770
Epoch:   200  |  train loss: 5.9880510330
Epoch:   300  |  train loss: 4.9792861938
Epoch:   400  |  train loss: 4.4172373772
Epoch:   500  |  train loss: 4.0173505306
Epoch:   600  |  train loss: 3.7092300415
Epoch:   700  |  train loss: 3.4900890350
Epoch:   800  |  train loss: 3.2965954304
Epoch:   900  |  train loss: 3.1604975700
Epoch:  1000  |  train loss: 3.0386733055
Processing class: 57
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 9.5970708847
Epoch:   200  |  train loss: 8.8071088791
Epoch:   300  |  train loss: 8.1238895416
Epoch:   400  |  train loss: 7.4601017952
Epoch:   500  |  train loss: 6.8660755157
Epoch:   600  |  train loss: 6.4810379982
Epoch:   700  |  train loss: 6.1563747406
Epoch:   800  |  train loss: 5.8328751564
Epoch:   900  |  train loss: 5.5268387794
Epoch:  1000  |  train loss: 5.2246189117
Processing class: 58
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 7.9167829514
Epoch:   200  |  train loss: 5.8895005226
Epoch:   300  |  train loss: 5.1668870926
Epoch:   400  |  train loss: 4.5056834221
Epoch:   500  |  train loss: 4.2049057007
Epoch:   600  |  train loss: 3.9733485222
Epoch:   700  |  train loss: 3.7557254791
Epoch:   800  |  train loss: 3.5881514072
Epoch:   900  |  train loss: 3.4282049656
Epoch:  1000  |  train loss: 3.2871309757
Processing class: 59
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 8.7974138260
Epoch:   200  |  train loss: 7.6940748215
Epoch:   300  |  train loss: 6.7148525238
Epoch:   400  |  train loss: 5.9692603111
Epoch:   500  |  train loss: 5.4433502197
Epoch:   600  |  train loss: 5.1341737747
Epoch:   700  |  train loss: 4.8620250702
Epoch:   800  |  train loss: 4.6605930328
Epoch:   900  |  train loss: 4.4608229637
Epoch:  1000  |  train loss: 4.3216940880
2024-03-05 23:25:47,584 [trainer.py] => CNN: {'total': 71.75, '00-09': 82.2, '10-19': 72.8, '20-29': 78.5, '30-39': 74.9, '40-49': 68.4, '50-59': 53.7, 'old': 75.36, 'new': 53.7}
2024-03-05 23:25:47,585 [trainer.py] => No NME accuracy
2024-03-05 23:25:47,585 [trainer.py] => FeCAM: {'total': 72.78, '00-09': 82.6, '10-19': 74.0, '20-29': 81.0, '30-39': 76.4, '40-49': 76.8, '50-59': 45.9, 'old': 78.16, 'new': 45.9}
2024-03-05 23:25:47,585 [trainer.py] => CNN top1 curve: [83.44, 71.75]
2024-03-05 23:25:47,585 [trainer.py] => CNN top5 curve: [96.5, 89.6]
2024-03-05 23:25:47,585 [trainer.py] => FeCAM top1 curve: [82.9, 72.78]
2024-03-05 23:25:47,585 [trainer.py] => FeCAM top5 curve: [95.84, 91.18]

2024-03-05 23:25:47,589 [fecam.py] => Learning on 60-70
Processing class: 60
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 8.5659482956
Epoch:   200  |  train loss: 7.6220466614
Epoch:   300  |  train loss: 6.7868415833
Epoch:   400  |  train loss: 6.0766623497
Epoch:   500  |  train loss: 5.5082770348
Epoch:   600  |  train loss: 5.0678800583
Epoch:   700  |  train loss: 4.7120962143
Epoch:   800  |  train loss: 4.3945574760
Epoch:   900  |  train loss: 4.1902070522
Epoch:  1000  |  train loss: 3.9888538361
Processing class: 61
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 7.3198139191
Epoch:   200  |  train loss: 5.9672760010
Epoch:   300  |  train loss: 5.0822413445
Epoch:   400  |  train loss: 4.4780601501
Epoch:   500  |  train loss: 4.1260693550
Epoch:   600  |  train loss: 3.8563220501
Epoch:   700  |  train loss: 3.6265194893
Epoch:   800  |  train loss: 3.4244332790
Epoch:   900  |  train loss: 3.2875645638
Epoch:  1000  |  train loss: 3.1626910210
Processing class: 62
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 9.6149328232
Epoch:   200  |  train loss: 8.8605630875
Epoch:   300  |  train loss: 8.2716955185
Epoch:   400  |  train loss: 7.8061539650
Epoch:   500  |  train loss: 7.3933416367
Epoch:   600  |  train loss: 7.0214015961
Epoch:   700  |  train loss: 6.6489976883
Epoch:   800  |  train loss: 6.3105159760
Epoch:   900  |  train loss: 6.0147402763
Epoch:  1000  |  train loss: 5.7462203979
Processing class: 63
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 8.1458399773
Epoch:   200  |  train loss: 7.3658873558
Epoch:   300  |  train loss: 5.9043193817
Epoch:   400  |  train loss: 5.1623801231
Epoch:   500  |  train loss: 4.6783288002
Epoch:   600  |  train loss: 4.3556008339
Epoch:   700  |  train loss: 4.1173577785
Epoch:   800  |  train loss: 3.9415720940
Epoch:   900  |  train loss: 3.7605653763
Epoch:  1000  |  train loss: 3.6116920948
Processing class: 64
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 7.3548100471
Epoch:   200  |  train loss: 5.4263970375
Epoch:   300  |  train loss: 4.6866521835
Epoch:   400  |  train loss: 4.2468899727
Epoch:   500  |  train loss: 3.8243631363
Epoch:   600  |  train loss: 3.5322516441
Epoch:   700  |  train loss: 3.3316552639
Epoch:   800  |  train loss: 3.1744040966
Epoch:   900  |  train loss: 3.0425682545
Epoch:  1000  |  train loss: 2.8987160683
Processing class: 65
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 7.3651809692
Epoch:   200  |  train loss: 5.9398178101
Epoch:   300  |  train loss: 4.7516236305
Epoch:   400  |  train loss: 4.2560187340
Epoch:   500  |  train loss: 3.9306627274
Epoch:   600  |  train loss: 3.6477719307
Epoch:   700  |  train loss: 3.4195665836
Epoch:   800  |  train loss: 3.2497491837
Epoch:   900  |  train loss: 3.0795273304
Epoch:  1000  |  train loss: 2.9560525417
Processing class: 66
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 8.1394504547
Epoch:   200  |  train loss: 7.1045499802
Epoch:   300  |  train loss: 6.2335162163
Epoch:   400  |  train loss: 5.6995300293
Epoch:   500  |  train loss: 5.3909985542
Epoch:   600  |  train loss: 5.1168501854
Epoch:   700  |  train loss: 4.9198873520
Epoch:   800  |  train loss: 4.7656236649
Epoch:   900  |  train loss: 4.6076439857
Epoch:  1000  |  train loss: 4.4798724174
Processing class: 67
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 8.5212139130
Epoch:   200  |  train loss: 7.4508869171
Epoch:   300  |  train loss: 6.7042233467
Epoch:   400  |  train loss: 6.1815367699
Epoch:   500  |  train loss: 5.8048874855
Epoch:   600  |  train loss: 5.4795145035
Epoch:   700  |  train loss: 5.1793264389
Epoch:   800  |  train loss: 4.9504117966
Epoch:   900  |  train loss: 4.7380816460
Epoch:  1000  |  train loss: 4.5665829659
Processing class: 68
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 7.2054618835
Epoch:   200  |  train loss: 6.4872008324
Epoch:   300  |  train loss: 5.4136595726
Epoch:   400  |  train loss: 4.9908256531
Epoch:   500  |  train loss: 4.6836104393
Epoch:   600  |  train loss: 4.4122436523
Epoch:   700  |  train loss: 4.2058590889
Epoch:   800  |  train loss: 4.0200562954
Epoch:   900  |  train loss: 3.8618452549
Epoch:  1000  |  train loss: 3.7335164070
Processing class: 69
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 4.2290983677
Epoch:   200  |  train loss: 3.6712479591
Epoch:   300  |  train loss: 3.1883761883
Epoch:   400  |  train loss: 2.9015459538
Epoch:   500  |  train loss: 2.7282575130
Epoch:   600  |  train loss: 2.6328200340
Epoch:   700  |  train loss: 2.5669070721
Epoch:   800  |  train loss: 2.4906709194
Epoch:   900  |  train loss: 2.4513504505
Epoch:  1000  |  train loss: 2.3927026749
2024-03-05 23:32:17,024 [trainer.py] => CNN: {'total': 64.66, '00-09': 75.0, '10-19': 70.1, '20-29': 77.1, '30-39': 70.6, '40-49': 64.0, '50-59': 45.8, '60-69': 50.0, 'old': 67.1, 'new': 50.0}
2024-03-05 23:32:17,025 [trainer.py] => No NME accuracy
2024-03-05 23:32:17,025 [trainer.py] => FeCAM: {'total': 67.27, '00-09': 79.9, '10-19': 70.6, '20-29': 79.1, '30-39': 71.9, '40-49': 74.1, '50-59': 41.2, '60-69': 54.1, 'old': 69.47, 'new': 54.1}
2024-03-05 23:32:17,025 [trainer.py] => CNN top1 curve: [83.44, 71.75, 64.66]
2024-03-05 23:32:17,025 [trainer.py] => CNN top5 curve: [96.5, 89.6, 86.54]
2024-03-05 23:32:17,025 [trainer.py] => FeCAM top1 curve: [82.9, 72.78, 67.27]
2024-03-05 23:32:17,025 [trainer.py] => FeCAM top5 curve: [95.84, 91.18, 87.21]

2024-03-05 23:32:17,029 [fecam.py] => Learning on 70-80
Processing class: 70
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 7.7576669693
Epoch:   200  |  train loss: 6.8766466141
Epoch:   300  |  train loss: 5.7940627098
Epoch:   400  |  train loss: 5.2664015770
Epoch:   500  |  train loss: 4.8242143631
Epoch:   600  |  train loss: 4.4528409958
Epoch:   700  |  train loss: 4.1281535149
Epoch:   800  |  train loss: 3.8685562134
Epoch:   900  |  train loss: 3.7263129711
Epoch:  1000  |  train loss: 3.5979406357
Processing class: 71
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 8.7159400940
Epoch:   200  |  train loss: 7.7567225456
Epoch:   300  |  train loss: 6.5071582794
Epoch:   400  |  train loss: 5.8323939323
Epoch:   500  |  train loss: 5.3141852379
Epoch:   600  |  train loss: 4.9142873764
Epoch:   700  |  train loss: 4.5617297173
Epoch:   800  |  train loss: 4.2924401760
Epoch:   900  |  train loss: 4.0545704365
Epoch:  1000  |  train loss: 3.8499114990
Processing class: 72
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 8.3945899963
Epoch:   200  |  train loss: 7.4585220337
Epoch:   300  |  train loss: 6.5314796448
Epoch:   400  |  train loss: 5.9159566879
Epoch:   500  |  train loss: 5.5245966911
Epoch:   600  |  train loss: 5.1565977097
Epoch:   700  |  train loss: 4.8648265839
Epoch:   800  |  train loss: 4.5975517273
Epoch:   900  |  train loss: 4.3666940689
Epoch:  1000  |  train loss: 4.1856501579
Processing class: 73
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 8.1490947723
Epoch:   200  |  train loss: 7.0006649971
Epoch:   300  |  train loss: 5.7797000885
Epoch:   400  |  train loss: 5.3232910156
Epoch:   500  |  train loss: 5.0577346802
Epoch:   600  |  train loss: 4.8214866638
Epoch:   700  |  train loss: 4.6031841278
Epoch:   800  |  train loss: 4.4033069611
Epoch:   900  |  train loss: 4.2239684105
Epoch:  1000  |  train loss: 4.0788004875
Processing class: 74
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 9.3442975998
Epoch:   200  |  train loss: 8.9179283142
Epoch:   300  |  train loss: 7.9450721741
Epoch:   400  |  train loss: 7.1640416145
Epoch:   500  |  train loss: 6.4995786667
Epoch:   600  |  train loss: 6.0293351173
Epoch:   700  |  train loss: 5.6844950676
Epoch:   800  |  train loss: 5.3548017502
Epoch:   900  |  train loss: 5.0955557823
Epoch:  1000  |  train loss: 4.8806681633
Processing class: 75
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 8.3588293076
Epoch:   200  |  train loss: 7.4299923897
Epoch:   300  |  train loss: 6.4200539589
Epoch:   400  |  train loss: 5.9250196457
Epoch:   500  |  train loss: 5.4848385811
Epoch:   600  |  train loss: 5.1331521988
Epoch:   700  |  train loss: 4.8522353172
Epoch:   800  |  train loss: 4.6326065063
Epoch:   900  |  train loss: 4.4563426018
Epoch:  1000  |  train loss: 4.2843975067
Processing class: 76
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 4.7832659721
Epoch:   200  |  train loss: 3.9840849400
Epoch:   300  |  train loss: 3.4053608418
Epoch:   400  |  train loss: 3.1443593979
Epoch:   500  |  train loss: 3.0001176357
Epoch:   600  |  train loss: 2.8495613575
Epoch:   700  |  train loss: 2.7715159893
Epoch:   800  |  train loss: 2.6407186031
Epoch:   900  |  train loss: 2.5702245712
Epoch:  1000  |  train loss: 2.5027976513
Processing class: 77
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 7.3311001778
Epoch:   200  |  train loss: 6.4365200996
Epoch:   300  |  train loss: 5.5648285866
Epoch:   400  |  train loss: 5.0371422768
Epoch:   500  |  train loss: 4.4947400093
Epoch:   600  |  train loss: 4.0833286762
Epoch:   700  |  train loss: 3.8013685226
Epoch:   800  |  train loss: 3.5833573818
Epoch:   900  |  train loss: 3.4205490589
Epoch:  1000  |  train loss: 3.2485243320
Processing class: 78
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 7.6993843079
Epoch:   200  |  train loss: 6.1665795326
Epoch:   300  |  train loss: 5.6413467407
Epoch:   400  |  train loss: 5.1016666412
Epoch:   500  |  train loss: 4.6214096069
Epoch:   600  |  train loss: 4.3113916397
Epoch:   700  |  train loss: 4.0924377918
Epoch:   800  |  train loss: 3.8863167286
Epoch:   900  |  train loss: 3.7547701359
Epoch:  1000  |  train loss: 3.6250463963
Processing class: 79
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 9.6129108429
Epoch:   200  |  train loss: 8.5631679535
Epoch:   300  |  train loss: 7.8321484566
Epoch:   400  |  train loss: 7.1145227432
Epoch:   500  |  train loss: 6.6176834106
Epoch:   600  |  train loss: 6.1789887428
Epoch:   700  |  train loss: 5.8047264099
Epoch:   800  |  train loss: 5.4476801872
Epoch:   900  |  train loss: 5.1527491570
Epoch:  1000  |  train loss: 4.8936814308
2024-03-05 23:39:52,254 [trainer.py] => CNN: {'total': 59.18, '00-09': 73.6, '10-19': 68.4, '20-29': 76.9, '30-39': 69.1, '40-49': 60.6, '50-59': 37.2, '60-69': 45.7, '70-79': 41.9, 'old': 61.64, 'new': 41.9}
2024-03-05 23:39:52,299 [trainer.py] => No NME accuracy
2024-03-05 23:39:52,299 [trainer.py] => FeCAM: {'total': 61.72, '00-09': 78.1, '10-19': 69.3, '20-29': 78.0, '30-39': 69.5, '40-49': 72.1, '50-59': 35.7, '60-69': 49.1, '70-79': 42.0, 'old': 64.54, 'new': 42.0}
2024-03-05 23:39:52,299 [trainer.py] => CNN top1 curve: [83.44, 71.75, 64.66, 59.18]
2024-03-05 23:39:52,299 [trainer.py] => CNN top5 curve: [96.5, 89.6, 86.54, 84.09]
2024-03-05 23:39:52,299 [trainer.py] => FeCAM top1 curve: [82.9, 72.78, 67.27, 61.72]
2024-03-05 23:39:52,299 [trainer.py] => FeCAM top5 curve: [95.84, 91.18, 87.21, 84.09]

2024-03-05 23:39:52,356 [fecam.py] => Learning on 80-90
Processing class: 80
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 5.6404649734
Epoch:   200  |  train loss: 4.0938451767
Epoch:   300  |  train loss: 3.6446864128
Epoch:   400  |  train loss: 3.3654531002
Epoch:   500  |  train loss: 3.1389317036
Epoch:   600  |  train loss: 3.0172209740
Epoch:   700  |  train loss: 2.8890747070
Epoch:   800  |  train loss: 2.8237568378
Epoch:   900  |  train loss: 2.7408296585
Epoch:  1000  |  train loss: 2.6656373024
Processing class: 81
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 7.3269001007
Epoch:   200  |  train loss: 6.0355669975
Epoch:   300  |  train loss: 5.3950325012
Epoch:   400  |  train loss: 4.9662650108
Epoch:   500  |  train loss: 4.6083499908
Epoch:   600  |  train loss: 4.3510773659
Epoch:   700  |  train loss: 4.1346282005
Epoch:   800  |  train loss: 3.9817444324
Epoch:   900  |  train loss: 3.7958985329
Epoch:  1000  |  train loss: 3.6313440323
Processing class: 82
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 6.6413157463
Epoch:   200  |  train loss: 5.4388690948
Epoch:   300  |  train loss: 4.8545696259
Epoch:   400  |  train loss: 4.3832459450
Epoch:   500  |  train loss: 4.0158962727
Epoch:   600  |  train loss: 3.7856981277
Epoch:   700  |  train loss: 3.6136341572
Epoch:   800  |  train loss: 3.4590482712
Epoch:   900  |  train loss: 3.3284786224
Epoch:  1000  |  train loss: 3.2025101185
Processing class: 83
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 6.0975098610
Epoch:   200  |  train loss: 4.4384748459
Epoch:   300  |  train loss: 3.8045062542
Epoch:   400  |  train loss: 3.3831337929
Epoch:   500  |  train loss: 3.1349970341
Epoch:   600  |  train loss: 2.9723308086
Epoch:   700  |  train loss: 2.8677564144
Epoch:   800  |  train loss: 2.7596705437
Epoch:   900  |  train loss: 2.6619297981
Epoch:  1000  |  train loss: 2.5882186890
Processing class: 84
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 8.0041427612
Epoch:   200  |  train loss: 6.2197957993
Epoch:   300  |  train loss: 5.4507220268
Epoch:   400  |  train loss: 4.8211307526
Epoch:   500  |  train loss: 4.4410658836
Epoch:   600  |  train loss: 4.1622292042
Epoch:   700  |  train loss: 3.9448439598
Epoch:   800  |  train loss: 3.7796841621
Epoch:   900  |  train loss: 3.6039124966
Epoch:  1000  |  train loss: 3.4507962704
Processing class: 85
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 8.2466775894
Epoch:   200  |  train loss: 6.5155691147
Epoch:   300  |  train loss: 5.5604132652
Epoch:   400  |  train loss: 4.9933848381
Epoch:   500  |  train loss: 4.6517210960
Epoch:   600  |  train loss: 4.3521341324
Epoch:   700  |  train loss: 4.1485314846
Epoch:   800  |  train loss: 3.9620166779
Epoch:   900  |  train loss: 3.7908589363
Epoch:  1000  |  train loss: 3.6530959129
Processing class: 86
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 9.5023302078
Epoch:   200  |  train loss: 8.2759199142
Epoch:   300  |  train loss: 6.9038847923
Epoch:   400  |  train loss: 6.1489064217
Epoch:   500  |  train loss: 5.6454961777
Epoch:   600  |  train loss: 5.2418307304
Epoch:   700  |  train loss: 4.9181596756
Epoch:   800  |  train loss: 4.6587677002
Epoch:   900  |  train loss: 4.4601419449
Epoch:  1000  |  train loss: 4.2993478298
Processing class: 87
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 6.6392856598
Epoch:   200  |  train loss: 5.2704227448
Epoch:   300  |  train loss: 4.8402982712
Epoch:   400  |  train loss: 4.3191262245
Epoch:   500  |  train loss: 4.0246821404
Epoch:   600  |  train loss: 3.8668891907
Epoch:   700  |  train loss: 3.7117555141
Epoch:   800  |  train loss: 3.5612794876
Epoch:   900  |  train loss: 3.4305737495
Epoch:  1000  |  train loss: 3.3308910370
Processing class: 88
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 7.9650349617
Epoch:   200  |  train loss: 6.9322843552
Epoch:   300  |  train loss: 6.1090597153
Epoch:   400  |  train loss: 5.4848071098
Epoch:   500  |  train loss: 4.9404459953
Epoch:   600  |  train loss: 4.5361252785
Epoch:   700  |  train loss: 4.2818162441
Epoch:   800  |  train loss: 4.0686573505
Epoch:   900  |  train loss: 3.9053851604
Epoch:  1000  |  train loss: 3.7360836029
Processing class: 89
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 7.4859629631
Epoch:   200  |  train loss: 6.2883227348
Epoch:   300  |  train loss: 5.3027688026
Epoch:   400  |  train loss: 4.8574420929
Epoch:   500  |  train loss: 4.4351706982
Epoch:   600  |  train loss: 4.0907163620
Epoch:   700  |  train loss: 3.8398435116
Epoch:   800  |  train loss: 3.6497880936
Epoch:   900  |  train loss: 3.4662549973
Epoch:  1000  |  train loss: 3.3138383389
2024-03-05 23:48:51,121 [trainer.py] => CNN: {'total': 54.08, '00-09': 67.9, '10-19': 63.6, '20-29': 73.2, '30-39': 68.8, '40-49': 57.2, '50-59': 32.6, '60-69': 40.9, '70-79': 38.1, '80-89': 44.4, 'old': 55.29, 'new': 44.4}
2024-03-05 23:48:51,123 [trainer.py] => No NME accuracy
2024-03-05 23:48:51,123 [trainer.py] => FeCAM: {'total': 57.66, '00-09': 76.1, '10-19': 65.8, '20-29': 75.3, '30-39': 67.9, '40-49': 68.5, '50-59': 33.0, '60-69': 45.0, '70-79': 39.0, '80-89': 48.3, 'old': 58.82, 'new': 48.3}
2024-03-05 23:48:51,123 [trainer.py] => CNN top1 curve: [83.44, 71.75, 64.66, 59.18, 54.08]
2024-03-05 23:48:51,123 [trainer.py] => CNN top5 curve: [96.5, 89.6, 86.54, 84.09, 81.74]
2024-03-05 23:48:51,123 [trainer.py] => FeCAM top1 curve: [82.9, 72.78, 67.27, 61.72, 57.66]
2024-03-05 23:48:51,123 [trainer.py] => FeCAM top5 curve: [95.84, 91.18, 87.21, 84.09, 81.63]

2024-03-05 23:48:51,129 [fecam.py] => Learning on 90-100
Processing class: 90
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 8.2092847824
Epoch:   200  |  train loss: 7.3494903564
Epoch:   300  |  train loss: 6.2558405876
Epoch:   400  |  train loss: 5.4709426880
Epoch:   500  |  train loss: 4.9255990982
Epoch:   600  |  train loss: 4.4839664459
Epoch:   700  |  train loss: 4.1393102169
Epoch:   800  |  train loss: 3.9255089283
Epoch:   900  |  train loss: 3.7639540672
Epoch:  1000  |  train loss: 3.6092610836
Processing class: 91
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 4.6228692055
Epoch:   200  |  train loss: 4.1639864922
Epoch:   300  |  train loss: 3.6772383213
Epoch:   400  |  train loss: 3.4769492626
Epoch:   500  |  train loss: 3.2142499447
Epoch:   600  |  train loss: 3.0338853836
Epoch:   700  |  train loss: 2.8184501648
Epoch:   800  |  train loss: 2.6488165379
Epoch:   900  |  train loss: 2.5289634705
Epoch:  1000  |  train loss: 2.4547520638
Processing class: 92
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 7.2280963898
Epoch:   200  |  train loss: 5.5661849976
Epoch:   300  |  train loss: 5.1302415848
Epoch:   400  |  train loss: 4.7321548462
Epoch:   500  |  train loss: 4.2660348415
Epoch:   600  |  train loss: 3.8384086132
Epoch:   700  |  train loss: 3.5704626083
Epoch:   800  |  train loss: 3.3907320023
Epoch:   900  |  train loss: 3.2472096443
Epoch:  1000  |  train loss: 3.1194032192
Processing class: 93
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 7.7589914322
Epoch:   200  |  train loss: 6.0122451782
Epoch:   300  |  train loss: 5.1579263687
Epoch:   400  |  train loss: 4.7082536697
Epoch:   500  |  train loss: 4.3072153091
Epoch:   600  |  train loss: 3.9819489956
Epoch:   700  |  train loss: 3.7525104523
Epoch:   800  |  train loss: 3.5625161171
Epoch:   900  |  train loss: 3.4058029175
Epoch:  1000  |  train loss: 3.2384819984
Processing class: 94
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 6.9559567451
Epoch:   200  |  train loss: 5.9619989395
Epoch:   300  |  train loss: 4.9483469963
Epoch:   400  |  train loss: 4.4371434212
Epoch:   500  |  train loss: 4.0909705162
Epoch:   600  |  train loss: 3.8510236263
Epoch:   700  |  train loss: 3.6538635731
Epoch:   800  |  train loss: 3.4498974323
Epoch:   900  |  train loss: 3.3098657608
Epoch:  1000  |  train loss: 3.1743579388
Processing class: 95
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 8.6441526413
Epoch:   200  |  train loss: 7.8188104630
Epoch:   300  |  train loss: 6.9566992760
Epoch:   400  |  train loss: 6.1175198555
Epoch:   500  |  train loss: 5.4882036209
Epoch:   600  |  train loss: 5.0956084251
Epoch:   700  |  train loss: 4.7526025772
Epoch:   800  |  train loss: 4.4837919235
Epoch:   900  |  train loss: 4.2567986012
Epoch:  1000  |  train loss: 4.0700083256
Processing class: 96
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 8.8123956680
Epoch:   200  |  train loss: 7.7794582367
Epoch:   300  |  train loss: 6.6317611694
Epoch:   400  |  train loss: 6.0306954384
Epoch:   500  |  train loss: 5.5184823036
Epoch:   600  |  train loss: 5.1592153549
Epoch:   700  |  train loss: 4.8560110092
Epoch:   800  |  train loss: 4.5828777313
Epoch:   900  |  train loss: 4.3215849400
Epoch:  1000  |  train loss: 4.1325613976
Processing class: 97
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 7.6646423340
Epoch:   200  |  train loss: 6.0630691528
Epoch:   300  |  train loss: 5.1299886703
Epoch:   400  |  train loss: 4.7029162407
Epoch:   500  |  train loss: 4.3322336197
Epoch:   600  |  train loss: 4.0604836941
Epoch:   700  |  train loss: 3.8257420063
Epoch:   800  |  train loss: 3.6516078472
Epoch:   900  |  train loss: 3.4807590485
Epoch:  1000  |  train loss: 3.3233422756
Processing class: 98
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 6.5155467033
Epoch:   200  |  train loss: 5.5517620087
Epoch:   300  |  train loss: 4.6497692108
Epoch:   400  |  train loss: 4.1762689114
Epoch:   500  |  train loss: 3.8422769547
Epoch:   600  |  train loss: 3.5795199394
Epoch:   700  |  train loss: 3.3937695980
Epoch:   800  |  train loss: 3.2614406109
Epoch:   900  |  train loss: 3.1405392647
Epoch:  1000  |  train loss: 3.0666667938
Processing class: 99
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 8.0865635872
Epoch:   200  |  train loss: 6.9446952820
Epoch:   300  |  train loss: 5.6810432434
Epoch:   400  |  train loss: 5.0001799583
Epoch:   500  |  train loss: 4.5064700603
Epoch:   600  |  train loss: 4.2043116570
Epoch:   700  |  train loss: 3.9804889679
Epoch:   800  |  train loss: 3.8034892082
Epoch:   900  |  train loss: 3.6418040276
Epoch:  1000  |  train loss: 3.4949217319
2024-03-05 23:59:03,421 [trainer.py] => CNN: {'total': 50.36, '00-09': 57.7, '10-19': 63.6, '20-29': 71.4, '30-39': 68.5, '40-49': 56.8, '50-59': 29.6, '60-69': 38.9, '70-79': 36.6, '80-89': 42.7, '90-99': 37.8, 'old': 51.76, 'new': 37.8}
2024-03-05 23:59:03,423 [trainer.py] => No NME accuracy
2024-03-05 23:59:03,423 [trainer.py] => FeCAM: {'total': 54.74, '00-09': 73.3, '10-19': 65.1, '20-29': 74.1, '30-39': 67.2, '40-49': 67.1, '50-59': 30.8, '60-69': 43.3, '70-79': 37.2, '80-89': 46.9, '90-99': 42.4, 'old': 56.11, 'new': 42.4}
2024-03-05 23:59:03,423 [trainer.py] => CNN top1 curve: [83.44, 71.75, 64.66, 59.18, 54.08, 50.36]
2024-03-05 23:59:03,423 [trainer.py] => CNN top5 curve: [96.5, 89.6, 86.54, 84.09, 81.74, 79.63]
2024-03-05 23:59:03,423 [trainer.py] => FeCAM top1 curve: [82.9, 72.78, 67.27, 61.72, 57.66, 54.74]
2024-03-05 23:59:03,423 [trainer.py] => FeCAM top5 curve: [95.84, 91.18, 87.21, 84.09, 81.63, 79.21]

=========================================
2024-03-05 23:59:12,697 [trainer.py] => config: ./exps/FeCAM_cifar100.json
2024-03-05 23:59:12,697 [trainer.py] => prefix: train
2024-03-05 23:59:12,697 [trainer.py] => dataset: cifar100
2024-03-05 23:59:12,697 [trainer.py] => memory_size: 0
2024-03-05 23:59:12,697 [trainer.py] => shuffle: True
2024-03-05 23:59:12,697 [trainer.py] => init_cls: 50
2024-03-05 23:59:12,697 [trainer.py] => increment: 10
2024-03-05 23:59:12,698 [trainer.py] => model_name: fecam
2024-03-05 23:59:12,698 [trainer.py] => convnet_type: resnet18
2024-03-05 23:59:12,698 [trainer.py] => device: [device(type='cuda', index=0)]
2024-03-05 23:59:12,698 [trainer.py] => seed: 1993
2024-03-05 23:59:12,698 [trainer.py] => init_epochs: 200
2024-03-05 23:59:12,698 [trainer.py] => init_lr: 0.1
2024-03-05 23:59:12,698 [trainer.py] => init_weight_decay: 0.0005
2024-03-05 23:59:12,698 [trainer.py] => batch_size: 128
2024-03-05 23:59:12,698 [trainer.py] => num_workers: 8
2024-03-05 23:59:12,698 [trainer.py] => T: 5
2024-03-05 23:59:12,698 [trainer.py] => beta: 0.5
2024-03-05 23:59:12,698 [trainer.py] => alpha1: 1
2024-03-05 23:59:12,698 [trainer.py] => alpha2: 1
2024-03-05 23:59:12,698 [trainer.py] => ncm: False
2024-03-05 23:59:12,698 [trainer.py] => tukey: False
2024-03-05 23:59:12,698 [trainer.py] => diagonal: False
2024-03-05 23:59:12,698 [trainer.py] => per_class: True
2024-03-05 23:59:12,698 [trainer.py] => full_cov: True
2024-03-05 23:59:12,698 [trainer.py] => shrink: True
2024-03-05 23:59:12,698 [trainer.py] => norm_cov: False
2024-03-05 23:59:12,698 [trainer.py] => vecnorm: False
2024-03-05 23:59:12,698 [trainer.py] => ae_type: wae
2024-03-05 23:59:12,698 [trainer.py] => epochs: 1000
2024-03-05 23:59:12,698 [trainer.py] => ae_latent_dim: 32
2024-03-05 23:59:12,698 [trainer.py] => wae_sigma: 5
2024-03-05 23:59:12,698 [trainer.py] => wae_C: 10
Files already downloaded and verified
Files already downloaded and verified
2024-03-05 23:59:14,355 [data_manager.py] => [68, 56, 78, 8, 23, 84, 90, 65, 74, 76, 40, 89, 3, 92, 55, 9, 26, 80, 43, 38, 58, 70, 77, 1, 85, 19, 17, 50, 28, 53, 13, 81, 45, 82, 6, 59, 83, 16, 15, 44, 91, 41, 72, 60, 79, 52, 20, 10, 31, 54, 37, 95, 14, 71, 96, 98, 97, 2, 64, 66, 42, 22, 35, 86, 24, 34, 87, 21, 99, 0, 88, 27, 18, 94, 11, 12, 47, 25, 30, 46, 62, 69, 36, 61, 7, 63, 75, 5, 32, 4, 51, 48, 73, 93, 39, 67, 29, 49, 57, 33]
2024-03-05 23:59:14,619 [fecam.py] => Learning on 0-50
/home/z1165703/miniconda3/lib/python3.11/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
/home/z1165703/miniconda3/lib/python3.11/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
Processing class: 0
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 5.6420847893
Epoch:   200  |  train loss: 5.2154714584
Epoch:   300  |  train loss: 5.1952045441
Epoch:   400  |  train loss: 5.0292251587
Epoch:   500  |  train loss: 4.9302660942
Epoch:   600  |  train loss: 4.8995404243
Epoch:   700  |  train loss: 4.8452501297
Epoch:   800  |  train loss: 4.8228712082
Epoch:   900  |  train loss: 4.7555837631
Epoch:  1000  |  train loss: 4.6899888039
Processing class: 1
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 6.2124413490
Epoch:   200  |  train loss: 6.1302505493
Epoch:   300  |  train loss: 5.8873278618
Epoch:   400  |  train loss: 5.7244130135
Epoch:   500  |  train loss: 5.5203766823
Epoch:   600  |  train loss: 5.4886210442
Epoch:   700  |  train loss: 5.3761384964
Epoch:   800  |  train loss: 5.2954979897
Epoch:   900  |  train loss: 5.2451914787
Epoch:  1000  |  train loss: 5.1361856461
Processing class: 2
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 6.5310849190
Epoch:   200  |  train loss: 6.2067770004
Epoch:   300  |  train loss: 5.9057847977
Epoch:   400  |  train loss: 5.6303708076
Epoch:   500  |  train loss: 5.3707569122
Epoch:   600  |  train loss: 5.1914443016
Epoch:   700  |  train loss: 5.0247610092
Epoch:   800  |  train loss: 4.9331755638
Epoch:   900  |  train loss: 4.7870609283
Epoch:  1000  |  train loss: 4.6645241737
Processing class: 3
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 5.5771481514
Epoch:   200  |  train loss: 5.6004343033
Epoch:   300  |  train loss: 5.4333476067
Epoch:   400  |  train loss: 5.1474894524
Epoch:   500  |  train loss: 4.9768532753
Epoch:   600  |  train loss: 4.9284127235
Epoch:   700  |  train loss: 4.7654668808
Epoch:   800  |  train loss: 4.6617527962
Epoch:   900  |  train loss: 4.5357373238
Epoch:  1000  |  train loss: 4.4249136925
Processing class: 4
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 6.1996082306
Epoch:   200  |  train loss: 6.0844276428
Epoch:   300  |  train loss: 5.7369571686
Epoch:   400  |  train loss: 5.5120610237
Epoch:   500  |  train loss: 5.3449126244
Epoch:   600  |  train loss: 5.1345945358
Epoch:   700  |  train loss: 5.0790809631
Epoch:   800  |  train loss: 5.0088899612
Epoch:   900  |  train loss: 4.9276584625
Epoch:  1000  |  train loss: 4.8538908005
Processing class: 5
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 6.6127643585
Epoch:   200  |  train loss: 6.1326581001
Epoch:   300  |  train loss: 5.9023061752
Epoch:   400  |  train loss: 5.5965112686
Epoch:   500  |  train loss: 5.4812787056
Epoch:   600  |  train loss: 5.3772588730
Epoch:   700  |  train loss: 5.1902967453
Epoch:   800  |  train loss: 5.0348570824
Epoch:   900  |  train loss: 4.9060826302
Epoch:  1000  |  train loss: 4.8323102951
Processing class: 6
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 6.1438604355
Epoch:   200  |  train loss: 6.1420319557
Epoch:   300  |  train loss: 5.8365984917
Epoch:   400  |  train loss: 5.5590151787
Epoch:   500  |  train loss: 5.4397792816
Epoch:   600  |  train loss: 5.3557910919
Epoch:   700  |  train loss: 5.2671397209
Epoch:   800  |  train loss: 5.1577030182
Epoch:   900  |  train loss: 4.9952979088
Epoch:  1000  |  train loss: 4.9309855461
Processing class: 7
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 6.5572471619
Epoch:   200  |  train loss: 6.2345582008
Epoch:   300  |  train loss: 6.0527034760
Epoch:   400  |  train loss: 5.7469821930
Epoch:   500  |  train loss: 5.5213529587
Epoch:   600  |  train loss: 5.3824353218
Epoch:   700  |  train loss: 5.2667075157
Epoch:   800  |  train loss: 5.1894639969
Epoch:   900  |  train loss: 5.0404491425
Epoch:  1000  |  train loss: 4.9570942879
Processing class: 8
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 6.3524032593
Epoch:   200  |  train loss: 6.1448831558
Epoch:   300  |  train loss: 5.7125934601
Epoch:   400  |  train loss: 5.5468190193
Epoch:   500  |  train loss: 5.3862448692
Epoch:   600  |  train loss: 5.2101215363
Epoch:   700  |  train loss: 5.1193930626
Epoch:   800  |  train loss: 4.9896682739
Epoch:   900  |  train loss: 4.9100407600
Epoch:  1000  |  train loss: 4.7843510628
Processing class: 9
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 6.0833225250
Epoch:   200  |  train loss: 6.0698266029
Epoch:   300  |  train loss: 5.9574830055
Epoch:   400  |  train loss: 5.7226712227
Epoch:   500  |  train loss: 5.6469429016
Epoch:   600  |  train loss: 5.5572360992
Epoch:   700  |  train loss: 5.5175134659
Epoch:   800  |  train loss: 5.4636481285
Epoch:   900  |  train loss: 5.4029996872
Epoch:  1000  |  train loss: 5.3049283981
Processing class: 10
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 6.5365214348
Epoch:   200  |  train loss: 6.1952202797
Epoch:   300  |  train loss: 6.0398686409
Epoch:   400  |  train loss: 5.8823188782
Epoch:   500  |  train loss: 5.6369916916
Epoch:   600  |  train loss: 5.5109177589
Epoch:   700  |  train loss: 5.3979592323
Epoch:   800  |  train loss: 5.2824158669
Epoch:   900  |  train loss: 5.2107562065
Epoch:  1000  |  train loss: 5.1354642868
Processing class: 11
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 6.6260150909
Epoch:   200  |  train loss: 6.1651555061
Epoch:   300  |  train loss: 5.9616426468
Epoch:   400  |  train loss: 5.7543541908
Epoch:   500  |  train loss: 5.5256501198
Epoch:   600  |  train loss: 5.4262203217
Epoch:   700  |  train loss: 5.3387138367
Epoch:   800  |  train loss: 5.2295548439
Epoch:   900  |  train loss: 5.1232241631
Epoch:  1000  |  train loss: 5.0305245399
Processing class: 12
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 6.3501959801
Epoch:   200  |  train loss: 6.2035275459
Epoch:   300  |  train loss: 5.9348971367
Epoch:   400  |  train loss: 5.6466736794
Epoch:   500  |  train loss: 5.4646432877
Epoch:   600  |  train loss: 5.2665503502
Epoch:   700  |  train loss: 5.1478902817
Epoch:   800  |  train loss: 5.0784576416
Epoch:   900  |  train loss: 5.0012712479
Epoch:  1000  |  train loss: 4.9026793480
Processing class: 13
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 6.1791263580
Epoch:   200  |  train loss: 5.8474755287
Epoch:   300  |  train loss: 5.5704128265
Epoch:   400  |  train loss: 5.3127783775
Epoch:   500  |  train loss: 5.1497061729
Epoch:   600  |  train loss: 5.0432783127
Epoch:   700  |  train loss: 4.9453537941
Epoch:   800  |  train loss: 4.8908175468
Epoch:   900  |  train loss: 4.8366854668
Epoch:  1000  |  train loss: 4.7661275864
Processing class: 14
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 6.8457260132
Epoch:   200  |  train loss: 6.8223425865
Epoch:   300  |  train loss: 6.5437636375
Epoch:   400  |  train loss: 6.2719501495
Epoch:   500  |  train loss: 6.0978366852
Epoch:   600  |  train loss: 5.9417293549
Epoch:   700  |  train loss: 5.8596145630
Epoch:   800  |  train loss: 5.7997583389
Epoch:   900  |  train loss: 5.7229445457
Epoch:  1000  |  train loss: 5.6363070488
Processing class: 15
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 6.0385472298
Epoch:   200  |  train loss: 5.8890675545
Epoch:   300  |  train loss: 5.5205330849
Epoch:   400  |  train loss: 5.3602298737
Epoch:   500  |  train loss: 5.2277775764
Epoch:   600  |  train loss: 5.1141515732
Epoch:   700  |  train loss: 5.0751835823
Epoch:   800  |  train loss: 4.9758324623
Epoch:   900  |  train loss: 4.9651594162
Epoch:  1000  |  train loss: 4.8919079781
Processing class: 16
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 6.2329785347
Epoch:   200  |  train loss: 5.7929492950
Epoch:   300  |  train loss: 5.6156841278
Epoch:   400  |  train loss: 5.4597660065
Epoch:   500  |  train loss: 5.4211132050
Epoch:   600  |  train loss: 5.3087726593
Epoch:   700  |  train loss: 5.1912648201
Epoch:   800  |  train loss: 5.1341118813
Epoch:   900  |  train loss: 5.0170715332
Epoch:  1000  |  train loss: 5.0112437248
Processing class: 17
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 6.1727107048
Epoch:   200  |  train loss: 6.1495668411
Epoch:   300  |  train loss: 6.0955136299
Epoch:   400  |  train loss: 5.8679775238
Epoch:   500  |  train loss: 5.5327928543
Epoch:   600  |  train loss: 5.4133890152
Epoch:   700  |  train loss: 5.3062942505
Epoch:   800  |  train loss: 5.2109290123
Epoch:   900  |  train loss: 5.1286828995
Epoch:  1000  |  train loss: 5.0615252495
Processing class: 18
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 6.1449804306
Epoch:   200  |  train loss: 6.1188973427
Epoch:   300  |  train loss: 5.5236438751
Epoch:   400  |  train loss: 5.3040042877
Epoch:   500  |  train loss: 5.0537754059
Epoch:   600  |  train loss: 4.8440309525
Epoch:   700  |  train loss: 4.7799530983
Epoch:   800  |  train loss: 4.6552676201
Epoch:   900  |  train loss: 4.5776032448
Epoch:  1000  |  train loss: 4.5081138611
Processing class: 19
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 5.8300824165
Epoch:   200  |  train loss: 5.6377380371
Epoch:   300  |  train loss: 5.4006684303
Epoch:   400  |  train loss: 5.3230051994
Epoch:   500  |  train loss: 5.0909758568
Epoch:   600  |  train loss: 5.0054901123
Epoch:   700  |  train loss: 4.9237511635
Epoch:   800  |  train loss: 4.8041423798
Epoch:   900  |  train loss: 4.7438026428
Epoch:  1000  |  train loss: 4.6709378242
Processing class: 20
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 6.1647316933
Epoch:   200  |  train loss: 5.8528796196
Epoch:   300  |  train loss: 5.7716395378
Epoch:   400  |  train loss: 5.5145280838
Epoch:   500  |  train loss: 5.4777675629
Epoch:   600  |  train loss: 5.3316794395
Epoch:   700  |  train loss: 5.2075599670
Epoch:   800  |  train loss: 5.1611605644
Epoch:   900  |  train loss: 5.0837355614
Epoch:  1000  |  train loss: 5.0617286682
Processing class: 21
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 6.2438362122
Epoch:   200  |  train loss: 5.9240936279
Epoch:   300  |  train loss: 5.3934953690
Epoch:   400  |  train loss: 5.1297444344
Epoch:   500  |  train loss: 4.9597337723
Epoch:   600  |  train loss: 4.8607158661
Epoch:   700  |  train loss: 4.7721234322
Epoch:   800  |  train loss: 4.6662613869
Epoch:   900  |  train loss: 4.5897173882
Epoch:  1000  |  train loss: 4.4733730316
Processing class: 22
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 6.5322109222
Epoch:   200  |  train loss: 6.4787823677
Epoch:   300  |  train loss: 6.3416625977
Epoch:   400  |  train loss: 5.9786385536
Epoch:   500  |  train loss: 5.8100109100
Epoch:   600  |  train loss: 5.6174607277
Epoch:   700  |  train loss: 5.4720229149
Epoch:   800  |  train loss: 5.3262120247
Epoch:   900  |  train loss: 5.2217468262
Epoch:  1000  |  train loss: 5.1893527985
Processing class: 23
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 6.0250769615
Epoch:   200  |  train loss: 5.9976771355
Epoch:   300  |  train loss: 5.7772265434
Epoch:   400  |  train loss: 5.5449379921
Epoch:   500  |  train loss: 5.4127840996
Epoch:   600  |  train loss: 5.1904197693
Epoch:   700  |  train loss: 5.0607439995
Epoch:   800  |  train loss: 4.9186281204
Epoch:   900  |  train loss: 4.8356954575
Epoch:  1000  |  train loss: 4.7059887886
Processing class: 24
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 6.1900867462
Epoch:   200  |  train loss: 6.1608549118
Epoch:   300  |  train loss: 6.1514113426
Epoch:   400  |  train loss: 5.8566955566
Epoch:   500  |  train loss: 5.6980080605
Epoch:   600  |  train loss: 5.5903589249
Epoch:   700  |  train loss: 5.4352594376
Epoch:   800  |  train loss: 5.3670575142
Epoch:   900  |  train loss: 5.2899413109
Epoch:  1000  |  train loss: 5.1801357269
Processing class: 25
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 6.3530829430
Epoch:   200  |  train loss: 6.1045091629
Epoch:   300  |  train loss: 5.9267160416
Epoch:   400  |  train loss: 5.5871398926
Epoch:   500  |  train loss: 5.3926234245
Epoch:   600  |  train loss: 5.2461759567
Epoch:   700  |  train loss: 5.1802616119
Epoch:   800  |  train loss: 5.0609710693
Epoch:   900  |  train loss: 4.9513022423
Epoch:  1000  |  train loss: 4.9064962387
Processing class: 26
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 5.9925255775
Epoch:   200  |  train loss: 5.9911240578
Epoch:   300  |  train loss: 6.0306162834
Epoch:   400  |  train loss: 5.8571476936
Epoch:   500  |  train loss: 5.6883488655
Epoch:   600  |  train loss: 5.3988142014
Epoch:   700  |  train loss: 5.3169195175
Epoch:   800  |  train loss: 5.2482924461
Epoch:   900  |  train loss: 5.1317488670
Epoch:  1000  |  train loss: 5.0830370903
Processing class: 27
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 6.5406293869
Epoch:   200  |  train loss: 6.1637172699
Epoch:   300  |  train loss: 6.0579508781
Epoch:   400  |  train loss: 5.7636388779
Epoch:   500  |  train loss: 5.5322836876
Epoch:   600  |  train loss: 5.4420444489
Epoch:   700  |  train loss: 5.3764364243
Epoch:   800  |  train loss: 5.2859151840
Epoch:   900  |  train loss: 5.1828227997
Epoch:  1000  |  train loss: 5.0205249786
Processing class: 28
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 6.2844980240
Epoch:   200  |  train loss: 6.2369079590
Epoch:   300  |  train loss: 5.9288806915
Epoch:   400  |  train loss: 5.6980233192
Epoch:   500  |  train loss: 5.5193709373
Epoch:   600  |  train loss: 5.4436537743
Epoch:   700  |  train loss: 5.3920529366
Epoch:   800  |  train loss: 5.3234276772
Epoch:   900  |  train loss: 5.2724861145
Epoch:  1000  |  train loss: 5.1878738403
Processing class: 29
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 6.2851380348
Epoch:   200  |  train loss: 5.9500372887
Epoch:   300  |  train loss: 5.6630546570
Epoch:   400  |  train loss: 5.3675868034
Epoch:   500  |  train loss: 5.2338340759
Epoch:   600  |  train loss: 5.1226577759
Epoch:   700  |  train loss: 4.9702713966
Epoch:   800  |  train loss: 4.8143004417
Epoch:   900  |  train loss: 4.7290421486
Epoch:  1000  |  train loss: 4.6477138519
Processing class: 30
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 5.9567546844
Epoch:   200  |  train loss: 5.9226638794
Epoch:   300  |  train loss: 5.5318874359
Epoch:   400  |  train loss: 5.4433460236
Epoch:   500  |  train loss: 5.3987532616
Epoch:   600  |  train loss: 5.2877398491
Epoch:   700  |  train loss: 5.1362174988
Epoch:   800  |  train loss: 5.0684850693
Epoch:   900  |  train loss: 5.0052812576
Epoch:  1000  |  train loss: 4.9232336044
Processing class: 31
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 6.3406251907
Epoch:   200  |  train loss: 6.0376253128
Epoch:   300  |  train loss: 5.7724563599
Epoch:   400  |  train loss: 5.7276441574
Epoch:   500  |  train loss: 5.5693478584
Epoch:   600  |  train loss: 5.4942920685
Epoch:   700  |  train loss: 5.3827966690
Epoch:   800  |  train loss: 5.3220207214
Epoch:   900  |  train loss: 5.2394058228
Epoch:  1000  |  train loss: 5.1360806465
Processing class: 32
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 6.6848600388
Epoch:   200  |  train loss: 6.5946923256
Epoch:   300  |  train loss: 6.3595802307
Epoch:   400  |  train loss: 6.0835544586
Epoch:   500  |  train loss: 5.8678619385
Epoch:   600  |  train loss: 5.6188519478
Epoch:   700  |  train loss: 5.4860302925
Epoch:   800  |  train loss: 5.3916834831
Epoch:   900  |  train loss: 5.2871533394
Epoch:  1000  |  train loss: 5.1941285133
Processing class: 33
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 6.3480042458
Epoch:   200  |  train loss: 6.2587021828
Epoch:   300  |  train loss: 5.8320953369
Epoch:   400  |  train loss: 5.5668575287
Epoch:   500  |  train loss: 5.3995333672
Epoch:   600  |  train loss: 5.2598505020
Epoch:   700  |  train loss: 5.1351554871
Epoch:   800  |  train loss: 5.0485741615
Epoch:   900  |  train loss: 4.9475749016
Epoch:  1000  |  train loss: 4.8394481659
Processing class: 34
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 6.4683005333
Epoch:   200  |  train loss: 6.3540079117
Epoch:   300  |  train loss: 5.9681673050
Epoch:   400  |  train loss: 5.7927223206
Epoch:   500  |  train loss: 5.5761582375
Epoch:   600  |  train loss: 5.4228399277
Epoch:   700  |  train loss: 5.2647783279
Epoch:   800  |  train loss: 5.1823523521
Epoch:   900  |  train loss: 5.0921851158
Epoch:  1000  |  train loss: 5.0037409782
Processing class: 35
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 7.0386359215
Epoch:   200  |  train loss: 6.5151347160
Epoch:   300  |  train loss: 6.3075022697
Epoch:   400  |  train loss: 6.1736743927
Epoch:   500  |  train loss: 5.9690314293
Epoch:   600  |  train loss: 5.9160185814
Epoch:   700  |  train loss: 5.7973233223
Epoch:   800  |  train loss: 5.6871247292
Epoch:   900  |  train loss: 5.6017409325
Epoch:  1000  |  train loss: 5.4997517586
Processing class: 36
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 5.7816152573
Epoch:   200  |  train loss: 5.5994077682
Epoch:   300  |  train loss: 5.3891643524
Epoch:   400  |  train loss: 5.2940902710
Epoch:   500  |  train loss: 5.1599100113
Epoch:   600  |  train loss: 4.9804461479
Epoch:   700  |  train loss: 4.8280442238
Epoch:   800  |  train loss: 4.7533721924
Epoch:   900  |  train loss: 4.6210804939
Epoch:  1000  |  train loss: 4.5491765976
Processing class: 37
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 6.6740660667
Epoch:   200  |  train loss: 6.5127153397
Epoch:   300  |  train loss: 6.1928133965
Epoch:   400  |  train loss: 5.9507761955
Epoch:   500  |  train loss: 5.7904655457
Epoch:   600  |  train loss: 5.6340025902
Epoch:   700  |  train loss: 5.5157237053
Epoch:   800  |  train loss: 5.4421631813
Epoch:   900  |  train loss: 5.3010118484
Epoch:  1000  |  train loss: 5.2323833466
Processing class: 38
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 5.9945947647
Epoch:   200  |  train loss: 5.9453419685
Epoch:   300  |  train loss: 5.8148045540
Epoch:   400  |  train loss: 5.7268263817
Epoch:   500  |  train loss: 5.5178472519
Epoch:   600  |  train loss: 5.3304083824
Epoch:   700  |  train loss: 5.1577530861
Epoch:   800  |  train loss: 5.1156658173
Epoch:   900  |  train loss: 5.0169076920
Epoch:  1000  |  train loss: 4.9804103851
Processing class: 39
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 6.2962137222
Epoch:   200  |  train loss: 6.1386855125
Epoch:   300  |  train loss: 5.9581774712
Epoch:   400  |  train loss: 5.7735322952
Epoch:   500  |  train loss: 5.5547928810
Epoch:   600  |  train loss: 5.4440827370
Epoch:   700  |  train loss: 5.3230225563
Epoch:   800  |  train loss: 5.2200852394
Epoch:   900  |  train loss: 5.1036105156
Epoch:  1000  |  train loss: 5.0074131012
Processing class: 40
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 6.1497304916
Epoch:   200  |  train loss: 5.7833284378
Epoch:   300  |  train loss: 5.7093260765
Epoch:   400  |  train loss: 5.5681724548
Epoch:   500  |  train loss: 5.4572428703
Epoch:   600  |  train loss: 5.2273720741
Epoch:   700  |  train loss: 5.1711017609
Epoch:   800  |  train loss: 5.0842192650
Epoch:   900  |  train loss: 5.0053958893
Epoch:  1000  |  train loss: 4.9649502754
Processing class: 41
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 5.7952877998
Epoch:   200  |  train loss: 5.8220315933
Epoch:   300  |  train loss: 5.4402173042
Epoch:   400  |  train loss: 5.1812149048
Epoch:   500  |  train loss: 5.0483401299
Epoch:   600  |  train loss: 4.9544145584
Epoch:   700  |  train loss: 4.8828021049
Epoch:   800  |  train loss: 4.7990465164
Epoch:   900  |  train loss: 4.7260349274
Epoch:  1000  |  train loss: 4.6664804459
Processing class: 42
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 6.4837125778
Epoch:   200  |  train loss: 6.2129699707
Epoch:   300  |  train loss: 5.9434185982
Epoch:   400  |  train loss: 5.6916967392
Epoch:   500  |  train loss: 5.4970289230
Epoch:   600  |  train loss: 5.2992146492
Epoch:   700  |  train loss: 5.1763768196
Epoch:   800  |  train loss: 5.0446384430
Epoch:   900  |  train loss: 4.9122804642
Epoch:  1000  |  train loss: 4.8114740372
Processing class: 43
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 6.6486556053
Epoch:   200  |  train loss: 6.3889619827
Epoch:   300  |  train loss: 6.0059955597
Epoch:   400  |  train loss: 5.9176851273
Epoch:   500  |  train loss: 5.8285840034
Epoch:   600  |  train loss: 5.7837927818
Epoch:   700  |  train loss: 5.7586113930
Epoch:   800  |  train loss: 5.6984994888
Epoch:   900  |  train loss: 5.5958792686
Epoch:  1000  |  train loss: 5.5562956810
Processing class: 44
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 6.6799236298
Epoch:   200  |  train loss: 6.2960816383
Epoch:   300  |  train loss: 6.1446082115
Epoch:   400  |  train loss: 5.8939925194
Epoch:   500  |  train loss: 5.7793400764
Epoch:   600  |  train loss: 5.6731641769
Epoch:   700  |  train loss: 5.5956077576
Epoch:   800  |  train loss: 5.4877483368
Epoch:   900  |  train loss: 5.3660145760
Epoch:  1000  |  train loss: 5.2899396896
Processing class: 45
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 6.1909363747
Epoch:   200  |  train loss: 5.9661755562
Epoch:   300  |  train loss: 5.6634512901
Epoch:   400  |  train loss: 5.7173727989
Epoch:   500  |  train loss: 5.6647439003
Epoch:   600  |  train loss: 5.5812109947
Epoch:   700  |  train loss: 5.4775547028
Epoch:   800  |  train loss: 5.4160937309
Epoch:   900  |  train loss: 5.3966743469
Epoch:  1000  |  train loss: 5.3593997955
Processing class: 46
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 5.9171926498
Epoch:   200  |  train loss: 5.7854458809
Epoch:   300  |  train loss: 5.5204290390
Epoch:   400  |  train loss: 5.4999565125
Epoch:   500  |  train loss: 5.4354969978
Epoch:   600  |  train loss: 5.3929106712
Epoch:   700  |  train loss: 5.3259050369
Epoch:   800  |  train loss: 5.2739582062
Epoch:   900  |  train loss: 5.1772049904
Epoch:  1000  |  train loss: 5.1361083984
Processing class: 47
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 6.5546646118
Epoch:   200  |  train loss: 6.3057380676
Epoch:   300  |  train loss: 5.8750449181
Epoch:   400  |  train loss: 5.6777464867
Epoch:   500  |  train loss: 5.5136525154
Epoch:   600  |  train loss: 5.3308349609
Epoch:   700  |  train loss: 5.1901865959
Epoch:   800  |  train loss: 5.0621097565
Epoch:   900  |  train loss: 4.9748526573
Epoch:  1000  |  train loss: 4.8915581703
Processing class: 48
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 6.4570756912
Epoch:   200  |  train loss: 6.3556048393
Epoch:   300  |  train loss: 5.8233846664
Epoch:   400  |  train loss: 5.6092956543
Epoch:   500  |  train loss: 5.6087999344
Epoch:   600  |  train loss: 5.4566248894
Epoch:   700  |  train loss: 5.3554042816
Epoch:   800  |  train loss: 5.3016048431
Epoch:   900  |  train loss: 5.2816382408
Epoch:  1000  |  train loss: 5.2126986504
Processing class: 49
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 6.5762191772
Epoch:   200  |  train loss: 6.3459772110
Epoch:   300  |  train loss: 6.0601730347
Epoch:   400  |  train loss: 5.8718766212
Epoch:   500  |  train loss: 5.6649174690
Epoch:   600  |  train loss: 5.5563559532
Epoch:   700  |  train loss: 5.4189085007
Epoch:   800  |  train loss: 5.3288849831
Epoch:   900  |  train loss: 5.2433815956
Epoch:  1000  |  train loss: 5.1609921455
/home/z1165703/FeCAM/models/base.py:87: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  vec = torch.tensor(vec).cuda()
2024-03-06 00:16:53,584 [trainer.py] => CNN: {'total': 83.44, '00-09': 87.7, '10-19': 79.6, '20-29': 84.9, '30-39': 81.0, '40-49': 84.0, 'old': 0, 'new': 83.44}
2024-03-06 00:16:53,586 [trainer.py] => No NME accuracy
2024-03-06 00:16:53,586 [trainer.py] => FeCAM: {'total': 82.9, '00-09': 86.2, '10-19': 79.7, '20-29': 85.0, '30-39': 80.6, '40-49': 83.0, 'old': 0, 'new': 82.9}
2024-03-06 00:16:53,586 [trainer.py] => CNN top1 curve: [83.44]
2024-03-06 00:16:53,586 [trainer.py] => CNN top5 curve: [96.5]
2024-03-06 00:16:53,586 [trainer.py] => FeCAM top1 curve: [82.9]
2024-03-06 00:16:53,586 [trainer.py] => FeCAM top5 curve: [95.84]

2024-03-06 00:16:53,600 [fecam.py] => Learning on 50-60
Processing class: 50
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 7.4055074692
Epoch:   200  |  train loss: 6.9823511124
Epoch:   300  |  train loss: 6.7283020020
Epoch:   400  |  train loss: 6.4844761848
Epoch:   500  |  train loss: 6.2135274887
Epoch:   600  |  train loss: 5.9909060478
Epoch:   700  |  train loss: 5.8452082634
Epoch:   800  |  train loss: 5.7175607681
Epoch:   900  |  train loss: 5.6292094231
Epoch:  1000  |  train loss: 5.5384678841
Processing class: 51
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 7.4724494934
Epoch:   200  |  train loss: 7.0096584320
Epoch:   300  |  train loss: 6.7431897163
Epoch:   400  |  train loss: 6.5418836594
Epoch:   500  |  train loss: 6.2395086288
Epoch:   600  |  train loss: 5.9952835083
Epoch:   700  |  train loss: 5.7990506172
Epoch:   800  |  train loss: 5.6876139641
Epoch:   900  |  train loss: 5.6102396011
Epoch:  1000  |  train loss: 5.4632204056
Processing class: 52
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 7.8805801392
Epoch:   200  |  train loss: 7.6823328018
Epoch:   300  |  train loss: 7.3948102951
Epoch:   400  |  train loss: 7.1555904388
Epoch:   500  |  train loss: 6.9263980865
Epoch:   600  |  train loss: 6.7015377045
Epoch:   700  |  train loss: 6.5569156647
Epoch:   800  |  train loss: 6.4003139496
Epoch:   900  |  train loss: 6.2493546486
Epoch:  1000  |  train loss: 6.0866868973
Processing class: 53
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 6.9302356720
Epoch:   200  |  train loss: 6.4883090973
Epoch:   300  |  train loss: 6.2775392532
Epoch:   400  |  train loss: 6.0539530754
Epoch:   500  |  train loss: 5.9573114395
Epoch:   600  |  train loss: 5.8715117455
Epoch:   700  |  train loss: 5.7805710793
Epoch:   800  |  train loss: 5.6223513603
Epoch:   900  |  train loss: 5.5715213776
Epoch:  1000  |  train loss: 5.4684118271
Processing class: 54
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 6.4780664444
Epoch:   200  |  train loss: 6.1237638474
Epoch:   300  |  train loss: 5.7828999519
Epoch:   400  |  train loss: 5.6109502792
Epoch:   500  |  train loss: 5.4553270340
Epoch:   600  |  train loss: 5.3432602882
Epoch:   700  |  train loss: 5.2230657578
Epoch:   800  |  train loss: 5.1422472000
Epoch:   900  |  train loss: 5.0697835922
Epoch:  1000  |  train loss: 4.9555429459
Processing class: 55
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 8.0297231674
Epoch:   200  |  train loss: 7.8844881058
Epoch:   300  |  train loss: 7.6876742363
Epoch:   400  |  train loss: 7.5322598457
Epoch:   500  |  train loss: 7.3788485527
Epoch:   600  |  train loss: 7.1704844475
Epoch:   700  |  train loss: 7.0079444885
Epoch:   800  |  train loss: 6.8768692017
Epoch:   900  |  train loss: 6.6674045563
Epoch:  1000  |  train loss: 6.5558728218
Processing class: 56
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 7.2692094803
Epoch:   200  |  train loss: 6.7133164406
Epoch:   300  |  train loss: 6.1334202766
Epoch:   400  |  train loss: 5.8298194885
Epoch:   500  |  train loss: 5.5635292053
Epoch:   600  |  train loss: 5.3362745285
Epoch:   700  |  train loss: 5.1700035095
Epoch:   800  |  train loss: 5.0008358955
Epoch:   900  |  train loss: 4.9061748505
Epoch:  1000  |  train loss: 4.8145762444
Processing class: 57
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 7.9991641998
Epoch:   200  |  train loss: 7.8614976883
Epoch:   300  |  train loss: 7.6551228523
Epoch:   400  |  train loss: 7.4413638115
Epoch:   500  |  train loss: 7.2483464241
Epoch:   600  |  train loss: 7.0587126732
Epoch:   700  |  train loss: 6.9596183777
Epoch:   800  |  train loss: 6.8033637047
Epoch:   900  |  train loss: 6.6720516205
Epoch:  1000  |  train loss: 6.4734011650
Processing class: 58
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 7.4924119949
Epoch:   200  |  train loss: 6.6656339645
Epoch:   300  |  train loss: 6.2918166161
Epoch:   400  |  train loss: 5.9441929817
Epoch:   500  |  train loss: 5.7417591095
Epoch:   600  |  train loss: 5.6060657501
Epoch:   700  |  train loss: 5.4359829903
Epoch:   800  |  train loss: 5.3357702255
Epoch:   900  |  train loss: 5.2144134521
Epoch:  1000  |  train loss: 5.0991375923
Processing class: 59
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 7.8632540703
Epoch:   200  |  train loss: 7.5861415863
Epoch:   300  |  train loss: 7.1794876099
Epoch:   400  |  train loss: 6.8557278633
Epoch:   500  |  train loss: 6.6158640862
Epoch:   600  |  train loss: 6.4524705887
Epoch:   700  |  train loss: 6.3142621994
Epoch:   800  |  train loss: 6.1941491127
Epoch:   900  |  train loss: 6.0667831421
Epoch:  1000  |  train loss: 5.9758022308
2024-03-06 00:22:30,455 [trainer.py] => CNN: {'total': 71.75, '00-09': 82.2, '10-19': 72.8, '20-29': 78.5, '30-39': 74.9, '40-49': 68.4, '50-59': 53.7, 'old': 75.36, 'new': 53.7}
2024-03-06 00:22:30,457 [trainer.py] => No NME accuracy
2024-03-06 00:22:30,457 [trainer.py] => FeCAM: {'total': 72.78, '00-09': 82.6, '10-19': 74.0, '20-29': 81.0, '30-39': 76.4, '40-49': 76.8, '50-59': 45.9, 'old': 78.16, 'new': 45.9}
2024-03-06 00:22:30,457 [trainer.py] => CNN top1 curve: [83.44, 71.75]
2024-03-06 00:22:30,457 [trainer.py] => CNN top5 curve: [96.5, 89.6]
2024-03-06 00:22:30,457 [trainer.py] => FeCAM top1 curve: [82.9, 72.78]
2024-03-06 00:22:30,457 [trainer.py] => FeCAM top5 curve: [95.84, 91.18]

2024-03-06 00:22:30,468 [fecam.py] => Learning on 60-70
Processing class: 60
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 7.7929561615
Epoch:   200  |  train loss: 7.4886987686
Epoch:   300  |  train loss: 7.1230463028
Epoch:   400  |  train loss: 6.8030728340
Epoch:   500  |  train loss: 6.5498434067
Epoch:   600  |  train loss: 6.3010428429
Epoch:   700  |  train loss: 6.1491771698
Epoch:   800  |  train loss: 5.9014519691
Epoch:   900  |  train loss: 5.7715534210
Epoch:  1000  |  train loss: 5.6481692314
Processing class: 61
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 7.2199785233
Epoch:   200  |  train loss: 6.6292138100
Epoch:   300  |  train loss: 6.1970616341
Epoch:   400  |  train loss: 5.8273725510
Epoch:   500  |  train loss: 5.6282760620
Epoch:   600  |  train loss: 5.4168891907
Epoch:   700  |  train loss: 5.2743020058
Epoch:   800  |  train loss: 5.1470853806
Epoch:   900  |  train loss: 4.9951855659
Epoch:  1000  |  train loss: 4.9189983368
Processing class: 62
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 8.0608851433
Epoch:   200  |  train loss: 7.8618224144
Epoch:   300  |  train loss: 7.7264282227
Epoch:   400  |  train loss: 7.5798125267
Epoch:   500  |  train loss: 7.4144084930
Epoch:   600  |  train loss: 7.3103910446
Epoch:   700  |  train loss: 7.1814347267
Epoch:   800  |  train loss: 7.0367204666
Epoch:   900  |  train loss: 6.8811112404
Epoch:  1000  |  train loss: 6.7664285660
Processing class: 63
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 7.5680640221
Epoch:   200  |  train loss: 7.3530888557
Epoch:   300  |  train loss: 6.8215832710
Epoch:   400  |  train loss: 6.4432024956
Epoch:   500  |  train loss: 6.1644902229
Epoch:   600  |  train loss: 5.9979921341
Epoch:   700  |  train loss: 5.7902609825
Epoch:   800  |  train loss: 5.7248933792
Epoch:   900  |  train loss: 5.5557969093
Epoch:  1000  |  train loss: 5.4627935410
Processing class: 64
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 7.3491547585
Epoch:   200  |  train loss: 6.4569497108
Epoch:   300  |  train loss: 6.0773632050
Epoch:   400  |  train loss: 5.7994043350
Epoch:   500  |  train loss: 5.5006657600
Epoch:   600  |  train loss: 5.2783990860
Epoch:   700  |  train loss: 5.0928756714
Epoch:   800  |  train loss: 4.9870639801
Epoch:   900  |  train loss: 4.9095859528
Epoch:  1000  |  train loss: 4.7734924316
Processing class: 65
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 7.2479628563
Epoch:   200  |  train loss: 6.6335668564
Epoch:   300  |  train loss: 6.0924983025
Epoch:   400  |  train loss: 5.7939827919
Epoch:   500  |  train loss: 5.5595048904
Epoch:   600  |  train loss: 5.4012557030
Epoch:   700  |  train loss: 5.2374164581
Epoch:   800  |  train loss: 5.1364494324
Epoch:   900  |  train loss: 4.9810190201
Epoch:  1000  |  train loss: 4.9120401382
Processing class: 66
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 7.6484380722
Epoch:   200  |  train loss: 7.2868167877
Epoch:   300  |  train loss: 6.9324403763
Epoch:   400  |  train loss: 6.6725998878
Epoch:   500  |  train loss: 6.5411655426
Epoch:   600  |  train loss: 6.3911580086
Epoch:   700  |  train loss: 6.3049917221
Epoch:   800  |  train loss: 6.2174977303
Epoch:   900  |  train loss: 6.1167592049
Epoch:  1000  |  train loss: 6.0369059563
Processing class: 67
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 7.6946134567
Epoch:   200  |  train loss: 7.3904501915
Epoch:   300  |  train loss: 7.0972404480
Epoch:   400  |  train loss: 6.8851729393
Epoch:   500  |  train loss: 6.7059507370
Epoch:   600  |  train loss: 6.5494567871
Epoch:   700  |  train loss: 6.4080255508
Epoch:   800  |  train loss: 6.3015405655
Epoch:   900  |  train loss: 6.1737308502
Epoch:  1000  |  train loss: 6.0686485291
Processing class: 68
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 7.1060070992
Epoch:   200  |  train loss: 6.8612977982
Epoch:   300  |  train loss: 6.3165143013
Epoch:   400  |  train loss: 6.0789610863
Epoch:   500  |  train loss: 5.9120051384
Epoch:   600  |  train loss: 5.7453236580
Epoch:   700  |  train loss: 5.6279631615
Epoch:   800  |  train loss: 5.4976417542
Epoch:   900  |  train loss: 5.3939046860
Epoch:  1000  |  train loss: 5.3434835434
Processing class: 69
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 5.5528792381
Epoch:   200  |  train loss: 5.2153133392
Epoch:   300  |  train loss: 4.8845624924
Epoch:   400  |  train loss: 4.6644258499
Epoch:   500  |  train loss: 4.5170866966
Epoch:   600  |  train loss: 4.4062822342
Epoch:   700  |  train loss: 4.3665266991
Epoch:   800  |  train loss: 4.2948690414
Epoch:   900  |  train loss: 4.2995213509
Epoch:  1000  |  train loss: 4.2046679020
2024-03-06 00:29:00,963 [trainer.py] => CNN: {'total': 64.66, '00-09': 75.0, '10-19': 70.1, '20-29': 77.1, '30-39': 70.6, '40-49': 64.0, '50-59': 45.8, '60-69': 50.0, 'old': 67.1, 'new': 50.0}
2024-03-06 00:29:00,964 [trainer.py] => No NME accuracy
2024-03-06 00:29:00,964 [trainer.py] => FeCAM: {'total': 67.27, '00-09': 79.9, '10-19': 70.6, '20-29': 79.1, '30-39': 71.9, '40-49': 74.1, '50-59': 41.2, '60-69': 54.1, 'old': 69.47, 'new': 54.1}
2024-03-06 00:29:00,964 [trainer.py] => CNN top1 curve: [83.44, 71.75, 64.66]
2024-03-06 00:29:00,964 [trainer.py] => CNN top5 curve: [96.5, 89.6, 86.54]
2024-03-06 00:29:00,964 [trainer.py] => FeCAM top1 curve: [82.9, 72.78, 67.27]
2024-03-06 00:29:00,964 [trainer.py] => FeCAM top5 curve: [95.84, 91.18, 87.21]

2024-03-06 00:29:00,968 [fecam.py] => Learning on 70-80
Processing class: 70
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 7.4179853439
Epoch:   200  |  train loss: 7.0876503944
Epoch:   300  |  train loss: 6.6206140518
Epoch:   400  |  train loss: 6.3734263420
Epoch:   500  |  train loss: 6.0913535118
Epoch:   600  |  train loss: 5.8992928505
Epoch:   700  |  train loss: 5.7150341034
Epoch:   800  |  train loss: 5.4906015396
Epoch:   900  |  train loss: 5.4154461861
Epoch:  1000  |  train loss: 5.3066037178
Processing class: 71
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 7.7858349800
Epoch:   200  |  train loss: 7.5407743454
Epoch:   300  |  train loss: 7.0613056183
Epoch:   400  |  train loss: 6.7904853821
Epoch:   500  |  train loss: 6.5487141609
Epoch:   600  |  train loss: 6.3370632172
Epoch:   700  |  train loss: 6.1361288071
Epoch:   800  |  train loss: 5.9742039680
Epoch:   900  |  train loss: 5.8367900848
Epoch:  1000  |  train loss: 5.6821193695
Processing class: 72
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 7.6248962402
Epoch:   200  |  train loss: 7.3406777382
Epoch:   300  |  train loss: 6.9999887466
Epoch:   400  |  train loss: 6.7093371391
Epoch:   500  |  train loss: 6.5359779358
Epoch:   600  |  train loss: 6.3268950462
Epoch:   700  |  train loss: 6.1816956520
Epoch:   800  |  train loss: 6.0206393242
Epoch:   900  |  train loss: 5.8902767181
Epoch:  1000  |  train loss: 5.7981778145
Processing class: 73
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 7.6280345917
Epoch:   200  |  train loss: 7.2279201508
Epoch:   300  |  train loss: 6.7740738869
Epoch:   400  |  train loss: 6.5235401154
Epoch:   500  |  train loss: 6.4423476219
Epoch:   600  |  train loss: 6.3095263481
Epoch:   700  |  train loss: 6.1540133476
Epoch:   800  |  train loss: 6.0387139320
Epoch:   900  |  train loss: 5.9190175056
Epoch:  1000  |  train loss: 5.8363007545
Processing class: 74
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 8.0049859047
Epoch:   200  |  train loss: 7.9180312157
Epoch:   300  |  train loss: 7.6132058144
Epoch:   400  |  train loss: 7.3164584160
Epoch:   500  |  train loss: 7.0671877861
Epoch:   600  |  train loss: 6.8449582100
Epoch:   700  |  train loss: 6.7321217537
Epoch:   800  |  train loss: 6.5197216988
Epoch:   900  |  train loss: 6.3853308678
Epoch:  1000  |  train loss: 6.2948626518
Processing class: 75
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 7.6884340286
Epoch:   200  |  train loss: 7.3776800156
Epoch:   300  |  train loss: 7.0223204613
Epoch:   400  |  train loss: 6.7873936653
Epoch:   500  |  train loss: 6.6015933037
Epoch:   600  |  train loss: 6.4422830582
Epoch:   700  |  train loss: 6.2130393028
Epoch:   800  |  train loss: 6.1313742638
Epoch:   900  |  train loss: 6.0175380707
Epoch:  1000  |  train loss: 5.9309013367
Processing class: 76
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 5.6584634781
Epoch:   200  |  train loss: 5.1895678520
Epoch:   300  |  train loss: 4.8749582291
Epoch:   400  |  train loss: 4.7227898598
Epoch:   500  |  train loss: 4.6295351982
Epoch:   600  |  train loss: 4.5397287369
Epoch:   700  |  train loss: 4.4963675499
Epoch:   800  |  train loss: 4.3610419273
Epoch:   900  |  train loss: 4.3297191143
Epoch:  1000  |  train loss: 4.2928918839
Processing class: 77
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 7.2111997604
Epoch:   200  |  train loss: 6.8372142792
Epoch:   300  |  train loss: 6.4071941376
Epoch:   400  |  train loss: 6.1362335205
Epoch:   500  |  train loss: 5.8184369087
Epoch:   600  |  train loss: 5.5642981529
Epoch:   700  |  train loss: 5.3854327202
Epoch:   800  |  train loss: 5.2586009026
Epoch:   900  |  train loss: 5.1229640007
Epoch:  1000  |  train loss: 4.9922366142
Processing class: 78
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 7.4530120850
Epoch:   200  |  train loss: 6.8812962532
Epoch:   300  |  train loss: 6.6338733673
Epoch:   400  |  train loss: 6.3426648140
Epoch:   500  |  train loss: 6.0520792007
Epoch:   600  |  train loss: 5.8852199554
Epoch:   700  |  train loss: 5.7444276810
Epoch:   800  |  train loss: 5.6102609634
Epoch:   900  |  train loss: 5.5231653214
Epoch:  1000  |  train loss: 5.4269090652
Processing class: 79
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 8.0421823502
Epoch:   200  |  train loss: 7.7696969986
Epoch:   300  |  train loss: 7.6033185959
Epoch:   400  |  train loss: 7.3502592087
Epoch:   500  |  train loss: 7.1623138428
Epoch:   600  |  train loss: 7.0048686981
Epoch:   700  |  train loss: 6.8331020355
Epoch:   800  |  train loss: 6.6677627563
Epoch:   900  |  train loss: 6.5357133865
Epoch:  1000  |  train loss: 6.3623681068
2024-03-06 00:36:32,291 [trainer.py] => CNN: {'total': 59.18, '00-09': 73.6, '10-19': 68.4, '20-29': 76.9, '30-39': 69.1, '40-49': 60.6, '50-59': 37.2, '60-69': 45.7, '70-79': 41.9, 'old': 61.64, 'new': 41.9}
2024-03-06 00:36:32,293 [trainer.py] => No NME accuracy
2024-03-06 00:36:32,293 [trainer.py] => FeCAM: {'total': 61.72, '00-09': 78.1, '10-19': 69.3, '20-29': 78.0, '30-39': 69.5, '40-49': 72.1, '50-59': 35.7, '60-69': 49.1, '70-79': 42.0, 'old': 64.54, 'new': 42.0}
2024-03-06 00:36:32,293 [trainer.py] => CNN top1 curve: [83.44, 71.75, 64.66, 59.18]
2024-03-06 00:36:32,293 [trainer.py] => CNN top5 curve: [96.5, 89.6, 86.54, 84.09]
2024-03-06 00:36:32,293 [trainer.py] => FeCAM top1 curve: [82.9, 72.78, 67.27, 61.72]
2024-03-06 00:36:32,293 [trainer.py] => FeCAM top5 curve: [95.84, 91.18, 87.21, 84.09]

2024-03-06 00:36:32,302 [fecam.py] => Learning on 80-90
Processing class: 80
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 6.3969552994
Epoch:   200  |  train loss: 5.5447012901
Epoch:   300  |  train loss: 5.2505673409
Epoch:   400  |  train loss: 5.0703017235
Epoch:   500  |  train loss: 4.8970200539
Epoch:   600  |  train loss: 4.8026149750
Epoch:   700  |  train loss: 4.6691391945
Epoch:   800  |  train loss: 4.6513002396
Epoch:   900  |  train loss: 4.5793174744
Epoch:  1000  |  train loss: 4.5306094170
Processing class: 81
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 7.2567086220
Epoch:   200  |  train loss: 6.8083758354
Epoch:   300  |  train loss: 6.4604076385
Epoch:   400  |  train loss: 6.2720252037
Epoch:   500  |  train loss: 6.0581735611
Epoch:   600  |  train loss: 5.9272321701
Epoch:   700  |  train loss: 5.7710667610
Epoch:   800  |  train loss: 5.6910095215
Epoch:   900  |  train loss: 5.5542435646
Epoch:  1000  |  train loss: 5.4324894905
Processing class: 82
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 6.8937809944
Epoch:   200  |  train loss: 6.4008157730
Epoch:   300  |  train loss: 6.0902401924
Epoch:   400  |  train loss: 5.8078395844
Epoch:   500  |  train loss: 5.5814966202
Epoch:   600  |  train loss: 5.4171957970
Epoch:   700  |  train loss: 5.2940640450
Epoch:   800  |  train loss: 5.1968379021
Epoch:   900  |  train loss: 5.0904682159
Epoch:  1000  |  train loss: 4.9915655136
Processing class: 83
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 6.6935266495
Epoch:   200  |  train loss: 5.8806938171
Epoch:   300  |  train loss: 5.4461746216
Epoch:   400  |  train loss: 5.1450258255
Epoch:   500  |  train loss: 4.9738237381
Epoch:   600  |  train loss: 4.8183925629
Epoch:   700  |  train loss: 4.7587810516
Epoch:   800  |  train loss: 4.7005731583
Epoch:   900  |  train loss: 4.5999690056
Epoch:  1000  |  train loss: 4.5142827034
Processing class: 84
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 7.5757370949
Epoch:   200  |  train loss: 6.8799791336
Epoch:   300  |  train loss: 6.5356650352
Epoch:   400  |  train loss: 6.1918791771
Epoch:   500  |  train loss: 5.9761599541
Epoch:   600  |  train loss: 5.7781277657
Epoch:   700  |  train loss: 5.6569269180
Epoch:   800  |  train loss: 5.5661902428
Epoch:   900  |  train loss: 5.4072707176
Epoch:  1000  |  train loss: 5.2927898407
Processing class: 85
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 7.6357378960
Epoch:   200  |  train loss: 7.0278912544
Epoch:   300  |  train loss: 6.6106575966
Epoch:   400  |  train loss: 6.2923813820
Epoch:   500  |  train loss: 6.1052371979
Epoch:   600  |  train loss: 5.9348826408
Epoch:   700  |  train loss: 5.8366815567
Epoch:   800  |  train loss: 5.6828104019
Epoch:   900  |  train loss: 5.5728408813
Epoch:  1000  |  train loss: 5.4935054779
Processing class: 86
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 8.0418186188
Epoch:   200  |  train loss: 7.7126896858
Epoch:   300  |  train loss: 7.3092176437
Epoch:   400  |  train loss: 6.9702603340
Epoch:   500  |  train loss: 6.7433279037
Epoch:   600  |  train loss: 6.5467638969
Epoch:   700  |  train loss: 6.3710032463
Epoch:   800  |  train loss: 6.2150402069
Epoch:   900  |  train loss: 6.0926587105
Epoch:  1000  |  train loss: 6.0228385925
Processing class: 87
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 6.9751547813
Epoch:   200  |  train loss: 6.2940948486
Epoch:   300  |  train loss: 6.0239192009
Epoch:   400  |  train loss: 5.7015388489
Epoch:   500  |  train loss: 5.4941684723
Epoch:   600  |  train loss: 5.4095566750
Epoch:   700  |  train loss: 5.3062796593
Epoch:   800  |  train loss: 5.1951070786
Epoch:   900  |  train loss: 5.0763271332
Epoch:  1000  |  train loss: 5.0362328529
Processing class: 88
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 7.4627935410
Epoch:   200  |  train loss: 7.0851775169
Epoch:   300  |  train loss: 6.6963209152
Epoch:   400  |  train loss: 6.3882488251
Epoch:   500  |  train loss: 6.0815946579
Epoch:   600  |  train loss: 5.8196290970
Epoch:   700  |  train loss: 5.6811531067
Epoch:   800  |  train loss: 5.5364583015
Epoch:   900  |  train loss: 5.4290484428
Epoch:  1000  |  train loss: 5.3020339966
Processing class: 89
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 7.3531745911
Epoch:   200  |  train loss: 6.8798145294
Epoch:   300  |  train loss: 6.4196216583
Epoch:   400  |  train loss: 6.1903440475
Epoch:   500  |  train loss: 5.9692759514
Epoch:   600  |  train loss: 5.7434517860
Epoch:   700  |  train loss: 5.5556685448
Epoch:   800  |  train loss: 5.4661431313
Epoch:   900  |  train loss: 5.3178395271
Epoch:  1000  |  train loss: 5.2140045166
2024-03-06 00:45:24,358 [trainer.py] => CNN: {'total': 54.08, '00-09': 67.9, '10-19': 63.6, '20-29': 73.2, '30-39': 68.8, '40-49': 57.2, '50-59': 32.6, '60-69': 40.9, '70-79': 38.1, '80-89': 44.4, 'old': 55.29, 'new': 44.4}
2024-03-06 00:45:24,359 [trainer.py] => No NME accuracy
2024-03-06 00:45:24,359 [trainer.py] => FeCAM: {'total': 57.66, '00-09': 76.1, '10-19': 65.8, '20-29': 75.3, '30-39': 67.9, '40-49': 68.5, '50-59': 33.0, '60-69': 45.0, '70-79': 39.0, '80-89': 48.3, 'old': 58.82, 'new': 48.3}
2024-03-06 00:45:24,360 [trainer.py] => CNN top1 curve: [83.44, 71.75, 64.66, 59.18, 54.08]
2024-03-06 00:45:24,360 [trainer.py] => CNN top5 curve: [96.5, 89.6, 86.54, 84.09, 81.74]
2024-03-06 00:45:24,360 [trainer.py] => FeCAM top1 curve: [82.9, 72.78, 67.27, 61.72, 57.66]
2024-03-06 00:45:24,360 [trainer.py] => FeCAM top5 curve: [95.84, 91.18, 87.21, 84.09, 81.63]

2024-03-06 00:45:24,369 [fecam.py] => Learning on 90-100
Processing class: 90
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 7.6156896591
Epoch:   200  |  train loss: 7.3097418785
Epoch:   300  |  train loss: 6.7941896439
Epoch:   400  |  train loss: 6.4101949692
Epoch:   500  |  train loss: 6.1031431198
Epoch:   600  |  train loss: 5.8527464867
Epoch:   700  |  train loss: 5.6156083107
Epoch:   800  |  train loss: 5.4739033699
Epoch:   900  |  train loss: 5.3763509750
Epoch:  1000  |  train loss: 5.2885620117
Processing class: 91
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 5.7150341034
Epoch:   200  |  train loss: 5.4083885193
Epoch:   300  |  train loss: 5.1428565979
Epoch:   400  |  train loss: 5.0313877106
Epoch:   500  |  train loss: 4.8521327019
Epoch:   600  |  train loss: 4.7333667755
Epoch:   700  |  train loss: 4.5600453377
Epoch:   800  |  train loss: 4.4282769203
Epoch:   900  |  train loss: 4.3293901443
Epoch:  1000  |  train loss: 4.2657427311
Processing class: 92
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 7.2567009926
Epoch:   200  |  train loss: 6.4783659935
Epoch:   300  |  train loss: 6.1978837013
Epoch:   400  |  train loss: 5.9797688484
Epoch:   500  |  train loss: 5.7115399361
Epoch:   600  |  train loss: 5.4341579437
Epoch:   700  |  train loss: 5.2408472061
Epoch:   800  |  train loss: 5.0577101707
Epoch:   900  |  train loss: 4.9534811020
Epoch:  1000  |  train loss: 4.8533264160
Processing class: 93
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 7.4633002281
Epoch:   200  |  train loss: 6.7893809319
Epoch:   300  |  train loss: 6.3946388245
Epoch:   400  |  train loss: 6.1495065689
Epoch:   500  |  train loss: 5.8836648941
Epoch:   600  |  train loss: 5.6412335396
Epoch:   700  |  train loss: 5.5194951057
Epoch:   800  |  train loss: 5.3648030281
Epoch:   900  |  train loss: 5.2517309189
Epoch:  1000  |  train loss: 5.1103111267
Processing class: 94
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 6.7977180481
Epoch:   200  |  train loss: 6.3707088470
Epoch:   300  |  train loss: 5.8229096413
Epoch:   400  |  train loss: 5.5306558609
Epoch:   500  |  train loss: 5.2706641197
Epoch:   600  |  train loss: 5.1373505592
Epoch:   700  |  train loss: 5.0239315987
Epoch:   800  |  train loss: 4.8255039215
Epoch:   900  |  train loss: 4.7580490112
Epoch:  1000  |  train loss: 4.6454060555
Processing class: 95
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 7.7602071762
Epoch:   200  |  train loss: 7.5632890701
Epoch:   300  |  train loss: 7.2591506004
Epoch:   400  |  train loss: 6.9142506599
Epoch:   500  |  train loss: 6.6553374290
Epoch:   600  |  train loss: 6.4457977295
Epoch:   700  |  train loss: 6.2679223061
Epoch:   800  |  train loss: 6.0937190056
Epoch:   900  |  train loss: 5.9681570053
Epoch:  1000  |  train loss: 5.8623501778
Processing class: 96
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 7.8649239540
Epoch:   200  |  train loss: 7.4909391403
Epoch:   300  |  train loss: 7.0789681435
Epoch:   400  |  train loss: 6.8523083687
Epoch:   500  |  train loss: 6.5879838943
Epoch:   600  |  train loss: 6.3939383507
Epoch:   700  |  train loss: 6.2207845688
Epoch:   800  |  train loss: 6.0368441582
Epoch:   900  |  train loss: 5.9030069351
Epoch:  1000  |  train loss: 5.7630501747
Processing class: 97
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 7.4590894699
Epoch:   200  |  train loss: 6.7986010551
Epoch:   300  |  train loss: 6.3407364845
Epoch:   400  |  train loss: 6.0942110062
Epoch:   500  |  train loss: 5.8913967133
Epoch:   600  |  train loss: 5.7213708878
Epoch:   700  |  train loss: 5.5778285027
Epoch:   800  |  train loss: 5.4419271469
Epoch:   900  |  train loss: 5.3195231438
Epoch:  1000  |  train loss: 5.2020962715
Processing class: 98
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 6.8230480194
Epoch:   200  |  train loss: 6.3591249466
Epoch:   300  |  train loss: 5.8799083710
Epoch:   400  |  train loss: 5.5878055573
Epoch:   500  |  train loss: 5.3441760063
Epoch:   600  |  train loss: 5.1639698029
Epoch:   700  |  train loss: 4.9998035431
Epoch:   800  |  train loss: 4.9147671700
Epoch:   900  |  train loss: 4.8263703346
Epoch:  1000  |  train loss: 4.7471399307
Processing class: 99
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 7.6297237396
Epoch:   200  |  train loss: 7.1912311554
Epoch:   300  |  train loss: 6.6778183937
Epoch:   400  |  train loss: 6.3436288834
Epoch:   500  |  train loss: 6.0375499725
Epoch:   600  |  train loss: 5.8267795563
Epoch:   700  |  train loss: 5.6718263626
Epoch:   800  |  train loss: 5.5664941788
Epoch:   900  |  train loss: 5.4257168770
Epoch:  1000  |  train loss: 5.3204030991
2024-03-06 00:55:34,888 [trainer.py] => CNN: {'total': 50.36, '00-09': 57.7, '10-19': 63.6, '20-29': 71.4, '30-39': 68.5, '40-49': 56.8, '50-59': 29.6, '60-69': 38.9, '70-79': 36.6, '80-89': 42.7, '90-99': 37.8, 'old': 51.76, 'new': 37.8}
2024-03-06 00:55:37,415 [trainer.py] => No NME accuracy
2024-03-06 00:55:37,416 [trainer.py] => FeCAM: {'total': 54.74, '00-09': 73.3, '10-19': 65.1, '20-29': 74.1, '30-39': 67.2, '40-49': 67.1, '50-59': 30.8, '60-69': 43.3, '70-79': 37.2, '80-89': 46.9, '90-99': 42.4, 'old': 56.11, 'new': 42.4}
2024-03-06 00:55:37,418 [trainer.py] => CNN top1 curve: [83.44, 71.75, 64.66, 59.18, 54.08, 50.36]
2024-03-06 00:55:37,418 [trainer.py] => CNN top5 curve: [96.5, 89.6, 86.54, 84.09, 81.74, 79.63]
2024-03-06 00:55:37,418 [trainer.py] => FeCAM top1 curve: [82.9, 72.78, 67.27, 61.72, 57.66, 54.74]
2024-03-06 00:55:37,418 [trainer.py] => FeCAM top5 curve: [95.84, 91.18, 87.21, 84.09, 81.63, 79.21]

=========================================
2024-03-06 00:55:50,488 [trainer.py] => config: ./exps/FeCAM_cifar100.json
2024-03-06 00:55:50,488 [trainer.py] => prefix: train
2024-03-06 00:55:50,488 [trainer.py] => dataset: cifar100
2024-03-06 00:55:50,488 [trainer.py] => memory_size: 0
2024-03-06 00:55:50,488 [trainer.py] => shuffle: True
2024-03-06 00:55:50,488 [trainer.py] => init_cls: 50
2024-03-06 00:55:50,488 [trainer.py] => increment: 10
2024-03-06 00:55:50,488 [trainer.py] => model_name: fecam
2024-03-06 00:55:50,488 [trainer.py] => convnet_type: resnet18
2024-03-06 00:55:50,488 [trainer.py] => device: [device(type='cuda', index=0)]
2024-03-06 00:55:50,489 [trainer.py] => seed: 1993
2024-03-06 00:55:50,489 [trainer.py] => init_epochs: 200
2024-03-06 00:55:50,489 [trainer.py] => init_lr: 0.1
2024-03-06 00:55:50,489 [trainer.py] => init_weight_decay: 0.0005
2024-03-06 00:55:50,489 [trainer.py] => batch_size: 128
2024-03-06 00:55:50,489 [trainer.py] => num_workers: 8
2024-03-06 00:55:50,489 [trainer.py] => T: 5
2024-03-06 00:55:50,489 [trainer.py] => beta: 0.5
2024-03-06 00:55:50,489 [trainer.py] => alpha1: 1
2024-03-06 00:55:50,489 [trainer.py] => alpha2: 1
2024-03-06 00:55:50,489 [trainer.py] => ncm: False
2024-03-06 00:55:50,489 [trainer.py] => tukey: False
2024-03-06 00:55:50,489 [trainer.py] => diagonal: False
2024-03-06 00:55:50,489 [trainer.py] => per_class: True
2024-03-06 00:55:50,489 [trainer.py] => full_cov: True
2024-03-06 00:55:50,489 [trainer.py] => shrink: True
2024-03-06 00:55:50,489 [trainer.py] => norm_cov: False
2024-03-06 00:55:50,489 [trainer.py] => vecnorm: False
2024-03-06 00:55:50,489 [trainer.py] => ae_type: wae
2024-03-06 00:55:50,489 [trainer.py] => epochs: 1000
2024-03-06 00:55:50,489 [trainer.py] => ae_latent_dim: 32
2024-03-06 00:55:50,489 [trainer.py] => wae_sigma: 10
2024-03-06 00:55:50,489 [trainer.py] => wae_C: 10
Files already downloaded and verified
Files already downloaded and verified
2024-03-06 00:55:52,147 [data_manager.py] => [68, 56, 78, 8, 23, 84, 90, 65, 74, 76, 40, 89, 3, 92, 55, 9, 26, 80, 43, 38, 58, 70, 77, 1, 85, 19, 17, 50, 28, 53, 13, 81, 45, 82, 6, 59, 83, 16, 15, 44, 91, 41, 72, 60, 79, 52, 20, 10, 31, 54, 37, 95, 14, 71, 96, 98, 97, 2, 64, 66, 42, 22, 35, 86, 24, 34, 87, 21, 99, 0, 88, 27, 18, 94, 11, 12, 47, 25, 30, 46, 62, 69, 36, 61, 7, 63, 75, 5, 32, 4, 51, 48, 73, 93, 39, 67, 29, 49, 57, 33]
2024-03-06 00:55:52,436 [fecam.py] => Learning on 0-50
/home/z1165703/miniconda3/lib/python3.11/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
/home/z1165703/miniconda3/lib/python3.11/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
Processing class: 0
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 5.3228344917
Epoch:   200  |  train loss: 5.0807981491
Epoch:   300  |  train loss: 5.1007150650
Epoch:   400  |  train loss: 5.0161797523
Epoch:   500  |  train loss: 4.9435167313
Epoch:   600  |  train loss: 4.9305191994
Epoch:   700  |  train loss: 4.9061886787
Epoch:   800  |  train loss: 4.8960962296
Epoch:   900  |  train loss: 4.8422478676
Epoch:  1000  |  train loss: 4.7988644600
Processing class: 1
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 5.6555083275
Epoch:   200  |  train loss: 5.5990096092
Epoch:   300  |  train loss: 5.5275980949
Epoch:   400  |  train loss: 5.4545052528
Epoch:   500  |  train loss: 5.2982922554
Epoch:   600  |  train loss: 5.3271788597
Epoch:   700  |  train loss: 5.2397349358
Epoch:   800  |  train loss: 5.2137875557
Epoch:   900  |  train loss: 5.2155745506
Epoch:  1000  |  train loss: 5.1151285172
Processing class: 2
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 5.8402848244
Epoch:   200  |  train loss: 5.6621467590
Epoch:   300  |  train loss: 5.5413626671
Epoch:   400  |  train loss: 5.3669196129
Epoch:   500  |  train loss: 5.1502899170
Epoch:   600  |  train loss: 5.0880004883
Epoch:   700  |  train loss: 4.9741223335
Epoch:   800  |  train loss: 4.9467870712
Epoch:   900  |  train loss: 4.8288693428
Epoch:  1000  |  train loss: 4.7387851715
Processing class: 3
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 5.3337429047
Epoch:   200  |  train loss: 5.3792537689
Epoch:   300  |  train loss: 5.3038974762
Epoch:   400  |  train loss: 5.1456337929
Epoch:   500  |  train loss: 5.0258629799
Epoch:   600  |  train loss: 5.0276815414
Epoch:   700  |  train loss: 4.9477116585
Epoch:   800  |  train loss: 4.8577014923
Epoch:   900  |  train loss: 4.7916534424
Epoch:  1000  |  train loss: 4.7229440689
Processing class: 4
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 5.6402216911
Epoch:   200  |  train loss: 5.5545779228
Epoch:   300  |  train loss: 5.4111831665
Epoch:   400  |  train loss: 5.2750637054
Epoch:   500  |  train loss: 5.1939898491
Epoch:   600  |  train loss: 5.0524010658
Epoch:   700  |  train loss: 5.0413955688
Epoch:   800  |  train loss: 5.0029932022
Epoch:   900  |  train loss: 4.9605383873
Epoch:  1000  |  train loss: 4.9398598671
Processing class: 5
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 5.8985848427
Epoch:   200  |  train loss: 5.6246842384
Epoch:   300  |  train loss: 5.5015177727
Epoch:   400  |  train loss: 5.3329268456
Epoch:   500  |  train loss: 5.2617633820
Epoch:   600  |  train loss: 5.2512007713
Epoch:   700  |  train loss: 5.1479654312
Epoch:   800  |  train loss: 5.0244064331
Epoch:   900  |  train loss: 4.9328640938
Epoch:  1000  |  train loss: 4.9067422867
Processing class: 6
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 5.6185566902
Epoch:   200  |  train loss: 5.6349080086
Epoch:   300  |  train loss: 5.5122419357
Epoch:   400  |  train loss: 5.3486659050
Epoch:   500  |  train loss: 5.3111431122
Epoch:   600  |  train loss: 5.2448193550
Epoch:   700  |  train loss: 5.1785454750
Epoch:   800  |  train loss: 5.1255920410
Epoch:   900  |  train loss: 5.0012165070
Epoch:  1000  |  train loss: 4.9775757790
Processing class: 7
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 5.8372511864
Epoch:   200  |  train loss: 5.6730738640
Epoch:   300  |  train loss: 5.5857090950
Epoch:   400  |  train loss: 5.4239014626
Epoch:   500  |  train loss: 5.3046263695
Epoch:   600  |  train loss: 5.2106626511
Epoch:   700  |  train loss: 5.1602519989
Epoch:   800  |  train loss: 5.1441252708
Epoch:   900  |  train loss: 5.0192938805
Epoch:  1000  |  train loss: 4.9879891396
Processing class: 8
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 5.7254534721
Epoch:   200  |  train loss: 5.6415582657
Epoch:   300  |  train loss: 5.3910503387
Epoch:   400  |  train loss: 5.3402096748
Epoch:   500  |  train loss: 5.2260274887
Epoch:   600  |  train loss: 5.1289316177
Epoch:   700  |  train loss: 5.1062788963
Epoch:   800  |  train loss: 4.9954027176
Epoch:   900  |  train loss: 4.9650573730
Epoch:  1000  |  train loss: 4.8670971870
Processing class: 9
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 5.5429623604
Epoch:   200  |  train loss: 5.5608901978
Epoch:   300  |  train loss: 5.5473161697
Epoch:   400  |  train loss: 5.3781871796
Epoch:   500  |  train loss: 5.3737911224
Epoch:   600  |  train loss: 5.2955994606
Epoch:   700  |  train loss: 5.2894599915
Epoch:   800  |  train loss: 5.2692533493
Epoch:   900  |  train loss: 5.2343729019
Epoch:  1000  |  train loss: 5.1494718552
Processing class: 10
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 5.7905884743
Epoch:   200  |  train loss: 5.5951517105
Epoch:   300  |  train loss: 5.5234072685
Epoch:   400  |  train loss: 5.4855579376
Epoch:   500  |  train loss: 5.2873845100
Epoch:   600  |  train loss: 5.2303260803
Epoch:   700  |  train loss: 5.1800405502
Epoch:   800  |  train loss: 5.0949978828
Epoch:   900  |  train loss: 5.0659220695
Epoch:  1000  |  train loss: 5.0169867516
Processing class: 11
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 5.9043961525
Epoch:   200  |  train loss: 5.6688166618
Epoch:   300  |  train loss: 5.5316872597
Epoch:   400  |  train loss: 5.4384460449
Epoch:   500  |  train loss: 5.2999363899
Epoch:   600  |  train loss: 5.2644186974
Epoch:   700  |  train loss: 5.2510923386
Epoch:   800  |  train loss: 5.1816527367
Epoch:   900  |  train loss: 5.1150237083
Epoch:  1000  |  train loss: 5.0753668785
Processing class: 12
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 5.6903656960
Epoch:   200  |  train loss: 5.6810220718
Epoch:   300  |  train loss: 5.5304562569
Epoch:   400  |  train loss: 5.3874997139
Epoch:   500  |  train loss: 5.2877038002
Epoch:   600  |  train loss: 5.1511589050
Epoch:   700  |  train loss: 5.0881330490
Epoch:   800  |  train loss: 5.0560501099
Epoch:   900  |  train loss: 5.0300711632
Epoch:  1000  |  train loss: 4.9514751434
Processing class: 13
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 5.6025328636
Epoch:   200  |  train loss: 5.4636220932
Epoch:   300  |  train loss: 5.3370384216
Epoch:   400  |  train loss: 5.1882235527
Epoch:   500  |  train loss: 5.1109637260
Epoch:   600  |  train loss: 5.0238554955
Epoch:   700  |  train loss: 4.9759985924
Epoch:   800  |  train loss: 4.9614069939
Epoch:   900  |  train loss: 4.9396478653
Epoch:  1000  |  train loss: 4.9036410332
Processing class: 14
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 6.0197848320
Epoch:   200  |  train loss: 6.0295237541
Epoch:   300  |  train loss: 5.8992980003
Epoch:   400  |  train loss: 5.7852395058
Epoch:   500  |  train loss: 5.6718675613
Epoch:   600  |  train loss: 5.5810231209
Epoch:   700  |  train loss: 5.5605695724
Epoch:   800  |  train loss: 5.5653501511
Epoch:   900  |  train loss: 5.5077919006
Epoch:  1000  |  train loss: 5.4310868263
Processing class: 15
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 5.5477169991
Epoch:   200  |  train loss: 5.4871698380
Epoch:   300  |  train loss: 5.2989619255
Epoch:   400  |  train loss: 5.2144882202
Epoch:   500  |  train loss: 5.1606767654
Epoch:   600  |  train loss: 5.0636576653
Epoch:   700  |  train loss: 5.0720848083
Epoch:   800  |  train loss: 4.9847430229
Epoch:   900  |  train loss: 5.0114448547
Epoch:  1000  |  train loss: 4.9571887970
Processing class: 16
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 5.6623196602
Epoch:   200  |  train loss: 5.4891546249
Epoch:   300  |  train loss: 5.4177438736
Epoch:   400  |  train loss: 5.3093927383
Epoch:   500  |  train loss: 5.3127809525
Epoch:   600  |  train loss: 5.2306145668
Epoch:   700  |  train loss: 5.1506832123
Epoch:   800  |  train loss: 5.1534642220
Epoch:   900  |  train loss: 5.0441127777
Epoch:  1000  |  train loss: 5.0766822815
Processing class: 17
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 5.5688953400
Epoch:   200  |  train loss: 5.5842620850
Epoch:   300  |  train loss: 5.5958094597
Epoch:   400  |  train loss: 5.4816366196
Epoch:   500  |  train loss: 5.2374891281
Epoch:   600  |  train loss: 5.1924881935
Epoch:   700  |  train loss: 5.1196969986
Epoch:   800  |  train loss: 5.0633409500
Epoch:   900  |  train loss: 5.0210261345
Epoch:  1000  |  train loss: 4.9813599586
Processing class: 18
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 5.5887465477
Epoch:   200  |  train loss: 5.5839295387
Epoch:   300  |  train loss: 5.2835404396
Epoch:   400  |  train loss: 5.1538139343
Epoch:   500  |  train loss: 5.0350644112
Epoch:   600  |  train loss: 4.8854755402
Epoch:   700  |  train loss: 4.8957451820
Epoch:   800  |  train loss: 4.8096439362
Epoch:   900  |  train loss: 4.7674283028
Epoch:  1000  |  train loss: 4.7474730492
Processing class: 19
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 5.4056652069
Epoch:   200  |  train loss: 5.3602902412
Epoch:   300  |  train loss: 5.2299151421
Epoch:   400  |  train loss: 5.1822837830
Epoch:   500  |  train loss: 5.0227462769
Epoch:   600  |  train loss: 4.9811802864
Epoch:   700  |  train loss: 4.9431158066
Epoch:   800  |  train loss: 4.8592776299
Epoch:   900  |  train loss: 4.8709107399
Epoch:  1000  |  train loss: 4.8356991768
Processing class: 20
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 5.6246777534
Epoch:   200  |  train loss: 5.4764113426
Epoch:   300  |  train loss: 5.4510258675
Epoch:   400  |  train loss: 5.3174717903
Epoch:   500  |  train loss: 5.3307816505
Epoch:   600  |  train loss: 5.2253268242
Epoch:   700  |  train loss: 5.1580577850
Epoch:   800  |  train loss: 5.1319443703
Epoch:   900  |  train loss: 5.0951113701
Epoch:  1000  |  train loss: 5.1044171333
Processing class: 21
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 5.6753403664
Epoch:   200  |  train loss: 5.5353518486
Epoch:   300  |  train loss: 5.2402932167
Epoch:   400  |  train loss: 5.0870762825
Epoch:   500  |  train loss: 4.9948047638
Epoch:   600  |  train loss: 4.9483858109
Epoch:   700  |  train loss: 4.8899255753
Epoch:   800  |  train loss: 4.8085381508
Epoch:   900  |  train loss: 4.7847414017
Epoch:  1000  |  train loss: 4.6965705872
Processing class: 22
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 5.8320306778
Epoch:   200  |  train loss: 5.8068041801
Epoch:   300  |  train loss: 5.7768070221
Epoch:   400  |  train loss: 5.6147507668
Epoch:   500  |  train loss: 5.5158030510
Epoch:   600  |  train loss: 5.3872781754
Epoch:   700  |  train loss: 5.3156403542
Epoch:   800  |  train loss: 5.2129141808
Epoch:   900  |  train loss: 5.1394326210
Epoch:  1000  |  train loss: 5.1679769516
Processing class: 23
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 5.5650885582
Epoch:   200  |  train loss: 5.5786343575
Epoch:   300  |  train loss: 5.4996386528
Epoch:   400  |  train loss: 5.3342861176
Epoch:   500  |  train loss: 5.2508245468
Epoch:   600  |  train loss: 5.0975283623
Epoch:   700  |  train loss: 5.0316562653
Epoch:   800  |  train loss: 4.9326026917
Epoch:   900  |  train loss: 4.8999812126
Epoch:  1000  |  train loss: 4.7966822624
Processing class: 24
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 5.6535772324
Epoch:   200  |  train loss: 5.6305466652
Epoch:   300  |  train loss: 5.6005992889
Epoch:   400  |  train loss: 5.4824702263
Epoch:   500  |  train loss: 5.3743293762
Epoch:   600  |  train loss: 5.3339601517
Epoch:   700  |  train loss: 5.1991777420
Epoch:   800  |  train loss: 5.1748485565
Epoch:   900  |  train loss: 5.1554602623
Epoch:  1000  |  train loss: 5.0742619514
Processing class: 25
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 5.7199665070
Epoch:   200  |  train loss: 5.6105751991
Epoch:   300  |  train loss: 5.5417456627
Epoch:   400  |  train loss: 5.3195098877
Epoch:   500  |  train loss: 5.2343695641
Epoch:   600  |  train loss: 5.1294947624
Epoch:   700  |  train loss: 5.1238616943
Epoch:   800  |  train loss: 5.0368587494
Epoch:   900  |  train loss: 4.9550862312
Epoch:  1000  |  train loss: 4.9395573616
Processing class: 26
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 5.5595700264
Epoch:   200  |  train loss: 5.5936839104
Epoch:   300  |  train loss: 5.6000162125
Epoch:   400  |  train loss: 5.5403330803
Epoch:   500  |  train loss: 5.4762799263
Epoch:   600  |  train loss: 5.2616094589
Epoch:   700  |  train loss: 5.2259119034
Epoch:   800  |  train loss: 5.2040929794
Epoch:   900  |  train loss: 5.1232460022
Epoch:  1000  |  train loss: 5.1210490227
Processing class: 27
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 5.8476827621
Epoch:   200  |  train loss: 5.6204526901
Epoch:   300  |  train loss: 5.5811859131
Epoch:   400  |  train loss: 5.4254083633
Epoch:   500  |  train loss: 5.3114183426
Epoch:   600  |  train loss: 5.2630610466
Epoch:   700  |  train loss: 5.2288355827
Epoch:   800  |  train loss: 5.1739294052
Epoch:   900  |  train loss: 5.1245236397
Epoch:  1000  |  train loss: 4.9898707390
Processing class: 28
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 5.7294432640
Epoch:   200  |  train loss: 5.7328497887
Epoch:   300  |  train loss: 5.5889552116
Epoch:   400  |  train loss: 5.4721415520
Epoch:   500  |  train loss: 5.3825143814
Epoch:   600  |  train loss: 5.3343236923
Epoch:   700  |  train loss: 5.3152946472
Epoch:   800  |  train loss: 5.2877058983
Epoch:   900  |  train loss: 5.2636787415
Epoch:  1000  |  train loss: 5.1919132233
Processing class: 29
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 5.6982253075
Epoch:   200  |  train loss: 5.5298673630
Epoch:   300  |  train loss: 5.3638401031
Epoch:   400  |  train loss: 5.1758383751
Epoch:   500  |  train loss: 5.1248888016
Epoch:   600  |  train loss: 5.0654062271
Epoch:   700  |  train loss: 4.9706980705
Epoch:   800  |  train loss: 4.8538389206
Epoch:   900  |  train loss: 4.8040410995
Epoch:  1000  |  train loss: 4.7437421799
Processing class: 30
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 5.5447006226
Epoch:   200  |  train loss: 5.5556680679
Epoch:   300  |  train loss: 5.3909511566
Epoch:   400  |  train loss: 5.3781667709
Epoch:   500  |  train loss: 5.3350929260
Epoch:   600  |  train loss: 5.2600892067
Epoch:   700  |  train loss: 5.1595644951
Epoch:   800  |  train loss: 5.1480938911
Epoch:   900  |  train loss: 5.1290411949
Epoch:  1000  |  train loss: 5.0521256447
Processing class: 31
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 5.7569400787
Epoch:   200  |  train loss: 5.6333506584
Epoch:   300  |  train loss: 5.4819689751
Epoch:   400  |  train loss: 5.4921887398
Epoch:   500  |  train loss: 5.4178277016
Epoch:   600  |  train loss: 5.3975147247
Epoch:   700  |  train loss: 5.3228231430
Epoch:   800  |  train loss: 5.2904193878
Epoch:   900  |  train loss: 5.2659138680
Epoch:  1000  |  train loss: 5.1728582382
Processing class: 32
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 5.9284930229
Epoch:   200  |  train loss: 5.9172993660
Epoch:   300  |  train loss: 5.7840204239
Epoch:   400  |  train loss: 5.6006293297
Epoch:   500  |  train loss: 5.5391178131
Epoch:   600  |  train loss: 5.3707620621
Epoch:   700  |  train loss: 5.3350319862
Epoch:   800  |  train loss: 5.2889864922
Epoch:   900  |  train loss: 5.2157427788
Epoch:  1000  |  train loss: 5.1449749947
Processing class: 33
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 5.6761991501
Epoch:   200  |  train loss: 5.7343089104
Epoch:   300  |  train loss: 5.4358393669
Epoch:   400  |  train loss: 5.2882576942
Epoch:   500  |  train loss: 5.1892434120
Epoch:   600  |  train loss: 5.1533396721
Epoch:   700  |  train loss: 5.0523405075
Epoch:   800  |  train loss: 5.0088101387
Epoch:   900  |  train loss: 4.9410454750
Epoch:  1000  |  train loss: 4.8633872032
Processing class: 34
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 5.8114938736
Epoch:   200  |  train loss: 5.7364809036
Epoch:   300  |  train loss: 5.5626261711
Epoch:   400  |  train loss: 5.4429185867
Epoch:   500  |  train loss: 5.3151746750
Epoch:   600  |  train loss: 5.2566441536
Epoch:   700  |  train loss: 5.1329600334
Epoch:   800  |  train loss: 5.1114120483
Epoch:   900  |  train loss: 5.0660153389
Epoch:  1000  |  train loss: 5.0199913025
Processing class: 35
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 6.0990861893
Epoch:   200  |  train loss: 5.8639640808
Epoch:   300  |  train loss: 5.7820020676
Epoch:   400  |  train loss: 5.7153786659
Epoch:   500  |  train loss: 5.5780884743
Epoch:   600  |  train loss: 5.5712701797
Epoch:   700  |  train loss: 5.5083413124
Epoch:   800  |  train loss: 5.4419562340
Epoch:   900  |  train loss: 5.3997255325
Epoch:  1000  |  train loss: 5.3259197235
Processing class: 36
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 5.3856818199
Epoch:   200  |  train loss: 5.2897686005
Epoch:   300  |  train loss: 5.1889575958
Epoch:   400  |  train loss: 5.1322497368
Epoch:   500  |  train loss: 5.0859771729
Epoch:   600  |  train loss: 4.9597863197
Epoch:   700  |  train loss: 4.8458119392
Epoch:   800  |  train loss: 4.8368327141
Epoch:   900  |  train loss: 4.7178364754
Epoch:  1000  |  train loss: 4.6930511475
Processing class: 37
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 5.9501152992
Epoch:   200  |  train loss: 5.8559403419
Epoch:   300  |  train loss: 5.6784107208
Epoch:   400  |  train loss: 5.5666699409
Epoch:   500  |  train loss: 5.4914502144
Epoch:   600  |  train loss: 5.3766759872
Epoch:   700  |  train loss: 5.3082268715
Epoch:   800  |  train loss: 5.2768022537
Epoch:   900  |  train loss: 5.1707371712
Epoch:  1000  |  train loss: 5.1444211006
Processing class: 38
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 5.5213065147
Epoch:   200  |  train loss: 5.5067432404
Epoch:   300  |  train loss: 5.4615432739
Epoch:   400  |  train loss: 5.4352694511
Epoch:   500  |  train loss: 5.3185977936
Epoch:   600  |  train loss: 5.1946810722
Epoch:   700  |  train loss: 5.0944910049
Epoch:   800  |  train loss: 5.0968368530
Epoch:   900  |  train loss: 4.9995113373
Epoch:  1000  |  train loss: 4.9992962837
Processing class: 39
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 5.7012987137
Epoch:   200  |  train loss: 5.6026667595
Epoch:   300  |  train loss: 5.5256864548
Epoch:   400  |  train loss: 5.4380371094
Epoch:   500  |  train loss: 5.3040636063
Epoch:   600  |  train loss: 5.2501251221
Epoch:   700  |  train loss: 5.1878012657
Epoch:   800  |  train loss: 5.1427342415
Epoch:   900  |  train loss: 5.0768102646
Epoch:  1000  |  train loss: 5.0237271309
Processing class: 40
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 5.6258354187
Epoch:   200  |  train loss: 5.3894433022
Epoch:   300  |  train loss: 5.4036081314
Epoch:   400  |  train loss: 5.3404829025
Epoch:   500  |  train loss: 5.2893765450
Epoch:   600  |  train loss: 5.1304964066
Epoch:   700  |  train loss: 5.1131863594
Epoch:   800  |  train loss: 5.0592138290
Epoch:   900  |  train loss: 4.9971946716
Epoch:  1000  |  train loss: 5.0210616112
Processing class: 41
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 5.4029416084
Epoch:   200  |  train loss: 5.4259017944
Epoch:   300  |  train loss: 5.2361191750
Epoch:   400  |  train loss: 5.0624105453
Epoch:   500  |  train loss: 4.9778519630
Epoch:   600  |  train loss: 4.9212514877
Epoch:   700  |  train loss: 4.9010727882
Epoch:   800  |  train loss: 4.8364147186
Epoch:   900  |  train loss: 4.7827683449
Epoch:  1000  |  train loss: 4.7436238289
Processing class: 42
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 5.7915518761
Epoch:   200  |  train loss: 5.6891748428
Epoch:   300  |  train loss: 5.5627085686
Epoch:   400  |  train loss: 5.4227183342
Epoch:   500  |  train loss: 5.3220776558
Epoch:   600  |  train loss: 5.2148713112
Epoch:   700  |  train loss: 5.1523565292
Epoch:   800  |  train loss: 5.0746382713
Epoch:   900  |  train loss: 4.9745946884
Epoch:  1000  |  train loss: 4.9087245941
Processing class: 43
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 5.9033939362
Epoch:   200  |  train loss: 5.7606066704
Epoch:   300  |  train loss: 5.5997819901
Epoch:   400  |  train loss: 5.6028538704
Epoch:   500  |  train loss: 5.5653424263
Epoch:   600  |  train loss: 5.5274338722
Epoch:   700  |  train loss: 5.5336786270
Epoch:   800  |  train loss: 5.4887578964
Epoch:   900  |  train loss: 5.3957685471
Epoch:  1000  |  train loss: 5.4084147453
Processing class: 44
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 5.8979378700
Epoch:   200  |  train loss: 5.7395590782
Epoch:   300  |  train loss: 5.6413343430
Epoch:   400  |  train loss: 5.5092442513
Epoch:   500  |  train loss: 5.4612377167
Epoch:   600  |  train loss: 5.3886990547
Epoch:   700  |  train loss: 5.3730609894
Epoch:   800  |  train loss: 5.3005130768
Epoch:   900  |  train loss: 5.2160430908
Epoch:  1000  |  train loss: 5.1997622490
Processing class: 45
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 5.6536128998
Epoch:   200  |  train loss: 5.5913315773
Epoch:   300  |  train loss: 5.3943508148
Epoch:   400  |  train loss: 5.4637223244
Epoch:   500  |  train loss: 5.4262420654
Epoch:   600  |  train loss: 5.4221243858
Epoch:   700  |  train loss: 5.3696403503
Epoch:   800  |  train loss: 5.3219117165
Epoch:   900  |  train loss: 5.3169217110
Epoch:  1000  |  train loss: 5.3148513794
Processing class: 46
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 5.5057540894
Epoch:   200  |  train loss: 5.4247220039
Epoch:   300  |  train loss: 5.2892733574
Epoch:   400  |  train loss: 5.2920245171
Epoch:   500  |  train loss: 5.2593473434
Epoch:   600  |  train loss: 5.2399786949
Epoch:   700  |  train loss: 5.2083141327
Epoch:   800  |  train loss: 5.2078694344
Epoch:   900  |  train loss: 5.1279444695
Epoch:  1000  |  train loss: 5.1224525452
Processing class: 47
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 5.8336541176
Epoch:   200  |  train loss: 5.7374127388
Epoch:   300  |  train loss: 5.4778849602
Epoch:   400  |  train loss: 5.3971621513
Epoch:   500  |  train loss: 5.2995171547
Epoch:   600  |  train loss: 5.1868067741
Epoch:   700  |  train loss: 5.1087851524
Epoch:   800  |  train loss: 5.0095726967
Epoch:   900  |  train loss: 4.9630164146
Epoch:  1000  |  train loss: 4.8963213921
Processing class: 48
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 5.7963252068
Epoch:   200  |  train loss: 5.7761827469
Epoch:   300  |  train loss: 5.4779474258
Epoch:   400  |  train loss: 5.3216002464
Epoch:   500  |  train loss: 5.3703487396
Epoch:   600  |  train loss: 5.2809369087
Epoch:   700  |  train loss: 5.1939411163
Epoch:   800  |  train loss: 5.1630142212
Epoch:   900  |  train loss: 5.1985589981
Epoch:  1000  |  train loss: 5.1649132729
Processing class: 49
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 5.8564679146
Epoch:   200  |  train loss: 5.7480391502
Epoch:   300  |  train loss: 5.6476864815
Epoch:   400  |  train loss: 5.5482100487
Epoch:   500  |  train loss: 5.4422677040
Epoch:   600  |  train loss: 5.3832010269
Epoch:   700  |  train loss: 5.3009978294
Epoch:   800  |  train loss: 5.2341174126
Epoch:   900  |  train loss: 5.1777286530
Epoch:  1000  |  train loss: 5.1366027832
/home/z1165703/FeCAM/models/base.py:87: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  vec = torch.tensor(vec).cuda()
2024-03-06 01:13:31,056 [trainer.py] => CNN: {'total': 83.44, '00-09': 87.7, '10-19': 79.6, '20-29': 84.9, '30-39': 81.0, '40-49': 84.0, 'old': 0, 'new': 83.44}
2024-03-06 01:13:33,241 [trainer.py] => No NME accuracy
2024-03-06 01:13:33,241 [trainer.py] => FeCAM: {'total': 82.9, '00-09': 86.2, '10-19': 79.7, '20-29': 85.0, '30-39': 80.6, '40-49': 83.0, 'old': 0, 'new': 82.9}
2024-03-06 01:13:33,241 [trainer.py] => CNN top1 curve: [83.44]
2024-03-06 01:13:33,241 [trainer.py] => CNN top5 curve: [96.5]
2024-03-06 01:13:33,241 [trainer.py] => FeCAM top1 curve: [82.9]
2024-03-06 01:13:33,241 [trainer.py] => FeCAM top5 curve: [95.84]

2024-03-06 01:13:33,258 [fecam.py] => Learning on 50-60
Processing class: 50
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 6.2689753532
Epoch:   200  |  train loss: 6.0722729683
Epoch:   300  |  train loss: 5.9431638718
Epoch:   400  |  train loss: 5.8480489731
Epoch:   500  |  train loss: 5.7125467300
Epoch:   600  |  train loss: 5.5662199020
Epoch:   700  |  train loss: 5.5119959831
Epoch:   800  |  train loss: 5.4519434929
Epoch:   900  |  train loss: 5.4200445175
Epoch:  1000  |  train loss: 5.3702901840
Processing class: 51
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 6.3210518837
Epoch:   200  |  train loss: 6.0561392784
Epoch:   300  |  train loss: 5.9783648491
Epoch:   400  |  train loss: 5.8797447205
Epoch:   500  |  train loss: 5.7203751564
Epoch:   600  |  train loss: 5.5977879524
Epoch:   700  |  train loss: 5.4581083298
Epoch:   800  |  train loss: 5.3977578163
Epoch:   900  |  train loss: 5.3892337799
Epoch:  1000  |  train loss: 5.2789048195
Processing class: 52
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 6.5288069725
Epoch:   200  |  train loss: 6.4207424164
Epoch:   300  |  train loss: 6.3098411560
Epoch:   400  |  train loss: 6.2102461815
Epoch:   500  |  train loss: 6.0874377251
Epoch:   600  |  train loss: 5.9467708588
Epoch:   700  |  train loss: 5.9057799339
Epoch:   800  |  train loss: 5.8233471870
Epoch:   900  |  train loss: 5.7476259232
Epoch:  1000  |  train loss: 5.6346893311
Processing class: 53
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 6.0303519249
Epoch:   200  |  train loss: 5.8089755058
Epoch:   300  |  train loss: 5.6591125488
Epoch:   400  |  train loss: 5.5495944977
Epoch:   500  |  train loss: 5.5120493889
Epoch:   600  |  train loss: 5.4900668144
Epoch:   700  |  train loss: 5.4649410248
Epoch:   800  |  train loss: 5.3123608589
Epoch:   900  |  train loss: 5.3141297340
Epoch:  1000  |  train loss: 5.2272936821
Processing class: 54
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 5.6687088966
Epoch:   200  |  train loss: 5.4844552040
Epoch:   300  |  train loss: 5.3178149223
Epoch:   400  |  train loss: 5.2235754967
Epoch:   500  |  train loss: 5.1467294693
Epoch:   600  |  train loss: 5.1130425453
Epoch:   700  |  train loss: 5.0496840477
Epoch:   800  |  train loss: 5.0258070946
Epoch:   900  |  train loss: 4.9997706413
Epoch:  1000  |  train loss: 4.9154167175
Processing class: 55
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 6.5725779533
Epoch:   200  |  train loss: 6.5122478485
Epoch:   300  |  train loss: 6.4008565903
Epoch:   400  |  train loss: 6.3713147163
Epoch:   500  |  train loss: 6.3513019562
Epoch:   600  |  train loss: 6.2574576378
Epoch:   700  |  train loss: 6.1927955627
Epoch:   800  |  train loss: 6.1645419121
Epoch:   900  |  train loss: 6.0030009270
Epoch:  1000  |  train loss: 5.9719429970
Processing class: 56
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 6.1846513748
Epoch:   200  |  train loss: 5.9193390846
Epoch:   300  |  train loss: 5.5599457741
Epoch:   400  |  train loss: 5.4360972404
Epoch:   500  |  train loss: 5.2865412712
Epoch:   600  |  train loss: 5.1510245323
Epoch:   700  |  train loss: 5.0579651833
Epoch:   800  |  train loss: 4.9356166840
Epoch:   900  |  train loss: 4.8990078926
Epoch:  1000  |  train loss: 4.8545395851
Processing class: 57
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 6.5328465462
Epoch:   200  |  train loss: 6.5184680939
Epoch:   300  |  train loss: 6.4090703011
Epoch:   400  |  train loss: 6.3110692024
Epoch:   500  |  train loss: 6.2472889900
Epoch:   600  |  train loss: 6.1174028397
Epoch:   700  |  train loss: 6.1164198875
Epoch:   800  |  train loss: 6.0319388390
Epoch:   900  |  train loss: 5.9905712128
Epoch:  1000  |  train loss: 5.8408970833
Processing class: 58
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 6.3036098480
Epoch:   200  |  train loss: 5.8685336113
Epoch:   300  |  train loss: 5.6935354233
Epoch:   400  |  train loss: 5.5342852592
Epoch:   500  |  train loss: 5.4049913406
Epoch:   600  |  train loss: 5.3525854111
Epoch:   700  |  train loss: 5.2377926826
Epoch:   800  |  train loss: 5.2011743546
Epoch:   900  |  train loss: 5.1269979477
Epoch:  1000  |  train loss: 5.0531830788
Processing class: 59
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 6.5329770088
Epoch:   200  |  train loss: 6.4581509590
Epoch:   300  |  train loss: 6.2286604881
Epoch:   400  |  train loss: 6.0776727676
Epoch:   500  |  train loss: 5.9785655975
Epoch:   600  |  train loss: 5.8950609207
Epoch:   700  |  train loss: 5.8356904984
Epoch:   800  |  train loss: 5.7733889580
Epoch:   900  |  train loss: 5.6944598198
Epoch:  1000  |  train loss: 5.6377438545
2024-03-06 01:19:17,897 [trainer.py] => CNN: {'total': 71.75, '00-09': 82.2, '10-19': 72.8, '20-29': 78.5, '30-39': 74.9, '40-49': 68.4, '50-59': 53.7, 'old': 75.36, 'new': 53.7}
2024-03-06 01:19:17,897 [trainer.py] => No NME accuracy
2024-03-06 01:19:17,898 [trainer.py] => FeCAM: {'total': 72.78, '00-09': 82.6, '10-19': 74.0, '20-29': 81.0, '30-39': 76.4, '40-49': 76.8, '50-59': 45.9, 'old': 78.16, 'new': 45.9}
2024-03-06 01:19:17,898 [trainer.py] => CNN top1 curve: [83.44, 71.75]
2024-03-06 01:19:17,898 [trainer.py] => CNN top5 curve: [96.5, 89.6]
2024-03-06 01:19:17,898 [trainer.py] => FeCAM top1 curve: [82.9, 72.78]
2024-03-06 01:19:17,898 [trainer.py] => FeCAM top5 curve: [95.84, 91.18]

2024-03-06 01:19:17,902 [fecam.py] => Learning on 60-70
Processing class: 60
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 6.5132063866
Epoch:   200  |  train loss: 6.3695812225
Epoch:   300  |  train loss: 6.1615102768
Epoch:   400  |  train loss: 5.9876561165
Epoch:   500  |  train loss: 5.8899621964
Epoch:   600  |  train loss: 5.7529513359
Epoch:   700  |  train loss: 5.7457793236
Epoch:   800  |  train loss: 5.5533322334
Epoch:   900  |  train loss: 5.4829195023
Epoch:  1000  |  train loss: 5.4299159050
Processing class: 61
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 6.1568121910
Epoch:   200  |  train loss: 5.8244836807
Epoch:   300  |  train loss: 5.6242691040
Epoch:   400  |  train loss: 5.4153911591
Epoch:   500  |  train loss: 5.3433116913
Epoch:   600  |  train loss: 5.1837757111
Epoch:   700  |  train loss: 5.1303469658
Epoch:   800  |  train loss: 5.0770477295
Epoch:   900  |  train loss: 4.9411312103
Epoch:  1000  |  train loss: 4.9129124641
Processing class: 62
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 6.6200515747
Epoch:   200  |  train loss: 6.5068035126
Epoch:   300  |  train loss: 6.4787878990
Epoch:   400  |  train loss: 6.4240018845
Epoch:   500  |  train loss: 6.3192115784
Epoch:   600  |  train loss: 6.3123025894
Epoch:   700  |  train loss: 6.2668129921
Epoch:   800  |  train loss: 6.1993106842
Epoch:   900  |  train loss: 6.1029677391
Epoch:  1000  |  train loss: 6.0585256577
Processing class: 63
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 6.3187613487
Epoch:   200  |  train loss: 6.2348064423
Epoch:   300  |  train loss: 6.0354253769
Epoch:   400  |  train loss: 5.8363466263
Epoch:   500  |  train loss: 5.6945441246
Epoch:   600  |  train loss: 5.6418902397
Epoch:   700  |  train loss: 5.4743771553
Epoch:   800  |  train loss: 5.4987642288
Epoch:   900  |  train loss: 5.3580746651
Epoch:  1000  |  train loss: 5.3162961006
Processing class: 64
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 6.2975011826
Epoch:   200  |  train loss: 5.7964161873
Epoch:   300  |  train loss: 5.6472334862
Epoch:   400  |  train loss: 5.4914491653
Epoch:   500  |  train loss: 5.3108748436
Epoch:   600  |  train loss: 5.1735645294
Epoch:   700  |  train loss: 5.0275989532
Epoch:   800  |  train loss: 4.9803174019
Epoch:   900  |  train loss: 4.9619604111
Epoch:  1000  |  train loss: 4.8611859322
Processing class: 65
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 6.1874723434
Epoch:   200  |  train loss: 5.8537356377
Epoch:   300  |  train loss: 5.6253063202
Epoch:   400  |  train loss: 5.4764454842
Epoch:   500  |  train loss: 5.3244853020
Epoch:   600  |  train loss: 5.2772543907
Epoch:   700  |  train loss: 5.1801872253
Epoch:   800  |  train loss: 5.1597481728
Epoch:   900  |  train loss: 5.0372153282
Epoch:  1000  |  train loss: 5.0357538223
Processing class: 66
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 6.4360106468
Epoch:   200  |  train loss: 6.2843069077
Epoch:   300  |  train loss: 6.1289254189
Epoch:   400  |  train loss: 5.9885728836
Epoch:   500  |  train loss: 5.9525109291
Epoch:   600  |  train loss: 5.8715688705
Epoch:   700  |  train loss: 5.8570658684
Epoch:   800  |  train loss: 5.8121158600
Epoch:   900  |  train loss: 5.7488959312
Epoch:  1000  |  train loss: 5.7019054413
Processing class: 67
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 6.3845306396
Epoch:   200  |  train loss: 6.2726432800
Epoch:   300  |  train loss: 6.1255056381
Epoch:   400  |  train loss: 6.0257663727
Epoch:   500  |  train loss: 5.9301544189
Epoch:   600  |  train loss: 5.8496064186
Epoch:   700  |  train loss: 5.7854494095
Epoch:   800  |  train loss: 5.7493173599
Epoch:   900  |  train loss: 5.6731934547
Epoch:  1000  |  train loss: 5.6119029045
Processing class: 68
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 6.0759191513
Epoch:   200  |  train loss: 5.9566668510
Epoch:   300  |  train loss: 5.6441683769
Epoch:   400  |  train loss: 5.5212942123
Epoch:   500  |  train loss: 5.4326449394
Epoch:   600  |  train loss: 5.3284380913
Epoch:   700  |  train loss: 5.2770391464
Epoch:   800  |  train loss: 5.1831281662
Epoch:   900  |  train loss: 5.1280266762
Epoch:  1000  |  train loss: 5.1404806137
Processing class: 69
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 5.1812221527
Epoch:   200  |  train loss: 5.0075438499
Epoch:   300  |  train loss: 4.8497135162
Epoch:   400  |  train loss: 4.7300064087
Epoch:   500  |  train loss: 4.6355557442
Epoch:   600  |  train loss: 4.5316669464
Epoch:   700  |  train loss: 4.5302077293
Epoch:   800  |  train loss: 4.4808454514
Epoch:   900  |  train loss: 4.5292494774
Epoch:  1000  |  train loss: 4.4219063759
2024-03-06 01:25:49,981 [trainer.py] => CNN: {'total': 64.66, '00-09': 75.0, '10-19': 70.1, '20-29': 77.1, '30-39': 70.6, '40-49': 64.0, '50-59': 45.8, '60-69': 50.0, 'old': 67.1, 'new': 50.0}
2024-03-06 01:25:49,981 [trainer.py] => No NME accuracy
2024-03-06 01:25:49,982 [trainer.py] => FeCAM: {'total': 67.27, '00-09': 79.9, '10-19': 70.6, '20-29': 79.1, '30-39': 71.9, '40-49': 74.1, '50-59': 41.2, '60-69': 54.1, 'old': 69.47, 'new': 54.1}
2024-03-06 01:25:49,982 [trainer.py] => CNN top1 curve: [83.44, 71.75, 64.66]
2024-03-06 01:25:49,982 [trainer.py] => CNN top5 curve: [96.5, 89.6, 86.54]
2024-03-06 01:25:49,982 [trainer.py] => FeCAM top1 curve: [82.9, 72.78, 67.27]
2024-03-06 01:25:49,982 [trainer.py] => FeCAM top5 curve: [95.84, 91.18, 87.21]

2024-03-06 01:25:49,986 [fecam.py] => Learning on 70-80
Processing class: 70
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 6.2581965446
Epoch:   200  |  train loss: 6.0778866768
Epoch:   300  |  train loss: 5.8518980026
Epoch:   400  |  train loss: 5.7404503822
Epoch:   500  |  train loss: 5.5581937790
Epoch:   600  |  train loss: 5.4869053841
Epoch:   700  |  train loss: 5.4157214165
Epoch:   800  |  train loss: 5.2373350143
Epoch:   900  |  train loss: 5.2230525970
Epoch:  1000  |  train loss: 5.1442831039
Processing class: 71
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 6.4670101166
Epoch:   200  |  train loss: 6.3729499817
Epoch:   300  |  train loss: 6.1080297470
Epoch:   400  |  train loss: 5.9994224548
Epoch:   500  |  train loss: 5.8922155380
Epoch:   600  |  train loss: 5.7829915047
Epoch:   700  |  train loss: 5.6783363342
Epoch:   800  |  train loss: 5.5976631165
Epoch:   900  |  train loss: 5.5506372452
Epoch:  1000  |  train loss: 5.4527384758
Processing class: 72
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 6.3505288124
Epoch:   200  |  train loss: 6.2318098068
Epoch:   300  |  train loss: 6.0732787132
Epoch:   400  |  train loss: 5.9184091568
Epoch:   500  |  train loss: 5.8452353477
Epoch:   600  |  train loss: 5.7101075172
Epoch:   700  |  train loss: 5.6465337753
Epoch:   800  |  train loss: 5.5471319199
Epoch:   900  |  train loss: 5.4832780838
Epoch:  1000  |  train loss: 5.4577491760
Processing class: 73
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 6.4074870110
Epoch:   200  |  train loss: 6.2049429893
Epoch:   300  |  train loss: 6.0239093781
Epoch:   400  |  train loss: 5.8687490463
Epoch:   500  |  train loss: 5.8804435730
Epoch:   600  |  train loss: 5.8070085526
Epoch:   700  |  train loss: 5.6931259155
Epoch:   800  |  train loss: 5.6334723473
Epoch:   900  |  train loss: 5.5667706490
Epoch:  1000  |  train loss: 5.5351013184
Processing class: 74
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 6.6045468330
Epoch:   200  |  train loss: 6.5930319786
Epoch:   300  |  train loss: 6.4115049362
Epoch:   400  |  train loss: 6.2489006042
Epoch:   500  |  train loss: 6.1494125366
Epoch:   600  |  train loss: 6.0218909264
Epoch:   700  |  train loss: 6.0178716660
Epoch:   800  |  train loss: 5.8559579849
Epoch:   900  |  train loss: 5.7910475731
Epoch:  1000  |  train loss: 5.7785191536
Processing class: 75
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 6.4070532799
Epoch:   200  |  train loss: 6.2520308495
Epoch:   300  |  train loss: 6.1208127975
Epoch:   400  |  train loss: 5.9908588409
Epoch:   500  |  train loss: 5.9242740631
Epoch:   600  |  train loss: 5.8684041977
Epoch:   700  |  train loss: 5.6649715424
Epoch:   800  |  train loss: 5.6712058067
Epoch:   900  |  train loss: 5.6021348953
Epoch:  1000  |  train loss: 5.5836026192
Processing class: 76
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 5.1630008698
Epoch:   200  |  train loss: 4.8938708305
Epoch:   300  |  train loss: 4.7552058220
Epoch:   400  |  train loss: 4.6913455963
Epoch:   500  |  train loss: 4.6464620590
Epoch:   600  |  train loss: 4.6143181801
Epoch:   700  |  train loss: 4.5977811813
Epoch:   800  |  train loss: 4.4773005486
Epoch:   900  |  train loss: 4.4947340965
Epoch:  1000  |  train loss: 4.4906169891
Processing class: 77
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 6.1542073250
Epoch:   200  |  train loss: 5.9702360153
Epoch:   300  |  train loss: 5.7116239548
Epoch:   400  |  train loss: 5.5796117783
Epoch:   500  |  train loss: 5.4029251099
Epoch:   600  |  train loss: 5.2552057266
Epoch:   700  |  train loss: 5.1593742371
Epoch:   800  |  train loss: 5.1154734612
Epoch:   900  |  train loss: 5.0175672531
Epoch:  1000  |  train loss: 4.9343082428
Processing class: 78
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 6.3174254417
Epoch:   200  |  train loss: 6.0621056557
Epoch:   300  |  train loss: 5.9578495026
Epoch:   400  |  train loss: 5.7976440430
Epoch:   500  |  train loss: 5.6265035629
Epoch:   600  |  train loss: 5.5603658676
Epoch:   700  |  train loss: 5.4821687698
Epoch:   800  |  train loss: 5.4113878250
Epoch:   900  |  train loss: 5.3615426064
Epoch:  1000  |  train loss: 5.2989782333
Processing class: 79
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 6.5915070534
Epoch:   200  |  train loss: 6.4473718643
Epoch:   300  |  train loss: 6.4153336525
Epoch:   400  |  train loss: 6.2915709496
Epoch:   500  |  train loss: 6.2043925285
Epoch:   600  |  train loss: 6.1624395370
Epoch:   700  |  train loss: 6.0773645401
Epoch:   800  |  train loss: 6.0069032669
Epoch:   900  |  train loss: 5.9667475700
Epoch:  1000  |  train loss: 5.8458554268
2024-03-06 01:33:24,942 [trainer.py] => CNN: {'total': 59.18, '00-09': 73.6, '10-19': 68.4, '20-29': 76.9, '30-39': 69.1, '40-49': 60.6, '50-59': 37.2, '60-69': 45.7, '70-79': 41.9, 'old': 61.64, 'new': 41.9}
2024-03-06 01:33:24,942 [trainer.py] => No NME accuracy
2024-03-06 01:33:24,942 [trainer.py] => FeCAM: {'total': 61.72, '00-09': 78.1, '10-19': 69.3, '20-29': 78.0, '30-39': 69.5, '40-49': 72.1, '50-59': 35.7, '60-69': 49.1, '70-79': 42.0, 'old': 64.54, 'new': 42.0}
2024-03-06 01:33:24,943 [trainer.py] => CNN top1 curve: [83.44, 71.75, 64.66, 59.18]
2024-03-06 01:33:24,943 [trainer.py] => CNN top5 curve: [96.5, 89.6, 86.54, 84.09]
2024-03-06 01:33:24,943 [trainer.py] => FeCAM top1 curve: [82.9, 72.78, 67.27, 61.72]
2024-03-06 01:33:24,943 [trainer.py] => FeCAM top5 curve: [95.84, 91.18, 87.21, 84.09]

2024-03-06 01:33:24,948 [fecam.py] => Learning on 80-90
Processing class: 80
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 5.6868577003
Epoch:   200  |  train loss: 5.2576250076
Epoch:   300  |  train loss: 5.0996444702
Epoch:   400  |  train loss: 5.0044351578
Epoch:   500  |  train loss: 4.8968660355
Epoch:   600  |  train loss: 4.8387117386
Epoch:   700  |  train loss: 4.7249635696
Epoch:   800  |  train loss: 4.7568487167
Epoch:   900  |  train loss: 4.7023397446
Epoch:  1000  |  train loss: 4.6930414200
Processing class: 81
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 6.1850977898
Epoch:   200  |  train loss: 6.0206547737
Epoch:   300  |  train loss: 5.8024944305
Epoch:   400  |  train loss: 5.7454395294
Epoch:   500  |  train loss: 5.6156673431
Epoch:   600  |  train loss: 5.5752731323
Epoch:   700  |  train loss: 5.4769923210
Epoch:   800  |  train loss: 5.4543526649
Epoch:   900  |  train loss: 5.3727504730
Epoch:  1000  |  train loss: 5.3064187050
Processing class: 82
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 5.9654965401
Epoch:   200  |  train loss: 5.7424053192
Epoch:   300  |  train loss: 5.5782061577
Epoch:   400  |  train loss: 5.4149820328
Epoch:   500  |  train loss: 5.2934447289
Epoch:   600  |  train loss: 5.1897103310
Epoch:   700  |  train loss: 5.1160479546
Epoch:   800  |  train loss: 5.0717207909
Epoch:   900  |  train loss: 5.0065893173
Epoch:  1000  |  train loss: 4.9477758408
Processing class: 83
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 5.8914454460
Epoch:   200  |  train loss: 5.4943305969
Epoch:   300  |  train loss: 5.2256019592
Epoch:   400  |  train loss: 5.0593959808
Epoch:   500  |  train loss: 4.9864829063
Epoch:   600  |  train loss: 4.8651611328
Epoch:   700  |  train loss: 4.8584516525
Epoch:   800  |  train loss: 4.8609048843
Epoch:   900  |  train loss: 4.7823579788
Epoch:  1000  |  train loss: 4.7072939873
Processing class: 84
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 6.3767485619
Epoch:   200  |  train loss: 6.0282639503
Epoch:   300  |  train loss: 5.8809803963
Epoch:   400  |  train loss: 5.6980376244
Epoch:   500  |  train loss: 5.5994681358
Epoch:   600  |  train loss: 5.4619697571
Epoch:   700  |  train loss: 5.4208363533
Epoch:   800  |  train loss: 5.3941522598
Epoch:   900  |  train loss: 5.2650070190
Epoch:  1000  |  train loss: 5.1934451103
Processing class: 85
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 6.3904290199
Epoch:   200  |  train loss: 6.0903458595
Epoch:   300  |  train loss: 5.9114297867
Epoch:   400  |  train loss: 5.7245682716
Epoch:   500  |  train loss: 5.6395186424
Epoch:   600  |  train loss: 5.5675276756
Epoch:   700  |  train loss: 5.5504759789
Epoch:   800  |  train loss: 5.4313139915
Epoch:   900  |  train loss: 5.3734401703
Epoch:  1000  |  train loss: 5.3535331726
Processing class: 86
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 6.6173813820
Epoch:   200  |  train loss: 6.4386403084
Epoch:   300  |  train loss: 6.3067720413
Epoch:   400  |  train loss: 6.1186066628
Epoch:   500  |  train loss: 6.0078426361
Epoch:   600  |  train loss: 5.9193223953
Epoch:   700  |  train loss: 5.8294760704
Epoch:   800  |  train loss: 5.7397792816
Epoch:   900  |  train loss: 5.6728378296
Epoch:  1000  |  train loss: 5.6729752541
Processing class: 87
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 6.0244409561
Epoch:   200  |  train loss: 5.6829602242
Epoch:   300  |  train loss: 5.5288232803
Epoch:   400  |  train loss: 5.3607393265
Epoch:   500  |  train loss: 5.2338006973
Epoch:   600  |  train loss: 5.2032341957
Epoch:   700  |  train loss: 5.1490587234
Epoch:   800  |  train loss: 5.0812573433
Epoch:   900  |  train loss: 4.9871536255
Epoch:  1000  |  train loss: 5.0048180580
Processing class: 88
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 6.2781552315
Epoch:   200  |  train loss: 6.0824048996
Epoch:   300  |  train loss: 5.8760914803
Epoch:   400  |  train loss: 5.7221250534
Epoch:   500  |  train loss: 5.5500083923
Epoch:   600  |  train loss: 5.3836005211
Epoch:   700  |  train loss: 5.3298872948
Epoch:   800  |  train loss: 5.2423713684
Epoch:   900  |  train loss: 5.1860948563
Epoch:  1000  |  train loss: 5.0955133438
Processing class: 89
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 6.2511256218
Epoch:   200  |  train loss: 6.0391815186
Epoch:   300  |  train loss: 5.8102660179
Epoch:   400  |  train loss: 5.6958518982
Epoch:   500  |  train loss: 5.6050491333
Epoch:   600  |  train loss: 5.4751749992
Epoch:   700  |  train loss: 5.3449881554
Epoch:   800  |  train loss: 5.3444179535
Epoch:   900  |  train loss: 5.2436157227
Epoch:  1000  |  train loss: 5.1955094337
2024-03-06 01:42:11,671 [trainer.py] => CNN: {'total': 54.08, '00-09': 67.9, '10-19': 63.6, '20-29': 73.2, '30-39': 68.8, '40-49': 57.2, '50-59': 32.6, '60-69': 40.9, '70-79': 38.1, '80-89': 44.4, 'old': 55.29, 'new': 44.4}
2024-03-06 01:42:11,672 [trainer.py] => No NME accuracy
2024-03-06 01:42:11,672 [trainer.py] => FeCAM: {'total': 57.66, '00-09': 76.1, '10-19': 65.8, '20-29': 75.3, '30-39': 67.9, '40-49': 68.5, '50-59': 33.0, '60-69': 45.0, '70-79': 39.0, '80-89': 48.3, 'old': 58.82, 'new': 48.3}
2024-03-06 01:42:11,672 [trainer.py] => CNN top1 curve: [83.44, 71.75, 64.66, 59.18, 54.08]
2024-03-06 01:42:11,672 [trainer.py] => CNN top5 curve: [96.5, 89.6, 86.54, 84.09, 81.74]
2024-03-06 01:42:11,672 [trainer.py] => FeCAM top1 curve: [82.9, 72.78, 67.27, 61.72, 57.66]
2024-03-06 01:42:11,672 [trainer.py] => FeCAM top5 curve: [95.84, 91.18, 87.21, 84.09, 81.63]

2024-03-06 01:42:11,680 [fecam.py] => Learning on 90-100
Processing class: 90
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 6.3841921806
Epoch:   200  |  train loss: 6.2420618057
Epoch:   300  |  train loss: 5.9450949669
Epoch:   400  |  train loss: 5.7281031609
Epoch:   500  |  train loss: 5.5498192787
Epoch:   600  |  train loss: 5.4229487419
Epoch:   700  |  train loss: 5.2759873390
Epoch:   800  |  train loss: 5.2001119614
Epoch:   900  |  train loss: 5.1584460258
Epoch:  1000  |  train loss: 5.1349505424
Processing class: 91
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 5.2553566933
Epoch:   200  |  train loss: 5.0625872612
Epoch:   300  |  train loss: 4.9380674362
Epoch:   400  |  train loss: 4.8993724823
Epoch:   500  |  train loss: 4.8068739891
Epoch:   600  |  train loss: 4.7475669861
Epoch:   700  |  train loss: 4.6422302246
Epoch:   800  |  train loss: 4.5699636459
Epoch:   900  |  train loss: 4.5197271347
Epoch:  1000  |  train loss: 4.4837213516
Processing class: 92
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 6.2348464012
Epoch:   200  |  train loss: 5.7934934616
Epoch:   300  |  train loss: 5.6015045166
Epoch:   400  |  train loss: 5.5018352509
Epoch:   500  |  train loss: 5.3644487381
Epoch:   600  |  train loss: 5.2214005470
Epoch:   700  |  train loss: 5.1063582420
Epoch:   800  |  train loss: 4.9519512177
Epoch:   900  |  train loss: 4.8966979027
Epoch:  1000  |  train loss: 4.8391554832
Processing class: 93
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 6.2960555077
Epoch:   200  |  train loss: 5.9854029655
Epoch:   300  |  train loss: 5.8398099899
Epoch:   400  |  train loss: 5.7196866035
Epoch:   500  |  train loss: 5.5609361649
Epoch:   600  |  train loss: 5.3961765289
Epoch:   700  |  train loss: 5.3734141350
Epoch:   800  |  train loss: 5.2730928421
Epoch:   900  |  train loss: 5.2093414307
Epoch:  1000  |  train loss: 5.1149388313
Processing class: 94
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 5.7872615814
Epoch:   200  |  train loss: 5.5595070839
Epoch:   300  |  train loss: 5.2151111603
Epoch:   400  |  train loss: 5.0602544785
Epoch:   500  |  train loss: 4.8581201553
Epoch:   600  |  train loss: 4.8050261497
Epoch:   700  |  train loss: 4.7619076729
Epoch:   800  |  train loss: 4.5827724457
Epoch:   900  |  train loss: 4.5807707787
Epoch:  1000  |  train loss: 4.5006801605
Processing class: 95
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 6.4359119415
Epoch:   200  |  train loss: 6.3887864113
Epoch:   300  |  train loss: 6.2264643669
Epoch:   400  |  train loss: 6.0480945587
Epoch:   500  |  train loss: 5.9614322662
Epoch:   600  |  train loss: 5.8468118668
Epoch:   700  |  train loss: 5.7679880142
Epoch:   800  |  train loss: 5.6594163895
Epoch:   900  |  train loss: 5.6167477608
Epoch:  1000  |  train loss: 5.5805459976
Processing class: 96
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 6.5325733185
Epoch:   200  |  train loss: 6.3038784981
Epoch:   300  |  train loss: 6.1390607834
Epoch:   400  |  train loss: 6.0765984535
Epoch:   500  |  train loss: 5.9211660385
Epoch:   600  |  train loss: 5.8151888847
Epoch:   700  |  train loss: 5.7244133949
Epoch:   800  |  train loss: 5.6015678406
Epoch:   900  |  train loss: 5.5711216927
Epoch:  1000  |  train loss: 5.4862807274
Processing class: 97
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 6.3276278496
Epoch:   200  |  train loss: 5.9970139503
Epoch:   300  |  train loss: 5.7617457390
Epoch:   400  |  train loss: 5.6363801003
Epoch:   500  |  train loss: 5.5515345573
Epoch:   600  |  train loss: 5.4699878693
Epoch:   700  |  train loss: 5.4069210052
Epoch:   800  |  train loss: 5.3237253189
Epoch:   900  |  train loss: 5.2616199493
Epoch:  1000  |  train loss: 5.1903312683
Processing class: 98
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 5.9314135551
Epoch:   200  |  train loss: 5.6740981102
Epoch:   300  |  train loss: 5.4149123192
Epoch:   400  |  train loss: 5.2585325241
Epoch:   500  |  train loss: 5.1041066170
Epoch:   600  |  train loss: 5.0122593880
Epoch:   700  |  train loss: 4.8821095467
Epoch:   800  |  train loss: 4.8517086983
Epoch:   900  |  train loss: 4.7996516228
Epoch:  1000  |  train loss: 4.7353819847
Processing class: 99
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 6.4380859375
Epoch:   200  |  train loss: 6.1906030655
Epoch:   300  |  train loss: 5.9803552628
Epoch:   400  |  train loss: 5.8431855202
Epoch:   500  |  train loss: 5.6610860825
Epoch:   600  |  train loss: 5.5306773186
Epoch:   700  |  train loss: 5.4350798607
Epoch:   800  |  train loss: 5.4019979477
Epoch:   900  |  train loss: 5.2981537819
Epoch:  1000  |  train loss: 5.2434103966
2024-03-06 01:52:23,678 [trainer.py] => CNN: {'total': 50.36, '00-09': 57.7, '10-19': 63.6, '20-29': 71.4, '30-39': 68.5, '40-49': 56.8, '50-59': 29.6, '60-69': 38.9, '70-79': 36.6, '80-89': 42.7, '90-99': 37.8, 'old': 51.76, 'new': 37.8}
2024-03-06 01:52:23,679 [trainer.py] => No NME accuracy
2024-03-06 01:52:23,679 [trainer.py] => FeCAM: {'total': 54.74, '00-09': 73.3, '10-19': 65.1, '20-29': 74.1, '30-39': 67.2, '40-49': 67.1, '50-59': 30.8, '60-69': 43.3, '70-79': 37.2, '80-89': 46.9, '90-99': 42.4, 'old': 56.11, 'new': 42.4}
2024-03-06 01:52:23,679 [trainer.py] => CNN top1 curve: [83.44, 71.75, 64.66, 59.18, 54.08, 50.36]
2024-03-06 01:52:23,679 [trainer.py] => CNN top5 curve: [96.5, 89.6, 86.54, 84.09, 81.74, 79.63]
2024-03-06 01:52:23,679 [trainer.py] => FeCAM top1 curve: [82.9, 72.78, 67.27, 61.72, 57.66, 54.74]
2024-03-06 01:52:23,679 [trainer.py] => FeCAM top5 curve: [95.84, 91.18, 87.21, 84.09, 81.63, 79.21]

=========================================
2024-03-06 01:52:31,867 [trainer.py] => config: ./exps/FeCAM_cifar100.json
2024-03-06 01:52:31,867 [trainer.py] => prefix: train
2024-03-06 01:52:31,867 [trainer.py] => dataset: cifar100
2024-03-06 01:52:31,867 [trainer.py] => memory_size: 0
2024-03-06 01:52:31,867 [trainer.py] => shuffle: True
2024-03-06 01:52:31,867 [trainer.py] => init_cls: 50
2024-03-06 01:52:31,867 [trainer.py] => increment: 10
2024-03-06 01:52:31,867 [trainer.py] => model_name: fecam
2024-03-06 01:52:31,867 [trainer.py] => convnet_type: resnet18
2024-03-06 01:52:31,867 [trainer.py] => device: [device(type='cuda', index=0)]
2024-03-06 01:52:31,867 [trainer.py] => seed: 1993
2024-03-06 01:52:31,867 [trainer.py] => init_epochs: 200
2024-03-06 01:52:31,867 [trainer.py] => init_lr: 0.1
2024-03-06 01:52:31,867 [trainer.py] => init_weight_decay: 0.0005
2024-03-06 01:52:31,867 [trainer.py] => batch_size: 128
2024-03-06 01:52:31,867 [trainer.py] => num_workers: 8
2024-03-06 01:52:31,867 [trainer.py] => T: 5
2024-03-06 01:52:31,867 [trainer.py] => beta: 0.5
2024-03-06 01:52:31,867 [trainer.py] => alpha1: 1
2024-03-06 01:52:31,867 [trainer.py] => alpha2: 1
2024-03-06 01:52:31,867 [trainer.py] => ncm: False
2024-03-06 01:52:31,868 [trainer.py] => tukey: False
2024-03-06 01:52:31,868 [trainer.py] => diagonal: False
2024-03-06 01:52:31,868 [trainer.py] => per_class: True
2024-03-06 01:52:31,868 [trainer.py] => full_cov: True
2024-03-06 01:52:31,868 [trainer.py] => shrink: True
2024-03-06 01:52:31,868 [trainer.py] => norm_cov: False
2024-03-06 01:52:31,868 [trainer.py] => vecnorm: False
2024-03-06 01:52:31,868 [trainer.py] => ae_type: wae
2024-03-06 01:52:31,868 [trainer.py] => epochs: 1000
2024-03-06 01:52:31,868 [trainer.py] => ae_latent_dim: 32
2024-03-06 01:52:31,868 [trainer.py] => wae_sigma: 20
2024-03-06 01:52:31,868 [trainer.py] => wae_C: 10
Files already downloaded and verified
Files already downloaded and verified
2024-03-06 01:52:33,523 [data_manager.py] => [68, 56, 78, 8, 23, 84, 90, 65, 74, 76, 40, 89, 3, 92, 55, 9, 26, 80, 43, 38, 58, 70, 77, 1, 85, 19, 17, 50, 28, 53, 13, 81, 45, 82, 6, 59, 83, 16, 15, 44, 91, 41, 72, 60, 79, 52, 20, 10, 31, 54, 37, 95, 14, 71, 96, 98, 97, 2, 64, 66, 42, 22, 35, 86, 24, 34, 87, 21, 99, 0, 88, 27, 18, 94, 11, 12, 47, 25, 30, 46, 62, 69, 36, 61, 7, 63, 75, 5, 32, 4, 51, 48, 73, 93, 39, 67, 29, 49, 57, 33]
2024-03-06 01:52:33,803 [fecam.py] => Learning on 0-50
/home/z1165703/miniconda3/lib/python3.11/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
/home/z1165703/miniconda3/lib/python3.11/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
Processing class: 0
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 4.3347705841
Epoch:   200  |  train loss: 4.2723728180
Epoch:   300  |  train loss: 4.3227501869
Epoch:   400  |  train loss: 4.3089864731
Epoch:   500  |  train loss: 4.2611455917
Epoch:   600  |  train loss: 4.2595345497
Epoch:   700  |  train loss: 4.2655301094
Epoch:   800  |  train loss: 4.2673287392
Epoch:   900  |  train loss: 4.2281884193
Epoch:  1000  |  train loss: 4.2096961021
Processing class: 1
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 4.4311667442
Epoch:   200  |  train loss: 4.3961235046
Epoch:   300  |  train loss: 4.4635061264
Epoch:   400  |  train loss: 4.4625982285
Epoch:   500  |  train loss: 4.3536913872
Epoch:   600  |  train loss: 4.4333425522
Epoch:   700  |  train loss: 4.3712586403
Epoch:   800  |  train loss: 4.3960852623
Epoch:   900  |  train loss: 4.4484082222
Epoch:  1000  |  train loss: 4.3604773521
Processing class: 2
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 4.4848405838
Epoch:   200  |  train loss: 4.4222746849
Epoch:   300  |  train loss: 4.4462444305
Epoch:   400  |  train loss: 4.3705915451
Epoch:   500  |  train loss: 4.1998757362
Epoch:   600  |  train loss: 4.2511208534
Epoch:   700  |  train loss: 4.1968785286
Epoch:   800  |  train loss: 4.2388206959
Epoch:   900  |  train loss: 4.1598842621
Epoch:  1000  |  train loss: 4.1085771561
Processing class: 3
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 4.4337692261
Epoch:   200  |  train loss: 4.5049965858
Epoch:   300  |  train loss: 4.5048915863
Epoch:   400  |  train loss: 4.4602079391
Epoch:   500  |  train loss: 4.3920907974
Epoch:   600  |  train loss: 4.4463734627
Epoch:   700  |  train loss: 4.4516832352
Epoch:   800  |  train loss: 4.3780376434
Epoch:   900  |  train loss: 4.3802769661
Epoch:  1000  |  train loss: 4.3611591339
Processing class: 4
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 4.4107081413
Epoch:   200  |  train loss: 4.3536537170
Epoch:   300  |  train loss: 4.3806403160
Epoch:   400  |  train loss: 4.3324126244
Epoch:   500  |  train loss: 4.3326451302
Epoch:   600  |  train loss: 4.2649020195
Epoch:   700  |  train loss: 4.2937982559
Epoch:   800  |  train loss: 4.2882376671
Epoch:   900  |  train loss: 4.2866607666
Epoch:  1000  |  train loss: 4.3216798782
Processing class: 5
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 4.5198302269
Epoch:   200  |  train loss: 4.4114692688
Epoch:   300  |  train loss: 4.3893013954
Epoch:   400  |  train loss: 4.3335118294
Epoch:   500  |  train loss: 4.3025895119
Epoch:   600  |  train loss: 4.3794184685
Epoch:   700  |  train loss: 4.3571084023
Epoch:   800  |  train loss: 4.2724107742
Epoch:   900  |  train loss: 4.2270442963
Epoch:  1000  |  train loss: 4.2523940086
Processing class: 6
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 4.4251273155
Epoch:   200  |  train loss: 4.4586421967
Epoch:   300  |  train loss: 4.4927902222
Epoch:   400  |  train loss: 4.4268737793
Epoch:   500  |  train loss: 4.4707041740
Epoch:   600  |  train loss: 4.4218302727
Epoch:   700  |  train loss: 4.3779953003
Epoch:   800  |  train loss: 4.3780242920
Epoch:   900  |  train loss: 4.3016861916
Epoch:  1000  |  train loss: 4.3177487373
Processing class: 7
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 4.4576228142
Epoch:   200  |  train loss: 4.4172911644
Epoch:   300  |  train loss: 4.4104182243
Epoch:   400  |  train loss: 4.3723591805
Epoch:   500  |  train loss: 4.3573267937
Epoch:   600  |  train loss: 4.3041117668
Epoch:   700  |  train loss: 4.3169531822
Epoch:   800  |  train loss: 4.3637052536
Epoch:   900  |  train loss: 4.2688313484
Epoch:  1000  |  train loss: 4.2962318420
Processing class: 8
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 4.4339400291
Epoch:   200  |  train loss: 4.4473332405
Epoch:   300  |  train loss: 4.3445748329
Epoch:   400  |  train loss: 4.3973279953
Epoch:   500  |  train loss: 4.3261549950
Epoch:   600  |  train loss: 4.3071913719
Epoch:   700  |  train loss: 4.3492425919
Epoch:   800  |  train loss: 4.2637228012
Epoch:   900  |  train loss: 4.2829282761
Epoch:  1000  |  train loss: 4.2187505722
Processing class: 9
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 4.3287567139
Epoch:   200  |  train loss: 4.3748563766
Epoch:   300  |  train loss: 4.4326063156
Epoch:   400  |  train loss: 4.3201513290
Epoch:   500  |  train loss: 4.3765137672
Epoch:   600  |  train loss: 4.3112469673
Epoch:   700  |  train loss: 4.3314743042
Epoch:   800  |  train loss: 4.3406113625
Epoch:   900  |  train loss: 4.3295425415
Epoch:  1000  |  train loss: 4.2611496925
Processing class: 10
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 4.3870120049
Epoch:   200  |  train loss: 4.3145346642
Epoch:   300  |  train loss: 4.3102928162
Epoch:   400  |  train loss: 4.3673757553
Epoch:   500  |  train loss: 4.2180110931
Epoch:   600  |  train loss: 4.2264460564
Epoch:   700  |  train loss: 4.2331409454
Epoch:   800  |  train loss: 4.1831836700
Epoch:   900  |  train loss: 4.1927505493
Epoch:  1000  |  train loss: 4.1715292931
Processing class: 11
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 4.5233854294
Epoch:   200  |  train loss: 4.4672353745
Epoch:   300  |  train loss: 4.3898715019
Epoch:   400  |  train loss: 4.3902185440
Epoch:   500  |  train loss: 4.3406861305
Epoch:   600  |  train loss: 4.3674349785
Epoch:   700  |  train loss: 4.4236166000
Epoch:   800  |  train loss: 4.3895074844
Epoch:   900  |  train loss: 4.3662641525
Epoch:  1000  |  train loss: 4.3816878319
Processing class: 12
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 4.3627804756
Epoch:   200  |  train loss: 4.4596487045
Epoch:   300  |  train loss: 4.4100627899
Epoch:   400  |  train loss: 4.4040950775
Epoch:   500  |  train loss: 4.3780540466
Epoch:   600  |  train loss: 4.3013797760
Epoch:   700  |  train loss: 4.2941082001
Epoch:   800  |  train loss: 4.2982635498
Epoch:   900  |  train loss: 4.3211303711
Epoch:  1000  |  train loss: 4.2680583954
Processing class: 13
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 4.3640226364
Epoch:   200  |  train loss: 4.3900060654
Epoch:   300  |  train loss: 4.3991533279
Epoch:   400  |  train loss: 4.3586035728
Epoch:   500  |  train loss: 4.3720642090
Epoch:   600  |  train loss: 4.3035925865
Epoch:   700  |  train loss: 4.3104564667
Epoch:   800  |  train loss: 4.3338233948
Epoch:   900  |  train loss: 4.3428731918
Epoch:  1000  |  train loss: 4.3450573921
Processing class: 14
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 4.5482942581
Epoch:   200  |  train loss: 4.5824212074
Epoch:   300  |  train loss: 4.5602407455
Epoch:   400  |  train loss: 4.5724122047
Epoch:   500  |  train loss: 4.5169993401
Epoch:   600  |  train loss: 4.4835848808
Epoch:   700  |  train loss: 4.5098453522
Epoch:   800  |  train loss: 4.5737696648
Epoch:   900  |  train loss: 4.5319125175
Epoch:  1000  |  train loss: 4.4638440132
Processing class: 15
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 4.3766986847
Epoch:   200  |  train loss: 4.3872291565
Epoch:   300  |  train loss: 4.3786181450
Epoch:   400  |  train loss: 4.3704793930
Epoch:   500  |  train loss: 4.3870972633
Epoch:   600  |  train loss: 4.3146718025
Epoch:   700  |  train loss: 4.3698267937
Epoch:   800  |  train loss: 4.2910132408
Epoch:   900  |  train loss: 4.3528904915
Epoch:  1000  |  train loss: 4.3137375832
Processing class: 16
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 4.4302902222
Epoch:   200  |  train loss: 4.4921113014
Epoch:   300  |  train loss: 4.5177173615
Epoch:   400  |  train loss: 4.4501991272
Epoch:   500  |  train loss: 4.4952590942
Epoch:   600  |  train loss: 4.4326232910
Epoch:   700  |  train loss: 4.3891812325
Epoch:   800  |  train loss: 4.4521834373
Epoch:   900  |  train loss: 4.3534019470
Epoch:  1000  |  train loss: 4.4229217529
Processing class: 17
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 4.3052401543
Epoch:   200  |  train loss: 4.3491633415
Epoch:   300  |  train loss: 4.4095615387
Epoch:   400  |  train loss: 4.3822577477
Epoch:   500  |  train loss: 4.2279891968
Epoch:   600  |  train loss: 4.2517456055
Epoch:   700  |  train loss: 4.2130586624
Epoch:   800  |  train loss: 4.1962070465
Epoch:   900  |  train loss: 4.1951923370
Epoch:  1000  |  train loss: 4.1780228615
Processing class: 18
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 4.3641989708
Epoch:   200  |  train loss: 4.3774455070
Epoch:   300  |  train loss: 4.3221518517
Epoch:   400  |  train loss: 4.2715518951
Epoch:   500  |  train loss: 4.2840632439
Epoch:   600  |  train loss: 4.2045156956
Epoch:   700  |  train loss: 4.2897694588
Epoch:   800  |  train loss: 4.2508597374
Epoch:   900  |  train loss: 4.2489667892
Epoch:  1000  |  train loss: 4.2865043640
Processing class: 19
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 4.3184768677
Epoch:   200  |  train loss: 4.4046916962
Epoch:   300  |  train loss: 4.3609385490
Epoch:   400  |  train loss: 4.3503896713
Epoch:   500  |  train loss: 4.2588296890
Epoch:   600  |  train loss: 4.2540105820
Epoch:   700  |  train loss: 4.2620567322
Epoch:   800  |  train loss: 4.2151320457
Epoch:   900  |  train loss: 4.3052275658
Epoch:  1000  |  train loss: 4.3099529266
Processing class: 20
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 4.4107121468
Epoch:   200  |  train loss: 4.3954166412
Epoch:   300  |  train loss: 4.4215601921
Epoch:   400  |  train loss: 4.3973786354
Epoch:   500  |  train loss: 4.4534467697
Epoch:   600  |  train loss: 4.3853227615
Epoch:   700  |  train loss: 4.3719984055
Epoch:   800  |  train loss: 4.3639917374
Epoch:   900  |  train loss: 4.3703336716
Epoch:  1000  |  train loss: 4.4090327263
Processing class: 21
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 4.4362313271
Epoch:   200  |  train loss: 4.4355288506
Epoch:   300  |  train loss: 4.3528418541
Epoch:   400  |  train loss: 4.3115745544
Epoch:   500  |  train loss: 4.2976992607
Epoch:   600  |  train loss: 4.3083413124
Epoch:   700  |  train loss: 4.2791360855
Epoch:   800  |  train loss: 4.2327835560
Epoch:   900  |  train loss: 4.2659269333
Epoch:  1000  |  train loss: 4.2169940948
Processing class: 22
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 4.4657150269
Epoch:   200  |  train loss: 4.4697196007
Epoch:   300  |  train loss: 4.5116169930
Epoch:   400  |  train loss: 4.5130349159
Epoch:   500  |  train loss: 4.4847643852
Epoch:   600  |  train loss: 4.4136607170
Epoch:   700  |  train loss: 4.4096027374
Epoch:   800  |  train loss: 4.3545300484
Epoch:   900  |  train loss: 4.3089125633
Epoch:  1000  |  train loss: 4.3924094200
Processing class: 23
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 4.4289080620
Epoch:   200  |  train loss: 4.4797480583
Epoch:   300  |  train loss: 4.5054697990
Epoch:   400  |  train loss: 4.3972631454
Epoch:   500  |  train loss: 4.3633072853
Epoch:   600  |  train loss: 4.2800763130
Epoch:   700  |  train loss: 4.2831804276
Epoch:   800  |  train loss: 4.2256060600
Epoch:   900  |  train loss: 4.2442657471
Epoch:  1000  |  train loss: 4.1755247116
Processing class: 24
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 4.4384060860
Epoch:   200  |  train loss: 4.4276503563
Epoch:   300  |  train loss: 4.3730491638
Epoch:   400  |  train loss: 4.3877188683
Epoch:   500  |  train loss: 4.3257949829
Epoch:   600  |  train loss: 4.3425648689
Epoch:   700  |  train loss: 4.2295362473
Epoch:   800  |  train loss: 4.2430823326
Epoch:   900  |  train loss: 4.2751293182
Epoch:  1000  |  train loss: 4.2277684212
Processing class: 25
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 4.4222327232
Epoch:   200  |  train loss: 4.4243442535
Epoch:   300  |  train loss: 4.4526158333
Epoch:   400  |  train loss: 4.3373021126
Epoch:   500  |  train loss: 4.3607573509
Epoch:   600  |  train loss: 4.2964727402
Epoch:   700  |  train loss: 4.3474364281
Epoch:   800  |  train loss: 4.2947087288
Epoch:   900  |  train loss: 4.2464413643
Epoch:  1000  |  train loss: 4.2598835945
Processing class: 26
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 4.4517580986
Epoch:   200  |  train loss: 4.5230725288
Epoch:   300  |  train loss: 4.4925205231
Epoch:   400  |  train loss: 4.5218125343
Epoch:   500  |  train loss: 4.5464163780
Epoch:   600  |  train loss: 4.4009553909
Epoch:   700  |  train loss: 4.4122725487
Epoch:   800  |  train loss: 4.4292481422
Epoch:   900  |  train loss: 4.3873071671
Epoch:  1000  |  train loss: 4.4274418831
Processing class: 27
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 4.4834616661
Epoch:   200  |  train loss: 4.3823617935
Epoch:   300  |  train loss: 4.3926972389
Epoch:   400  |  train loss: 4.3553667068
Epoch:   500  |  train loss: 4.3463223457
Epoch:   600  |  train loss: 4.3324305534
Epoch:   700  |  train loss: 4.3339686394
Epoch:   800  |  train loss: 4.3125871658
Epoch:   900  |  train loss: 4.3117394447
Epoch:  1000  |  train loss: 4.2120954514
Processing class: 28
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 4.4934853554
Epoch:   200  |  train loss: 4.5452168465
Epoch:   300  |  train loss: 4.5468487740
Epoch:   400  |  train loss: 4.5315559387
Epoch:   500  |  train loss: 4.5174111366
Epoch:   600  |  train loss: 4.4992665291
Epoch:   700  |  train loss: 4.5056373596
Epoch:   800  |  train loss: 4.5128550529
Epoch:   900  |  train loss: 4.5142930984
Epoch:  1000  |  train loss: 4.4513525009
Processing class: 29
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 4.4332161903
Epoch:   200  |  train loss: 4.4077989578
Epoch:   300  |  train loss: 4.3472966194
Epoch:   400  |  train loss: 4.2597183228
Epoch:   500  |  train loss: 4.2858692169
Epoch:   600  |  train loss: 4.2826467514
Epoch:   700  |  train loss: 4.2463060379
Epoch:   800  |  train loss: 4.1732371330
Epoch:   900  |  train loss: 4.1623893738
Epoch:  1000  |  train loss: 4.1286211967
Processing class: 30
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 4.4470771790
Epoch:   200  |  train loss: 4.5032228470
Epoch:   300  |  train loss: 4.5270290375
Epoch:   400  |  train loss: 4.5874043465
Epoch:   500  |  train loss: 4.5426450729
Epoch:   600  |  train loss: 4.5067090988
Epoch:   700  |  train loss: 4.4587099075
Epoch:   800  |  train loss: 4.5001831055
Epoch:   900  |  train loss: 4.5228891373
Epoch:  1000  |  train loss: 4.4592363358
Processing class: 31
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 4.5044385910
Epoch:   200  |  train loss: 4.5280239105
Epoch:   300  |  train loss: 4.4787125587
Epoch:   400  |  train loss: 4.5415540695
Epoch:   500  |  train loss: 4.5377027512
Epoch:   600  |  train loss: 4.5736597061
Epoch:   700  |  train loss: 4.5352010727
Epoch:   800  |  train loss: 4.5280639648
Epoch:   900  |  train loss: 4.5568247795
Epoch:  1000  |  train loss: 4.4781646729
Processing class: 32
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 4.5176644325
Epoch:   200  |  train loss: 4.5665994644
Epoch:   300  |  train loss: 4.5114382744
Epoch:   400  |  train loss: 4.4040597916
Epoch:   500  |  train loss: 4.4714282036
Epoch:   600  |  train loss: 4.3852950096
Epoch:   700  |  train loss: 4.4383385658
Epoch:   800  |  train loss: 4.4429729462
Epoch:   900  |  train loss: 4.4024219513
Epoch:  1000  |  train loss: 4.3575179100
Processing class: 33
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 4.3430751801
Epoch:   200  |  train loss: 4.4955521584
Epoch:   300  |  train loss: 4.3061064720
Epoch:   400  |  train loss: 4.2657216072
Epoch:   500  |  train loss: 4.2378855705
Epoch:   600  |  train loss: 4.2947778702
Epoch:   700  |  train loss: 4.2217045307
Epoch:   800  |  train loss: 4.2214225769
Epoch:   900  |  train loss: 4.1900809765
Epoch:  1000  |  train loss: 4.1438481331
Processing class: 34
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 4.4904131889
Epoch:   200  |  train loss: 4.4506375313
Epoch:   300  |  train loss: 4.4375709534
Epoch:   400  |  train loss: 4.3704158783
Epoch:   500  |  train loss: 4.3221369743
Epoch:   600  |  train loss: 4.3447697639
Epoch:   700  |  train loss: 4.2613855362
Epoch:   800  |  train loss: 4.2991295815
Epoch:   900  |  train loss: 4.2973897934
Epoch:  1000  |  train loss: 4.2922760963
Processing class: 35
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 4.5387912750
Epoch:   200  |  train loss: 4.5183248520
Epoch:   300  |  train loss: 4.5333223343
Epoch:   400  |  train loss: 4.5269720078
Epoch:   500  |  train loss: 4.4461561203
Epoch:   600  |  train loss: 4.4749040604
Epoch:   700  |  train loss: 4.4659318924
Epoch:   800  |  train loss: 4.4380605698
Epoch:   900  |  train loss: 4.4325722694
Epoch:  1000  |  train loss: 4.3863905907
Processing class: 36
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 4.3392971039
Epoch:   200  |  train loss: 4.3188770294
Epoch:   300  |  train loss: 4.3180713654
Epoch:   400  |  train loss: 4.2983617783
Epoch:   500  |  train loss: 4.3385476112
Epoch:   600  |  train loss: 4.2646498680
Epoch:   700  |  train loss: 4.1932529449
Epoch:   800  |  train loss: 4.2533115387
Epoch:   900  |  train loss: 4.1541537285
Epoch:  1000  |  train loss: 4.1821094513
Processing class: 37
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 4.5589969635
Epoch:   200  |  train loss: 4.5175055504
Epoch:   300  |  train loss: 4.4531275749
Epoch:   400  |  train loss: 4.4456817627
Epoch:   500  |  train loss: 4.4453402519
Epoch:   600  |  train loss: 4.3722915649
Epoch:   700  |  train loss: 4.3536359787
Epoch:   800  |  train loss: 4.3600112915
Epoch:   900  |  train loss: 4.2927817345
Epoch:  1000  |  train loss: 4.3037158012
Processing class: 38
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 4.3724687576
Epoch:   200  |  train loss: 4.3885655403
Epoch:   300  |  train loss: 4.4192250252
Epoch:   400  |  train loss: 4.4339145660
Epoch:   500  |  train loss: 4.4050595284
Epoch:   600  |  train loss: 4.3457134247
Epoch:   700  |  train loss: 4.3164260864
Epoch:   800  |  train loss: 4.3602893829
Epoch:   900  |  train loss: 4.2714556694
Epoch:  1000  |  train loss: 4.3070431709
Processing class: 39
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 4.4493372917
Epoch:   200  |  train loss: 4.3992015839
Epoch:   300  |  train loss: 4.4072969437
Epoch:   400  |  train loss: 4.4029801369
Epoch:   500  |  train loss: 4.3411982536
Epoch:   600  |  train loss: 4.3382839203
Epoch:   700  |  train loss: 4.3343916893
Epoch:   800  |  train loss: 4.3468860626
Epoch:   900  |  train loss: 4.3358630180
Epoch:  1000  |  train loss: 4.3271016121
Processing class: 40
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 4.4327371597
Epoch:   200  |  train loss: 4.3167098999
Epoch:   300  |  train loss: 4.4082722664
Epoch:   400  |  train loss: 4.4077104568
Epoch:   500  |  train loss: 4.4133936882
Epoch:   600  |  train loss: 4.3286902428
Epoch:   700  |  train loss: 4.3458767891
Epoch:   800  |  train loss: 4.3234831810
Epoch:   900  |  train loss: 4.2789188862
Epoch:  1000  |  train loss: 4.3633072853
Processing class: 41
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 4.3335335732
Epoch:   200  |  train loss: 4.3512694359
Epoch:   300  |  train loss: 4.3271588326
Epoch:   400  |  train loss: 4.2450425148
Epoch:   500  |  train loss: 4.2135467052
Epoch:   600  |  train loss: 4.1965843201
Epoch:   700  |  train loss: 4.2256787300
Epoch:   800  |  train loss: 4.1816511154
Epoch:   900  |  train loss: 4.1509050369
Epoch:  1000  |  train loss: 4.1368330479
Processing class: 42
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 4.4364162445
Epoch:   200  |  train loss: 4.4629868507
Epoch:   300  |  train loss: 4.4584987640
Epoch:   400  |  train loss: 4.4222346306
Epoch:   500  |  train loss: 4.4091969490
Epoch:   600  |  train loss: 4.3873620033
Epoch:   700  |  train loss: 4.3875402451
Epoch:   800  |  train loss: 4.3718558311
Epoch:   900  |  train loss: 4.3113043785
Epoch:  1000  |  train loss: 4.2828428268
Processing class: 43
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 4.4985898018
Epoch:   200  |  train loss: 4.4455860138
Epoch:   300  |  train loss: 4.4839249611
Epoch:   400  |  train loss: 4.5698447227
Epoch:   500  |  train loss: 4.5778285027
Epoch:   600  |  train loss: 4.5446288109
Epoch:   700  |  train loss: 4.5794646263
Epoch:   800  |  train loss: 4.5485174179
Epoch:   900  |  train loss: 4.4683878899
Epoch:  1000  |  train loss: 4.5293114662
Processing class: 44
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 4.4697115898
Epoch:   200  |  train loss: 4.4869834900
Epoch:   300  |  train loss: 4.4318846703
Epoch:   400  |  train loss: 4.4002750397
Epoch:   500  |  train loss: 4.4111711502
Epoch:   600  |  train loss: 4.3664175987
Epoch:   700  |  train loss: 4.4054176331
Epoch:   800  |  train loss: 4.3666139603
Epoch:   900  |  train loss: 4.3181923866
Epoch:  1000  |  train loss: 4.3592672348
Processing class: 45
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 4.4447701454
Epoch:   200  |  train loss: 4.5108743668
Epoch:   300  |  train loss: 4.4280105591
Epoch:   400  |  train loss: 4.5069119453
Epoch:   500  |  train loss: 4.4836633682
Epoch:   600  |  train loss: 4.5500075340
Epoch:   700  |  train loss: 4.5467158318
Epoch:   800  |  train loss: 4.5116423607
Epoch:   900  |  train loss: 4.5182403564
Epoch:  1000  |  train loss: 4.5520828247
Processing class: 46
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 4.4169522285
Epoch:   200  |  train loss: 4.3931305885
Epoch:   300  |  train loss: 4.3757169724
Epoch:   400  |  train loss: 4.3981010437
Epoch:   500  |  train loss: 4.3920105934
Epoch:   600  |  train loss: 4.3886064529
Epoch:   700  |  train loss: 4.3850565910
Epoch:   800  |  train loss: 4.4372179031
Epoch:   900  |  train loss: 4.3759592056
Epoch:  1000  |  train loss: 4.3999344826
Processing class: 47
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 4.4499773979
Epoch:   200  |  train loss: 4.4712567329
Epoch:   300  |  train loss: 4.3572800636
Epoch:   400  |  train loss: 4.3823273659
Epoch:   500  |  train loss: 4.3488520622
Epoch:   600  |  train loss: 4.3071083069
Epoch:   700  |  train loss: 4.2933759689
Epoch:   800  |  train loss: 4.2290564537
Epoch:   900  |  train loss: 4.2270718575
Epoch:  1000  |  train loss: 4.1847918987
Processing class: 48
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 4.4678683281
Epoch:   200  |  train loss: 4.5218643188
Epoch:   300  |  train loss: 4.4126307487
Epoch:   400  |  train loss: 4.3109123230
Epoch:   500  |  train loss: 4.3971644402
Epoch:   600  |  train loss: 4.3586199760
Epoch:   700  |  train loss: 4.2930616379
Epoch:   800  |  train loss: 4.2823822021
Epoch:   900  |  train loss: 4.3638648987
Epoch:  1000  |  train loss: 4.3675818443
Processing class: 49
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 4.4778763771
Epoch:   200  |  train loss: 4.4612601280
Epoch:   300  |  train loss: 4.5140521049
Epoch:   400  |  train loss: 4.4944756508
Epoch:   500  |  train loss: 4.4778878212
Epoch:   600  |  train loss: 4.4606292725
Epoch:   700  |  train loss: 4.4307826996
Epoch:   800  |  train loss: 4.3909378052
Epoch:   900  |  train loss: 4.3620392799
Epoch:  1000  |  train loss: 4.3587427139
/home/z1165703/FeCAM/models/base.py:87: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  vec = torch.tensor(vec).cuda()
2024-03-06 02:10:02,736 [trainer.py] => CNN: {'total': 83.44, '00-09': 87.7, '10-19': 79.6, '20-29': 84.9, '30-39': 81.0, '40-49': 84.0, 'old': 0, 'new': 83.44}
2024-03-06 02:10:02,738 [trainer.py] => No NME accuracy
2024-03-06 02:10:02,738 [trainer.py] => FeCAM: {'total': 82.9, '00-09': 86.2, '10-19': 79.7, '20-29': 85.0, '30-39': 80.6, '40-49': 83.0, 'old': 0, 'new': 82.9}
2024-03-06 02:10:02,738 [trainer.py] => CNN top1 curve: [83.44]
2024-03-06 02:10:02,738 [trainer.py] => CNN top5 curve: [96.5]
2024-03-06 02:10:02,738 [trainer.py] => FeCAM top1 curve: [82.9]
2024-03-06 02:10:02,738 [trainer.py] => FeCAM top5 curve: [95.84]

2024-03-06 02:10:02,750 [fecam.py] => Learning on 50-60
Processing class: 50
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 4.5782714844
Epoch:   200  |  train loss: 4.5365536690
Epoch:   300  |  train loss: 4.5044751167
Epoch:   400  |  train loss: 4.5225441933
Epoch:   500  |  train loss: 4.5033486366
Epoch:   600  |  train loss: 4.4248130798
Epoch:   700  |  train loss: 4.4550286293
Epoch:   800  |  train loss: 4.4574481964
Epoch:   900  |  train loss: 4.4767216682
Epoch:  1000  |  train loss: 4.4692750931
Processing class: 51
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 4.6166866302
Epoch:   200  |  train loss: 4.4855868340
Epoch:   300  |  train loss: 4.5438562393
Epoch:   400  |  train loss: 4.5279301643
Epoch:   500  |  train loss: 4.4867808342
Epoch:   600  |  train loss: 4.4729527473
Epoch:   700  |  train loss: 4.3856255531
Epoch:   800  |  train loss: 4.3752000809
Epoch:   900  |  train loss: 4.4306821823
Epoch:  1000  |  train loss: 4.3649423599
Processing class: 52
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 4.6947564125
Epoch:   200  |  train loss: 4.6438363075
Epoch:   300  |  train loss: 4.6459043503
Epoch:   400  |  train loss: 4.6420261383
Epoch:   500  |  train loss: 4.5968382835
Epoch:   600  |  train loss: 4.5210225105
Epoch:   700  |  train loss: 4.5574696541
Epoch:   800  |  train loss: 4.5353373528
Epoch:   900  |  train loss: 4.5229640007
Epoch:  1000  |  train loss: 4.4527527809
Processing class: 53
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 4.4964814186
Epoch:   200  |  train loss: 4.4474009514
Epoch:   300  |  train loss: 4.3493244171
Epoch:   400  |  train loss: 4.3342066765
Epoch:   500  |  train loss: 4.3469849586
Epoch:   600  |  train loss: 4.3811573029
Epoch:   700  |  train loss: 4.4164765358
Epoch:   800  |  train loss: 4.2770624161
Epoch:   900  |  train loss: 4.3223061562
Epoch:  1000  |  train loss: 4.2589044571
Processing class: 54
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 4.2342720985
Epoch:   200  |  train loss: 4.1904340744
Epoch:   300  |  train loss: 4.1771136284
Epoch:   400  |  train loss: 4.1581533432
Epoch:   500  |  train loss: 4.1536353111
Epoch:   600  |  train loss: 4.1932888985
Epoch:   700  |  train loss: 4.1853492737
Epoch:   800  |  train loss: 4.2175091743
Epoch:   900  |  train loss: 4.2366269112
Epoch:  1000  |  train loss: 4.1904766083
Processing class: 55
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 4.6789694786
Epoch:   200  |  train loss: 4.6700600624
Epoch:   300  |  train loss: 4.6108208656
Epoch:   400  |  train loss: 4.6598134041
Epoch:   500  |  train loss: 4.7281542778
Epoch:   600  |  train loss: 4.7102092743
Epoch:   700  |  train loss: 4.7146011353
Epoch:   800  |  train loss: 4.7617302895
Epoch:   900  |  train loss: 4.6427103996
Epoch:  1000  |  train loss: 4.6737547874
Processing class: 56
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 4.5288786888
Epoch:   200  |  train loss: 4.4716709137
Epoch:   300  |  train loss: 4.3044235229
Epoch:   400  |  train loss: 4.3322140694
Epoch:   500  |  train loss: 4.3006001472
Epoch:   600  |  train loss: 4.2636725426
Epoch:   700  |  train loss: 4.2527228355
Epoch:   800  |  train loss: 4.1826995850
Epoch:   900  |  train loss: 4.2097679138
Epoch:  1000  |  train loss: 4.2173867226
Processing class: 57
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 4.6365960121
Epoch:   200  |  train loss: 4.6888218880
Epoch:   300  |  train loss: 4.6380293846
Epoch:   400  |  train loss: 4.6123234749
Epoch:   500  |  train loss: 4.6337206841
Epoch:   600  |  train loss: 4.5465664864
Epoch:   700  |  train loss: 4.6166656494
Epoch:   800  |  train loss: 4.5881450653
Epoch:   900  |  train loss: 4.6147684097
Epoch:  1000  |  train loss: 4.5082986832
Processing class: 58
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 4.5751759529
Epoch:   200  |  train loss: 4.4223422050
Epoch:   300  |  train loss: 4.4143863678
Epoch:   400  |  train loss: 4.4192102432
Epoch:   500  |  train loss: 4.3596335411
Epoch:   600  |  train loss: 4.3832811356
Epoch:   700  |  train loss: 4.3291532516
Epoch:   800  |  train loss: 4.3504312515
Epoch:   900  |  train loss: 4.3239021301
Epoch:  1000  |  train loss: 4.2975551605
Processing class: 59
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 4.7101043701
Epoch:   200  |  train loss: 4.7603055000
Epoch:   300  |  train loss: 4.6602828026
Epoch:   400  |  train loss: 4.6419090271
Epoch:   500  |  train loss: 4.6483906746
Epoch:   600  |  train loss: 4.6293345451
Epoch:   700  |  train loss: 4.6353822708
Epoch:   800  |  train loss: 4.6248071671
Epoch:   900  |  train loss: 4.5895398140
Epoch:  1000  |  train loss: 4.5592933655
2024-03-06 02:15:40,123 [trainer.py] => CNN: {'total': 71.75, '00-09': 82.2, '10-19': 72.8, '20-29': 78.5, '30-39': 74.9, '40-49': 68.4, '50-59': 53.7, 'old': 75.36, 'new': 53.7}
2024-03-06 02:15:40,124 [trainer.py] => No NME accuracy
2024-03-06 02:15:40,124 [trainer.py] => FeCAM: {'total': 72.78, '00-09': 82.6, '10-19': 74.0, '20-29': 81.0, '30-39': 76.4, '40-49': 76.8, '50-59': 45.9, 'old': 78.16, 'new': 45.9}
2024-03-06 02:15:40,124 [trainer.py] => CNN top1 curve: [83.44, 71.75]
2024-03-06 02:15:40,124 [trainer.py] => CNN top5 curve: [96.5, 89.6]
2024-03-06 02:15:40,124 [trainer.py] => FeCAM top1 curve: [82.9, 72.78]
2024-03-06 02:15:40,124 [trainer.py] => FeCAM top5 curve: [95.84, 91.18]

2024-03-06 02:15:40,129 [fecam.py] => Learning on 60-70
Processing class: 60
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 4.7264769554
Epoch:   200  |  train loss: 4.6804964066
Epoch:   300  |  train loss: 4.5860474586
Epoch:   400  |  train loss: 4.5213311195
Epoch:   500  |  train loss: 4.5507795334
Epoch:   600  |  train loss: 4.5089490891
Epoch:   700  |  train loss: 4.6336242676
Epoch:   800  |  train loss: 4.4980519295
Epoch:   900  |  train loss: 4.4845236778
Epoch:  1000  |  train loss: 4.4997946739
Processing class: 61
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 4.5193298340
Epoch:   200  |  train loss: 4.3750866890
Epoch:   300  |  train loss: 4.3689476013
Epoch:   400  |  train loss: 4.3071613312
Epoch:   500  |  train loss: 4.3535762787
Epoch:   600  |  train loss: 4.2494215012
Epoch:   700  |  train loss: 4.2890957832
Epoch:   800  |  train loss: 4.3080777168
Epoch:   900  |  train loss: 4.2011270523
Epoch:  1000  |  train loss: 4.2175895691
Processing class: 62
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 4.7352316856
Epoch:   200  |  train loss: 4.6711825371
Epoch:   300  |  train loss: 4.7090081215
Epoch:   400  |  train loss: 4.7141973495
Epoch:   500  |  train loss: 4.6505469322
Epoch:   600  |  train loss: 4.7139380455
Epoch:   700  |  train loss: 4.7228570938
Epoch:   800  |  train loss: 4.7149669647
Epoch:   900  |  train loss: 4.6666633606
Epoch:  1000  |  train loss: 4.6730143547
Processing class: 63
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 4.5485931396
Epoch:   200  |  train loss: 4.5439708710
Epoch:   300  |  train loss: 4.5761006355
Epoch:   400  |  train loss: 4.5190461159
Epoch:   500  |  train loss: 4.4990827560
Epoch:   600  |  train loss: 4.5425537109
Epoch:   700  |  train loss: 4.4183043480
Epoch:   800  |  train loss: 4.5236577988
Epoch:   900  |  train loss: 4.4142045975
Epoch:  1000  |  train loss: 4.4178256989
Processing class: 64
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 4.6598278999
Epoch:   200  |  train loss: 4.4557836533
Epoch:   300  |  train loss: 4.5056816101
Epoch:   400  |  train loss: 4.4610152245
Epoch:   500  |  train loss: 4.4020905495
Epoch:   600  |  train loss: 4.3515432358
Epoch:   700  |  train loss: 4.2542210102
Epoch:   800  |  train loss: 4.2649580002
Epoch:   900  |  train loss: 4.3056878090
Epoch:  1000  |  train loss: 4.2505727768
Processing class: 65
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 4.5593658447
Epoch:   200  |  train loss: 4.4420900345
Epoch:   300  |  train loss: 4.4669176102
Epoch:   400  |  train loss: 4.4553955078
Epoch:   500  |  train loss: 4.3867103577
Epoch:   600  |  train loss: 4.4481490135
Epoch:   700  |  train loss: 4.4164337158
Epoch:   800  |  train loss: 4.4798616409
Epoch:   900  |  train loss: 4.3928534508
Epoch:  1000  |  train loss: 4.4635749817
Processing class: 66
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 4.6900625229
Epoch:   200  |  train loss: 4.6836293221
Epoch:   300  |  train loss: 4.6791280746
Epoch:   400  |  train loss: 4.6350888252
Epoch:   500  |  train loss: 4.6750811577
Epoch:   600  |  train loss: 4.6546268463
Epoch:   700  |  train loss: 4.7036732674
Epoch:   800  |  train loss: 4.6957108498
Epoch:   900  |  train loss: 4.6702111244
Epoch:  1000  |  train loss: 4.6499135971
Processing class: 67
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 4.5810279846
Epoch:   200  |  train loss: 4.5842736244
Epoch:   300  |  train loss: 4.5399451256
Epoch:   400  |  train loss: 4.5219841957
Epoch:   500  |  train loss: 4.4931196213
Epoch:   600  |  train loss: 4.4682210922
Epoch:   700  |  train loss: 4.4623355865
Epoch:   800  |  train loss: 4.4844380379
Epoch:   900  |  train loss: 4.4521070480
Epoch:  1000  |  train loss: 4.4290249825
Processing class: 68
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 4.4708278656
Epoch:   200  |  train loss: 4.4363012314
Epoch:   300  |  train loss: 4.3175868988
Epoch:   400  |  train loss: 4.2970021248
Epoch:   500  |  train loss: 4.2745854378
Epoch:   600  |  train loss: 4.2297624588
Epoch:   700  |  train loss: 4.2377257347
Epoch:   800  |  train loss: 4.1808541298
Epoch:   900  |  train loss: 4.1765789986
Epoch:  1000  |  train loss: 4.2450132370
Processing class: 69
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 4.1137356758
Epoch:   200  |  train loss: 4.0964410305
Epoch:   300  |  train loss: 4.1226029873
Epoch:   400  |  train loss: 4.1149197578
Epoch:   500  |  train loss: 4.0839261055
Epoch:   600  |  train loss: 3.9921698570
Epoch:   700  |  train loss: 4.0378790379
Epoch:   800  |  train loss: 4.0156398773
Epoch:   900  |  train loss: 4.1059138298
Epoch:  1000  |  train loss: 3.9951796532
2024-03-06 02:22:03,699 [trainer.py] => CNN: {'total': 64.66, '00-09': 75.0, '10-19': 70.1, '20-29': 77.1, '30-39': 70.6, '40-49': 64.0, '50-59': 45.8, '60-69': 50.0, 'old': 67.1, 'new': 50.0}
2024-03-06 02:22:03,700 [trainer.py] => No NME accuracy
2024-03-06 02:22:03,700 [trainer.py] => FeCAM: {'total': 67.27, '00-09': 79.9, '10-19': 70.6, '20-29': 79.1, '30-39': 71.9, '40-49': 74.1, '50-59': 41.2, '60-69': 54.1, 'old': 69.47, 'new': 54.1}
2024-03-06 02:22:03,700 [trainer.py] => CNN top1 curve: [83.44, 71.75, 64.66]
2024-03-06 02:22:03,700 [trainer.py] => CNN top5 curve: [96.5, 89.6, 86.54]
2024-03-06 02:22:03,700 [trainer.py] => FeCAM top1 curve: [82.9, 72.78, 67.27]
2024-03-06 02:22:03,700 [trainer.py] => FeCAM top5 curve: [95.84, 91.18, 87.21]

2024-03-06 02:22:03,705 [fecam.py] => Learning on 70-80
Processing class: 70
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 4.5506665230
Epoch:   200  |  train loss: 4.4729645729
Epoch:   300  |  train loss: 4.4285993576
Epoch:   400  |  train loss: 4.4226655006
Epoch:   500  |  train loss: 4.3365898132
Epoch:   600  |  train loss: 4.3638734818
Epoch:   700  |  train loss: 4.3967276573
Epoch:   800  |  train loss: 4.2719146729
Epoch:   900  |  train loss: 4.3132813454
Epoch:  1000  |  train loss: 4.2682050705
Processing class: 71
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 4.6562457085
Epoch:   200  |  train loss: 4.6532280922
Epoch:   300  |  train loss: 4.5319534302
Epoch:   400  |  train loss: 4.5414428711
Epoch:   500  |  train loss: 4.5380829811
Epoch:   600  |  train loss: 4.5104204178
Epoch:   700  |  train loss: 4.4850055695
Epoch:   800  |  train loss: 4.4729725838
Epoch:   900  |  train loss: 4.5074423790
Epoch:  1000  |  train loss: 4.4637762070
Processing class: 72
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 4.5751176834
Epoch:   200  |  train loss: 4.5629343987
Epoch:   300  |  train loss: 4.5289757729
Epoch:   400  |  train loss: 4.4795919418
Epoch:   500  |  train loss: 4.4884903908
Epoch:   600  |  train loss: 4.4124537468
Epoch:   700  |  train loss: 4.4146029472
Epoch:   800  |  train loss: 4.3675848961
Epoch:   900  |  train loss: 4.3615242004
Epoch:  1000  |  train loss: 4.3963297844
Processing class: 73
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 4.6563354492
Epoch:   200  |  train loss: 4.5844168663
Epoch:   300  |  train loss: 4.5958134651
Epoch:   400  |  train loss: 4.5182535172
Epoch:   500  |  train loss: 4.5935706139
Epoch:   600  |  train loss: 4.5681484222
Epoch:   700  |  train loss: 4.4950645447
Epoch:   800  |  train loss: 4.4789416313
Epoch:   900  |  train loss: 4.4625530243
Epoch:  1000  |  train loss: 4.4781620979
Processing class: 74
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 4.7449390411
Epoch:   200  |  train loss: 4.7774201393
Epoch:   300  |  train loss: 4.6708188057
Epoch:   400  |  train loss: 4.5989315987
Epoch:   500  |  train loss: 4.6023060799
Epoch:   600  |  train loss: 4.5453464508
Epoch:   700  |  train loss: 4.6186039925
Epoch:   800  |  train loss: 4.5012720108
Epoch:   900  |  train loss: 4.4906411171
Epoch:  1000  |  train loss: 4.5429775238
Processing class: 75
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 4.6131507874
Epoch:   200  |  train loss: 4.5584323883
Epoch:   300  |  train loss: 4.5814273834
Epoch:   400  |  train loss: 4.5335987091
Epoch:   500  |  train loss: 4.5571168900
Epoch:   600  |  train loss: 4.5831183434
Epoch:   700  |  train loss: 4.4079589844
Epoch:   800  |  train loss: 4.4821798325
Epoch:   900  |  train loss: 4.4540130615
Epoch:  1000  |  train loss: 4.4953042030
Processing class: 76
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 4.0435732841
Epoch:   200  |  train loss: 3.9759945869
Epoch:   300  |  train loss: 4.0018913746
Epoch:   400  |  train loss: 4.0276744843
Epoch:   500  |  train loss: 4.0322779179
Epoch:   600  |  train loss: 4.0590530872
Epoch:   700  |  train loss: 4.0651263714
Epoch:   800  |  train loss: 3.9678663254
Epoch:   900  |  train loss: 4.0359920502
Epoch:  1000  |  train loss: 4.0633994102
Processing class: 77
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 4.5304555893
Epoch:   200  |  train loss: 4.4858842850
Epoch:   300  |  train loss: 4.3687947273
Epoch:   400  |  train loss: 4.3533887863
Epoch:   500  |  train loss: 4.3076836586
Epoch:   600  |  train loss: 4.2591719627
Epoch:   700  |  train loss: 4.2421601295
Epoch:   800  |  train loss: 4.2760723591
Epoch:   900  |  train loss: 4.2191748619
Epoch:  1000  |  train loss: 4.1849909782
Processing class: 78
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 4.6239713669
Epoch:   200  |  train loss: 4.5926515579
Epoch:   300  |  train loss: 4.6042167664
Epoch:   400  |  train loss: 4.5527040482
Epoch:   500  |  train loss: 4.4880467415
Epoch:   600  |  train loss: 4.5083295822
Epoch:   700  |  train loss: 4.4927813530
Epoch:   800  |  train loss: 4.4828346252
Epoch:   900  |  train loss: 4.4710115433
Epoch:  1000  |  train loss: 4.4394268036
Processing class: 79
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 4.7009788513
Epoch:   200  |  train loss: 4.6282491684
Epoch:   300  |  train loss: 4.6753308296
Epoch:   400  |  train loss: 4.6408663750
Epoch:   500  |  train loss: 4.6187073708
Epoch:   600  |  train loss: 4.6618216515
Epoch:   700  |  train loss: 4.6372378349
Epoch:   800  |  train loss: 4.6412045479
Epoch:   900  |  train loss: 4.6721611977
Epoch:  1000  |  train loss: 4.5979018211
2024-03-06 02:29:34,082 [trainer.py] => CNN: {'total': 59.18, '00-09': 73.6, '10-19': 68.4, '20-29': 76.9, '30-39': 69.1, '40-49': 60.6, '50-59': 37.2, '60-69': 45.7, '70-79': 41.9, 'old': 61.64, 'new': 41.9}
2024-03-06 02:29:34,082 [trainer.py] => No NME accuracy
2024-03-06 02:29:34,082 [trainer.py] => FeCAM: {'total': 61.72, '00-09': 78.1, '10-19': 69.3, '20-29': 78.0, '30-39': 69.5, '40-49': 72.1, '50-59': 35.7, '60-69': 49.1, '70-79': 42.0, 'old': 64.54, 'new': 42.0}
2024-03-06 02:29:34,082 [trainer.py] => CNN top1 curve: [83.44, 71.75, 64.66, 59.18]
2024-03-06 02:29:34,082 [trainer.py] => CNN top5 curve: [96.5, 89.6, 86.54, 84.09]
2024-03-06 02:29:34,082 [trainer.py] => FeCAM top1 curve: [82.9, 72.78, 67.27, 61.72]
2024-03-06 02:29:34,082 [trainer.py] => FeCAM top5 curve: [95.84, 91.18, 87.21, 84.09]

2024-03-06 02:29:34,086 [fecam.py] => Learning on 80-90
Processing class: 80
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 4.3308562279
Epoch:   200  |  train loss: 4.2815454483
Epoch:   300  |  train loss: 4.2623277664
Epoch:   400  |  train loss: 4.2518832207
Epoch:   500  |  train loss: 4.2133275986
Epoch:   600  |  train loss: 4.1948225975
Epoch:   700  |  train loss: 4.1106758118
Epoch:   800  |  train loss: 4.1924152374
Epoch:   900  |  train loss: 4.1576256752
Epoch:  1000  |  train loss: 4.1917583466
Processing class: 81
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 4.5357063293
Epoch:   200  |  train loss: 4.5671021461
Epoch:   300  |  train loss: 4.4570413589
Epoch:   400  |  train loss: 4.5097337723
Epoch:   500  |  train loss: 4.4533237457
Epoch:   600  |  train loss: 4.4910631180
Epoch:   700  |  train loss: 4.4516577721
Epoch:   800  |  train loss: 4.4784334183
Epoch:   900  |  train loss: 4.4548594475
Epoch:  1000  |  train loss: 4.4475788116
Processing class: 82
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 4.4327364922
Epoch:   200  |  train loss: 4.4111274719
Epoch:   300  |  train loss: 4.3699567795
Epoch:   400  |  train loss: 4.3136123657
Epoch:   500  |  train loss: 4.2904230118
Epoch:   600  |  train loss: 4.2456043243
Epoch:   700  |  train loss: 4.2228804588
Epoch:   800  |  train loss: 4.2242909431
Epoch:   900  |  train loss: 4.2075180531
Epoch:  1000  |  train loss: 4.1903035641
Processing class: 83
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 4.4453897476
Epoch:   200  |  train loss: 4.3900701523
Epoch:   300  |  train loss: 4.2886563301
Epoch:   400  |  train loss: 4.2589832306
Epoch:   500  |  train loss: 4.2903209686
Epoch:   600  |  train loss: 4.2134501934
Epoch:   700  |  train loss: 4.2647305489
Epoch:   800  |  train loss: 4.3285745621
Epoch:   900  |  train loss: 4.2775379181
Epoch:  1000  |  train loss: 4.2270695686
Processing class: 84
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 4.6421707153
Epoch:   200  |  train loss: 4.5354574203
Epoch:   300  |  train loss: 4.5429918289
Epoch:   400  |  train loss: 4.4924015045
Epoch:   500  |  train loss: 4.5001410484
Epoch:   600  |  train loss: 4.4179476738
Epoch:   700  |  train loss: 4.4501981735
Epoch:   800  |  train loss: 4.4813021660
Epoch:   900  |  train loss: 4.3865608215
Epoch:  1000  |  train loss: 4.3557641029
Processing class: 85
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 4.6243162155
Epoch:   200  |  train loss: 4.5293880463
Epoch:   300  |  train loss: 4.5291009903
Epoch:   400  |  train loss: 4.4522412300
Epoch:   500  |  train loss: 4.4580756187
Epoch:   600  |  train loss: 4.4746453285
Epoch:   700  |  train loss: 4.5280758858
Epoch:   800  |  train loss: 4.4432159424
Epoch:   900  |  train loss: 4.4328515053
Epoch:  1000  |  train loss: 4.4710367203
Processing class: 86
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 4.7448257446
Epoch:   200  |  train loss: 4.6510973930
Epoch:   300  |  train loss: 4.6869950294
Epoch:   400  |  train loss: 4.6100158691
Epoch:   500  |  train loss: 4.5820265770
Epoch:   600  |  train loss: 4.5802438736
Epoch:   700  |  train loss: 4.5586464882
Epoch:   800  |  train loss: 4.5255228996
Epoch:   900  |  train loss: 4.5056534767
Epoch:  1000  |  train loss: 4.5679574966
Processing class: 87
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 4.4593209267
Epoch:   200  |  train loss: 4.4000195503
Epoch:   300  |  train loss: 4.3543419838
Epoch:   400  |  train loss: 4.3319307327
Epoch:   500  |  train loss: 4.2860893250
Epoch:   600  |  train loss: 4.3053610802
Epoch:   700  |  train loss: 4.3021194458
Epoch:   800  |  train loss: 4.2805214882
Epoch:   900  |  train loss: 4.2192975998
Epoch:  1000  |  train loss: 4.2951449394
Processing class: 88
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 4.5658358574
Epoch:   200  |  train loss: 4.4922470093
Epoch:   300  |  train loss: 4.4312231064
Epoch:   400  |  train loss: 4.4010932922
Epoch:   500  |  train loss: 4.3451395988
Epoch:   600  |  train loss: 4.2719923973
Epoch:   700  |  train loss: 4.2956377029
Epoch:   800  |  train loss: 4.2684238434
Epoch:   900  |  train loss: 4.2624905586
Epoch:  1000  |  train loss: 4.2080583572
Processing class: 89
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 4.5868126869
Epoch:   200  |  train loss: 4.5708478928
Epoch:   300  |  train loss: 4.5262659073
Epoch:   400  |  train loss: 4.5056151390
Epoch:   500  |  train loss: 4.5245786667
Epoch:   600  |  train loss: 4.4866516113
Epoch:   700  |  train loss: 4.4073377609
Epoch:   800  |  train loss: 4.4922393799
Epoch:   900  |  train loss: 4.4389016151
Epoch:  1000  |  train loss: 4.4474632263
2024-03-06 02:38:17,532 [trainer.py] => CNN: {'total': 54.08, '00-09': 67.9, '10-19': 63.6, '20-29': 73.2, '30-39': 68.8, '40-49': 57.2, '50-59': 32.6, '60-69': 40.9, '70-79': 38.1, '80-89': 44.4, 'old': 55.29, 'new': 44.4}
2024-03-06 02:38:17,534 [trainer.py] => No NME accuracy
2024-03-06 02:38:17,534 [trainer.py] => FeCAM: {'total': 57.66, '00-09': 76.1, '10-19': 65.8, '20-29': 75.3, '30-39': 67.9, '40-49': 68.5, '50-59': 33.0, '60-69': 45.0, '70-79': 39.0, '80-89': 48.3, 'old': 58.82, 'new': 48.3}
2024-03-06 02:38:17,534 [trainer.py] => CNN top1 curve: [83.44, 71.75, 64.66, 59.18, 54.08]
2024-03-06 02:38:17,535 [trainer.py] => CNN top5 curve: [96.5, 89.6, 86.54, 84.09, 81.74]
2024-03-06 02:38:17,535 [trainer.py] => FeCAM top1 curve: [82.9, 72.78, 67.27, 61.72, 57.66]
2024-03-06 02:38:17,535 [trainer.py] => FeCAM top5 curve: [95.84, 91.18, 87.21, 84.09, 81.63]

2024-03-06 02:38:17,543 [fecam.py] => Learning on 90-100
Processing class: 90
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 4.6322465897
Epoch:   200  |  train loss: 4.6024829865
Epoch:   300  |  train loss: 4.4690762520
Epoch:   400  |  train loss: 4.3888748169
Epoch:   500  |  train loss: 4.3188045502
Epoch:   600  |  train loss: 4.3014260292
Epoch:   700  |  train loss: 4.2424377441
Epoch:   800  |  train loss: 4.2309436798
Epoch:   900  |  train loss: 4.2425392151
Epoch:  1000  |  train loss: 4.2814365387
Processing class: 91
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 4.1341881752
Epoch:   200  |  train loss: 4.0601986408
Epoch:   300  |  train loss: 4.0662524700
Epoch:   400  |  train loss: 4.0958726883
Epoch:   500  |  train loss: 4.0989362717
Epoch:   600  |  train loss: 4.1001547813
Epoch:   700  |  train loss: 4.0740031242
Epoch:   800  |  train loss: 4.0706399918
Epoch:   900  |  train loss: 4.0800892830
Epoch:  1000  |  train loss: 4.0762770653
Processing class: 92
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 4.6243429184
Epoch:   200  |  train loss: 4.4410325050
Epoch:   300  |  train loss: 4.3274459839
Epoch:   400  |  train loss: 4.3313922882
Epoch:   500  |  train loss: 4.3128879547
Epoch:   600  |  train loss: 4.3062848091
Epoch:   700  |  train loss: 4.2695186615
Epoch:   800  |  train loss: 4.1568212509
Epoch:   900  |  train loss: 4.1538435936
Epoch:  1000  |  train loss: 4.1436408997
Processing class: 93
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 4.5816793442
Epoch:   200  |  train loss: 4.5273027420
Epoch:   300  |  train loss: 4.5870717049
Epoch:   400  |  train loss: 4.5705321312
Epoch:   500  |  train loss: 4.5138627052
Epoch:   600  |  train loss: 4.4278264046
Epoch:   700  |  train loss: 4.5029009819
Epoch:   800  |  train loss: 4.4622571945
Epoch:   900  |  train loss: 4.4495242119
Epoch:  1000  |  train loss: 4.4081099510
Processing class: 94
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 4.2447201729
Epoch:   200  |  train loss: 4.1695356846
Epoch:   300  |  train loss: 4.0015702248
Epoch:   400  |  train loss: 3.9757525921
Epoch:   500  |  train loss: 3.8329306602
Epoch:   600  |  train loss: 3.8548462868
Epoch:   700  |  train loss: 3.8796080112
Epoch:   800  |  train loss: 3.7283145428
Epoch:   900  |  train loss: 3.7880955219
Epoch:  1000  |  train loss: 3.7434072018
Processing class: 95
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 4.6172039032
Epoch:   200  |  train loss: 4.6613236427
Epoch:   300  |  train loss: 4.5892521858
Epoch:   400  |  train loss: 4.5249513626
Epoch:   500  |  train loss: 4.5689618111
Epoch:   600  |  train loss: 4.5290728569
Epoch:   700  |  train loss: 4.5316067696
Epoch:   800  |  train loss: 4.4783951759
Epoch:   900  |  train loss: 4.5091582298
Epoch:  1000  |  train loss: 4.5367934227
Processing class: 96
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 4.7096334457
Epoch:   200  |  train loss: 4.5752048492
Epoch:   300  |  train loss: 4.5811558723
Epoch:   400  |  train loss: 4.6431232452
Epoch:   500  |  train loss: 4.5729284286
Epoch:   600  |  train loss: 4.5368848801
Epoch:   700  |  train loss: 4.5190761566
Epoch:   800  |  train loss: 4.4519433022
Epoch:   900  |  train loss: 4.5172184944
Epoch:  1000  |  train loss: 4.4882800102
Processing class: 97
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 4.6318207741
Epoch:   200  |  train loss: 4.5413669586
Epoch:   300  |  train loss: 4.4848683357
Epoch:   400  |  train loss: 4.4655884743
Epoch:   500  |  train loss: 4.4872822762
Epoch:   600  |  train loss: 4.4913897514
Epoch:   700  |  train loss: 4.5033695221
Epoch:   800  |  train loss: 4.4818719864
Epoch:   900  |  train loss: 4.4799277306
Epoch:  1000  |  train loss: 4.4534235954
Processing class: 98
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 4.4307896614
Epoch:   200  |  train loss: 4.3441682816
Epoch:   300  |  train loss: 4.2762098312
Epoch:   400  |  train loss: 4.2495402336
Epoch:   500  |  train loss: 4.1894339561
Epoch:   600  |  train loss: 4.1878623009
Epoch:   700  |  train loss: 4.0983928204
Epoch:   800  |  train loss: 4.1273200035
Epoch:   900  |  train loss: 4.1125587940
Epoch:  1000  |  train loss: 4.0696190834
Processing class: 99
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 4.7060057640
Epoch:   200  |  train loss: 4.5887770653
Epoch:   300  |  train loss: 4.6075501442
Epoch:   400  |  train loss: 4.6287209511
Epoch:   500  |  train loss: 4.5598525047
Epoch:   600  |  train loss: 4.5087745667
Epoch:   700  |  train loss: 4.4677230835
Epoch:   800  |  train loss: 4.5051267624
Epoch:   900  |  train loss: 4.4468230247
Epoch:  1000  |  train loss: 4.4406752586
2024-03-06 02:48:19,705 [trainer.py] => CNN: {'total': 50.36, '00-09': 57.7, '10-19': 63.6, '20-29': 71.4, '30-39': 68.5, '40-49': 56.8, '50-59': 29.6, '60-69': 38.9, '70-79': 36.6, '80-89': 42.7, '90-99': 37.8, 'old': 51.76, 'new': 37.8}
2024-03-06 02:48:19,705 [trainer.py] => No NME accuracy
2024-03-06 02:48:19,705 [trainer.py] => FeCAM: {'total': 54.74, '00-09': 73.3, '10-19': 65.1, '20-29': 74.1, '30-39': 67.2, '40-49': 67.1, '50-59': 30.8, '60-69': 43.3, '70-79': 37.2, '80-89': 46.9, '90-99': 42.4, 'old': 56.11, 'new': 42.4}
2024-03-06 02:48:19,705 [trainer.py] => CNN top1 curve: [83.44, 71.75, 64.66, 59.18, 54.08, 50.36]
2024-03-06 02:48:19,705 [trainer.py] => CNN top5 curve: [96.5, 89.6, 86.54, 84.09, 81.74, 79.63]
2024-03-06 02:48:19,706 [trainer.py] => FeCAM top1 curve: [82.9, 72.78, 67.27, 61.72, 57.66, 54.74]
2024-03-06 02:48:19,706 [trainer.py] => FeCAM top5 curve: [95.84, 91.18, 87.21, 84.09, 81.63, 79.21]

=========================================
2024-03-06 02:48:28,411 [trainer.py] => config: ./exps/FeCAM_cifar100.json
2024-03-06 02:48:28,411 [trainer.py] => prefix: train
2024-03-06 02:48:28,411 [trainer.py] => dataset: cifar100
2024-03-06 02:48:28,411 [trainer.py] => memory_size: 0
2024-03-06 02:48:28,411 [trainer.py] => shuffle: True
2024-03-06 02:48:28,411 [trainer.py] => init_cls: 50
2024-03-06 02:48:28,411 [trainer.py] => increment: 10
2024-03-06 02:48:28,411 [trainer.py] => model_name: fecam
2024-03-06 02:48:28,411 [trainer.py] => convnet_type: resnet18
2024-03-06 02:48:28,412 [trainer.py] => device: [device(type='cuda', index=0)]
2024-03-06 02:48:28,412 [trainer.py] => seed: 1993
2024-03-06 02:48:28,412 [trainer.py] => init_epochs: 200
2024-03-06 02:48:28,412 [trainer.py] => init_lr: 0.1
2024-03-06 02:48:28,412 [trainer.py] => init_weight_decay: 0.0005
2024-03-06 02:48:28,412 [trainer.py] => batch_size: 128
2024-03-06 02:48:28,412 [trainer.py] => num_workers: 8
2024-03-06 02:48:28,412 [trainer.py] => T: 5
2024-03-06 02:48:28,412 [trainer.py] => beta: 0.5
2024-03-06 02:48:28,412 [trainer.py] => alpha1: 1
2024-03-06 02:48:28,412 [trainer.py] => alpha2: 1
2024-03-06 02:48:28,412 [trainer.py] => ncm: False
2024-03-06 02:48:28,412 [trainer.py] => tukey: False
2024-03-06 02:48:28,412 [trainer.py] => diagonal: False
2024-03-06 02:48:28,412 [trainer.py] => per_class: True
2024-03-06 02:48:28,412 [trainer.py] => full_cov: True
2024-03-06 02:48:28,412 [trainer.py] => shrink: True
2024-03-06 02:48:28,412 [trainer.py] => norm_cov: False
2024-03-06 02:48:28,412 [trainer.py] => vecnorm: False
2024-03-06 02:48:28,412 [trainer.py] => ae_type: wae
2024-03-06 02:48:28,412 [trainer.py] => epochs: 1000
2024-03-06 02:48:28,412 [trainer.py] => ae_latent_dim: 32
2024-03-06 02:48:28,412 [trainer.py] => wae_sigma: 30
2024-03-06 02:48:28,412 [trainer.py] => wae_C: 10
Files already downloaded and verified
Files already downloaded and verified
2024-03-06 02:48:30,067 [data_manager.py] => [68, 56, 78, 8, 23, 84, 90, 65, 74, 76, 40, 89, 3, 92, 55, 9, 26, 80, 43, 38, 58, 70, 77, 1, 85, 19, 17, 50, 28, 53, 13, 81, 45, 82, 6, 59, 83, 16, 15, 44, 91, 41, 72, 60, 79, 52, 20, 10, 31, 54, 37, 95, 14, 71, 96, 98, 97, 2, 64, 66, 42, 22, 35, 86, 24, 34, 87, 21, 99, 0, 88, 27, 18, 94, 11, 12, 47, 25, 30, 46, 62, 69, 36, 61, 7, 63, 75, 5, 32, 4, 51, 48, 73, 93, 39, 67, 29, 49, 57, 33]
2024-03-06 02:48:30,332 [fecam.py] => Learning on 0-50
/home/z1165703/miniconda3/lib/python3.11/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
/home/z1165703/miniconda3/lib/python3.11/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
Processing class: 0
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 3.5475455284
Epoch:   200  |  train loss: 3.5689865112
Epoch:   300  |  train loss: 3.6299810410
Epoch:   400  |  train loss: 3.6463878632
Epoch:   500  |  train loss: 3.6119067192
Epoch:   600  |  train loss: 3.6122682095
Epoch:   700  |  train loss: 3.6331785679
Epoch:   800  |  train loss: 3.6405388355
Epoch:   900  |  train loss: 3.6094374657
Epoch:  1000  |  train loss: 3.6024434566
Processing class: 1
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 3.5372488499
Epoch:   200  |  train loss: 3.5101948738
Epoch:   300  |  train loss: 3.6335551262
Epoch:   400  |  train loss: 3.6583991051
Epoch:   500  |  train loss: 3.5774660587
Epoch:   600  |  train loss: 3.6728787899
Epoch:   700  |  train loss: 3.6258085728
Epoch:   800  |  train loss: 3.6720106602
Epoch:   900  |  train loss: 3.7436671257
Epoch:  1000  |  train loss: 3.6667371750
Processing class: 2
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 3.5307072639
Epoch:   200  |  train loss: 3.5139507771
Epoch:   300  |  train loss: 3.5941355705
Epoch:   400  |  train loss: 3.5650550365
Epoch:   500  |  train loss: 3.4237063408
Epoch:   600  |  train loss: 3.5204541206
Epoch:   700  |  train loss: 3.4965387821
Epoch:   800  |  train loss: 3.5700284481
Epoch:   900  |  train loss: 3.5151915073
Epoch:  1000  |  train loss: 3.4836479664
Processing class: 3
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 3.6868587971
Epoch:   200  |  train loss: 3.7693808079
Epoch:   300  |  train loss: 3.8016681194
Epoch:   400  |  train loss: 3.8067300320
Epoch:   500  |  train loss: 3.7643957615
Epoch:   600  |  train loss: 3.8446749210
Epoch:   700  |  train loss: 3.8894427299
Epoch:   800  |  train loss: 3.8264680862
Epoch:   900  |  train loss: 3.8621178150
Epoch:  1000  |  train loss: 3.8696211338
Processing class: 4
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 3.5126622200
Epoch:   200  |  train loss: 3.4703138828
Epoch:   300  |  train loss: 3.5653176308
Epoch:   400  |  train loss: 3.5597868919
Epoch:   500  |  train loss: 3.5953978062
Epoch:   600  |  train loss: 3.5663449764
Epoch:   700  |  train loss: 3.6104570389
Epoch:   800  |  train loss: 3.6198163509
Epoch:   900  |  train loss: 3.6375537872
Epoch:  1000  |  train loss: 3.6973744869
Processing class: 5
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 3.5521785259
Epoch:   200  |  train loss: 3.5127448082
Epoch:   300  |  train loss: 3.5367568493
Epoch:   400  |  train loss: 3.5280420303
Epoch:   500  |  train loss: 3.5147868633
Epoch:   600  |  train loss: 3.6264343262
Epoch:   700  |  train loss: 3.6401751041
Epoch:   800  |  train loss: 3.5781643391
Epoch:   900  |  train loss: 3.5582467079
Epoch:  1000  |  train loss: 3.6067415237
Processing class: 6
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 3.5440470219
Epoch:   200  |  train loss: 3.5843722820
Epoch:   300  |  train loss: 3.6826968670
Epoch:   400  |  train loss: 3.6586108208
Epoch:   500  |  train loss: 3.7395399094
Epoch:   600  |  train loss: 3.7008014202
Epoch:   700  |  train loss: 3.6684910774
Epoch:   800  |  train loss: 3.6901596069
Epoch:   900  |  train loss: 3.6432847977
Epoch:  1000  |  train loss: 3.6757198811
Processing class: 7
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 3.4937144279
Epoch:   200  |  train loss: 3.5036495686
Epoch:   300  |  train loss: 3.5296226501
Epoch:   400  |  train loss: 3.5426198006
Epoch:   500  |  train loss: 3.5759033203
Epoch:   600  |  train loss: 3.5406906128
Epoch:   700  |  train loss: 3.5810110092
Epoch:   800  |  train loss: 3.6553217888
Epoch:   900  |  train loss: 3.5792240143
Epoch:  1000  |  train loss: 3.6354987144
Processing class: 8
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 3.5104006290
Epoch:   200  |  train loss: 3.5610784054
Epoch:   300  |  train loss: 3.5201897621
Epoch:   400  |  train loss: 3.6151405334
Epoch:   500  |  train loss: 3.5636196613
Epoch:   600  |  train loss: 3.5817764282
Epoch:   700  |  train loss: 3.6497191429
Epoch:   800  |  train loss: 3.5811696053
Epoch:   900  |  train loss: 3.6205217361
Epoch:  1000  |  train loss: 3.5753410339
Processing class: 9
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 3.4400033951
Epoch:   200  |  train loss: 3.4962782383
Epoch:   300  |  train loss: 3.5771404743
Epoch:   400  |  train loss: 3.4918587208
Epoch:   500  |  train loss: 3.5706236362
Epoch:   600  |  train loss: 3.5152072430
Epoch:   700  |  train loss: 3.5443993092
Epoch:   800  |  train loss: 3.5653316975
Epoch:   900  |  train loss: 3.5649029255
Epoch:  1000  |  train loss: 3.5086678028
Processing class: 10
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 3.4147675037
Epoch:   200  |  train loss: 3.3954450607
Epoch:   300  |  train loss: 3.4198228836
Epoch:   400  |  train loss: 3.5088239670
Epoch:   500  |  train loss: 3.3879170895
Epoch:   600  |  train loss: 3.4251935482
Epoch:   700  |  train loss: 3.4552690029
Epoch:   800  |  train loss: 3.4251553535
Epoch:   900  |  train loss: 3.4494431496
Epoch:  1000  |  train loss: 3.4409098625
Processing class: 11
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 3.5563477516
Epoch:   200  |  train loss: 3.5728670120
Epoch:   300  |  train loss: 3.5249135017
Epoch:   400  |  train loss: 3.5615788937
Epoch:   500  |  train loss: 3.5546572685
Epoch:   600  |  train loss: 3.6091289520
Epoch:   700  |  train loss: 3.6929244995
Epoch:   800  |  train loss: 3.6729804039
Epoch:   900  |  train loss: 3.6717884541
Epoch:  1000  |  train loss: 3.7117706776
Processing class: 12
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 3.4240838528
Epoch:   200  |  train loss: 3.5566366673
Epoch:   300  |  train loss: 3.5526051521
Epoch:   400  |  train loss: 3.6076314926
Epoch:   500  |  train loss: 3.6114305019
Epoch:   600  |  train loss: 3.5638319969
Epoch:   700  |  train loss: 3.5833470821
Epoch:   800  |  train loss: 3.6030198574
Epoch:   900  |  train loss: 3.6450638294
Epoch:  1000  |  train loss: 3.6069907188
Processing class: 13
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 3.4676632404
Epoch:   200  |  train loss: 3.5598712444
Epoch:   300  |  train loss: 3.6259890079
Epoch:   400  |  train loss: 3.6357541084
Epoch:   500  |  train loss: 3.6936376572
Epoch:   600  |  train loss: 3.6346433640
Epoch:   700  |  train loss: 3.6690425873
Epoch:   800  |  train loss: 3.7072757244
Epoch:   900  |  train loss: 3.7290240765
Epoch:  1000  |  train loss: 3.7507049561
Processing class: 14
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 3.5450686932
Epoch:   200  |  train loss: 3.5857052803
Epoch:   300  |  train loss: 3.6039445877
Epoch:   400  |  train loss: 3.6654504299
Epoch:   500  |  train loss: 3.6397432804
Epoch:   600  |  train loss: 3.6327044487
Epoch:   700  |  train loss: 3.6725774765
Epoch:   800  |  train loss: 3.7602955818
Epoch:   900  |  train loss: 3.7260309696
Epoch:  1000  |  train loss: 3.6630108356
Processing class: 15
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 3.5024127960
Epoch:   200  |  train loss: 3.5419721603
Epoch:   300  |  train loss: 3.6145045757
Epoch:   400  |  train loss: 3.6440637112
Epoch:   500  |  train loss: 3.6871526718
Epoch:   600  |  train loss: 3.6326803207
Epoch:   700  |  train loss: 3.7073800087
Epoch:   800  |  train loss: 3.6340132713
Epoch:   900  |  train loss: 3.7083575726
Epoch:  1000  |  train loss: 3.6746488571
Processing class: 16
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 3.5344945908
Epoch:   200  |  train loss: 3.6910420418
Epoch:   300  |  train loss: 3.7583322525
Epoch:   400  |  train loss: 3.7107860565
Epoch:   500  |  train loss: 3.7741967678
Epoch:   600  |  train loss: 3.7177516937
Epoch:   700  |  train loss: 3.6932489872
Epoch:   800  |  train loss: 3.7825147152
Epoch:   900  |  train loss: 3.6918876171
Epoch:  1000  |  train loss: 3.7747780323
Processing class: 17
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 3.3986978054
Epoch:   200  |  train loss: 3.4498627186
Epoch:   300  |  train loss: 3.5262406349
Epoch:   400  |  train loss: 3.5323101521
Epoch:   500  |  train loss: 3.4272619247
Epoch:   600  |  train loss: 3.4792793751
Epoch:   700  |  train loss: 3.4579861164
Epoch:   800  |  train loss: 3.4597464085
Epoch:   900  |  train loss: 3.4778636456
Epoch:  1000  |  train loss: 3.4679610252
Processing class: 18
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 3.4710778236
Epoch:   200  |  train loss: 3.4919287205
Epoch:   300  |  train loss: 3.5368267059
Epoch:   400  |  train loss: 3.5203104019
Epoch:   500  |  train loss: 3.5913547516
Epoch:   600  |  train loss: 3.5490687847
Epoch:   700  |  train loss: 3.6653855324
Epoch:   800  |  train loss: 3.6521150589
Epoch:   900  |  train loss: 3.6711081982
Epoch:  1000  |  train loss: 3.7349167824
Processing class: 19
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 3.4886026859
Epoch:   200  |  train loss: 3.6281003952
Epoch:   300  |  train loss: 3.6206505775
Epoch:   400  |  train loss: 3.6309790134
Epoch:   500  |  train loss: 3.5729310513
Epoch:   600  |  train loss: 3.5822172165
Epoch:   700  |  train loss: 3.6122880459
Epoch:   800  |  train loss: 3.5838556290
Epoch:   900  |  train loss: 3.7098886013
Epoch:  1000  |  train loss: 3.7326599598
Processing class: 20
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 3.5194184780
Epoch:   200  |  train loss: 3.5575088978
Epoch:   300  |  train loss: 3.6045150757
Epoch:   400  |  train loss: 3.6296361446
Epoch:   500  |  train loss: 3.7009271622
Epoch:   600  |  train loss: 3.6511964798
Epoch:   700  |  train loss: 3.6616420269
Epoch:   800  |  train loss: 3.6611365795
Epoch:   900  |  train loss: 3.6873362064
Epoch:  1000  |  train loss: 3.7378037453
Processing class: 21
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 3.5336950302
Epoch:   200  |  train loss: 3.5846258640
Epoch:   300  |  train loss: 3.5973223686
Epoch:   400  |  train loss: 3.6089721203
Epoch:   500  |  train loss: 3.6301997185
Epoch:   600  |  train loss: 3.6678944111
Epoch:   700  |  train loss: 3.6512596607
Epoch:   800  |  train loss: 3.6259853363
Epoch:   900  |  train loss: 3.6846674919
Epoch:  1000  |  train loss: 3.6589437008
Processing class: 22
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 3.5063181877
Epoch:   200  |  train loss: 3.5260106564
Epoch:   300  |  train loss: 3.5886442661
Epoch:   400  |  train loss: 3.6527333260
Epoch:   500  |  train loss: 3.6610026836
Epoch:   600  |  train loss: 3.6162290096
Epoch:   700  |  train loss: 3.6420359612
Epoch:   800  |  train loss: 3.6124471188
Epoch:   900  |  train loss: 3.5788300514
Epoch:  1000  |  train loss: 3.6810416698
Processing class: 23
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 3.5715150833
Epoch:   200  |  train loss: 3.6364843845
Epoch:   300  |  train loss: 3.6974305153
Epoch:   400  |  train loss: 3.6174492836
Epoch:   500  |  train loss: 3.6084334850
Epoch:   600  |  train loss: 3.5596000195
Epoch:   700  |  train loss: 3.5959095478
Epoch:   800  |  train loss: 3.5570235729
Epoch:   900  |  train loss: 3.5981192112
Epoch:  1000  |  train loss: 3.5501119614
Processing class: 24
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 3.5416539669
Epoch:   200  |  train loss: 3.5399651527
Epoch:   300  |  train loss: 3.4776180744
Epoch:   400  |  train loss: 3.5395129204
Epoch:   500  |  train loss: 3.5003927708
Epoch:   600  |  train loss: 3.5392348289
Epoch:   700  |  train loss: 3.4419457912
Epoch:   800  |  train loss: 3.4704426765
Epoch:   900  |  train loss: 3.5221323490
Epoch:  1000  |  train loss: 3.4943935871
Processing class: 25
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 3.4968214512
Epoch:   200  |  train loss: 3.5409192562
Epoch:   300  |  train loss: 3.6086750984
Epoch:   400  |  train loss: 3.5437063217
Epoch:   500  |  train loss: 3.6171220303
Epoch:   600  |  train loss: 3.5715822220
Epoch:   700  |  train loss: 3.6455489159
Epoch:   800  |  train loss: 3.6100243092
Epoch:   900  |  train loss: 3.5819626808
Epoch:  1000  |  train loss: 3.6069036484
Processing class: 26
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 3.6065038681
Epoch:   200  |  train loss: 3.6930872440
Epoch:   300  |  train loss: 3.6451454163
Epoch:   400  |  train loss: 3.7086763382
Epoch:   500  |  train loss: 3.7694449425
Epoch:   600  |  train loss: 3.6600390434
Epoch:   700  |  train loss: 3.6929121017
Epoch:   800  |  train loss: 3.7231987476
Epoch:   900  |  train loss: 3.7020340919
Epoch:  1000  |  train loss: 3.7585171223
Processing class: 27
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 3.5221278667
Epoch:   200  |  train loss: 3.4787188530
Epoch:   300  |  train loss: 3.5060059071
Epoch:   400  |  train loss: 3.5176633358
Epoch:   500  |  train loss: 3.5530585289
Epoch:   600  |  train loss: 3.5523708344
Epoch:   700  |  train loss: 3.5732823849
Epoch:   800  |  train loss: 3.5669041634
Epoch:   900  |  train loss: 3.5867990494
Epoch:  1000  |  train loss: 3.5098293304
Processing class: 28
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 3.5882561684
Epoch:   200  |  train loss: 3.6586852074
Epoch:   300  |  train loss: 3.7234735489
Epoch:   400  |  train loss: 3.7518317699
Epoch:   500  |  train loss: 3.7678501606
Epoch:   600  |  train loss: 3.7669631481
Epoch:   700  |  train loss: 3.7820937157
Epoch:   800  |  train loss: 3.8031173229
Epoch:   900  |  train loss: 3.8152386189
Epoch:  1000  |  train loss: 3.7563686371
Processing class: 29
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 3.5161246777
Epoch:   200  |  train loss: 3.5540890217
Epoch:   300  |  train loss: 3.5391041756
Epoch:   400  |  train loss: 3.4976599693
Epoch:   500  |  train loss: 3.5538928032
Epoch:   600  |  train loss: 3.5778654099
Epoch:   700  |  train loss: 3.5694525242
Epoch:   800  |  train loss: 3.5198400974
Epoch:   900  |  train loss: 3.5277200222
Epoch:  1000  |  train loss: 3.5082516670
Processing class: 30
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 3.6014323711
Epoch:   200  |  train loss: 3.6777494907
Epoch:   300  |  train loss: 3.7773012161
Epoch:   400  |  train loss: 3.8692165375
Epoch:   500  |  train loss: 3.8238453865
Epoch:   600  |  train loss: 3.8114325523
Epoch:   700  |  train loss: 3.7892951965
Epoch:   800  |  train loss: 3.8515804768
Epoch:   900  |  train loss: 3.8907367706
Epoch:  1000  |  train loss: 3.8382451057
Processing class: 31
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 3.5949900627
Epoch:   200  |  train loss: 3.6752289772
Epoch:   300  |  train loss: 3.6737344265
Epoch:   400  |  train loss: 3.7573802471
Epoch:   500  |  train loss: 3.7813012600
Epoch:   600  |  train loss: 3.8446368217
Epoch:   700  |  train loss: 3.8229153633
Epoch:   800  |  train loss: 3.8256361961
Epoch:   900  |  train loss: 3.8761399269
Epoch:  1000  |  train loss: 3.8089246273
Processing class: 32
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 3.5390341282
Epoch:   200  |  train loss: 3.6094433308
Epoch:   300  |  train loss: 3.5880357265
Epoch:   400  |  train loss: 3.5162520409
Epoch:   500  |  train loss: 3.6299406528
Epoch:   600  |  train loss: 3.5858557224
Epoch:   700  |  train loss: 3.6725310326
Epoch:   800  |  train loss: 3.7014313221
Epoch:   900  |  train loss: 3.6768313408
Epoch:  1000  |  train loss: 3.6470891476
Processing class: 33
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 3.4074537277
Epoch:   200  |  train loss: 3.5791163445
Epoch:   300  |  train loss: 3.4443799496
Epoch:   400  |  train loss: 3.4511113167
Epoch:   500  |  train loss: 3.4568668365
Epoch:   600  |  train loss: 3.5480603695
Epoch:   700  |  train loss: 3.4918681622
Epoch:   800  |  train loss: 3.5107318401
Epoch:   900  |  train loss: 3.4975453854
Epoch:  1000  |  train loss: 3.4666182995
Processing class: 34
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 3.5495822430
Epoch:   200  |  train loss: 3.5273993969
Epoch:   300  |  train loss: 3.5744146824
Epoch:   400  |  train loss: 3.5341943264
Epoch:   500  |  train loss: 3.5216367245
Epoch:   600  |  train loss: 3.5745399475
Epoch:   700  |  train loss: 3.5159171104
Epoch:   800  |  train loss: 3.5793494701
Epoch:   900  |  train loss: 3.5968495369
Epoch:  1000  |  train loss: 3.6078809261
Processing class: 35
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 3.5027282238
Epoch:   200  |  train loss: 3.5614418030
Epoch:   300  |  train loss: 3.6121288776
Epoch:   400  |  train loss: 3.6320812225
Epoch:   500  |  train loss: 3.5782742023
Epoch:   600  |  train loss: 3.6171709061
Epoch:   700  |  train loss: 3.6341747284
Epoch:   800  |  train loss: 3.6225958347
Epoch:   900  |  train loss: 3.6307388306
Epoch:  1000  |  train loss: 3.5985511303
Processing class: 36
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 3.5293567657
Epoch:   200  |  train loss: 3.5445539474
Epoch:   300  |  train loss: 3.5872620106
Epoch:   400  |  train loss: 3.5853115082
Epoch:   500  |  train loss: 3.6637626648
Epoch:   600  |  train loss: 3.6176372528
Epoch:   700  |  train loss: 3.5692367554
Epoch:   800  |  train loss: 3.6604995251
Epoch:   900  |  train loss: 3.5757709026
Epoch:  1000  |  train loss: 3.6283154964
Processing class: 37
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 3.5852880478
Epoch:   200  |  train loss: 3.5672510147
Epoch:   300  |  train loss: 3.5491114616
Epoch:   400  |  train loss: 3.5816559792
Epoch:   500  |  train loss: 3.6121856689
Epoch:   600  |  train loss: 3.5610683441
Epoch:   700  |  train loss: 3.5656544685
Epoch:   800  |  train loss: 3.5864741325
Epoch:   900  |  train loss: 3.5409339905
Epoch:  1000  |  train loss: 3.5650997162
Processing class: 38
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 3.5099646568
Epoch:   200  |  train loss: 3.5395505905
Epoch:   300  |  train loss: 3.6030471802
Epoch:   400  |  train loss: 3.6277685165
Epoch:   500  |  train loss: 3.6405806065
Epoch:   600  |  train loss: 3.6131908417
Epoch:   700  |  train loss: 3.6159075737
Epoch:   800  |  train loss: 3.6756042957
Epoch:   900  |  train loss: 3.5971061707
Epoch:  1000  |  train loss: 3.6474599838
Processing class: 39
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 3.5441448689
Epoch:   200  |  train loss: 3.5166543007
Epoch:   300  |  train loss: 3.5592785835
Epoch:   400  |  train loss: 3.5896348000
Epoch:   500  |  train loss: 3.5595773697
Epoch:   600  |  train loss: 3.5778941631
Epoch:   700  |  train loss: 3.6012023926
Epoch:   800  |  train loss: 3.6388320446
Epoch:   900  |  train loss: 3.6550836563
Epoch:  1000  |  train loss: 3.6669921875
Processing class: 40
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 3.5501215458
Epoch:   200  |  train loss: 3.4919057369
Epoch:   300  |  train loss: 3.6127994537
Epoch:   400  |  train loss: 3.6354218483
Epoch:   500  |  train loss: 3.6658188343
Epoch:   600  |  train loss: 3.6206294537
Epoch:   700  |  train loss: 3.6506058693
Epoch:   800  |  train loss: 3.6437429905
Epoch:   900  |  train loss: 3.6089244843
Epoch:  1000  |  train loss: 3.7166680336
Processing class: 41
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 3.5065637589
Epoch:   200  |  train loss: 3.5206303596
Epoch:   300  |  train loss: 3.5659851551
Epoch:   400  |  train loss: 3.5314684868
Epoch:   500  |  train loss: 3.5275453568
Epoch:   600  |  train loss: 3.5286221981
Epoch:   700  |  train loss: 3.5776777744
Epoch:   800  |  train loss: 3.5443811417
Epoch:   900  |  train loss: 3.5273018360
Epoch:  1000  |  train loss: 3.5270025730
Processing class: 42
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 3.4849966526
Epoch:   200  |  train loss: 3.5597940922
Epoch:   300  |  train loss: 3.6052246571
Epoch:   400  |  train loss: 3.6158747196
Epoch:   500  |  train loss: 3.6402374744
Epoch:   600  |  train loss: 3.6544544697
Epoch:   700  |  train loss: 3.6836316109
Epoch:   800  |  train loss: 3.7002611637
Epoch:   900  |  train loss: 3.6615967751
Epoch:  1000  |  train loss: 3.6515553474
Processing class: 43
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 3.5235909939
Epoch:   200  |  train loss: 3.5091289043
Epoch:   300  |  train loss: 3.6299674034
Epoch:   400  |  train loss: 3.7452200413
Epoch:   500  |  train loss: 3.7719259739
Epoch:   600  |  train loss: 3.7417450905
Epoch:   700  |  train loss: 3.7884118080
Epoch:   800  |  train loss: 3.7644923687
Epoch:   900  |  train loss: 3.6948897839
Epoch:  1000  |  train loss: 3.7746263027
Processing class: 44
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 3.4886007786
Epoch:   200  |  train loss: 3.5719498634
Epoch:   300  |  train loss: 3.5359008789
Epoch:   400  |  train loss: 3.5473562717
Epoch:   500  |  train loss: 3.5817348003
Epoch:   600  |  train loss: 3.5506815910
Epoch:   700  |  train loss: 3.6100608826
Epoch:   800  |  train loss: 3.5874663353
Epoch:   900  |  train loss: 3.5569385052
Epoch:  1000  |  train loss: 3.6209452152
Processing class: 45
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 3.5543981075
Epoch:   200  |  train loss: 3.6679010391
Epoch:   300  |  train loss: 3.6457365513
Epoch:   400  |  train loss: 3.7225044250
Epoch:   500  |  train loss: 3.7070339680
Epoch:   600  |  train loss: 3.8009104729
Epoch:   700  |  train loss: 3.8193621635
Epoch:   800  |  train loss: 3.7917477608
Epoch:   900  |  train loss: 3.8025104046
Epoch:  1000  |  train loss: 3.8517326355
Processing class: 46
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 3.5776634216
Epoch:   200  |  train loss: 3.5835791111
Epoch:   300  |  train loss: 3.6197701931
Epoch:   400  |  train loss: 3.6492845535
Epoch:   500  |  train loss: 3.6552956104
Epoch:   600  |  train loss: 3.6568670273
Epoch:   700  |  train loss: 3.6655188560
Epoch:   800  |  train loss: 3.7402122498
Epoch:   900  |  train loss: 3.6911003113
Epoch:  1000  |  train loss: 3.7252104282
Processing class: 47
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 3.4845886707
Epoch:   200  |  train loss: 3.5496951103
Epoch:   300  |  train loss: 3.5013628960
Epoch:   400  |  train loss: 3.5684394836
Epoch:   500  |  train loss: 3.5637450218
Epoch:   600  |  train loss: 3.5553505421
Epoch:   700  |  train loss: 3.5725585938
Epoch:   800  |  train loss: 3.5280616283
Epoch:   900  |  train loss: 3.5468539715
Epoch:  1000  |  train loss: 3.5203258991
Processing class: 48
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 3.5249419212
Epoch:   200  |  train loss: 3.6084463120
Epoch:   300  |  train loss: 3.5783859253
Epoch:   400  |  train loss: 3.5061110973
Epoch:   500  |  train loss: 3.6013109207
Epoch:   600  |  train loss: 3.5834026337
Epoch:   700  |  train loss: 3.5347146511
Epoch:   800  |  train loss: 3.5318957329
Epoch:   900  |  train loss: 3.6265953064
Epoch:  1000  |  train loss: 3.6482249737
Processing class: 49
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 3.5150704384
Epoch:   200  |  train loss: 3.5338149071
Epoch:   300  |  train loss: 3.6453871727
Epoch:   400  |  train loss: 3.6593533993
Epoch:   500  |  train loss: 3.6815563202
Epoch:   600  |  train loss: 3.6812898159
Epoch:   700  |  train loss: 3.6755862713
Epoch:   800  |  train loss: 3.6516081333
Epoch:   900  |  train loss: 3.6358961582
Epoch:  1000  |  train loss: 3.6476409912
/home/z1165703/FeCAM/models/base.py:87: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  vec = torch.tensor(vec).cuda()
2024-03-06 03:06:07,554 [trainer.py] => CNN: {'total': 83.44, '00-09': 87.7, '10-19': 79.6, '20-29': 84.9, '30-39': 81.0, '40-49': 84.0, 'old': 0, 'new': 83.44}
2024-03-06 03:06:07,555 [trainer.py] => No NME accuracy
2024-03-06 03:06:07,555 [trainer.py] => FeCAM: {'total': 82.9, '00-09': 86.2, '10-19': 79.7, '20-29': 85.0, '30-39': 80.6, '40-49': 83.0, 'old': 0, 'new': 82.9}
2024-03-06 03:06:07,555 [trainer.py] => CNN top1 curve: [83.44]
2024-03-06 03:06:07,555 [trainer.py] => CNN top5 curve: [96.5]
2024-03-06 03:06:07,555 [trainer.py] => FeCAM top1 curve: [82.9]
2024-03-06 03:06:07,555 [trainer.py] => FeCAM top5 curve: [95.84]

2024-03-06 03:06:07,565 [fecam.py] => Learning on 50-60
Processing class: 50
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 3.4960061073
Epoch:   200  |  train loss: 3.5100173950
Epoch:   300  |  train loss: 3.5179953098
Epoch:   400  |  train loss: 3.5761940956
Epoch:   500  |  train loss: 3.6061851025
Epoch:   600  |  train loss: 3.5593410969
Epoch:   700  |  train loss: 3.6248356819
Epoch:   800  |  train loss: 3.6542880535
Epoch:   900  |  train loss: 3.6937651634
Epoch:  1000  |  train loss: 3.7068704605
Processing class: 51
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 3.5273444176
Epoch:   200  |  train loss: 3.4496554852
Epoch:   300  |  train loss: 3.5515187263
Epoch:   400  |  train loss: 3.5688366890
Epoch:   500  |  train loss: 3.5768278122
Epoch:   600  |  train loss: 3.6092829227
Epoch:   700  |  train loss: 3.5474932671
Epoch:   800  |  train loss: 3.5596461296
Epoch:   900  |  train loss: 3.6396067619
Epoch:  1000  |  train loss: 3.6005066395
Processing class: 52
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 3.5571611404
Epoch:   200  |  train loss: 3.5284012794
Epoch:   300  |  train loss: 3.5693779469
Epoch:   400  |  train loss: 3.5981359482
Epoch:   500  |  train loss: 3.5835201263
Epoch:   600  |  train loss: 3.5374897957
Epoch:   700  |  train loss: 3.5993184566
Epoch:   800  |  train loss: 3.6006244183
Epoch:   900  |  train loss: 3.6139961243
Epoch:  1000  |  train loss: 3.5632946968
Processing class: 53
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 3.4694396973
Epoch:   200  |  train loss: 3.4868048191
Epoch:   300  |  train loss: 3.4165873528
Epoch:   400  |  train loss: 3.4388358116
Epoch:   500  |  train loss: 3.4713810921
Epoch:   600  |  train loss: 3.5255147457
Epoch:   700  |  train loss: 3.5857555866
Epoch:   800  |  train loss: 3.4613778114
Epoch:   900  |  train loss: 3.5204374790
Epoch:  1000  |  train loss: 3.4741015434
Processing class: 54
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 3.2642556190
Epoch:   200  |  train loss: 3.2760792255
Epoch:   300  |  train loss: 3.3265881062
Epoch:   400  |  train loss: 3.3433798790
Epoch:   500  |  train loss: 3.3697732925
Epoch:   600  |  train loss: 3.4394296646
Epoch:   700  |  train loss: 3.4564697266
Epoch:   800  |  train loss: 3.5129704475
Epoch:   900  |  train loss: 3.5517393589
Epoch:  1000  |  train loss: 3.5281516552
Processing class: 55
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 3.5241875172
Epoch:   200  |  train loss: 3.5331410408
Epoch:   300  |  train loss: 3.4955411911
Epoch:   400  |  train loss: 3.5682755470
Epoch:   500  |  train loss: 3.6640588760
Epoch:   600  |  train loss: 3.6725987434
Epoch:   700  |  train loss: 3.7020092964
Epoch:   800  |  train loss: 3.7736646175
Epoch:   900  |  train loss: 3.6802155495
Epoch:  1000  |  train loss: 3.7325123310
Processing class: 56
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 3.4602530956
Epoch:   200  |  train loss: 3.4799432755
Epoch:   300  |  train loss: 3.4011716366
Epoch:   400  |  train loss: 3.4838819027
Epoch:   500  |  train loss: 3.5075662613
Epoch:   600  |  train loss: 3.5200495720
Epoch:   700  |  train loss: 3.5493231773
Epoch:   800  |  train loss: 3.5064835548
Epoch:   900  |  train loss: 3.5629724503
Epoch:  1000  |  train loss: 3.5950723171
Processing class: 57
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 3.4856484890
Epoch:   200  |  train loss: 3.5523853779
Epoch:   300  |  train loss: 3.5242528439
Epoch:   400  |  train loss: 3.5237353802
Epoch:   500  |  train loss: 3.5720509529
Epoch:   600  |  train loss: 3.5055781364
Epoch:   700  |  train loss: 3.5972316265
Epoch:   800  |  train loss: 3.5926012039
Epoch:   900  |  train loss: 3.6416521549
Epoch:  1000  |  train loss: 3.5597043037
Processing class: 58
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 3.4780037403
Epoch:   200  |  train loss: 3.4354118347
Epoch:   300  |  train loss: 3.4944427013
Epoch:   400  |  train loss: 3.5677383423
Epoch:   500  |  train loss: 3.5419907093
Epoch:   600  |  train loss: 3.5956390858
Epoch:   700  |  train loss: 3.5739079952
Epoch:   800  |  train loss: 3.6179275513
Epoch:   900  |  train loss: 3.6130595207
Epoch:  1000  |  train loss: 3.6117619514
Processing class: 59
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 3.5733887196
Epoch:   200  |  train loss: 3.6614251137
Epoch:   300  |  train loss: 3.6180445671
Epoch:   400  |  train loss: 3.6530235767
Epoch:   500  |  train loss: 3.6972456932
Epoch:   600  |  train loss: 3.7035525799
Epoch:   700  |  train loss: 3.7352615356
Epoch:   800  |  train loss: 3.7472037792
Epoch:   900  |  train loss: 3.7329282284
Epoch:  1000  |  train loss: 3.7120935917
2024-03-06 03:11:46,761 [trainer.py] => CNN: {'total': 71.75, '00-09': 82.2, '10-19': 72.8, '20-29': 78.5, '30-39': 74.9, '40-49': 68.4, '50-59': 53.7, 'old': 75.36, 'new': 53.7}
2024-03-06 03:11:46,761 [trainer.py] => No NME accuracy
2024-03-06 03:11:46,762 [trainer.py] => FeCAM: {'total': 72.78, '00-09': 82.6, '10-19': 74.0, '20-29': 81.0, '30-39': 76.4, '40-49': 76.8, '50-59': 45.9, 'old': 78.16, 'new': 45.9}
2024-03-06 03:11:46,762 [trainer.py] => CNN top1 curve: [83.44, 71.75]
2024-03-06 03:11:46,762 [trainer.py] => CNN top5 curve: [96.5, 89.6]
2024-03-06 03:11:46,762 [trainer.py] => FeCAM top1 curve: [82.9, 72.78]
2024-03-06 03:11:46,762 [trainer.py] => FeCAM top5 curve: [95.84, 91.18]

2024-03-06 03:11:46,766 [fecam.py] => Learning on 60-70
Processing class: 60
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 3.6037433147
Epoch:   200  |  train loss: 3.5900048256
Epoch:   300  |  train loss: 3.5419282436
Epoch:   400  |  train loss: 3.5211549282
Epoch:   500  |  train loss: 3.6000978470
Epoch:   600  |  train loss: 3.5973275661
Epoch:   700  |  train loss: 3.7735077381
Epoch:   800  |  train loss: 3.6697269440
Epoch:   900  |  train loss: 3.6822712898
Epoch:  1000  |  train loss: 3.7269652367
Processing class: 61
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 3.4573042870
Epoch:   200  |  train loss: 3.3884881020
Epoch:   300  |  train loss: 3.4603676319
Epoch:   400  |  train loss: 3.4631269455
Epoch:   500  |  train loss: 3.5568065643
Epoch:   600  |  train loss: 3.4845089436
Epoch:   700  |  train loss: 3.5666121483
Epoch:   800  |  train loss: 3.6164353848
Epoch:   900  |  train loss: 3.5306069851
Epoch:  1000  |  train loss: 3.5648653030
Processing class: 62
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 3.5784828186
Epoch:   200  |  train loss: 3.5347996235
Epoch:   300  |  train loss: 3.5923104286
Epoch:   400  |  train loss: 3.6168637276
Epoch:   500  |  train loss: 3.5729770184
Epoch:   600  |  train loss: 3.6589525700
Epoch:   700  |  train loss: 3.6847259521
Epoch:   800  |  train loss: 3.7010770321
Epoch:   900  |  train loss: 3.6749148846
Epoch:  1000  |  train loss: 3.6979785919
Processing class: 63
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 3.4398414612
Epoch:   200  |  train loss: 3.4608459949
Epoch:   300  |  train loss: 3.5720252514
Epoch:   400  |  train loss: 3.5733427525
Epoch:   500  |  train loss: 3.6065427303
Epoch:   600  |  train loss: 3.6836563587
Epoch:   700  |  train loss: 3.5849764347
Epoch:   800  |  train loss: 3.7203445435
Epoch:   900  |  train loss: 3.6295386791
Epoch:  1000  |  train loss: 3.6504549026
Processing class: 64
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 3.5896535397
Epoch:   200  |  train loss: 3.5090064049
Epoch:   300  |  train loss: 3.6372198105
Epoch:   400  |  train loss: 3.6398526669
Epoch:   500  |  train loss: 3.6411121845
Epoch:   600  |  train loss: 3.6306916237
Epoch:   700  |  train loss: 3.5619453907
Epoch:   800  |  train loss: 3.5963712692
Epoch:   900  |  train loss: 3.6622144699
Epoch:  1000  |  train loss: 3.6341569424
Processing class: 65
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 3.5002619743
Epoch:   200  |  train loss: 3.4743594646
Epoch:   300  |  train loss: 3.5988842964
Epoch:   400  |  train loss: 3.6461928844
Epoch:   500  |  train loss: 3.6191606998
Epoch:   600  |  train loss: 3.7280298233
Epoch:   700  |  train loss: 3.7265296459
Epoch:   800  |  train loss: 3.8269718170
Epoch:   900  |  train loss: 3.7598375320
Epoch:  1000  |  train loss: 3.8630493164
Processing class: 66
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 3.5817652225
Epoch:   200  |  train loss: 3.6270368099
Epoch:   300  |  train loss: 3.6802212715
Epoch:   400  |  train loss: 3.6779633045
Epoch:   500  |  train loss: 3.7451396465
Epoch:   600  |  train loss: 3.7511404991
Epoch:   700  |  train loss: 3.8253004551
Epoch:   800  |  train loss: 3.8327847004
Epoch:   900  |  train loss: 3.8275716305
Epoch:  1000  |  train loss: 3.8168689728
Processing class: 67
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 3.4625671387
Epoch:   200  |  train loss: 3.4990755558
Epoch:   300  |  train loss: 3.4937073708
Epoch:   400  |  train loss: 3.5073941708
Epoch:   500  |  train loss: 3.5072620392
Epoch:   600  |  train loss: 3.5026169300
Epoch:   700  |  train loss: 3.5177872181
Epoch:   800  |  train loss: 3.5624115467
Epoch:   900  |  train loss: 3.5492048264
Epoch:  1000  |  train loss: 3.5429193497
Processing class: 68
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 3.4230667591
Epoch:   200  |  train loss: 3.4197386742
Epoch:   300  |  train loss: 3.3859694481
Epoch:   400  |  train loss: 3.4079896450
Epoch:   500  |  train loss: 3.4127103806
Epoch:   600  |  train loss: 3.3968108177
Epoch:   700  |  train loss: 3.4281375885
Epoch:   800  |  train loss: 3.3926240444
Epoch:   900  |  train loss: 3.4130715847
Epoch:  1000  |  train loss: 3.5014768124
Processing class: 69
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 3.2937612057
Epoch:   200  |  train loss: 3.3443233013
Epoch:   300  |  train loss: 3.4556487083
Epoch:   400  |  train loss: 3.5019689083
Epoch:   500  |  train loss: 3.5041419029
Epoch:   600  |  train loss: 3.4218391895
Epoch:   700  |  train loss: 3.4918167591
Epoch:   800  |  train loss: 3.4839124680
Epoch:   900  |  train loss: 3.5886416435
Epoch:  1000  |  train loss: 3.4836225986
2024-03-06 03:18:20,504 [trainer.py] => CNN: {'total': 64.66, '00-09': 75.0, '10-19': 70.1, '20-29': 77.1, '30-39': 70.6, '40-49': 64.0, '50-59': 45.8, '60-69': 50.0, 'old': 67.1, 'new': 50.0}
2024-03-06 03:18:20,506 [trainer.py] => No NME accuracy
2024-03-06 03:18:20,506 [trainer.py] => FeCAM: {'total': 67.27, '00-09': 79.9, '10-19': 70.6, '20-29': 79.1, '30-39': 71.9, '40-49': 74.1, '50-59': 41.2, '60-69': 54.1, 'old': 69.47, 'new': 54.1}
2024-03-06 03:18:20,506 [trainer.py] => CNN top1 curve: [83.44, 71.75, 64.66]
2024-03-06 03:18:20,506 [trainer.py] => CNN top5 curve: [96.5, 89.6, 86.54]
2024-03-06 03:18:20,506 [trainer.py] => FeCAM top1 curve: [82.9, 72.78, 67.27]
2024-03-06 03:18:20,506 [trainer.py] => FeCAM top5 curve: [95.84, 91.18, 87.21]

2024-03-06 03:18:20,512 [fecam.py] => Learning on 70-80
Processing class: 70
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 3.4629995346
Epoch:   200  |  train loss: 3.4259513855
Epoch:   300  |  train loss: 3.4518439293
Epoch:   400  |  train loss: 3.4843623161
Epoch:   500  |  train loss: 3.4468807697
Epoch:   600  |  train loss: 3.5084271431
Epoch:   700  |  train loss: 3.5836830616
Epoch:   800  |  train loss: 3.4914421082
Epoch:   900  |  train loss: 3.5538901329
Epoch:  1000  |  train loss: 3.5271493435
Processing class: 71
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 3.5280250549
Epoch:   200  |  train loss: 3.5542263508
Epoch:   300  |  train loss: 3.4910698891
Epoch:   400  |  train loss: 3.5438742161
Epoch:   500  |  train loss: 3.5805460453
Epoch:   600  |  train loss: 3.5859564781
Epoch:   700  |  train loss: 3.5922338486
Epoch:   800  |  train loss: 3.6080742359
Epoch:   900  |  train loss: 3.6744204521
Epoch:  1000  |  train loss: 3.6558867931
Processing class: 72
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 3.4665853024
Epoch:   200  |  train loss: 3.4903037071
Epoch:   300  |  train loss: 3.5003968716
Epoch:   400  |  train loss: 3.4938598633
Epoch:   500  |  train loss: 3.5362624645
Epoch:   600  |  train loss: 3.4869022369
Epoch:   700  |  train loss: 3.5131587982
Epoch:   800  |  train loss: 3.4898641109
Epoch:   900  |  train loss: 3.5081228256
Epoch:  1000  |  train loss: 3.5673015594
Processing class: 73
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 3.5474144936
Epoch:   200  |  train loss: 3.5250979424
Epoch:   300  |  train loss: 3.6049818516
Epoch:   400  |  train loss: 3.5630476475
Epoch:   500  |  train loss: 3.6536096573
Epoch:   600  |  train loss: 3.6477098942
Epoch:   700  |  train loss: 3.5981924057
Epoch:   800  |  train loss: 3.5976831913
Epoch:   900  |  train loss: 3.6039968014
Epoch:  1000  |  train loss: 3.6397368431
Processing class: 74
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 3.5968361855
Epoch:   200  |  train loss: 3.6397287846
Epoch:   300  |  train loss: 3.5657207489
Epoch:   400  |  train loss: 3.5306361198
Epoch:   500  |  train loss: 3.5687529087
Epoch:   600  |  train loss: 3.5413517475
Epoch:   700  |  train loss: 3.6360077858
Epoch:   800  |  train loss: 3.5451745510
Epoch:   900  |  train loss: 3.5549135685
Epoch:  1000  |  train loss: 3.6302092552
Processing class: 75
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 3.4920429230
Epoch:   200  |  train loss: 3.4740242958
Epoch:   300  |  train loss: 3.5490966797
Epoch:   400  |  train loss: 3.5366416931
Epoch:   500  |  train loss: 3.5922117233
Epoch:   600  |  train loss: 3.6477967739
Epoch:   700  |  train loss: 3.4961452007
Epoch:   800  |  train loss: 3.5902729034
Epoch:   900  |  train loss: 3.5809535027
Epoch:  1000  |  train loss: 3.6448056221
Processing class: 76
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 3.2179008007
Epoch:   200  |  train loss: 3.2485970497
Epoch:   300  |  train loss: 3.3460965157
Epoch:   400  |  train loss: 3.4143604279
Epoch:   500  |  train loss: 3.4418173790
Epoch:   600  |  train loss: 3.4961339951
Epoch:   700  |  train loss: 3.5106148243
Epoch:   800  |  train loss: 3.4305743694
Epoch:   900  |  train loss: 3.5197547436
Epoch:  1000  |  train loss: 3.5599812984
Processing class: 77
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 3.4763768196
Epoch:   200  |  train loss: 3.4832908154
Epoch:   300  |  train loss: 3.4304540634
Epoch:   400  |  train loss: 3.4614121437
Epoch:   500  |  train loss: 3.4749036789
Epoch:   600  |  train loss: 3.4729457855
Epoch:   700  |  train loss: 3.4911217690
Epoch:   800  |  train loss: 3.5561314583
Epoch:   900  |  train loss: 3.5211932182
Epoch:  1000  |  train loss: 3.5111376762
Processing class: 78
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 3.5371598244
Epoch:   200  |  train loss: 3.5876652718
Epoch:   300  |  train loss: 3.6451511860
Epoch:   400  |  train loss: 3.6386214733
Epoch:   500  |  train loss: 3.6211168766
Epoch:   600  |  train loss: 3.6742431164
Epoch:   700  |  train loss: 3.6888682365
Epoch:   800  |  train loss: 3.7067505836
Epoch:   900  |  train loss: 3.7135998726
Epoch:  1000  |  train loss: 3.6957007408
Processing class: 79
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 3.5455873966
Epoch:   200  |  train loss: 3.4984585285
Epoch:   300  |  train loss: 3.5677100658
Epoch:   400  |  train loss: 3.5695083141
Epoch:   500  |  train loss: 3.5690161705
Epoch:   600  |  train loss: 3.6421446800
Epoch:   700  |  train loss: 3.6392123222
Epoch:   800  |  train loss: 3.6715322971
Epoch:   900  |  train loss: 3.7274003983
Epoch:  1000  |  train loss: 3.6764656067
2024-03-06 03:25:56,368 [trainer.py] => CNN: {'total': 59.18, '00-09': 73.6, '10-19': 68.4, '20-29': 76.9, '30-39': 69.1, '40-49': 60.6, '50-59': 37.2, '60-69': 45.7, '70-79': 41.9, 'old': 61.64, 'new': 41.9}
2024-03-06 03:25:56,368 [trainer.py] => No NME accuracy
2024-03-06 03:25:56,368 [trainer.py] => FeCAM: {'total': 61.72, '00-09': 78.1, '10-19': 69.3, '20-29': 78.0, '30-39': 69.5, '40-49': 72.1, '50-59': 35.7, '60-69': 49.1, '70-79': 42.0, 'old': 64.54, 'new': 42.0}
2024-03-06 03:25:56,368 [trainer.py] => CNN top1 curve: [83.44, 71.75, 64.66, 59.18]
2024-03-06 03:25:56,368 [trainer.py] => CNN top5 curve: [96.5, 89.6, 86.54, 84.09]
2024-03-06 03:25:56,368 [trainer.py] => FeCAM top1 curve: [82.9, 72.78, 67.27, 61.72]
2024-03-06 03:25:56,368 [trainer.py] => FeCAM top5 curve: [95.84, 91.18, 87.21, 84.09]

2024-03-06 03:25:56,372 [fecam.py] => Learning on 80-90
Processing class: 80
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 3.3851448536
Epoch:   200  |  train loss: 3.4978580952
Epoch:   300  |  train loss: 3.5447593212
Epoch:   400  |  train loss: 3.5736530304
Epoch:   500  |  train loss: 3.5687891483
Epoch:   600  |  train loss: 3.5700510979
Epoch:   700  |  train loss: 3.5063501835
Epoch:   800  |  train loss: 3.6077497482
Epoch:   900  |  train loss: 3.5843389511
Epoch:  1000  |  train loss: 3.6386168480
Processing class: 81
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 3.4670069218
Epoch:   200  |  train loss: 3.5635629654
Epoch:   300  |  train loss: 3.5040455341
Epoch:   400  |  train loss: 3.5993781090
Epoch:   500  |  train loss: 3.5763619423
Epoch:   600  |  train loss: 3.6433406353
Epoch:   700  |  train loss: 3.6329972267
Epoch:   800  |  train loss: 3.6776941299
Epoch:   900  |  train loss: 3.6827491760
Epoch:  1000  |  train loss: 3.7036322117
Processing class: 82
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 3.4149228573
Epoch:   200  |  train loss: 3.4680886269
Epoch:   300  |  train loss: 3.4780522346
Epoch:   400  |  train loss: 3.4685474396
Epoch:   500  |  train loss: 3.4901141644
Epoch:   600  |  train loss: 3.4725159168
Epoch:   700  |  train loss: 3.4752989292
Epoch:   800  |  train loss: 3.4926957130
Epoch:   900  |  train loss: 3.5014675617
Epoch:  1000  |  train loss: 3.5036880970
Processing class: 83
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 3.4562077522
Epoch:   200  |  train loss: 3.5374841213
Epoch:   300  |  train loss: 3.5186597347
Epoch:   400  |  train loss: 3.5511784554
Epoch:   500  |  train loss: 3.6301370621
Epoch:   600  |  train loss: 3.5802217484
Epoch:   700  |  train loss: 3.6584434032
Epoch:   800  |  train loss: 3.7474847317
Epoch:   900  |  train loss: 3.7117069721
Epoch:  1000  |  train loss: 3.6814434052
Processing class: 84
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 3.5417362213
Epoch:   200  |  train loss: 3.5259899139
Epoch:   300  |  train loss: 3.5934499264
Epoch:   400  |  train loss: 3.5969119072
Epoch:   500  |  train loss: 3.6497440338
Epoch:   600  |  train loss: 3.5935038567
Epoch:   700  |  train loss: 3.6553020477
Epoch:   800  |  train loss: 3.7093485832
Epoch:   900  |  train loss: 3.6355690002
Epoch:  1000  |  train loss: 3.6225362301
Processing class: 85
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 3.5126019478
Epoch:   200  |  train loss: 3.4942718506
Epoch:   300  |  train loss: 3.5593802929
Epoch:   400  |  train loss: 3.5303166866
Epoch:   500  |  train loss: 3.5744016171
Epoch:   600  |  train loss: 3.6275031090
Epoch:   700  |  train loss: 3.7071709156
Epoch:   800  |  train loss: 3.6411754131
Epoch:   900  |  train loss: 3.6502890110
Epoch:  1000  |  train loss: 3.7136252403
Processing class: 86
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 3.5931125641
Epoch:   200  |  train loss: 3.5323964119
Epoch:   300  |  train loss: 3.6194422245
Epoch:   400  |  train loss: 3.5893217564
Epoch:   500  |  train loss: 3.5905383587
Epoch:   600  |  train loss: 3.6239881516
Epoch:   700  |  train loss: 3.6284914017
Epoch:   800  |  train loss: 3.6203374386
Epoch:   900  |  train loss: 3.6198801517
Epoch:  1000  |  train loss: 3.7054449558
Processing class: 87
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 3.4245583057
Epoch:   200  |  train loss: 3.4813153267
Epoch:   300  |  train loss: 3.4845277786
Epoch:   400  |  train loss: 3.5256137848
Epoch:   500  |  train loss: 3.5176797867
Epoch:   600  |  train loss: 3.5571328640
Epoch:   700  |  train loss: 3.5784264565
Epoch:   800  |  train loss: 3.5789194107
Epoch:   900  |  train loss: 3.5384273529
Epoch:  1000  |  train loss: 3.6383011818
Processing class: 88
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 3.4784595966
Epoch:   200  |  train loss: 3.4510030270
Epoch:   300  |  train loss: 3.4492828369
Epoch:   400  |  train loss: 3.4674262524
Epoch:   500  |  train loss: 3.4607937336
Epoch:   600  |  train loss: 3.4340074539
Epoch:   700  |  train loss: 3.4893959522
Epoch:   800  |  train loss: 3.4931289196
Epoch:   900  |  train loss: 3.5098367214
Epoch:  1000  |  train loss: 3.4734763145
Processing class: 89
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 3.5134377003
Epoch:   200  |  train loss: 3.5734086990
Epoch:   300  |  train loss: 3.6043636799
Epoch:   400  |  train loss: 3.6218851566
Epoch:   500  |  train loss: 3.6832812786
Epoch:   600  |  train loss: 3.6890915394
Epoch:   700  |  train loss: 3.6320892811
Epoch:   800  |  train loss: 3.7521231174
Epoch:   900  |  train loss: 3.7216456413
Epoch:  1000  |  train loss: 3.7559931755
2024-03-06 03:34:50,816 [trainer.py] => CNN: {'total': 54.08, '00-09': 67.9, '10-19': 63.6, '20-29': 73.2, '30-39': 68.8, '40-49': 57.2, '50-59': 32.6, '60-69': 40.9, '70-79': 38.1, '80-89': 44.4, 'old': 55.29, 'new': 44.4}
2024-03-06 03:34:50,817 [trainer.py] => No NME accuracy
2024-03-06 03:34:50,817 [trainer.py] => FeCAM: {'total': 57.66, '00-09': 76.1, '10-19': 65.8, '20-29': 75.3, '30-39': 67.9, '40-49': 68.5, '50-59': 33.0, '60-69': 45.0, '70-79': 39.0, '80-89': 48.3, 'old': 58.82, 'new': 48.3}
2024-03-06 03:34:50,817 [trainer.py] => CNN top1 curve: [83.44, 71.75, 64.66, 59.18, 54.08]
2024-03-06 03:34:50,817 [trainer.py] => CNN top5 curve: [96.5, 89.6, 86.54, 84.09, 81.74]
2024-03-06 03:34:50,817 [trainer.py] => FeCAM top1 curve: [82.9, 72.78, 67.27, 61.72, 57.66]
2024-03-06 03:34:50,817 [trainer.py] => FeCAM top5 curve: [95.84, 91.18, 87.21, 84.09, 81.63]

2024-03-06 03:34:50,824 [fecam.py] => Learning on 90-100
Processing class: 90
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 3.5280260086
Epoch:   200  |  train loss: 3.5377297878
Epoch:   300  |  train loss: 3.4701691628
Epoch:   400  |  train loss: 3.4489923000
Epoch:   500  |  train loss: 3.4248600483
Epoch:   600  |  train loss: 3.4539427757
Epoch:   700  |  train loss: 3.4369051933
Epoch:   800  |  train loss: 3.4546926022
Epoch:   900  |  train loss: 3.4882679462
Epoch:  1000  |  train loss: 3.5530190468
Processing class: 91
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 3.2958138943
Epoch:   200  |  train loss: 3.2818232536
Epoch:   300  |  train loss: 3.3441459179
Epoch:   400  |  train loss: 3.4014281750
Epoch:   500  |  train loss: 3.4520720005
Epoch:   600  |  train loss: 3.4813474178
Epoch:   700  |  train loss: 3.4963309765
Epoch:   800  |  train loss: 3.5282609463
Epoch:   900  |  train loss: 3.5692234993
Epoch:  1000  |  train loss: 3.5808661461
Processing class: 92
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 3.5654624462
Epoch:   200  |  train loss: 3.4907237053
Epoch:   300  |  train loss: 3.4140218735
Epoch:   400  |  train loss: 3.4585291862
Epoch:   500  |  train loss: 3.4905888557
Epoch:   600  |  train loss: 3.5477216721
Epoch:   700  |  train loss: 3.5468575478
Epoch:   800  |  train loss: 3.4623183727
Epoch:   900  |  train loss: 3.4836102009
Epoch:  1000  |  train loss: 3.4966802120
Processing class: 93
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 3.4921144962
Epoch:   200  |  train loss: 3.5320147514
Epoch:   300  |  train loss: 3.6706850529
Epoch:   400  |  train loss: 3.6939708233
Epoch:   500  |  train loss: 3.6835379124
Epoch:   600  |  train loss: 3.6376656532
Epoch:   700  |  train loss: 3.7537132263
Epoch:   800  |  train loss: 3.7433748245
Epoch:   900  |  train loss: 3.7553600788
Epoch:  1000  |  train loss: 3.7402997971
Processing class: 94
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 3.2464410782
Epoch:   200  |  train loss: 3.2319092751
Epoch:   300  |  train loss: 3.1458889008
Epoch:   400  |  train loss: 3.1785837173
Epoch:   500  |  train loss: 3.0698121071
Epoch:   600  |  train loss: 3.1235555649
Epoch:   700  |  train loss: 3.1767400265
Epoch:   800  |  train loss: 3.0474354267
Epoch:   900  |  train loss: 3.1315437794
Epoch:  1000  |  train loss: 3.1048040390
Processing class: 95
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 3.4883907318
Epoch:   200  |  train loss: 3.5583185673
Epoch:   300  |  train loss: 3.5202579975
Epoch:   400  |  train loss: 3.4989110470
Epoch:   500  |  train loss: 3.5902234077
Epoch:   600  |  train loss: 3.5807016373
Epoch:   700  |  train loss: 3.6153306484
Epoch:   800  |  train loss: 3.5865871906
Epoch:   900  |  train loss: 3.6454716682
Epoch:  1000  |  train loss: 3.7000245571
Processing class: 96
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 3.5739680767
Epoch:   200  |  train loss: 3.4816447735
Epoch:   300  |  train loss: 3.5464420795
Epoch:   400  |  train loss: 3.6516024113
Epoch:   500  |  train loss: 3.6178891182
Epoch:   600  |  train loss: 3.6088234425
Epoch:   700  |  train loss: 3.6231287479
Epoch:   800  |  train loss: 3.5824197769
Epoch:   900  |  train loss: 3.6860657692
Epoch:  1000  |  train loss: 3.6833926201
Processing class: 97
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 3.5412195206
Epoch:   200  |  train loss: 3.5441462994
Epoch:   300  |  train loss: 3.5607872009
Epoch:   400  |  train loss: 3.5849255085
Epoch:   500  |  train loss: 3.6517054558
Epoch:   600  |  train loss: 3.6933629513
Epoch:   700  |  train loss: 3.7365974903
Epoch:   800  |  train loss: 3.7482910156
Epoch:   900  |  train loss: 3.7728380680
Epoch:  1000  |  train loss: 3.7656128407
Processing class: 98
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 3.4248480797
Epoch:   200  |  train loss: 3.4109835625
Epoch:   300  |  train loss: 3.4251381874
Epoch:   400  |  train loss: 3.4562022209
Epoch:   500  |  train loss: 3.4436972141
Epoch:   600  |  train loss: 3.4825614929
Epoch:   700  |  train loss: 3.4172353745
Epoch:   800  |  train loss: 3.4744296551
Epoch:   900  |  train loss: 3.4773237705
Epoch:  1000  |  train loss: 3.4475839615
Processing class: 99
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 3.6000572681
Epoch:   200  |  train loss: 3.5359009266
Epoch:   300  |  train loss: 3.6414764404
Epoch:   400  |  train loss: 3.7204286575
Epoch:   500  |  train loss: 3.7046863079
Epoch:   600  |  train loss: 3.6922857285
Epoch:   700  |  train loss: 3.6746550083
Epoch:   800  |  train loss: 3.7414483547
Epoch:   900  |  train loss: 3.7106549740
Epoch:  1000  |  train loss: 3.7247034550
2024-03-06 03:45:03,275 [trainer.py] => CNN: {'total': 50.36, '00-09': 57.7, '10-19': 63.6, '20-29': 71.4, '30-39': 68.5, '40-49': 56.8, '50-59': 29.6, '60-69': 38.9, '70-79': 36.6, '80-89': 42.7, '90-99': 37.8, 'old': 51.76, 'new': 37.8}
2024-03-06 03:45:03,277 [trainer.py] => No NME accuracy
2024-03-06 03:45:03,277 [trainer.py] => FeCAM: {'total': 54.74, '00-09': 73.3, '10-19': 65.1, '20-29': 74.1, '30-39': 67.2, '40-49': 67.1, '50-59': 30.8, '60-69': 43.3, '70-79': 37.2, '80-89': 46.9, '90-99': 42.4, 'old': 56.11, 'new': 42.4}
2024-03-06 03:45:03,277 [trainer.py] => CNN top1 curve: [83.44, 71.75, 64.66, 59.18, 54.08, 50.36]
2024-03-06 03:45:03,277 [trainer.py] => CNN top5 curve: [96.5, 89.6, 86.54, 84.09, 81.74, 79.63]
2024-03-06 03:45:03,277 [trainer.py] => FeCAM top1 curve: [82.9, 72.78, 67.27, 61.72, 57.66, 54.74]
2024-03-06 03:45:03,277 [trainer.py] => FeCAM top5 curve: [95.84, 91.18, 87.21, 84.09, 81.63, 79.21]

=========================================
2024-03-06 03:45:11,421 [trainer.py] => config: ./exps/FeCAM_cifar100.json
2024-03-06 03:45:11,421 [trainer.py] => prefix: train
2024-03-06 03:45:11,421 [trainer.py] => dataset: cifar100
2024-03-06 03:45:11,421 [trainer.py] => memory_size: 0
2024-03-06 03:45:11,421 [trainer.py] => shuffle: True
2024-03-06 03:45:11,421 [trainer.py] => init_cls: 50
2024-03-06 03:45:11,421 [trainer.py] => increment: 10
2024-03-06 03:45:11,421 [trainer.py] => model_name: fecam
2024-03-06 03:45:11,421 [trainer.py] => convnet_type: resnet18
2024-03-06 03:45:11,421 [trainer.py] => device: [device(type='cuda', index=0)]
2024-03-06 03:45:11,421 [trainer.py] => seed: 1993
2024-03-06 03:45:11,421 [trainer.py] => init_epochs: 200
2024-03-06 03:45:11,421 [trainer.py] => init_lr: 0.1
2024-03-06 03:45:11,421 [trainer.py] => init_weight_decay: 0.0005
2024-03-06 03:45:11,421 [trainer.py] => batch_size: 128
2024-03-06 03:45:11,421 [trainer.py] => num_workers: 8
2024-03-06 03:45:11,421 [trainer.py] => T: 5
2024-03-06 03:45:11,421 [trainer.py] => beta: 0.5
2024-03-06 03:45:11,421 [trainer.py] => alpha1: 1
2024-03-06 03:45:11,421 [trainer.py] => alpha2: 1
2024-03-06 03:45:11,421 [trainer.py] => ncm: False
2024-03-06 03:45:11,421 [trainer.py] => tukey: False
2024-03-06 03:45:11,421 [trainer.py] => diagonal: False
2024-03-06 03:45:11,421 [trainer.py] => per_class: True
2024-03-06 03:45:11,421 [trainer.py] => full_cov: True
2024-03-06 03:45:11,421 [trainer.py] => shrink: True
2024-03-06 03:45:11,421 [trainer.py] => norm_cov: False
2024-03-06 03:45:11,421 [trainer.py] => vecnorm: False
2024-03-06 03:45:11,421 [trainer.py] => ae_type: wae
2024-03-06 03:45:11,421 [trainer.py] => epochs: 1000
2024-03-06 03:45:11,421 [trainer.py] => ae_latent_dim: 32
2024-03-06 03:45:11,421 [trainer.py] => wae_sigma: 40
2024-03-06 03:45:11,421 [trainer.py] => wae_C: 10
Files already downloaded and verified
Files already downloaded and verified
2024-03-06 03:45:13,074 [data_manager.py] => [68, 56, 78, 8, 23, 84, 90, 65, 74, 76, 40, 89, 3, 92, 55, 9, 26, 80, 43, 38, 58, 70, 77, 1, 85, 19, 17, 50, 28, 53, 13, 81, 45, 82, 6, 59, 83, 16, 15, 44, 91, 41, 72, 60, 79, 52, 20, 10, 31, 54, 37, 95, 14, 71, 96, 98, 97, 2, 64, 66, 42, 22, 35, 86, 24, 34, 87, 21, 99, 0, 88, 27, 18, 94, 11, 12, 47, 25, 30, 46, 62, 69, 36, 61, 7, 63, 75, 5, 32, 4, 51, 48, 73, 93, 39, 67, 29, 49, 57, 33]
2024-03-06 03:45:13,354 [fecam.py] => Learning on 0-50
/home/z1165703/miniconda3/lib/python3.11/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
/home/z1165703/miniconda3/lib/python3.11/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
Processing class: 0
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 2.9633405209
Epoch:   200  |  train loss: 3.0289217472
Epoch:   300  |  train loss: 3.0934817314
Epoch:   400  |  train loss: 3.1247306824
Epoch:   500  |  train loss: 3.0990262985
Epoch:   600  |  train loss: 3.0988600254
Epoch:   700  |  train loss: 3.1278176308
Epoch:   800  |  train loss: 3.1382728100
Epoch:   900  |  train loss: 3.1123271465
Epoch:  1000  |  train loss: 3.1111498356
Processing class: 1
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 2.8985117912
Epoch:   200  |  train loss: 2.8752735615
Epoch:   300  |  train loss: 3.0239928246
Epoch:   400  |  train loss: 3.0587503910
Epoch:   500  |  train loss: 2.9972886086
Epoch:   600  |  train loss: 3.0959798336
Epoch:   700  |  train loss: 3.0593702316
Epoch:   800  |  train loss: 3.1152894974
Epoch:   900  |  train loss: 3.1937053680
Epoch:  1000  |  train loss: 3.1266362667
Processing class: 2
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 2.8607249737
Epoch:   200  |  train loss: 2.8657451153
Epoch:   300  |  train loss: 2.9707747459
Epoch:   400  |  train loss: 2.9670630932
Epoch:   500  |  train loss: 2.8477373123
Epoch:   600  |  train loss: 2.9631036282
Epoch:   700  |  train loss: 2.9564404488
Epoch:   800  |  train loss: 3.0449461460
Epoch:   900  |  train loss: 3.0061883450
Epoch:  1000  |  train loss: 2.9856873512
Processing class: 3
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 3.1207048416
Epoch:   200  |  train loss: 3.2075876713
Epoch:   300  |  train loss: 3.2560216904
Epoch:   400  |  train loss: 3.2866397858
Epoch:   500  |  train loss: 3.2592764378
Epoch:   600  |  train loss: 3.3532934666
Epoch:   700  |  train loss: 3.4181144238
Epoch:   800  |  train loss: 3.3631356716
Epoch:   900  |  train loss: 3.4162043571
Epoch:  1000  |  train loss: 3.4393828392
Processing class: 4
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 2.8712521076
Epoch:   200  |  train loss: 2.8383070946
Epoch:   300  |  train loss: 2.9638126373
Epoch:   400  |  train loss: 2.9820037842
Epoch:   500  |  train loss: 3.0346872330
Epoch:   600  |  train loss: 3.0286382198
Epoch:   700  |  train loss: 3.0785018921
Epoch:   800  |  train loss: 3.0953754902
Epoch:   900  |  train loss: 3.1230700016
Epoch:  1000  |  train loss: 3.1943818092
Processing class: 5
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 2.8732110023
Epoch:   200  |  train loss: 2.8685751438
Epoch:   300  |  train loss: 2.9168362141
Epoch:   400  |  train loss: 2.9317327023
Epoch:   500  |  train loss: 2.9277032375
Epoch:   600  |  train loss: 3.0535406590
Epoch:   700  |  train loss: 3.0853359222
Epoch:   800  |  train loss: 3.0382769108
Epoch:   900  |  train loss: 3.0336637020
Epoch:  1000  |  train loss: 3.0929161072
Processing class: 6
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 2.9112483978
Epoch:   200  |  train loss: 2.9542018890
Epoch:   300  |  train loss: 3.0818045616
Epoch:   400  |  train loss: 3.0790119171
Epoch:   500  |  train loss: 3.1782540798
Epoch:   600  |  train loss: 3.1466570854
Epoch:   700  |  train loss: 3.1214114666
Epoch:   800  |  train loss: 3.1525548458
Epoch:   900  |  train loss: 3.1252609730
Epoch:  1000  |  train loss: 3.1648770332
Processing class: 7
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 2.8193406105
Epoch:   200  |  train loss: 2.8539165497
Epoch:   300  |  train loss: 2.8951718330
Epoch:   400  |  train loss: 2.9326512814
Epoch:   500  |  train loss: 2.9912167072
Epoch:   600  |  train loss: 2.9656823158
Epoch:   700  |  train loss: 3.0189918041
Epoch:   800  |  train loss: 3.1057540894
Epoch:   900  |  train loss: 3.0428911686
Epoch:  1000  |  train loss: 3.1143154621
Processing class: 8
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 2.8573741913
Epoch:   200  |  train loss: 2.9242429256
Epoch:   300  |  train loss: 2.9149604797
Epoch:   400  |  train loss: 3.0285266399
Epoch:   500  |  train loss: 2.9878525734
Epoch:   600  |  train loss: 3.0260439873
Epoch:   700  |  train loss: 3.1044610977
Epoch:   800  |  train loss: 3.0484827042
Epoch:   900  |  train loss: 3.0961063385
Epoch:  1000  |  train loss: 3.0629868507
Processing class: 9
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 2.8061290741
Epoch:   200  |  train loss: 2.8656095982
Epoch:   300  |  train loss: 2.9536258221
Epoch:   400  |  train loss: 2.8849038601
Epoch:   500  |  train loss: 2.9719422817
Epoch:   600  |  train loss: 2.9246381760
Epoch:   700  |  train loss: 2.9567940712
Epoch:   800  |  train loss: 2.9828871727
Epoch:   900  |  train loss: 2.9879334450
Epoch:  1000  |  train loss: 2.9412263393
Processing class: 10
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 2.7384890079
Epoch:   200  |  train loss: 2.7464879990
Epoch:   300  |  train loss: 2.7852354527
Epoch:   400  |  train loss: 2.8848991394
Epoch:   500  |  train loss: 2.7836187363
Epoch:   600  |  train loss: 2.8349526882
Epoch:   700  |  train loss: 2.8754643917
Epoch:   800  |  train loss: 2.8581019878
Epoch:   900  |  train loss: 2.8882296562
Epoch:  1000  |  train loss: 2.8862638950
Processing class: 11
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 2.8781453609
Epoch:   200  |  train loss: 2.9297627926
Epoch:   300  |  train loss: 2.8994958401
Epoch:   400  |  train loss: 2.9520236969
Epoch:   500  |  train loss: 2.9685985088
Epoch:   600  |  train loss: 3.0365300179
Epoch:   700  |  train loss: 3.1314228535
Epoch:   800  |  train loss: 3.1181170464
Epoch:   900  |  train loss: 3.1294091225
Epoch:  1000  |  train loss: 3.1811913490
Processing class: 12
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 2.7651919365
Epoch:   200  |  train loss: 2.9091585159
Epoch:   300  |  train loss: 2.9298749924
Epoch:   400  |  train loss: 3.0154190063
Epoch:   500  |  train loss: 3.0327671528
Epoch:   600  |  train loss: 3.0019388676
Epoch:   700  |  train loss: 3.0359340668
Epoch:   800  |  train loss: 3.0629332542
Epoch:   900  |  train loss: 3.1126678944
Epoch:  1000  |  train loss: 3.0844265938
Processing class: 13
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 2.8307993412
Epoch:   200  |  train loss: 2.9525400162
Epoch:   300  |  train loss: 3.0454162121
Epoch:   400  |  train loss: 3.0814170837
Epoch:   500  |  train loss: 3.1629736900
Epoch:   600  |  train loss: 3.1102569103
Epoch:   700  |  train loss: 3.1596621990
Epoch:   800  |  train loss: 3.2037384510
Epoch:   900  |  train loss: 3.2311423302
Epoch:  1000  |  train loss: 3.2636145115
Processing class: 14
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 2.8500631332
Epoch:   200  |  train loss: 2.8915570736
Epoch:   300  |  train loss: 2.9281195164
Epoch:   400  |  train loss: 3.0118579388
Epoch:   500  |  train loss: 3.0039856434
Epoch:   600  |  train loss: 3.0114063740
Epoch:   700  |  train loss: 3.0544026375
Epoch:   800  |  train loss: 3.1518752098
Epoch:   900  |  train loss: 3.1226253986
Epoch:  1000  |  train loss: 3.0635782719
Processing class: 15
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 2.8724708557
Epoch:   200  |  train loss: 2.9259897709
Epoch:   300  |  train loss: 3.0388910770
Epoch:   400  |  train loss: 3.0892332077
Epoch:   500  |  train loss: 3.1426418304
Epoch:   600  |  train loss: 3.1015712261
Epoch:   700  |  train loss: 3.1844755173
Epoch:   800  |  train loss: 3.1160391331
Epoch:   900  |  train loss: 3.1938738346
Epoch:  1000  |  train loss: 3.1627742290
Processing class: 16
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 2.8956141949
Epoch:   200  |  train loss: 3.0938323498
Epoch:   300  |  train loss: 3.1809067249
Epoch:   400  |  train loss: 3.1455602646
Epoch:   500  |  train loss: 3.2172303200
Epoch:   600  |  train loss: 3.1638481617
Epoch:   700  |  train loss: 3.1508883476
Epoch:   800  |  train loss: 3.2520005703
Epoch:   900  |  train loss: 3.1684781551
Epoch:  1000  |  train loss: 3.2552277565
Processing class: 17
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 2.7576613426
Epoch:   200  |  train loss: 2.8093237877
Epoch:   300  |  train loss: 2.8905949116
Epoch:   400  |  train loss: 2.9117378235
Epoch:   500  |  train loss: 2.8385940552
Epoch:   600  |  train loss: 2.9030211926
Epoch:   700  |  train loss: 2.8921548367
Epoch:   800  |  train loss: 2.9038874149
Epoch:   900  |  train loss: 2.9315884590
Epoch:  1000  |  train loss: 2.9241733551
Processing class: 18
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 2.8342332363
Epoch:   200  |  train loss: 2.8585771561
Epoch:   300  |  train loss: 2.9514205933
Epoch:   400  |  train loss: 2.9522482872
Epoch:   500  |  train loss: 3.0519561291
Epoch:   600  |  train loss: 3.0317998886
Epoch:   700  |  train loss: 3.1609928131
Epoch:   800  |  train loss: 3.1628536701
Epoch:   900  |  train loss: 3.1936288357
Epoch:  1000  |  train loss: 3.2696167469
Processing class: 19
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 2.8839540005
Epoch:   200  |  train loss: 3.0463255882
Epoch:   300  |  train loss: 3.0572769165
Epoch:   400  |  train loss: 3.0799960136
Epoch:   500  |  train loss: 3.0417600632
Epoch:   600  |  train loss: 3.0572103024
Epoch:   700  |  train loss: 3.0985996246
Epoch:   800  |  train loss: 3.0811719418
Epoch:   900  |  train loss: 3.2238629341
Epoch:  1000  |  train loss: 3.2553350449
Processing class: 20
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 2.8816262722
Epoch:   200  |  train loss: 2.9447871208
Epoch:   300  |  train loss: 3.0006259441
Epoch:   400  |  train loss: 3.0516719341
Epoch:   500  |  train loss: 3.1279642582
Epoch:   600  |  train loss: 3.0893332005
Epoch:   700  |  train loss: 3.1117479801
Epoch:   800  |  train loss: 3.1147810936
Epoch:   900  |  train loss: 3.1508097172
Epoch:  1000  |  train loss: 3.2059574127
Processing class: 21
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 2.8898766041
Epoch:   200  |  train loss: 2.9625885963
Epoch:   300  |  train loss: 3.0251462460
Epoch:   400  |  train loss: 3.0651255608
Epoch:   500  |  train loss: 3.1039176941
Epoch:   600  |  train loss: 3.1553976536
Epoch:   700  |  train loss: 3.1449909687
Epoch:   800  |  train loss: 3.1330567360
Epoch:   900  |  train loss: 3.2037348747
Epoch:  1000  |  train loss: 3.1927351952
Processing class: 22
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 2.8337080002
Epoch:   200  |  train loss: 2.8627081394
Epoch:   300  |  train loss: 2.9304968357
Epoch:   400  |  train loss: 3.0217334747
Epoch:   500  |  train loss: 3.0514400959
Epoch:   600  |  train loss: 3.0213541508
Epoch:   700  |  train loss: 3.0624920368
Epoch:   800  |  train loss: 3.0481990337
Epoch:   900  |  train loss: 3.0210721493
Epoch:  1000  |  train loss: 3.1283633709
Processing class: 23
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 2.9497800827
Epoch:   200  |  train loss: 3.0198573112
Epoch:   300  |  train loss: 3.0933413029
Epoch:   400  |  train loss: 3.0308569908
Epoch:   500  |  train loss: 3.0363558292
Epoch:   600  |  train loss: 3.0072847843
Epoch:   700  |  train loss: 3.0607995033
Epoch:   800  |  train loss: 3.0321019650
Epoch:   900  |  train loss: 3.0834669590
Epoch:  1000  |  train loss: 3.0490711689
Processing class: 24
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 2.8982063293
Epoch:   200  |  train loss: 2.9028614044
Epoch:   300  |  train loss: 2.8395662785
Epoch:   400  |  train loss: 2.9202082634
Epoch:   500  |  train loss: 2.8947265625
Epoch:   600  |  train loss: 2.9428870678
Epoch:   700  |  train loss: 2.8584025383
Epoch:   800  |  train loss: 2.8934335232
Epoch:   900  |  train loss: 2.9527625561
Epoch:  1000  |  train loss: 2.9376914978
Processing class: 25
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 2.8434605598
Epoch:   200  |  train loss: 2.9055772305
Epoch:   300  |  train loss: 2.9906582832
Epoch:   400  |  train loss: 2.9540978909
Epoch:   500  |  train loss: 3.0527402401
Epoch:   600  |  train loss: 3.0176554680
Epoch:   700  |  train loss: 3.1010546207
Epoch:   800  |  train loss: 3.0755962849
Epoch:   900  |  train loss: 3.0609738827
Epoch:  1000  |  train loss: 3.0904402256
Processing class: 26
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 2.9902164459
Epoch:   200  |  train loss: 3.0824901581
Epoch:   300  |  train loss: 3.0260374546
Epoch:   400  |  train loss: 3.1042600632
Epoch:   500  |  train loss: 3.1816527367
Epoch:   600  |  train loss: 3.0953337669
Epoch:   700  |  train loss: 3.1389440060
Epoch:   800  |  train loss: 3.1737678051
Epoch:   900  |  train loss: 3.1649700642
Epoch:  1000  |  train loss: 3.2280306339
Processing class: 27
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 2.8469872475
Epoch:   200  |  train loss: 2.8357075214
Epoch:   300  |  train loss: 2.8693839073
Epoch:   400  |  train loss: 2.9044677734
Epoch:   500  |  train loss: 2.9613884926
Epoch:   600  |  train loss: 2.9667501450
Epoch:   700  |  train loss: 2.9988523006
Epoch:   800  |  train loss: 3.0002267361
Epoch:   900  |  train loss: 3.0302521229
Epoch:  1000  |  train loss: 2.9695291996
Processing class: 28
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 2.9408545971
Epoch:   200  |  train loss: 3.0182002544
Epoch:   300  |  train loss: 3.1142115116
Epoch:   400  |  train loss: 3.1641829014
Epoch:   500  |  train loss: 3.1938691616
Epoch:   600  |  train loss: 3.2039785862
Epoch:   700  |  train loss: 3.2222315311
Epoch:   800  |  train loss: 3.2493729591
Epoch:   900  |  train loss: 3.2664582729
Epoch:  1000  |  train loss: 3.2106379986
Processing class: 29
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 2.8638883114
Epoch:   200  |  train loss: 2.9345649242
Epoch:   300  |  train loss: 2.9430436611
Epoch:   400  |  train loss: 2.9265548706
Epoch:   500  |  train loss: 2.9950130939
Epoch:   600  |  train loss: 3.0332732677
Epoch:   700  |  train loss: 3.0402890205
Epoch:   800  |  train loss: 3.0050303459
Epoch:   900  |  train loss: 3.0228079319
Epoch:  1000  |  train loss: 3.0118194580
Processing class: 30
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 2.9826144695
Epoch:   200  |  train loss: 3.0681643009
Epoch:   300  |  train loss: 3.2023170948
Epoch:   400  |  train loss: 3.3083581448
Epoch:   500  |  train loss: 3.2635804653
Epoch:   600  |  train loss: 3.2669219494
Epoch:   700  |  train loss: 3.2593718052
Epoch:   800  |  train loss: 3.3299228191
Epoch:   900  |  train loss: 3.3758076191
Epoch:  1000  |  train loss: 3.3320604324
Processing class: 31
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 2.9462511063
Epoch:   200  |  train loss: 3.0507443428
Epoch:   300  |  train loss: 3.0756999016
Epoch:   400  |  train loss: 3.1673380852
Epoch:   500  |  train loss: 3.2036441326
Epoch:   600  |  train loss: 3.2813796997
Epoch:   700  |  train loss: 3.2686197758
Epoch:   800  |  train loss: 3.2756211758
Epoch:   900  |  train loss: 3.3356622696
Epoch:  1000  |  train loss: 3.2777701855
Processing class: 32
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 2.8559580326
Epoch:   200  |  train loss: 2.9341732502
Epoch:   300  |  train loss: 2.9312211990
Epoch:   400  |  train loss: 2.8805467606
Epoch:   500  |  train loss: 3.0110107899
Epoch:   600  |  train loss: 2.9920088291
Epoch:   700  |  train loss: 3.0914287090
Epoch:   800  |  train loss: 3.1330943584
Epoch:   900  |  train loss: 3.1173382759
Epoch:  1000  |  train loss: 3.0977697849
Processing class: 33
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 2.7531421185
Epoch:   200  |  train loss: 2.9221305847
Epoch:   300  |  train loss: 2.8217838287
Epoch:   400  |  train loss: 2.8525449753
Epoch:   500  |  train loss: 2.8759681225
Epoch:   600  |  train loss: 2.9799079418
Epoch:   700  |  train loss: 2.9354096889
Epoch:   800  |  train loss: 2.9636390209
Epoch:   900  |  train loss: 2.9606899261
Epoch:  1000  |  train loss: 2.9385097504
Processing class: 34
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 2.8845208645
Epoch:   200  |  train loss: 2.8730311394
Epoch:   300  |  train loss: 2.9462301731
Epoch:   400  |  train loss: 2.9222615242
Epoch:   500  |  train loss: 2.9287190437
Epoch:   600  |  train loss: 2.9936546803
Epoch:   700  |  train loss: 2.9520637989
Epoch:   800  |  train loss: 3.0272354126
Epoch:   900  |  train loss: 3.0542101860
Epoch:  1000  |  train loss: 3.0720299244
Processing class: 35
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 2.7940197468
Epoch:   200  |  train loss: 2.8872316360
Epoch:   300  |  train loss: 2.9528293610
Epoch:   400  |  train loss: 2.9860626221
Epoch:   500  |  train loss: 2.9484686375
Epoch:   600  |  train loss: 2.9892431259
Epoch:   700  |  train loss: 3.0203680992
Epoch:   800  |  train loss: 3.0171311378
Epoch:   900  |  train loss: 3.0307568073
Epoch:  1000  |  train loss: 3.0071053028
Processing class: 36
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 2.9339803696
Epoch:   200  |  train loss: 2.9689100266
Epoch:   300  |  train loss: 3.0329230309
Epoch:   400  |  train loss: 3.0409349918
Epoch:   500  |  train loss: 3.1375131607
Epoch:   600  |  train loss: 3.1089617729
Epoch:   700  |  train loss: 3.0747151375
Epoch:   800  |  train loss: 3.1805700302
Epoch:   900  |  train loss: 3.1071094036
Epoch:  1000  |  train loss: 3.1715581417
Processing class: 37
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 2.9028308392
Epoch:   200  |  train loss: 2.8976711273
Epoch:   300  |  train loss: 2.9025608540
Epoch:   400  |  train loss: 2.9529170513
Epoch:   500  |  train loss: 2.9978762627
Epoch:   600  |  train loss: 2.9604788780
Epoch:   700  |  train loss: 2.9773148060
Epoch:   800  |  train loss: 3.0039546490
Epoch:   900  |  train loss: 2.9723449707
Epoch:  1000  |  train loss: 3.0011652470
Processing class: 38
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 2.8867412567
Epoch:   200  |  train loss: 2.9229261398
Epoch:   300  |  train loss: 3.0021099091
Epoch:   400  |  train loss: 3.0279454231
Epoch:   500  |  train loss: 3.0635231018
Epoch:   600  |  train loss: 3.0543747425
Epoch:   700  |  train loss: 3.0734023571
Epoch:   800  |  train loss: 3.1388463497
Epoch:   900  |  train loss: 3.0700425625
Epoch:  1000  |  train loss: 3.1264116287
Processing class: 39
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 2.8995524406
Epoch:   200  |  train loss: 2.8851359367
Epoch:   300  |  train loss: 2.9437839031
Epoch:   400  |  train loss: 2.9906369686
Epoch:   500  |  train loss: 2.9775660515
Epoch:   600  |  train loss: 3.0058341503
Epoch:   700  |  train loss: 3.0432447910
Epoch:   800  |  train loss: 3.0925867081
Epoch:   900  |  train loss: 3.1238158703
Epoch:  1000  |  train loss: 3.1466119289
Processing class: 40
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 2.9155509949
Epoch:   200  |  train loss: 2.8901820183
Epoch:   300  |  train loss: 3.0220073223
Epoch:   400  |  train loss: 3.0541659832
Epoch:   500  |  train loss: 3.0965178967
Epoch:   600  |  train loss: 3.0754099846
Epoch:   700  |  train loss: 3.1100263119
Epoch:   800  |  train loss: 3.1122437477
Epoch:   900  |  train loss: 3.0837958813
Epoch:  1000  |  train loss: 3.2001257896
Processing class: 41
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 2.9014953613
Epoch:   200  |  train loss: 2.9128116131
Epoch:   300  |  train loss: 2.9916965961
Epoch:   400  |  train loss: 2.9849810600
Epoch:   500  |  train loss: 2.9969704628
Epoch:   600  |  train loss: 3.0069662094
Epoch:   700  |  train loss: 3.0644758701
Epoch:   800  |  train loss: 3.0377621174
Epoch:   900  |  train loss: 3.0295775414
Epoch:  1000  |  train loss: 3.0374364376
Processing class: 42
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 2.8181467533
Epoch:   200  |  train loss: 2.9136318207
Epoch:   300  |  train loss: 2.9824114799
Epoch:   400  |  train loss: 3.0177425861
Epoch:   500  |  train loss: 3.0598950386
Epoch:   600  |  train loss: 3.0913215637
Epoch:   700  |  train loss: 3.1351778030
Epoch:   800  |  train loss: 3.1701688766
Epoch:   900  |  train loss: 3.1449645996
Epoch:  1000  |  train loss: 3.1452299118
Processing class: 43
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 2.8433895588
Epoch:   200  |  train loss: 2.8496190071
Epoch:   300  |  train loss: 3.0085145473
Epoch:   400  |  train loss: 3.1327860832
Epoch:   500  |  train loss: 3.1681877613
Epoch:   600  |  train loss: 3.1405527115
Epoch:   700  |  train loss: 3.1924461365
Epoch:   800  |  train loss: 3.1727188587
Epoch:   900  |  train loss: 3.1120149612
Epoch:  1000  |  train loss: 3.1989955902
Processing class: 44
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 2.8074095249
Epoch:   200  |  train loss: 2.9191898346
Epoch:   300  |  train loss: 2.8937246323
Epoch:   400  |  train loss: 2.9269984722
Epoch:   500  |  train loss: 2.9714459896
Epoch:   600  |  train loss: 2.9491477489
Epoch:   700  |  train loss: 3.0161489964
Epoch:   800  |  train loss: 3.0028485775
Epoch:   900  |  train loss: 2.9829058170
Epoch:  1000  |  train loss: 3.0561699390
Processing class: 45
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 2.9155534744
Epoch:   200  |  train loss: 3.0480563641
Epoch:   300  |  train loss: 3.0620585918
Epoch:   400  |  train loss: 3.1332707882
Epoch:   500  |  train loss: 3.1229875088
Epoch:   600  |  train loss: 3.2277185440
Epoch:   700  |  train loss: 3.2568138599
Epoch:   800  |  train loss: 3.2345630646
Epoch:   900  |  train loss: 3.2468205929
Epoch:  1000  |  train loss: 3.3027801991
Processing class: 46
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 2.9630269051
Epoch:   200  |  train loss: 2.9860845089
Epoch:   300  |  train loss: 3.0500746250
Epoch:   400  |  train loss: 3.0821122169
Epoch:   500  |  train loss: 3.0948021889
Epoch:   600  |  train loss: 3.0979849815
Epoch:   700  |  train loss: 3.1129665852
Epoch:   800  |  train loss: 3.1974039078
Epoch:   900  |  train loss: 3.1571684361
Epoch:  1000  |  train loss: 3.1946260929
Processing class: 47
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 2.8098756313
Epoch:   200  |  train loss: 2.8936876297
Epoch:   300  |  train loss: 2.8812333584
Epoch:   400  |  train loss: 2.9664036751
Epoch:   500  |  train loss: 2.9764901638
Epoch:   600  |  train loss: 2.9859652996
Epoch:   700  |  train loss: 3.0199922562
Epoch:   800  |  train loss: 2.9879603386
Epoch:   900  |  train loss: 3.0172960281
Epoch:  1000  |  train loss: 3.0012868881
Processing class: 48
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 2.8604485989
Epoch:   200  |  train loss: 2.9559489250
Epoch:   300  |  train loss: 2.9662228107
Epoch:   400  |  train loss: 2.9130135059
Epoch:   500  |  train loss: 3.0077767849
Epoch:   600  |  train loss: 3.0001119614
Epoch:   700  |  train loss: 2.9645250320
Epoch:   800  |  train loss: 2.9649963379
Epoch:   900  |  train loss: 3.0614373684
Epoch:  1000  |  train loss: 3.0924766541
Processing class: 49
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 2.8413599968
Epoch:   200  |  train loss: 2.8763329983
Epoch:   300  |  train loss: 3.0127700329
Epoch:   400  |  train loss: 3.0427618980
Epoch:   500  |  train loss: 3.0844344139
Epoch:   600  |  train loss: 3.0924751282
Epoch:   700  |  train loss: 3.0995609760
Epoch:   800  |  train loss: 3.0859035492
Epoch:   900  |  train loss: 3.0779098511
Epoch:  1000  |  train loss: 3.0961470604
/home/z1165703/FeCAM/models/base.py:87: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  vec = torch.tensor(vec).cuda()
2024-03-06 04:02:56,949 [trainer.py] => CNN: {'total': 83.44, '00-09': 87.7, '10-19': 79.6, '20-29': 84.9, '30-39': 81.0, '40-49': 84.0, 'old': 0, 'new': 83.44}
2024-03-06 04:02:56,950 [trainer.py] => No NME accuracy
2024-03-06 04:02:56,951 [trainer.py] => FeCAM: {'total': 82.9, '00-09': 86.2, '10-19': 79.7, '20-29': 85.0, '30-39': 80.6, '40-49': 83.0, 'old': 0, 'new': 82.9}
2024-03-06 04:02:56,951 [trainer.py] => CNN top1 curve: [83.44]
2024-03-06 04:02:56,951 [trainer.py] => CNN top5 curve: [96.5]
2024-03-06 04:02:56,951 [trainer.py] => FeCAM top1 curve: [82.9]
2024-03-06 04:02:56,951 [trainer.py] => FeCAM top5 curve: [95.84]

2024-03-06 04:02:56,966 [fecam.py] => Learning on 50-60
Processing class: 50
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 2.7667094707
Epoch:   200  |  train loss: 2.8054263115
Epoch:   300  |  train loss: 2.8334321499
Epoch:   400  |  train loss: 2.9074181557
Epoch:   500  |  train loss: 2.9615931511
Epoch:   600  |  train loss: 2.9329208374
Epoch:   700  |  train loss: 3.0143878937
Epoch:   800  |  train loss: 3.0571139812
Epoch:   900  |  train loss: 3.1049747944
Epoch:  1000  |  train loss: 3.1292049408
Processing class: 51
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 2.7931882381
Epoch:   200  |  train loss: 2.7443053246
Epoch:   300  |  train loss: 2.8602375507
Epoch:   400  |  train loss: 2.8933034420
Epoch:   500  |  train loss: 2.9257950306
Epoch:   600  |  train loss: 2.9806647778
Epoch:   700  |  train loss: 2.9341558933
Epoch:   800  |  train loss: 2.9577817440
Epoch:   900  |  train loss: 3.0467604160
Epoch:  1000  |  train loss: 3.0252182484
Processing class: 52
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 2.7995227337
Epoch:   200  |  train loss: 2.7824601650
Epoch:   300  |  train loss: 2.8398144722
Epoch:   400  |  train loss: 2.8816119671
Epoch:   500  |  train loss: 2.8822093964
Epoch:   600  |  train loss: 2.8537707806
Epoch:   700  |  train loss: 2.9239834309
Epoch:   800  |  train loss: 2.9357870579
Epoch:   900  |  train loss: 2.9612463474
Epoch:  1000  |  train loss: 2.9218599319
Processing class: 53
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 2.7648749352
Epoch:   200  |  train loss: 2.8126674652
Epoch:   300  |  train loss: 2.7609194279
Epoch:   400  |  train loss: 2.8002712727
Epoch:   500  |  train loss: 2.8414927959
Epoch:   600  |  train loss: 2.9024090767
Epoch:   700  |  train loss: 2.9735851288
Epoch:   800  |  train loss: 2.8634797096
Epoch:   900  |  train loss: 2.9260194778
Epoch:  1000  |  train loss: 2.8924464226
Processing class: 54
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 2.5979502678
Epoch:   200  |  train loss: 2.6353893280
Epoch:   300  |  train loss: 2.7161867619
Epoch:   400  |  train loss: 2.7520943165
Epoch:   500  |  train loss: 2.7934309006
Epoch:   600  |  train loss: 2.8760028362
Epoch:   700  |  train loss: 2.9056688786
Epoch:   800  |  train loss: 2.9733768463
Epoch:   900  |  train loss: 3.0214102745
Epoch:  1000  |  train loss: 3.0121070862
Processing class: 55
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 2.7612118244
Epoch:   200  |  train loss: 2.7778700829
Epoch:   300  |  train loss: 2.7526703835
Epoch:   400  |  train loss: 2.8327916622
Epoch:   500  |  train loss: 2.9371772289
Epoch:   600  |  train loss: 2.9570377350
Epoch:   700  |  train loss: 2.9975247383
Epoch:   800  |  train loss: 3.0770087719
Epoch:   900  |  train loss: 3.0020981789
Epoch:  1000  |  train loss: 3.0619961262
Processing class: 56
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 2.7383775711
Epoch:   200  |  train loss: 2.7926097870
Epoch:   300  |  train loss: 2.7631508350
Epoch:   400  |  train loss: 2.8667520046
Epoch:   500  |  train loss: 2.9193743229
Epoch:   600  |  train loss: 2.9594743252
Epoch:   700  |  train loss: 3.0098972321
Epoch:   800  |  train loss: 2.9831741810
Epoch:   900  |  train loss: 3.0538878918
Epoch:  1000  |  train loss: 3.0982403755
Processing class: 57
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 2.7276363373
Epoch:   200  |  train loss: 2.7950150967
Epoch:   300  |  train loss: 2.7790265560
Epoch:   400  |  train loss: 2.7900042057
Epoch:   500  |  train loss: 2.8476578712
Epoch:   600  |  train loss: 2.7946253777
Epoch:   700  |  train loss: 2.8915996552
Epoch:   800  |  train loss: 2.8997061729
Epoch:   900  |  train loss: 2.9560160637
Epoch:  1000  |  train loss: 2.8911315918
Processing class: 58
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 2.7415777683
Epoch:   200  |  train loss: 2.7529428482
Epoch:   300  |  train loss: 2.8427555561
Epoch:   400  |  train loss: 2.9485228062
Epoch:   500  |  train loss: 2.9420807362
Epoch:   600  |  train loss: 3.0083066940
Epoch:   700  |  train loss: 3.0059578896
Epoch:   800  |  train loss: 3.0596327305
Epoch:   900  |  train loss: 3.0658302307
Epoch:  1000  |  train loss: 3.0790983677
Processing class: 59
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 2.8141762257
Epoch:   200  |  train loss: 2.9144762516
Epoch:   300  |  train loss: 2.9026565075
Epoch:   400  |  train loss: 2.9635156631
Epoch:   500  |  train loss: 3.0233428955
Epoch:   600  |  train loss: 3.0413279057
Epoch:   700  |  train loss: 3.0844087601
Epoch:   800  |  train loss: 3.1076722145
Epoch:   900  |  train loss: 3.1056544781
Epoch:  1000  |  train loss: 3.0889374733
2024-03-06 04:08:33,952 [trainer.py] => CNN: {'total': 71.75, '00-09': 82.2, '10-19': 72.8, '20-29': 78.5, '30-39': 74.9, '40-49': 68.4, '50-59': 53.7, 'old': 75.36, 'new': 53.7}
2024-03-06 04:08:33,954 [trainer.py] => No NME accuracy
2024-03-06 04:08:33,954 [trainer.py] => FeCAM: {'total': 72.78, '00-09': 82.6, '10-19': 74.0, '20-29': 81.0, '30-39': 76.4, '40-49': 76.8, '50-59': 45.9, 'old': 78.16, 'new': 45.9}
2024-03-06 04:08:33,954 [trainer.py] => CNN top1 curve: [83.44, 71.75]
2024-03-06 04:08:33,954 [trainer.py] => CNN top5 curve: [96.5, 89.6]
2024-03-06 04:08:33,954 [trainer.py] => FeCAM top1 curve: [82.9, 72.78]
2024-03-06 04:08:33,954 [trainer.py] => FeCAM top5 curve: [95.84, 91.18]

2024-03-06 04:08:33,960 [fecam.py] => Learning on 60-70
Processing class: 60
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 2.8508694649
Epoch:   200  |  train loss: 2.8510159016
Epoch:   300  |  train loss: 2.8273693085
Epoch:   400  |  train loss: 2.8289044380
Epoch:   500  |  train loss: 2.9294747353
Epoch:   600  |  train loss: 2.9454059601
Epoch:   700  |  train loss: 3.1418004513
Epoch:   800  |  train loss: 3.0590058327
Epoch:   900  |  train loss: 3.0852364540
Epoch:  1000  |  train loss: 3.1435617447
Processing class: 61
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 2.7380273819
Epoch:   200  |  train loss: 2.7078341484
Epoch:   300  |  train loss: 2.8156553268
Epoch:   400  |  train loss: 2.8511031628
Epoch:   500  |  train loss: 2.9644393444
Epoch:   600  |  train loss: 2.9134233952
Epoch:   700  |  train loss: 3.0159699917
Epoch:   800  |  train loss: 3.0800726414
Epoch:   900  |  train loss: 3.0096848965
Epoch:  1000  |  train loss: 3.0515250206
Processing class: 62
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 2.8107661724
Epoch:   200  |  train loss: 2.7790070057
Epoch:   300  |  train loss: 2.8426364899
Epoch:   400  |  train loss: 2.8741799355
Epoch:   500  |  train loss: 2.8431693077
Epoch:   600  |  train loss: 2.9358183861
Epoch:   700  |  train loss: 2.9674832821
Epoch:   800  |  train loss: 2.9956949234
Epoch:   900  |  train loss: 2.9822953701
Epoch:  1000  |  train loss: 3.0113384247
Processing class: 63
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 2.7010486126
Epoch:   200  |  train loss: 2.7321267128
Epoch:   300  |  train loss: 2.8742516994
Epoch:   400  |  train loss: 2.9048610687
Epoch:   500  |  train loss: 2.9649298191
Epoch:   600  |  train loss: 3.0534873486
Epoch:   700  |  train loss: 2.9725488663
Epoch:   800  |  train loss: 3.1184468269
Epoch:   900  |  train loss: 3.0407467365
Epoch:  1000  |  train loss: 3.0688330650
Processing class: 64
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 2.8606782436
Epoch:   200  |  train loss: 2.8443144321
Epoch:   300  |  train loss: 3.0062536716
Epoch:   400  |  train loss: 3.0320096493
Epoch:   500  |  train loss: 3.0671772480
Epoch:   600  |  train loss: 3.0777053833
Epoch:   700  |  train loss: 3.0277144909
Epoch:   800  |  train loss: 3.0721317291
Epoch:   900  |  train loss: 3.1490671158
Epoch:  1000  |  train loss: 3.1383756638
Processing class: 65
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 2.7805313587
Epoch:   200  |  train loss: 2.8021193981
Epoch:   300  |  train loss: 2.9715860844
Epoch:   400  |  train loss: 3.0472939968
Epoch:   500  |  train loss: 3.0442791939
Epoch:   600  |  train loss: 3.1753706455
Epoch:   700  |  train loss: 3.1900122643
Epoch:   800  |  train loss: 3.3072811127
Epoch:   900  |  train loss: 3.2535574436
Epoch:  1000  |  train loss: 3.3716681480
Processing class: 66
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 2.8360608101
Epoch:   200  |  train loss: 2.9033177376
Epoch:   300  |  train loss: 2.9824424267
Epoch:   400  |  train loss: 3.0023034573
Epoch:   500  |  train loss: 3.0797802448
Epoch:   600  |  train loss: 3.0992352962
Epoch:   700  |  train loss: 3.1836991310
Epoch:   800  |  train loss: 3.1986512184
Epoch:   900  |  train loss: 3.2059998512
Epoch:  1000  |  train loss: 3.1991736412
Processing class: 67
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 2.7196961880
Epoch:   200  |  train loss: 2.7667963982
Epoch:   300  |  train loss: 2.7799128532
Epoch:   400  |  train loss: 2.8086422443
Epoch:   500  |  train loss: 2.8235470772
Epoch:   600  |  train loss: 2.8279264927
Epoch:   700  |  train loss: 2.8519520760
Epoch:   800  |  train loss: 2.9061963558
Epoch:   900  |  train loss: 2.9031177998
Epoch:  1000  |  train loss: 2.9055646896
Processing class: 68
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 2.7115465641
Epoch:   200  |  train loss: 2.7223716259
Epoch:   300  |  train loss: 2.7335834980
Epoch:   400  |  train loss: 2.7756417274
Epoch:   500  |  train loss: 2.7934081554
Epoch:   600  |  train loss: 2.7938414097
Epoch:   700  |  train loss: 2.8350609303
Epoch:   800  |  train loss: 2.8138874531
Epoch:   900  |  train loss: 2.8474437714
Epoch:  1000  |  train loss: 2.9423849583
Processing class: 69
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 2.6998290062
Epoch:   200  |  train loss: 2.7830743313
Epoch:   300  |  train loss: 2.9367246151
Epoch:   400  |  train loss: 3.0111917019
Epoch:   500  |  train loss: 3.0320569038
Epoch:   600  |  train loss: 2.9575496197
Epoch:   700  |  train loss: 3.0401882648
Epoch:   800  |  train loss: 3.0406183243
Epoch:   900  |  train loss: 3.1487958431
Epoch:  1000  |  train loss: 3.0517456055
2024-03-06 04:15:04,298 [trainer.py] => CNN: {'total': 64.66, '00-09': 75.0, '10-19': 70.1, '20-29': 77.1, '30-39': 70.6, '40-49': 64.0, '50-59': 45.8, '60-69': 50.0, 'old': 67.1, 'new': 50.0}
2024-03-06 04:15:04,299 [trainer.py] => No NME accuracy
2024-03-06 04:15:04,299 [trainer.py] => FeCAM: {'total': 67.27, '00-09': 79.9, '10-19': 70.6, '20-29': 79.1, '30-39': 71.9, '40-49': 74.1, '50-59': 41.2, '60-69': 54.1, 'old': 69.47, 'new': 54.1}
2024-03-06 04:15:04,299 [trainer.py] => CNN top1 curve: [83.44, 71.75, 64.66]
2024-03-06 04:15:04,299 [trainer.py] => CNN top5 curve: [96.5, 89.6, 86.54]
2024-03-06 04:15:04,299 [trainer.py] => FeCAM top1 curve: [82.9, 72.78, 67.27]
2024-03-06 04:15:04,299 [trainer.py] => FeCAM top5 curve: [95.84, 91.18, 87.21]

2024-03-06 04:15:04,303 [fecam.py] => Learning on 70-80
Processing class: 70
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 2.7322668552
Epoch:   200  |  train loss: 2.7161427975
Epoch:   300  |  train loss: 2.7748833656
Epoch:   400  |  train loss: 2.8232775688
Epoch:   500  |  train loss: 2.8143193245
Epoch:   600  |  train loss: 2.8881402016
Epoch:   700  |  train loss: 2.9819890499
Epoch:   800  |  train loss: 2.9120748043
Epoch:   900  |  train loss: 2.9825683594
Epoch:  1000  |  train loss: 2.9671333790
Processing class: 71
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 2.7756189346
Epoch:   200  |  train loss: 2.8129977703
Epoch:   300  |  train loss: 2.7811417103
Epoch:   400  |  train loss: 2.8525495529
Epoch:   500  |  train loss: 2.9073581696
Epoch:   600  |  train loss: 2.9290252209
Epoch:   700  |  train loss: 2.9502057076
Epoch:   800  |  train loss: 2.9795365810
Epoch:   900  |  train loss: 3.0590397835
Epoch:  1000  |  train loss: 3.0543105602
Processing class: 72
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 2.7275828362
Epoch:   200  |  train loss: 2.7660346508
Epoch:   300  |  train loss: 2.7952587605
Epoch:   400  |  train loss: 2.8100131512
Epoch:   500  |  train loss: 2.8680090427
Epoch:   600  |  train loss: 2.8340077400
Epoch:   700  |  train loss: 2.8698728085
Epoch:   800  |  train loss: 2.8597690582
Epoch:   900  |  train loss: 2.8899381638
Epoch:  1000  |  train loss: 2.9595423698
Processing class: 73
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 2.8026326180
Epoch:   200  |  train loss: 2.8046985626
Epoch:   300  |  train loss: 2.9134192944
Epoch:   400  |  train loss: 2.8918822765
Epoch:   500  |  train loss: 2.9831627369
Epoch:   600  |  train loss: 2.9868341446
Epoch:   700  |  train loss: 2.9533207417
Epoch:   800  |  train loss: 2.9593454838
Epoch:   900  |  train loss: 2.9772654533
Epoch:  1000  |  train loss: 3.0224110603
Processing class: 74
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 2.8325652122
Epoch:   200  |  train loss: 2.8766488075
Epoch:   300  |  train loss: 2.8225413322
Epoch:   400  |  train loss: 2.8070143700
Epoch:   500  |  train loss: 2.8587255955
Epoch:   600  |  train loss: 2.8471409321
Epoch:   700  |  train loss: 2.9456310272
Epoch:   800  |  train loss: 2.8739511967
Epoch:   900  |  train loss: 2.8927855968
Epoch:  1000  |  train loss: 2.9759167194
Processing class: 75
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 2.7447442055
Epoch:   200  |  train loss: 2.7442098141
Epoch:   300  |  train loss: 2.8393451691
Epoch:   400  |  train loss: 2.8457234859
Epoch:   500  |  train loss: 2.9140103340
Epoch:   600  |  train loss: 2.9810290337
Epoch:   700  |  train loss: 2.8497425079
Epoch:   800  |  train loss: 2.9478844166
Epoch:   900  |  train loss: 2.9488205910
Epoch:  1000  |  train loss: 3.0213672161
Processing class: 76
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 2.6266857147
Epoch:   200  |  train loss: 2.7108868122
Epoch:   300  |  train loss: 2.8430293083
Epoch:   400  |  train loss: 2.9334349155
Epoch:   500  |  train loss: 2.9723145962
Epoch:   600  |  train loss: 3.0406509876
Epoch:   700  |  train loss: 3.0585726738
Epoch:   800  |  train loss: 2.9917136192
Epoch:   900  |  train loss: 3.0892769337
Epoch:  1000  |  train loss: 3.1349400520
Processing class: 77
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 2.7616074085
Epoch:   200  |  train loss: 2.7913356781
Epoch:   300  |  train loss: 2.7739329815
Epoch:   400  |  train loss: 2.8260241032
Epoch:   500  |  train loss: 2.8700592041
Epoch:   600  |  train loss: 2.8937226772
Epoch:   700  |  train loss: 2.9297689915
Epoch:   800  |  train loss: 3.0076815128
Epoch:   900  |  train loss: 2.9862760067
Epoch:  1000  |  train loss: 2.9897976398
Processing class: 78
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 2.8034436226
Epoch:   200  |  train loss: 2.8896365166
Epoch:   300  |  train loss: 2.9683861732
Epoch:   400  |  train loss: 2.9840109825
Epoch:   500  |  train loss: 2.9911490440
Epoch:   600  |  train loss: 3.0578828335
Epoch:   700  |  train loss: 3.0887846947
Epoch:   800  |  train loss: 3.1211934090
Epoch:   900  |  train loss: 3.1383311272
Epoch:  1000  |  train loss: 3.1277846336
Processing class: 79
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 2.7812039375
Epoch:   200  |  train loss: 2.7469435215
Epoch:   300  |  train loss: 2.8224550724
Epoch:   400  |  train loss: 2.8428619385
Epoch:   500  |  train loss: 2.8514207840
Epoch:   600  |  train loss: 2.9360699177
Epoch:   700  |  train loss: 2.9431329727
Epoch:   800  |  train loss: 2.9877133369
Epoch:   900  |  train loss: 3.0530907154
Epoch:  1000  |  train loss: 3.0165106773
2024-03-06 04:22:39,305 [trainer.py] => CNN: {'total': 59.18, '00-09': 73.6, '10-19': 68.4, '20-29': 76.9, '30-39': 69.1, '40-49': 60.6, '50-59': 37.2, '60-69': 45.7, '70-79': 41.9, 'old': 61.64, 'new': 41.9}
2024-03-06 04:22:39,307 [trainer.py] => No NME accuracy
2024-03-06 04:22:39,307 [trainer.py] => FeCAM: {'total': 61.72, '00-09': 78.1, '10-19': 69.3, '20-29': 78.0, '30-39': 69.5, '40-49': 72.1, '50-59': 35.7, '60-69': 49.1, '70-79': 42.0, 'old': 64.54, 'new': 42.0}
2024-03-06 04:22:39,307 [trainer.py] => CNN top1 curve: [83.44, 71.75, 64.66, 59.18]
2024-03-06 04:22:39,307 [trainer.py] => CNN top5 curve: [96.5, 89.6, 86.54, 84.09]
2024-03-06 04:22:39,307 [trainer.py] => FeCAM top1 curve: [82.9, 72.78, 67.27, 61.72]
2024-03-06 04:22:39,307 [trainer.py] => FeCAM top5 curve: [95.84, 91.18, 87.21, 84.09]

2024-03-06 04:22:39,313 [fecam.py] => Learning on 80-90
Processing class: 80
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 2.7247141838
Epoch:   200  |  train loss: 2.9157077312
Epoch:   300  |  train loss: 2.9978280544
Epoch:   400  |  train loss: 3.0469938755
Epoch:   500  |  train loss: 3.0607316017
Epoch:   600  |  train loss: 3.0730035305
Epoch:   700  |  train loss: 3.0240736961
Epoch:   800  |  train loss: 3.1325311184
Epoch:   900  |  train loss: 3.1166343689
Epoch:  1000  |  train loss: 3.1805934429
Processing class: 81
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 2.7436210632
Epoch:   200  |  train loss: 2.8644143581
Epoch:   300  |  train loss: 2.8343338013
Epoch:   400  |  train loss: 2.9477560520
Epoch:   500  |  train loss: 2.9430235863
Epoch:   600  |  train loss: 3.0213277817
Epoch:   700  |  train loss: 3.0273486614
Epoch:   800  |  train loss: 3.0787504196
Epoch:   900  |  train loss: 3.0996713638
Epoch:  1000  |  train loss: 3.1351781845
Processing class: 82
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 2.7190177917
Epoch:   200  |  train loss: 2.8047487736
Epoch:   300  |  train loss: 2.8398339748
Epoch:   400  |  train loss: 2.8541275978
Epoch:   500  |  train loss: 2.8992298126
Epoch:   600  |  train loss: 2.8962065220
Epoch:   700  |  train loss: 2.9135054111
Epoch:   800  |  train loss: 2.9364674091
Epoch:   900  |  train loss: 2.9599031448
Epoch:  1000  |  train loss: 2.9723097324
Processing class: 83
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 2.7710453987
Epoch:   200  |  train loss: 2.9156599998
Epoch:   300  |  train loss: 2.9434316158
Epoch:   400  |  train loss: 3.0074495792
Epoch:   500  |  train loss: 3.1095659733
Epoch:   600  |  train loss: 3.0774683952
Epoch:   700  |  train loss: 3.1685720444
Epoch:   800  |  train loss: 3.2678478241
Epoch:   900  |  train loss: 3.2416157246
Epoch:  1000  |  train loss: 3.2266380310
Processing class: 84
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 2.8021918774
Epoch:   200  |  train loss: 2.8294759274
Epoch:   300  |  train loss: 2.9239637852
Epoch:   400  |  train loss: 2.9538206577
Epoch:   500  |  train loss: 3.0281435490
Epoch:   600  |  train loss: 2.9866738319
Epoch:   700  |  train loss: 3.0611963272
Epoch:   800  |  train loss: 3.1250488281
Epoch:   900  |  train loss: 3.0658412457
Epoch:  1000  |  train loss: 3.0616632938
Processing class: 85
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 2.7685116291
Epoch:   200  |  train loss: 2.7868171692
Epoch:   300  |  train loss: 2.8799242973
Epoch:   400  |  train loss: 2.8760562420
Epoch:   500  |  train loss: 2.9381962299
Epoch:   600  |  train loss: 3.0079352856
Epoch:   700  |  train loss: 3.0975285530
Epoch:   800  |  train loss: 3.0442664146
Epoch:   900  |  train loss: 3.0622009277
Epoch:  1000  |  train loss: 3.1372213840
Processing class: 86
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 2.8275690079
Epoch:   200  |  train loss: 2.7852126122
Epoch:   300  |  train loss: 2.8898768425
Epoch:   400  |  train loss: 2.8849394798
Epoch:   500  |  train loss: 2.8985903263
Epoch:   600  |  train loss: 2.9490037441
Epoch:   700  |  train loss: 2.9651824951
Epoch:   800  |  train loss: 2.9704150677
Epoch:   900  |  train loss: 2.9795706749
Epoch:  1000  |  train loss: 3.0734947205
Processing class: 87
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 2.7190983295
Epoch:   200  |  train loss: 2.8316304207
Epoch:   300  |  train loss: 2.8604273319
Epoch:   400  |  train loss: 2.9326661110
Epoch:   500  |  train loss: 2.9448943615
Epoch:   600  |  train loss: 2.9930423260
Epoch:   700  |  train loss: 3.0274309158
Epoch:   800  |  train loss: 3.0395867348
Epoch:   900  |  train loss: 3.0131403446
Epoch:  1000  |  train loss: 3.1226915836
Processing class: 88
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 2.7476516247
Epoch:   200  |  train loss: 2.7428042889
Epoch:   300  |  train loss: 2.7700318336
Epoch:   400  |  train loss: 2.8103923798
Epoch:   500  |  train loss: 2.8280922413
Epoch:   600  |  train loss: 2.8280347824
Epoch:   700  |  train loss: 2.8976511955
Epoch:   800  |  train loss: 2.9191296577
Epoch:   900  |  train loss: 2.9471776009
Epoch:  1000  |  train loss: 2.9215845585
Processing class: 89
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 2.7870200634
Epoch:   200  |  train loss: 2.8821999073
Epoch:   300  |  train loss: 2.9499806881
Epoch:   400  |  train loss: 2.9858051777
Epoch:   500  |  train loss: 3.0654529572
Epoch:   600  |  train loss: 3.0953822613
Epoch:   700  |  train loss: 3.0505875587
Epoch:   800  |  train loss: 3.1853651047
Epoch:   900  |  train loss: 3.1678986549
Epoch:  1000  |  train loss: 3.2151316166
2024-03-06 04:31:26,030 [trainer.py] => CNN: {'total': 54.08, '00-09': 67.9, '10-19': 63.6, '20-29': 73.2, '30-39': 68.8, '40-49': 57.2, '50-59': 32.6, '60-69': 40.9, '70-79': 38.1, '80-89': 44.4, 'old': 55.29, 'new': 44.4}
2024-03-06 04:31:26,031 [trainer.py] => No NME accuracy
2024-03-06 04:31:26,031 [trainer.py] => FeCAM: {'total': 57.66, '00-09': 76.1, '10-19': 65.8, '20-29': 75.3, '30-39': 67.9, '40-49': 68.5, '50-59': 33.0, '60-69': 45.0, '70-79': 39.0, '80-89': 48.3, 'old': 58.82, 'new': 48.3}
2024-03-06 04:31:26,031 [trainer.py] => CNN top1 curve: [83.44, 71.75, 64.66, 59.18, 54.08]
2024-03-06 04:31:26,031 [trainer.py] => CNN top5 curve: [96.5, 89.6, 86.54, 84.09, 81.74]
2024-03-06 04:31:26,031 [trainer.py] => FeCAM top1 curve: [82.9, 72.78, 67.27, 61.72, 57.66]
2024-03-06 04:31:26,031 [trainer.py] => FeCAM top5 curve: [95.84, 91.18, 87.21, 84.09, 81.63]

2024-03-06 04:31:26,037 [fecam.py] => Learning on 90-100
Processing class: 90
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 2.7881200790
Epoch:   200  |  train loss: 2.8144066811
Epoch:   300  |  train loss: 2.7806154728
Epoch:   400  |  train loss: 2.7901600361
Epoch:   500  |  train loss: 2.7894165993
Epoch:   600  |  train loss: 2.8410295010
Epoch:   700  |  train loss: 2.8472950459
Epoch:   800  |  train loss: 2.8800894260
Epoch:   900  |  train loss: 2.9233191490
Epoch:  1000  |  train loss: 2.9992677212
Processing class: 91
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 2.6929537773
Epoch:   200  |  train loss: 2.7129250526
Epoch:   300  |  train loss: 2.8021614552
Epoch:   400  |  train loss: 2.8712615490
Epoch:   500  |  train loss: 2.9477198601
Epoch:   600  |  train loss: 2.9912788868
Epoch:   700  |  train loss: 3.0294900894
Epoch:   800  |  train loss: 3.0809339046
Epoch:   900  |  train loss: 3.1394872189
Epoch:  1000  |  train loss: 3.1590674877
Processing class: 92
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 2.8418699265
Epoch:   200  |  train loss: 2.8245959282
Epoch:   300  |  train loss: 2.7692227840
Epoch:   400  |  train loss: 2.8310451984
Epoch:   500  |  train loss: 2.8874463081
Epoch:   600  |  train loss: 2.9775681019
Epoch:   700  |  train loss: 2.9953064919
Epoch:   800  |  train loss: 2.9308447838
Epoch:   900  |  train loss: 2.9642652512
Epoch:  1000  |  train loss: 2.9899324417
Processing class: 93
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 2.7607386112
Epoch:   200  |  train loss: 2.8428756237
Epoch:   300  |  train loss: 3.0152186394
Epoch:   400  |  train loss: 3.0560847759
Epoch:   500  |  train loss: 3.0696607113
Epoch:   600  |  train loss: 3.0477969170
Epoch:   700  |  train loss: 3.1812057018
Epoch:   800  |  train loss: 3.1881793022
Epoch:   900  |  train loss: 3.2137362480
Epoch:  1000  |  train loss: 3.2130260944
Processing class: 94
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 2.5710136414
Epoch:   200  |  train loss: 2.5858294487
Epoch:   300  |  train loss: 2.5455826283
Epoch:   400  |  train loss: 2.6083922863
Epoch:   500  |  train loss: 2.5225438118
Epoch:   600  |  train loss: 2.5911397457
Epoch:   700  |  train loss: 2.6568586349
Epoch:   800  |  train loss: 2.5453961372
Epoch:   900  |  train loss: 2.6391348839
Epoch:  1000  |  train loss: 2.6226271629
Processing class: 95
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 2.7381681442
Epoch:   200  |  train loss: 2.8148723125
Epoch:   300  |  train loss: 2.7940559387
Epoch:   400  |  train loss: 2.7935858727
Epoch:   500  |  train loss: 2.9039039135
Epoch:   600  |  train loss: 2.9095645428
Epoch:   700  |  train loss: 2.9587488174
Epoch:   800  |  train loss: 2.9435448647
Epoch:   900  |  train loss: 3.0136244297
Epoch:  1000  |  train loss: 3.0807560444
Processing class: 96
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 2.8158033371
Epoch:   200  |  train loss: 2.7493241787
Epoch:   300  |  train loss: 2.8377771378
Epoch:   400  |  train loss: 2.9586404800
Epoch:   500  |  train loss: 2.9446600914
Epoch:   600  |  train loss: 2.9479812145
Epoch:   700  |  train loss: 2.9788517952
Epoch:   800  |  train loss: 2.9533303261
Epoch:   900  |  train loss: 3.0728492260
Epoch:  1000  |  train loss: 3.0842226982
Processing class: 97
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 2.8042212963
Epoch:   200  |  train loss: 2.8521267414
Epoch:   300  |  train loss: 2.9046663284
Epoch:   400  |  train loss: 2.9490192890
Epoch:   500  |  train loss: 3.0372447491
Epoch:   600  |  train loss: 3.0969981194
Epoch:   700  |  train loss: 3.1545634747
Epoch:   800  |  train loss: 3.1856070518
Epoch:   900  |  train loss: 3.2230454445
Epoch:  1000  |  train loss: 3.2252164841
Processing class: 98
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 2.7336389065
Epoch:   200  |  train loss: 2.7568543911
Epoch:   300  |  train loss: 2.8120338917
Epoch:   400  |  train loss: 2.8720037460
Epoch:   500  |  train loss: 2.8866188526
Epoch:   600  |  train loss: 2.9450881958
Epoch:   700  |  train loss: 2.8959182739
Epoch:   800  |  train loss: 2.9673231125
Epoch:   900  |  train loss: 2.9797358990
Epoch:  1000  |  train loss: 2.9587182999
Processing class: 99
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 2.8532354832
Epoch:   200  |  train loss: 2.8178248882
Epoch:   300  |  train loss: 2.9618917942
Epoch:   400  |  train loss: 3.0638084888
Epoch:   500  |  train loss: 3.0777315140
Epoch:   600  |  train loss: 3.0872310638
Epoch:   700  |  train loss: 3.0812583923
Epoch:   800  |  train loss: 3.1608257294
Epoch:   900  |  train loss: 3.1482435226
Epoch:  1000  |  train loss: 3.1715197563
2024-03-06 04:41:38,016 [trainer.py] => CNN: {'total': 50.36, '00-09': 57.7, '10-19': 63.6, '20-29': 71.4, '30-39': 68.5, '40-49': 56.8, '50-59': 29.6, '60-69': 38.9, '70-79': 36.6, '80-89': 42.7, '90-99': 37.8, 'old': 51.76, 'new': 37.8}
2024-03-06 04:41:38,018 [trainer.py] => No NME accuracy
2024-03-06 04:41:38,018 [trainer.py] => FeCAM: {'total': 54.74, '00-09': 73.3, '10-19': 65.1, '20-29': 74.1, '30-39': 67.2, '40-49': 67.1, '50-59': 30.8, '60-69': 43.3, '70-79': 37.2, '80-89': 46.9, '90-99': 42.4, 'old': 56.11, 'new': 42.4}
2024-03-06 04:41:38,018 [trainer.py] => CNN top1 curve: [83.44, 71.75, 64.66, 59.18, 54.08, 50.36]
2024-03-06 04:41:38,018 [trainer.py] => CNN top5 curve: [96.5, 89.6, 86.54, 84.09, 81.74, 79.63]
2024-03-06 04:41:38,018 [trainer.py] => FeCAM top1 curve: [82.9, 72.78, 67.27, 61.72, 57.66, 54.74]
2024-03-06 04:41:38,018 [trainer.py] => FeCAM top5 curve: [95.84, 91.18, 87.21, 84.09, 81.63, 79.21]

=========================================
2024-03-06 04:41:48,325 [trainer.py] => config: ./exps/FeCAM_cifar100.json
2024-03-06 04:41:48,325 [trainer.py] => prefix: train
2024-03-06 04:41:48,325 [trainer.py] => dataset: cifar100
2024-03-06 04:41:48,325 [trainer.py] => memory_size: 0
2024-03-06 04:41:48,326 [trainer.py] => shuffle: True
2024-03-06 04:41:48,326 [trainer.py] => init_cls: 50
2024-03-06 04:41:48,326 [trainer.py] => increment: 10
2024-03-06 04:41:48,326 [trainer.py] => model_name: fecam
2024-03-06 04:41:48,326 [trainer.py] => convnet_type: resnet18
2024-03-06 04:41:48,326 [trainer.py] => device: [device(type='cuda', index=0)]
2024-03-06 04:41:48,326 [trainer.py] => seed: 1993
2024-03-06 04:41:48,326 [trainer.py] => init_epochs: 200
2024-03-06 04:41:48,326 [trainer.py] => init_lr: 0.1
2024-03-06 04:41:48,326 [trainer.py] => init_weight_decay: 0.0005
2024-03-06 04:41:48,326 [trainer.py] => batch_size: 128
2024-03-06 04:41:48,326 [trainer.py] => num_workers: 8
2024-03-06 04:41:48,326 [trainer.py] => T: 5
2024-03-06 04:41:48,326 [trainer.py] => beta: 0.5
2024-03-06 04:41:48,326 [trainer.py] => alpha1: 1
2024-03-06 04:41:48,326 [trainer.py] => alpha2: 1
2024-03-06 04:41:48,326 [trainer.py] => ncm: False
2024-03-06 04:41:48,326 [trainer.py] => tukey: False
2024-03-06 04:41:48,326 [trainer.py] => diagonal: False
2024-03-06 04:41:48,326 [trainer.py] => per_class: True
2024-03-06 04:41:48,326 [trainer.py] => full_cov: True
2024-03-06 04:41:48,326 [trainer.py] => shrink: True
2024-03-06 04:41:48,326 [trainer.py] => norm_cov: False
2024-03-06 04:41:48,326 [trainer.py] => vecnorm: False
2024-03-06 04:41:48,326 [trainer.py] => ae_type: wae
2024-03-06 04:41:48,326 [trainer.py] => epochs: 1000
2024-03-06 04:41:48,326 [trainer.py] => ae_latent_dim: 32
2024-03-06 04:41:48,326 [trainer.py] => wae_sigma: 50
2024-03-06 04:41:48,326 [trainer.py] => wae_C: 10
Files already downloaded and verified
Files already downloaded and verified
2024-03-06 04:41:49,986 [data_manager.py] => [68, 56, 78, 8, 23, 84, 90, 65, 74, 76, 40, 89, 3, 92, 55, 9, 26, 80, 43, 38, 58, 70, 77, 1, 85, 19, 17, 50, 28, 53, 13, 81, 45, 82, 6, 59, 83, 16, 15, 44, 91, 41, 72, 60, 79, 52, 20, 10, 31, 54, 37, 95, 14, 71, 96, 98, 97, 2, 64, 66, 42, 22, 35, 86, 24, 34, 87, 21, 99, 0, 88, 27, 18, 94, 11, 12, 47, 25, 30, 46, 62, 69, 36, 61, 7, 63, 75, 5, 32, 4, 51, 48, 73, 93, 39, 67, 29, 49, 57, 33]
2024-03-06 04:41:50,252 [fecam.py] => Learning on 0-50
/home/z1165703/miniconda3/lib/python3.11/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
/home/z1165703/miniconda3/lib/python3.11/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
Processing class: 0
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 2.5235266685
Epoch:   200  |  train loss: 2.6135546207
Epoch:   300  |  train loss: 2.6785706997
Epoch:   400  |  train loss: 2.7174664497
Epoch:   500  |  train loss: 2.6980124474
Epoch:   600  |  train loss: 2.6966640949
Epoch:   700  |  train loss: 2.7300460339
Epoch:   800  |  train loss: 2.7423645973
Epoch:   900  |  train loss: 2.7200352669
Epoch:  1000  |  train loss: 2.7219646454
Processing class: 1
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 2.4291492939
Epoch:   200  |  train loss: 2.4081500530
Epoch:   300  |  train loss: 2.5680051327
Epoch:   400  |  train loss: 2.6061522961
Epoch:   500  |  train loss: 2.5590116024
Epoch:   600  |  train loss: 2.6560017109
Epoch:   700  |  train loss: 2.6271092415
Epoch:   800  |  train loss: 2.6872528076
Epoch:   900  |  train loss: 2.7668166637
Epoch:  1000  |  train loss: 2.7083306313
Processing class: 2
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 2.3742828846
Epoch:   200  |  train loss: 2.3904681683
Epoch:   300  |  train loss: 2.5063956738
Epoch:   400  |  train loss: 2.5175580025
Epoch:   500  |  train loss: 2.4155192375
Epoch:   600  |  train loss: 2.5374370575
Epoch:   700  |  train loss: 2.5412485600
Epoch:   800  |  train loss: 2.6366372585
Epoch:   900  |  train loss: 2.6091519356
Epoch:  1000  |  train loss: 2.5952419758
Processing class: 3
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 2.6878675938
Epoch:   200  |  train loss: 2.7756234169
Epoch:   300  |  train loss: 2.8323694706
Epoch:   400  |  train loss: 2.8770771027
Epoch:   500  |  train loss: 2.8591777802
Epoch:   600  |  train loss: 2.9603183746
Epoch:   700  |  train loss: 3.0356003284
Epoch:   800  |  train loss: 2.9869291306
Epoch:   900  |  train loss: 3.0492146492
Epoch:  1000  |  train loss: 3.0820590496
Processing class: 4
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 2.4003363132
Epoch:   200  |  train loss: 2.3740410328
Epoch:   300  |  train loss: 2.5130808830
Epoch:   400  |  train loss: 2.5452991486
Epoch:   500  |  train loss: 2.6063701630
Epoch:   600  |  train loss: 2.6149054527
Epoch:   700  |  train loss: 2.6663217068
Epoch:   800  |  train loss: 2.6871537685
Epoch:   900  |  train loss: 2.7201055050
Epoch:  1000  |  train loss: 2.7964356422
Processing class: 5
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 2.3805075645
Epoch:   200  |  train loss: 2.3950286388
Epoch:   300  |  train loss: 2.4567608356
Epoch:   400  |  train loss: 2.4844742298
Epoch:   500  |  train loss: 2.4857386112
Epoch:   600  |  train loss: 2.6163777828
Epoch:   700  |  train loss: 2.6576086998
Epoch:   800  |  train loss: 2.6210677624
Epoch:   900  |  train loss: 2.6261560440
Epoch:  1000  |  train loss: 2.6901380539
Processing class: 6
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 2.4447414398
Epoch:   200  |  train loss: 2.4883701324
Epoch:   300  |  train loss: 2.6291640282
Epoch:   400  |  train loss: 2.6381397247
Epoch:   500  |  train loss: 2.7463385582
Epoch:   600  |  train loss: 2.7201293468
Epoch:   700  |  train loss: 2.6996419430
Epoch:   800  |  train loss: 2.7347682476
Epoch:   900  |  train loss: 2.7210381031
Epoch:  1000  |  train loss: 2.7636492729
Processing class: 7
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 2.3310529709
Epoch:   200  |  train loss: 2.3786098003
Epoch:   300  |  train loss: 2.4271411419
Epoch:   400  |  train loss: 2.4770324230
Epoch:   500  |  train loss: 2.5494571209
Epoch:   600  |  train loss: 2.5297632694
Epoch:   700  |  train loss: 2.5891698360
Epoch:   800  |  train loss: 2.6810843468
Epoch:   900  |  train loss: 2.6280664444
Epoch:  1000  |  train loss: 2.7074976921
Processing class: 8
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 2.3810636997
Epoch:   200  |  train loss: 2.4548246384
Epoch:   300  |  train loss: 2.4632917404
Epoch:   400  |  train loss: 2.5846417904
Epoch:   500  |  train loss: 2.5506720543
Epoch:   600  |  train loss: 2.6002491474
Epoch:   700  |  train loss: 2.6821210384
Epoch:   800  |  train loss: 2.6358445168
Epoch:   900  |  train loss: 2.6863680363
Epoch:  1000  |  train loss: 2.6614777088
Processing class: 9
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 2.3416312695
Epoch:   200  |  train loss: 2.4012434006
Epoch:   300  |  train loss: 2.4899455547
Epoch:   400  |  train loss: 2.4325615883
Epoch:   500  |  train loss: 2.5217131138
Epoch:   600  |  train loss: 2.4810878277
Epoch:   700  |  train loss: 2.5137341499
Epoch:   800  |  train loss: 2.5419584751
Epoch:   900  |  train loss: 2.5499930859
Epoch:  1000  |  train loss: 2.5108250141
Processing class: 10
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 2.2511819363
Epoch:   200  |  train loss: 2.2743469715
Epoch:   300  |  train loss: 2.3206842422
Epoch:   400  |  train loss: 2.4224548340
Epoch:   500  |  train loss: 2.3356793404
Epoch:   600  |  train loss: 2.3939517498
Epoch:   700  |  train loss: 2.4389657497
Epoch:   800  |  train loss: 2.4303366184
Epoch:   900  |  train loss: 2.4624760628
Epoch:  1000  |  train loss: 2.4641798019
Processing class: 11
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 2.3860053062
Epoch:   200  |  train loss: 2.4557046413
Epoch:   300  |  train loss: 2.4370760918
Epoch:   400  |  train loss: 2.4964451313
Epoch:   500  |  train loss: 2.5268813610
Epoch:   600  |  train loss: 2.6013050079
Epoch:   700  |  train loss: 2.6997905731
Epoch:   800  |  train loss: 2.6899030685
Epoch:   900  |  train loss: 2.7086374760
Epoch:  1000  |  train loss: 2.7660767078
Processing class: 12
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 2.2875048161
Epoch:   200  |  train loss: 2.4328623295
Epoch:   300  |  train loss: 2.4681252003
Epoch:   400  |  train loss: 2.5694591999
Epoch:   500  |  train loss: 2.5931209087
Epoch:   600  |  train loss: 2.5729005814
Epoch:   700  |  train loss: 2.6150706768
Epoch:   800  |  train loss: 2.6455772400
Epoch:   900  |  train loss: 2.6979060173
Epoch:  1000  |  train loss: 2.6765387535
Processing class: 13
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 2.3649258614
Epoch:   200  |  train loss: 2.4995831013
Epoch:   300  |  train loss: 2.6052120686
Epoch:   400  |  train loss: 2.6555709839
Epoch:   500  |  train loss: 2.7500141144
Epoch:   600  |  train loss: 2.7020504475
Epoch:   700  |  train loss: 2.7598159790
Epoch:   800  |  train loss: 2.8057256699
Epoch:   900  |  train loss: 2.8355852604
Epoch:  1000  |  train loss: 2.8741716862
Processing class: 14
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 2.3496530533
Epoch:   200  |  train loss: 2.3899742126
Epoch:   300  |  train loss: 2.4353428841
Epoch:   400  |  train loss: 2.5292329311
Epoch:   500  |  train loss: 2.5328080654
Epoch:   600  |  train loss: 2.5487856388
Epoch:   700  |  train loss: 2.5910726070
Epoch:   800  |  train loss: 2.6917719364
Epoch:   900  |  train loss: 2.6662867546
Epoch:  1000  |  train loss: 2.6106473446
Processing class: 15
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 2.4075815678
Epoch:   200  |  train loss: 2.4680868626
Epoch:   300  |  train loss: 2.6015532017
Epoch:   400  |  train loss: 2.6638110161
Epoch:   500  |  train loss: 2.7206482887
Epoch:   600  |  train loss: 2.6897068501
Epoch:   700  |  train loss: 2.7754343510
Epoch:   800  |  train loss: 2.7115624905
Epoch:   900  |  train loss: 2.7889846325
Epoch:  1000  |  train loss: 2.7594966888
Processing class: 16
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 2.4267545223
Epoch:   200  |  train loss: 2.6425567627
Epoch:   300  |  train loss: 2.7389997959
Epoch:   400  |  train loss: 2.7118805408
Epoch:   500  |  train loss: 2.7868213654
Epoch:   600  |  train loss: 2.7355943680
Epoch:   700  |  train loss: 2.7301871300
Epoch:   800  |  train loss: 2.8361220837
Epoch:   900  |  train loss: 2.7588905334
Epoch:  1000  |  train loss: 2.8453050137
Processing class: 17
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 2.2905039787
Epoch:   200  |  train loss: 2.3402781487
Epoch:   300  |  train loss: 2.4219007015
Epoch:   400  |  train loss: 2.4501258373
Epoch:   500  |  train loss: 2.3991714478
Epoch:   600  |  train loss: 2.4687274456
Epoch:   700  |  train loss: 2.4646114349
Epoch:   800  |  train loss: 2.4820310116
Epoch:   900  |  train loss: 2.5145831585
Epoch:  1000  |  train loss: 2.5079419613
Processing class: 18
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 2.3673269272
Epoch:   200  |  train loss: 2.3932086468
Epoch:   300  |  train loss: 2.5102246284
Epoch:   400  |  train loss: 2.5206040859
Epoch:   500  |  train loss: 2.6345923424
Epoch:   600  |  train loss: 2.6284614563
Epoch:   700  |  train loss: 2.7620100498
Epoch:   800  |  train loss: 2.7732445717
Epoch:   900  |  train loss: 2.8108926773
Epoch:  1000  |  train loss: 2.8921586514
Processing class: 19
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 2.4341883183
Epoch:   200  |  train loss: 2.6054386139
Epoch:   300  |  train loss: 2.6265299797
Epoch:   400  |  train loss: 2.6567881107
Epoch:   500  |  train loss: 2.6313372612
Epoch:   600  |  train loss: 2.6495345592
Epoch:   700  |  train loss: 2.6967833519
Epoch:   800  |  train loss: 2.6865504265
Epoch:   900  |  train loss: 2.8362022877
Epoch:  1000  |  train loss: 2.8718537807
Processing class: 20
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 2.4128035545
Epoch:   200  |  train loss: 2.4882813454
Epoch:   300  |  train loss: 2.5474444389
Epoch:   400  |  train loss: 2.6129304886
Epoch:   500  |  train loss: 2.6898183823
Epoch:   600  |  train loss: 2.6585875988
Epoch:   700  |  train loss: 2.6873771191
Epoch:   800  |  train loss: 2.6921503067
Epoch:   900  |  train loss: 2.7330696106
Epoch:  1000  |  train loss: 2.7896063328
Processing class: 21
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 2.4174983501
Epoch:   200  |  train loss: 2.4992707253
Epoch:   300  |  train loss: 2.5895791054
Epoch:   400  |  train loss: 2.6456000328
Epoch:   500  |  train loss: 2.6935661793
Epoch:   600  |  train loss: 2.7521337509
Epoch:   700  |  train loss: 2.7451027870
Epoch:   800  |  train loss: 2.7419548988
Epoch:   900  |  train loss: 2.8180624008
Epoch:  1000  |  train loss: 2.8168590069
Processing class: 22
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 2.3460270882
Epoch:   200  |  train loss: 2.3807082653
Epoch:   300  |  train loss: 2.4478723049
Epoch:   400  |  train loss: 2.5507485390
Epoch:   500  |  train loss: 2.5936709404
Epoch:   600  |  train loss: 2.5726809025
Epoch:   700  |  train loss: 2.6221665382
Epoch:   800  |  train loss: 2.6176146030
Epoch:   900  |  train loss: 2.5945327282
Epoch:  1000  |  train loss: 2.7011996746
Processing class: 23
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 2.4884885311
Epoch:   200  |  train loss: 2.5596438885
Epoch:   300  |  train loss: 2.6365322590
Epoch:   400  |  train loss: 2.5860294342
Epoch:   500  |  train loss: 2.6005699158
Epoch:   600  |  train loss: 2.5839025021
Epoch:   700  |  train loss: 2.6464649677
Epoch:   800  |  train loss: 2.6240400791
Epoch:   900  |  train loss: 2.6798427582
Epoch:  1000  |  train loss: 2.6548927784
Processing class: 24
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 2.4245050907
Epoch:   200  |  train loss: 2.4335754395
Epoch:   300  |  train loss: 2.3720393658
Epoch:   400  |  train loss: 2.4595363617
Epoch:   500  |  train loss: 2.4429901123
Epoch:   600  |  train loss: 2.4948334694
Epoch:   700  |  train loss: 2.4209020615
Epoch:   800  |  train loss: 2.4586967945
Epoch:   900  |  train loss: 2.5203539848
Epoch:  1000  |  train loss: 2.5139096260
Processing class: 25
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 2.3674011230
Epoch:   200  |  train loss: 2.4372504234
Epoch:   300  |  train loss: 2.5295171738
Epoch:   400  |  train loss: 2.5105303764
Epoch:   500  |  train loss: 2.6220636368
Epoch:   600  |  train loss: 2.5935456276
Epoch:   700  |  train loss: 2.6800725937
Epoch:   800  |  train loss: 2.6610601902
Epoch:   900  |  train loss: 2.6557863712
Epoch:  1000  |  train loss: 2.6866745472
Processing class: 26
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 2.5312435150
Epoch:   200  |  train loss: 2.6246561527
Epoch:   300  |  train loss: 2.5642042160
Epoch:   400  |  train loss: 2.6483677387
Epoch:   500  |  train loss: 2.7333719254
Epoch:   600  |  train loss: 2.6632418156
Epoch:   700  |  train loss: 2.7122340202
Epoch:   800  |  train loss: 2.7480675220
Epoch:   900  |  train loss: 2.7470270157
Epoch:  1000  |  train loss: 2.8123260021
Processing class: 27
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 2.3569548130
Epoch:   200  |  train loss: 2.3652068138
Epoch:   300  |  train loss: 2.4010120392
Epoch:   400  |  train loss: 2.4478427887
Epoch:   500  |  train loss: 2.5156334877
Epoch:   600  |  train loss: 2.5240125656
Epoch:   700  |  train loss: 2.5626748085
Epoch:   800  |  train loss: 2.5683495998
Epoch:   900  |  train loss: 2.6035305500
Epoch:  1000  |  train loss: 2.5549390793
Processing class: 28
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 2.4649818420
Epoch:   200  |  train loss: 2.5439527988
Epoch:   300  |  train loss: 2.6559061527
Epoch:   400  |  train loss: 2.7171206474
Epoch:   500  |  train loss: 2.7531224728
Epoch:   600  |  train loss: 2.7705058098
Epoch:   700  |  train loss: 2.7895730972
Epoch:   800  |  train loss: 2.8193705082
Epoch:   900  |  train loss: 2.8387920856
Epoch:  1000  |  train loss: 2.7856513023
Processing class: 29
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 2.3865786076
Epoch:   200  |  train loss: 2.4750209808
Epoch:   300  |  train loss: 2.4966959476
Epoch:   400  |  train loss: 2.4948825359
Epoch:   500  |  train loss: 2.5677233696
Epoch:   600  |  train loss: 2.6136896133
Epoch:   700  |  train loss: 2.6298944473
Epoch:   800  |  train loss: 2.6041027069
Epoch:   900  |  train loss: 2.6273520947
Epoch:  1000  |  train loss: 2.6217409134
Processing class: 30
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 2.5209478855
Epoch:   200  |  train loss: 2.6102704525
Epoch:   300  |  train loss: 2.7603627205
Epoch:   400  |  train loss: 2.8720983982
Epoch:   500  |  train loss: 2.8285892010
Epoch:   600  |  train loss: 2.8428490162
Epoch:   700  |  train loss: 2.8441352367
Epoch:   800  |  train loss: 2.9173119068
Epoch:   900  |  train loss: 2.9656131744
Epoch:  1000  |  train loss: 2.9286790848
Processing class: 31
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 2.4698957920
Epoch:   200  |  train loss: 2.5844107151
Epoch:   300  |  train loss: 2.6250530243
Epoch:   400  |  train loss: 2.7188226700
Epoch:   500  |  train loss: 2.7607031822
Epoch:   600  |  train loss: 2.8460072517
Epoch:   700  |  train loss: 2.8384623528
Epoch:   800  |  train loss: 2.8473614693
Epoch:   900  |  train loss: 2.9113076687
Epoch:  1000  |  train loss: 2.8610021114
Processing class: 32
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 2.3619261265
Epoch:   200  |  train loss: 2.4420176983
Epoch:   300  |  train loss: 2.4502410412
Epoch:   400  |  train loss: 2.4134790421
Epoch:   500  |  train loss: 2.5482226849
Epoch:   600  |  train loss: 2.5453437805
Epoch:   700  |  train loss: 2.6484183788
Epoch:   800  |  train loss: 2.6969823360
Epoch:   900  |  train loss: 2.6865137577
Epoch:  1000  |  train loss: 2.6742002010
Processing class: 33
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 2.2797155380
Epoch:   200  |  train loss: 2.4390187263
Epoch:   300  |  train loss: 2.3623764992
Epoch:   400  |  train loss: 2.4061910629
Epoch:   500  |  train loss: 2.4393779278
Epoch:   600  |  train loss: 2.5468222618
Epoch:   700  |  train loss: 2.5109375954
Epoch:   800  |  train loss: 2.5438675880
Epoch:   900  |  train loss: 2.5471154690
Epoch:  1000  |  train loss: 2.5304277420
Processing class: 34
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 2.3993659973
Epoch:   200  |  train loss: 2.3949728012
Epoch:   300  |  train loss: 2.4797718525
Epoch:   400  |  train loss: 2.4663534641
Epoch:   500  |  train loss: 2.4837713242
Epoch:   600  |  train loss: 2.5529355526
Epoch:   700  |  train loss: 2.5236206055
Epoch:   800  |  train loss: 2.6037966251
Epoch:   900  |  train loss: 2.6355962753
Epoch:  1000  |  train loss: 2.6561503410
Processing class: 35
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 2.2881203175
Epoch:   200  |  train loss: 2.3967726707
Epoch:   300  |  train loss: 2.4683971405
Epoch:   400  |  train loss: 2.5086859226
Epoch:   500  |  train loss: 2.4818510532
Epoch:   600  |  train loss: 2.5214601517
Epoch:   700  |  train loss: 2.5605754852
Epoch:   800  |  train loss: 2.5620878220
Epoch:   900  |  train loss: 2.5777743816
Epoch:  1000  |  train loss: 2.5598308563
Processing class: 36
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 2.4881395340
Epoch:   200  |  train loss: 2.5346532345
Epoch:   300  |  train loss: 2.6094055653
Epoch:   400  |  train loss: 2.6234270096
Epoch:   500  |  train loss: 2.7283648968
Epoch:   600  |  train loss: 2.7118152142
Epoch:   700  |  train loss: 2.6869416237
Epoch:   800  |  train loss: 2.7991469383
Epoch:   900  |  train loss: 2.7347451687
Epoch:  1000  |  train loss: 2.8047723770
Processing class: 37
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 2.4078080654
Epoch:   200  |  train loss: 2.4103666782
Epoch:   300  |  train loss: 2.4277891159
Epoch:   400  |  train loss: 2.4862977982
Epoch:   500  |  train loss: 2.5381461143
Epoch:   600  |  train loss: 2.5100837708
Epoch:   700  |  train loss: 2.5337215424
Epoch:   800  |  train loss: 2.5624577999
Epoch:   900  |  train loss: 2.5404265881
Epoch:  1000  |  train loss: 2.5703826904
Processing class: 38
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 2.4258285522
Epoch:   200  |  train loss: 2.4652273178
Epoch:   300  |  train loss: 2.5518085003
Epoch:   400  |  train loss: 2.5758408546
Epoch:   500  |  train loss: 2.6246157169
Epoch:   600  |  train loss: 2.6266787529
Epoch:   700  |  train loss: 2.6544777870
Epoch:   800  |  train loss: 2.7211646557
Epoch:   900  |  train loss: 2.6608094692
Epoch:  1000  |  train loss: 2.7190921783
Processing class: 39
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 2.4268468380
Epoch:   200  |  train loss: 2.4207170486
Epoch:   300  |  train loss: 2.4868821144
Epoch:   400  |  train loss: 2.5418445110
Epoch:   500  |  train loss: 2.5388167381
Epoch:   600  |  train loss: 2.5718617916
Epoch:   700  |  train loss: 2.6168374538
Epoch:   800  |  train loss: 2.6714913368
Epoch:   900  |  train loss: 2.7113089085
Epoch:  1000  |  train loss: 2.7401783466
Processing class: 40
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 2.4474671364
Epoch:   200  |  train loss: 2.4423565865
Epoch:   300  |  train loss: 2.5768829346
Epoch:   400  |  train loss: 2.6127699852
Epoch:   500  |  train loss: 2.6610156059
Epoch:   600  |  train loss: 2.6555660725
Epoch:   700  |  train loss: 2.6913114071
Epoch:   800  |  train loss: 2.6992757320
Epoch:   900  |  train loss: 2.6753182411
Epoch:  1000  |  train loss: 2.7935406208
Processing class: 41
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 2.4504958630
Epoch:   200  |  train loss: 2.4596870899
Epoch:   300  |  train loss: 2.5554656982
Epoch:   400  |  train loss: 2.5659892559
Epoch:   500  |  train loss: 2.5876558304
Epoch:   600  |  train loss: 2.6022025108
Epoch:   700  |  train loss: 2.6629475117
Epoch:   800  |  train loss: 2.6406956196
Epoch:   900  |  train loss: 2.6385257721
Epoch:  1000  |  train loss: 2.6514343262
Processing class: 42
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 2.3346641541
Epoch:   200  |  train loss: 2.4388729572
Epoch:   300  |  train loss: 2.5188740253
Epoch:   400  |  train loss: 2.5680706024
Epoch:   500  |  train loss: 2.6188611984
Epoch:   600  |  train loss: 2.6588865280
Epoch:   700  |  train loss: 2.7103485584
Epoch:   800  |  train loss: 2.7561825275
Epoch:   900  |  train loss: 2.7398188114
Epoch:  1000  |  train loss: 2.7462644100
Processing class: 43
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 2.3516092777
Epoch:   200  |  train loss: 2.3700545788
Epoch:   300  |  train loss: 2.5465034485
Epoch:   400  |  train loss: 2.6711774349
Epoch:   500  |  train loss: 2.7106675148
Epoch:   600  |  train loss: 2.6852390289
Epoch:   700  |  train loss: 2.7392353058
Epoch:   800  |  train loss: 2.7222218037
Epoch:   900  |  train loss: 2.6689686298
Epoch:  1000  |  train loss: 2.7579371929
Processing class: 44
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 2.3165328503
Epoch:   200  |  train loss: 2.4404021740
Epoch:   300  |  train loss: 2.4215371132
Epoch:   400  |  train loss: 2.4666043282
Epoch:   500  |  train loss: 2.5150699139
Epoch:   600  |  train loss: 2.4989676952
Epoch:   700  |  train loss: 2.5680396080
Epoch:   800  |  train loss: 2.5605782986
Epoch:   900  |  train loss: 2.5474671841
Epoch:  1000  |  train loss: 2.6237904549
Processing class: 45
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 2.4449345112
Epoch:   200  |  train loss: 2.5841362476
Epoch:   300  |  train loss: 2.6208654881
Epoch:   400  |  train loss: 2.6858582973
Epoch:   500  |  train loss: 2.6793024540
Epoch:   600  |  train loss: 2.7872894287
Epoch:   700  |  train loss: 2.8217122078
Epoch:   800  |  train loss: 2.8035487652
Epoch:   900  |  train loss: 2.8161653996
Epoch:  1000  |  train loss: 2.8747288227
Processing class: 46
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 2.5042287350
Epoch:   200  |  train loss: 2.5376973629
Epoch:   300  |  train loss: 2.6167726994
Epoch:   400  |  train loss: 2.6493269444
Epoch:   500  |  train loss: 2.6659913540
Epoch:   600  |  train loss: 2.6695199490
Epoch:   700  |  train loss: 2.6879609108
Epoch:   800  |  train loss: 2.7760283947
Epoch:   900  |  train loss: 2.7425290108
Epoch:  1000  |  train loss: 2.7805356979
Processing class: 47
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 2.3216859341
Epoch:   200  |  train loss: 2.4133302212
Epoch:   300  |  train loss: 2.4222792625
Epoch:   400  |  train loss: 2.5149037361
Epoch:   500  |  train loss: 2.5330759525
Epoch:   600  |  train loss: 2.5527961731
Epoch:   700  |  train loss: 2.5964731216
Epoch:   800  |  train loss: 2.5727689266
Epoch:   900  |  train loss: 2.6076266289
Epoch:  1000  |  train loss: 2.5988523006
Processing class: 48
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 2.3769352436
Epoch:   200  |  train loss: 2.4764429092
Epoch:   300  |  train loss: 2.5091686726
Epoch:   400  |  train loss: 2.4692810059
Epoch:   500  |  train loss: 2.5602660656
Epoch:   600  |  train loss: 2.5582245827
Epoch:   700  |  train loss: 2.5327213287
Epoch:   800  |  train loss: 2.5345058441
Epoch:   900  |  train loss: 2.6282763481
Epoch:  1000  |  train loss: 2.6643975258
Processing class: 49
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 2.3533355236
Epoch:   200  |  train loss: 2.3961176872
Epoch:   300  |  train loss: 2.5424357414
Epoch:   400  |  train loss: 2.5804031849
Epoch:   500  |  train loss: 2.6323496342
Epoch:   600  |  train loss: 2.6448375702
Epoch:   700  |  train loss: 2.6591391563
Epoch:   800  |  train loss: 2.6525148392
Epoch:   900  |  train loss: 2.6495141029
Epoch:  1000  |  train loss: 2.6704676151
/home/z1165703/FeCAM/models/base.py:87: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  vec = torch.tensor(vec).cuda()
2024-03-06 04:59:24,846 [trainer.py] => CNN: {'total': 83.44, '00-09': 87.7, '10-19': 79.6, '20-29': 84.9, '30-39': 81.0, '40-49': 84.0, 'old': 0, 'new': 83.44}
2024-03-06 04:59:24,848 [trainer.py] => No NME accuracy
2024-03-06 04:59:24,848 [trainer.py] => FeCAM: {'total': 82.9, '00-09': 86.2, '10-19': 79.7, '20-29': 85.0, '30-39': 80.6, '40-49': 83.0, 'old': 0, 'new': 82.9}
2024-03-06 04:59:24,848 [trainer.py] => CNN top1 curve: [83.44]
2024-03-06 04:59:24,848 [trainer.py] => CNN top5 curve: [96.5]
2024-03-06 04:59:24,848 [trainer.py] => FeCAM top1 curve: [82.9]
2024-03-06 04:59:24,848 [trainer.py] => FeCAM top5 curve: [95.84]

2024-03-06 04:59:24,860 [fecam.py] => Learning on 50-60
Processing class: 50
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 2.2508856297
Epoch:   200  |  train loss: 2.3013081074
Epoch:   300  |  train loss: 2.3400725842
Epoch:   400  |  train loss: 2.4197114944
Epoch:   500  |  train loss: 2.4863812923
Epoch:   600  |  train loss: 2.4692365170
Epoch:   700  |  train loss: 2.5576677799
Epoch:   800  |  train loss: 2.6071494579
Epoch:   900  |  train loss: 2.6580970764
Epoch:  1000  |  train loss: 2.6886000156
Processing class: 51
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 2.2736588955
Epoch:   200  |  train loss: 2.2425772667
Epoch:   300  |  train loss: 2.3609940529
Epoch:   400  |  train loss: 2.4019027233
Epoch:   500  |  train loss: 2.4475864887
Epoch:   600  |  train loss: 2.5137024403
Epoch:   700  |  train loss: 2.4773241997
Epoch:   800  |  train loss: 2.5070278168
Epoch:   900  |  train loss: 2.5981651306
Epoch:  1000  |  train loss: 2.5887403488
Processing class: 52
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 2.2671912670
Epoch:   200  |  train loss: 2.2571868420
Epoch:   300  |  train loss: 2.3215563774
Epoch:   400  |  train loss: 2.3684441090
Epoch:   500  |  train loss: 2.3774424553
Epoch:   600  |  train loss: 2.3605344772
Epoch:   700  |  train loss: 2.4323562145
Epoch:   800  |  train loss: 2.4490133286
Epoch:   900  |  train loss: 2.4803682327
Epoch:  1000  |  train loss: 2.4484140396
Processing class: 53
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 2.2612946033
Epoch:   200  |  train loss: 2.3238245010
Epoch:   300  |  train loss: 2.2852266788
Epoch:   400  |  train loss: 2.3326978683
Epoch:   500  |  train loss: 2.3776575565
Epoch:   600  |  train loss: 2.4398429394
Epoch:   700  |  train loss: 2.5154600143
Epoch:   800  |  train loss: 2.4179459572
Epoch:   900  |  train loss: 2.4798624992
Epoch:  1000  |  train loss: 2.4559294224
Processing class: 54
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 2.1220069408
Epoch:   200  |  train loss: 2.1717948914
Epoch:   300  |  train loss: 2.2674777508
Epoch:   400  |  train loss: 2.3140740871
Epoch:   500  |  train loss: 2.3629008293
Epoch:   600  |  train loss: 2.4505312920
Epoch:   700  |  train loss: 2.4868402481
Epoch:   800  |  train loss: 2.5594562054
Epoch:   900  |  train loss: 2.6117704391
Epoch:  1000  |  train loss: 2.6118948936
Processing class: 55
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 2.2279774666
Epoch:   200  |  train loss: 2.2480913639
Epoch:   300  |  train loss: 2.2309158802
Epoch:   400  |  train loss: 2.3120740414
Epoch:   500  |  train loss: 2.4176283360
Epoch:   600  |  train loss: 2.4425187588
Epoch:   700  |  train loss: 2.4881584644
Epoch:   800  |  train loss: 2.5687582970
Epoch:   900  |  train loss: 2.5079308510
Epoch:  1000  |  train loss: 2.5698439121
Processing class: 56
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 2.2273197174
Epoch:   200  |  train loss: 2.2980195045
Epoch:   300  |  train loss: 2.2986284733
Epoch:   400  |  train loss: 2.4089294434
Epoch:   500  |  train loss: 2.4773872852
Epoch:   600  |  train loss: 2.5337309837
Epoch:   700  |  train loss: 2.5955469608
Epoch:   800  |  train loss: 2.5791522026
Epoch:   900  |  train loss: 2.6565742016
Epoch:  1000  |  train loss: 2.7071037769
Processing class: 57
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 2.1991171837
Epoch:   200  |  train loss: 2.2626421452
Epoch:   300  |  train loss: 2.2540958405
Epoch:   400  |  train loss: 2.2709385872
Epoch:   500  |  train loss: 2.3310751438
Epoch:   600  |  train loss: 2.2877095699
Epoch:   700  |  train loss: 2.3836569786
Epoch:   800  |  train loss: 2.3992326736
Epoch:   900  |  train loss: 2.4567162991
Epoch:  1000  |  train loss: 2.4044887543
Processing class: 58
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 2.2221852779
Epoch:   200  |  train loss: 2.2626538754
Epoch:   300  |  train loss: 2.3669798374
Epoch:   400  |  train loss: 2.4885041714
Epoch:   500  |  train loss: 2.4939584732
Epoch:   600  |  train loss: 2.5650369644
Epoch:   700  |  train loss: 2.5749428749
Epoch:   800  |  train loss: 2.6324967861
Epoch:   900  |  train loss: 2.6446778774
Epoch:  1000  |  train loss: 2.6668480396
Processing class: 59
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 2.2797564507
Epoch:   200  |  train loss: 2.3824912071
Epoch:   300  |  train loss: 2.3900193691
Epoch:   400  |  train loss: 2.4642344952
Epoch:   500  |  train loss: 2.5304368019
Epoch:   600  |  train loss: 2.5540577888
Epoch:   700  |  train loss: 2.6021401882
Epoch:   800  |  train loss: 2.6313862801
Epoch:   900  |  train loss: 2.6371719837
Epoch:  1000  |  train loss: 2.6225192070
2024-03-06 05:05:02,870 [trainer.py] => CNN: {'total': 71.75, '00-09': 82.2, '10-19': 72.8, '20-29': 78.5, '30-39': 74.9, '40-49': 68.4, '50-59': 53.7, 'old': 75.36, 'new': 53.7}
2024-03-06 05:05:02,870 [trainer.py] => No NME accuracy
2024-03-06 05:05:02,870 [trainer.py] => FeCAM: {'total': 72.78, '00-09': 82.6, '10-19': 74.0, '20-29': 81.0, '30-39': 76.4, '40-49': 76.8, '50-59': 45.9, 'old': 78.16, 'new': 45.9}
2024-03-06 05:05:02,870 [trainer.py] => CNN top1 curve: [83.44, 71.75]
2024-03-06 05:05:02,870 [trainer.py] => CNN top5 curve: [96.5, 89.6]
2024-03-06 05:05:02,870 [trainer.py] => FeCAM top1 curve: [82.9, 72.78]
2024-03-06 05:05:02,870 [trainer.py] => FeCAM top5 curve: [95.84, 91.18]

2024-03-06 05:05:02,876 [fecam.py] => Learning on 60-70
Processing class: 60
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 2.3192718506
Epoch:   200  |  train loss: 2.3261056900
Epoch:   300  |  train loss: 2.3169091702
Epoch:   400  |  train loss: 2.3310137272
Epoch:   500  |  train loss: 2.4407869816
Epoch:   600  |  train loss: 2.4662437439
Epoch:   700  |  train loss: 2.6687150478
Epoch:   800  |  train loss: 2.6008726120
Epoch:   900  |  train loss: 2.6347173691
Epoch:  1000  |  train loss: 2.6992019653
Processing class: 61
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 2.2279375076
Epoch:   200  |  train loss: 2.2199045658
Epoch:   300  |  train loss: 2.3447047234
Epoch:   400  |  train loss: 2.3977533340
Epoch:   500  |  train loss: 2.5183670044
Epoch:   600  |  train loss: 2.4824896812
Epoch:   700  |  train loss: 2.5946245193
Epoch:   800  |  train loss: 2.6652770996
Epoch:   900  |  train loss: 2.6064326763
Epoch:  1000  |  train loss: 2.6513010502
Processing class: 62
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 2.2723996162
Epoch:   200  |  train loss: 2.2485349655
Epoch:   300  |  train loss: 2.3130740166
Epoch:   400  |  train loss: 2.3467050552
Epoch:   500  |  train loss: 2.3249040604
Epoch:   600  |  train loss: 2.4177932262
Epoch:   700  |  train loss: 2.4511661530
Epoch:   800  |  train loss: 2.4855406761
Epoch:   900  |  train loss: 2.4802523613
Epoch:  1000  |  train loss: 2.5110477924
Processing class: 63
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 2.1826217175
Epoch:   200  |  train loss: 2.2175986767
Epoch:   300  |  train loss: 2.3710320950
Epoch:   400  |  train loss: 2.4176023960
Epoch:   500  |  train loss: 2.4919650555
Epoch:   600  |  train loss: 2.5828176022
Epoch:   700  |  train loss: 2.5150059700
Epoch:   800  |  train loss: 2.6628551483
Epoch:   900  |  train loss: 2.5950186729
Epoch:  1000  |  train loss: 2.6259502888
Processing class: 64
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 2.3412032604
Epoch:   200  |  train loss: 2.3618966103
Epoch:   300  |  train loss: 2.5376564980
Epoch:   400  |  train loss: 2.5753694057
Epoch:   500  |  train loss: 2.6308476448
Epoch:   600  |  train loss: 2.6530102730
Epoch:   700  |  train loss: 2.6160991192
Epoch:   800  |  train loss: 2.6643527508
Epoch:   900  |  train loss: 2.7457573414
Epoch:  1000  |  train loss: 2.7467260361
Processing class: 65
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 2.2686244965
Epoch:   200  |  train loss: 2.3171485901
Epoch:   300  |  train loss: 2.5070575237
Epoch:   400  |  train loss: 2.5968805313
Epoch:   500  |  train loss: 2.6087051392
Epoch:   600  |  train loss: 2.7498773098
Epoch:   700  |  train loss: 2.7737812519
Epoch:   800  |  train loss: 2.8980649471
Epoch:   900  |  train loss: 2.8542109489
Epoch:  1000  |  train loss: 2.9785090446
Processing class: 66
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 2.3086803436
Epoch:   200  |  train loss: 2.3854570866
Epoch:   300  |  train loss: 2.4765756607
Epoch:   400  |  train loss: 2.5091291428
Epoch:   500  |  train loss: 2.5897974014
Epoch:   600  |  train loss: 2.6164920330
Epoch:   700  |  train loss: 2.7045094967
Epoch:   800  |  train loss: 2.7232605457
Epoch:   900  |  train loss: 2.7388281822
Epoch:  1000  |  train loss: 2.7335891247
Processing class: 67
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 2.1990999222
Epoch:   200  |  train loss: 2.2485564232
Epoch:   300  |  train loss: 2.2712873459
Epoch:   400  |  train loss: 2.3076876163
Epoch:   500  |  train loss: 2.3311004162
Epoch:   600  |  train loss: 2.3398612499
Epoch:   700  |  train loss: 2.3676497936
Epoch:   800  |  train loss: 2.4257593155
Epoch:   900  |  train loss: 2.4285611153
Epoch:  1000  |  train loss: 2.4358855724
Processing class: 68
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 2.2063125610
Epoch:   200  |  train loss: 2.2239630699
Epoch:   300  |  train loss: 2.2611278057
Epoch:   400  |  train loss: 2.3128929615
Epoch:   500  |  train loss: 2.3373560905
Epoch:   600  |  train loss: 2.3476531982
Epoch:   700  |  train loss: 2.3928330898
Epoch:   800  |  train loss: 2.3818372250
Epoch:   900  |  train loss: 2.4225070000
Epoch:  1000  |  train loss: 2.5182650089
Processing class: 69
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 2.2611230850
Epoch:   200  |  train loss: 2.3605069637
Epoch:   300  |  train loss: 2.5353404999
Epoch:   400  |  train loss: 2.6249119282
Epoch:   500  |  train loss: 2.6567703247
Epoch:   600  |  train loss: 2.5888237476
Epoch:   700  |  train loss: 2.6778399944
Epoch:   800  |  train loss: 2.6834197044
Epoch:   900  |  train loss: 2.7902797699
Epoch:  1000  |  train loss: 2.7013141632
2024-03-06 05:11:36,983 [trainer.py] => CNN: {'total': 64.66, '00-09': 75.0, '10-19': 70.1, '20-29': 77.1, '30-39': 70.6, '40-49': 64.0, '50-59': 45.8, '60-69': 50.0, 'old': 67.1, 'new': 50.0}
2024-03-06 05:11:36,984 [trainer.py] => No NME accuracy
2024-03-06 05:11:36,984 [trainer.py] => FeCAM: {'total': 67.27, '00-09': 79.9, '10-19': 70.6, '20-29': 79.1, '30-39': 71.9, '40-49': 74.1, '50-59': 41.2, '60-69': 54.1, 'old': 69.47, 'new': 54.1}
2024-03-06 05:11:36,984 [trainer.py] => CNN top1 curve: [83.44, 71.75, 64.66]
2024-03-06 05:11:36,984 [trainer.py] => CNN top5 curve: [96.5, 89.6, 86.54]
2024-03-06 05:11:36,984 [trainer.py] => FeCAM top1 curve: [82.9, 72.78, 67.27]
2024-03-06 05:11:36,984 [trainer.py] => FeCAM top5 curve: [95.84, 91.18, 87.21]

2024-03-06 05:11:36,989 [fecam.py] => Learning on 70-80
Processing class: 70
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 2.2166535378
Epoch:   200  |  train loss: 2.2126120567
Epoch:   300  |  train loss: 2.2877434254
Epoch:   400  |  train loss: 2.3425482273
Epoch:   500  |  train loss: 2.3514904499
Epoch:   600  |  train loss: 2.4286336899
Epoch:   700  |  train loss: 2.5302165985
Epoch:   800  |  train loss: 2.4764413834
Epoch:   900  |  train loss: 2.5492971897
Epoch:  1000  |  train loss: 2.5413080215
Processing class: 71
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 2.2467333317
Epoch:   200  |  train loss: 2.2881609917
Epoch:   300  |  train loss: 2.2753438473
Epoch:   400  |  train loss: 2.3547201157
Epoch:   500  |  train loss: 2.4179832935
Epoch:   600  |  train loss: 2.4482803822
Epoch:   700  |  train loss: 2.4768747330
Epoch:   800  |  train loss: 2.5132251263
Epoch:   900  |  train loss: 2.5975439072
Epoch:  1000  |  train loss: 2.6010873318
Processing class: 72
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 2.2083581924
Epoch:   200  |  train loss: 2.2530181885
Epoch:   300  |  train loss: 2.2911069393
Epoch:   400  |  train loss: 2.3174108028
Epoch:   500  |  train loss: 2.3827096939
Epoch:   600  |  train loss: 2.3585965633
Epoch:   700  |  train loss: 2.3979523182
Epoch:   800  |  train loss: 2.3959116459
Epoch:   900  |  train loss: 2.4322174072
Epoch:  1000  |  train loss: 2.5058661461
Processing class: 73
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 2.2767260551
Epoch:   200  |  train loss: 2.2921074390
Epoch:   300  |  train loss: 2.4130211353
Epoch:   400  |  train loss: 2.4043410778
Epoch:   500  |  train loss: 2.4914120197
Epoch:   600  |  train loss: 2.5001624584
Epoch:   700  |  train loss: 2.4781286716
Epoch:   800  |  train loss: 2.4869991779
Epoch:   900  |  train loss: 2.5111185074
Epoch:  1000  |  train loss: 2.5605978489
Processing class: 74
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 2.2954350948
Epoch:   200  |  train loss: 2.3376497269
Epoch:   300  |  train loss: 2.2972033978
Epoch:   400  |  train loss: 2.2932993412
Epoch:   500  |  train loss: 2.3500586033
Epoch:   600  |  train loss: 2.3479117393
Epoch:   700  |  train loss: 2.4437944889
Epoch:   800  |  train loss: 2.3866753578
Epoch:   900  |  train loss: 2.4098136902
Epoch:  1000  |  train loss: 2.4943674564
Processing class: 75
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 2.2199605942
Epoch:   200  |  train loss: 2.2287269115
Epoch:   300  |  train loss: 2.3309216022
Epoch:   400  |  train loss: 2.3482604980
Epoch:   500  |  train loss: 2.4211550236
Epoch:   600  |  train loss: 2.4918966293
Epoch:   700  |  train loss: 2.3778213024
Epoch:   800  |  train loss: 2.4739690781
Epoch:   900  |  train loss: 2.4809312820
Epoch:  1000  |  train loss: 2.5560890675
Processing class: 76
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 2.1925269127
Epoch:   200  |  train loss: 2.3072317600
Epoch:   300  |  train loss: 2.4564133167
Epoch:   400  |  train loss: 2.5586158752
Epoch:   500  |  train loss: 2.6032483578
Epoch:   600  |  train loss: 2.6787648678
Epoch:   700  |  train loss: 2.6980161667
Epoch:   800  |  train loss: 2.6414233208
Epoch:   900  |  train loss: 2.7413980961
Epoch:  1000  |  train loss: 2.7891136169
Processing class: 77
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 2.2541005611
Epoch:   200  |  train loss: 2.2945281029
Epoch:   300  |  train loss: 2.2984028339
Epoch:   400  |  train loss: 2.3604252815
Epoch:   500  |  train loss: 2.4210443020
Epoch:   600  |  train loss: 2.4598566532
Epoch:   700  |  train loss: 2.5054635525
Epoch:   800  |  train loss: 2.5880827904
Epoch:   900  |  train loss: 2.5756440163
Epoch:  1000  |  train loss: 2.5873581886
Processing class: 78
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 2.2837671757
Epoch:   200  |  train loss: 2.3859610558
Epoch:   300  |  train loss: 2.4748821259
Epoch:   400  |  train loss: 2.5022545338
Epoch:   500  |  train loss: 2.5233157635
Epoch:   600  |  train loss: 2.5953402996
Epoch:   700  |  train loss: 2.6353994846
Epoch:   800  |  train loss: 2.6757929325
Epoch:   900  |  train loss: 2.6988808632
Epoch:  1000  |  train loss: 2.6927622795
Processing class: 79
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 2.2464816093
Epoch:   200  |  train loss: 2.2197137356
Epoch:   300  |  train loss: 2.2956460953
Epoch:   400  |  train loss: 2.3264194012
Epoch:   500  |  train loss: 2.3390499592
Epoch:   600  |  train loss: 2.4273554802
Epoch:   700  |  train loss: 2.4396534920
Epoch:   800  |  train loss: 2.4894096851
Epoch:   900  |  train loss: 2.5579134464
Epoch:  1000  |  train loss: 2.5310003281
2024-03-06 05:19:13,271 [trainer.py] => CNN: {'total': 59.18, '00-09': 73.6, '10-19': 68.4, '20-29': 76.9, '30-39': 69.1, '40-49': 60.6, '50-59': 37.2, '60-69': 45.7, '70-79': 41.9, 'old': 61.64, 'new': 41.9}
2024-03-06 05:19:13,272 [trainer.py] => No NME accuracy
2024-03-06 05:19:13,272 [trainer.py] => FeCAM: {'total': 61.72, '00-09': 78.1, '10-19': 69.3, '20-29': 78.0, '30-39': 69.5, '40-49': 72.1, '50-59': 35.7, '60-69': 49.1, '70-79': 42.0, 'old': 64.54, 'new': 42.0}
2024-03-06 05:19:13,272 [trainer.py] => CNN top1 curve: [83.44, 71.75, 64.66, 59.18]
2024-03-06 05:19:13,272 [trainer.py] => CNN top5 curve: [96.5, 89.6, 86.54, 84.09]
2024-03-06 05:19:13,272 [trainer.py] => FeCAM top1 curve: [82.9, 72.78, 67.27, 61.72]
2024-03-06 05:19:13,272 [trainer.py] => FeCAM top5 curve: [95.84, 91.18, 87.21, 84.09]

2024-03-06 05:19:13,283 [fecam.py] => Learning on 80-90
Processing class: 80
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 2.2474256992
Epoch:   200  |  train loss: 2.4775101185
Epoch:   300  |  train loss: 2.5792499065
Epoch:   400  |  train loss: 2.6392307758
Epoch:   500  |  train loss: 2.6638990402
Epoch:   600  |  train loss: 2.6826746941
Epoch:   700  |  train loss: 2.6447124004
Epoch:   800  |  train loss: 2.7544288158
Epoch:   900  |  train loss: 2.7438127041
Epoch:  1000  |  train loss: 2.8122018337
Processing class: 81
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 2.2308380127
Epoch:   200  |  train loss: 2.3594290257
Epoch:   300  |  train loss: 2.3482252598
Epoch:   400  |  train loss: 2.4687435627
Epoch:   500  |  train loss: 2.4749120712
Epoch:   600  |  train loss: 2.5567629814
Epoch:   700  |  train loss: 2.5725995541
Epoch:   800  |  train loss: 2.6259994984
Epoch:   900  |  train loss: 2.6561985016
Epoch:  1000  |  train loss: 2.6994939804
Processing class: 82
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 2.2226617813
Epoch:   200  |  train loss: 2.3228981495
Epoch:   300  |  train loss: 2.3711745739
Epoch:   400  |  train loss: 2.3983513355
Epoch:   500  |  train loss: 2.4566107273
Epoch:   600  |  train loss: 2.4620152950
Epoch:   700  |  train loss: 2.4880629539
Epoch:   800  |  train loss: 2.5124018192
Epoch:   900  |  train loss: 2.5446084976
Epoch:  1000  |  train loss: 2.5626412868
Processing class: 83
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 2.2783097744
Epoch:   200  |  train loss: 2.4534574509
Epoch:   300  |  train loss: 2.5092301369
Epoch:   400  |  train loss: 2.5898508549
Epoch:   500  |  train loss: 2.7031180859
Epoch:   600  |  train loss: 2.6833950520
Epoch:   700  |  train loss: 2.7802907944
Epoch:   800  |  train loss: 2.8828680515
Epoch:   900  |  train loss: 2.8630866528
Epoch:  1000  |  train loss: 2.8594996452
Processing class: 84
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 2.2796989918
Epoch:   200  |  train loss: 2.3292992592
Epoch:   300  |  train loss: 2.4362545013
Epoch:   400  |  train loss: 2.4800004482
Epoch:   500  |  train loss: 2.5647577286
Epoch:   600  |  train loss: 2.5326085091
Epoch:   700  |  train loss: 2.6122310162
Epoch:   800  |  train loss: 2.6799744129
Epoch:   900  |  train loss: 2.6314821720
Epoch:  1000  |  train loss: 2.6320200443
Processing class: 85
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 2.2444032192
Epoch:   200  |  train loss: 2.2820869446
Epoch:   300  |  train loss: 2.3873219490
Epoch:   400  |  train loss: 2.3979881287
Epoch:   500  |  train loss: 2.4687514782
Epoch:   600  |  train loss: 2.5459666252
Epoch:   700  |  train loss: 2.6383703709
Epoch:   800  |  train loss: 2.5944616318
Epoch:   900  |  train loss: 2.6164027691
Epoch:  1000  |  train loss: 2.6966355324
Processing class: 86
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 2.2899758816
Epoch:   200  |  train loss: 2.2593697071
Epoch:   300  |  train loss: 2.3688673973
Epoch:   400  |  train loss: 2.3788917542
Epoch:   500  |  train loss: 2.3981516361
Epoch:   600  |  train loss: 2.4571292877
Epoch:   700  |  train loss: 2.4787506580
Epoch:   800  |  train loss: 2.4918179035
Epoch:   900  |  train loss: 2.5061710358
Epoch:  1000  |  train loss: 2.6018465042
Processing class: 87
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 2.2170438766
Epoch:   200  |  train loss: 2.3577496529
Epoch:   300  |  train loss: 2.4008817196
Epoch:   400  |  train loss: 2.4889889240
Epoch:   500  |  train loss: 2.5126796246
Epoch:   600  |  train loss: 2.5643094540
Epoch:   700  |  train loss: 2.6059489250
Epoch:   800  |  train loss: 2.6245624542
Epoch:   900  |  train loss: 2.6080305576
Epoch:  1000  |  train loss: 2.7205122471
Processing class: 88
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 2.2315385818
Epoch:   200  |  train loss: 2.2391393185
Epoch:   300  |  train loss: 2.2815283775
Epoch:   400  |  train loss: 2.3327407837
Epoch:   500  |  train loss: 2.3631564617
Epoch:   600  |  train loss: 2.3796059132
Epoch:   700  |  train loss: 2.4554328918
Epoch:   800  |  train loss: 2.4875991821
Epoch:   900  |  train loss: 2.5215253353
Epoch:  1000  |  train loss: 2.5029847145
Processing class: 89
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 2.2716454029
Epoch:   200  |  train loss: 2.3839768410
Epoch:   300  |  train loss: 2.4708098888
Epoch:   400  |  train loss: 2.5158950329
Epoch:   500  |  train loss: 2.6031695843
Epoch:   600  |  train loss: 2.6472807407
Epoch:   700  |  train loss: 2.6100858688
Epoch:   800  |  train loss: 2.7500878811
Epoch:   900  |  train loss: 2.7407341003
Epoch:  1000  |  train loss: 2.7945245266
2024-03-06 05:28:07,479 [trainer.py] => CNN: {'total': 54.08, '00-09': 67.9, '10-19': 63.6, '20-29': 73.2, '30-39': 68.8, '40-49': 57.2, '50-59': 32.6, '60-69': 40.9, '70-79': 38.1, '80-89': 44.4, 'old': 55.29, 'new': 44.4}
2024-03-06 05:28:07,481 [trainer.py] => No NME accuracy
2024-03-06 05:28:07,481 [trainer.py] => FeCAM: {'total': 57.66, '00-09': 76.1, '10-19': 65.8, '20-29': 75.3, '30-39': 67.9, '40-49': 68.5, '50-59': 33.0, '60-69': 45.0, '70-79': 39.0, '80-89': 48.3, 'old': 58.82, 'new': 48.3}
2024-03-06 05:28:07,481 [trainer.py] => CNN top1 curve: [83.44, 71.75, 64.66, 59.18, 54.08]
2024-03-06 05:28:07,481 [trainer.py] => CNN top5 curve: [96.5, 89.6, 86.54, 84.09, 81.74]
2024-03-06 05:28:07,481 [trainer.py] => FeCAM top1 curve: [82.9, 72.78, 67.27, 61.72, 57.66]
2024-03-06 05:28:07,481 [trainer.py] => FeCAM top5 curve: [95.84, 91.18, 87.21, 84.09, 81.63]

2024-03-06 05:28:07,492 [fecam.py] => Learning on 90-100
Processing class: 90
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 2.2663114548
Epoch:   200  |  train loss: 2.2997615814
Epoch:   300  |  train loss: 2.2854839802
Epoch:   400  |  train loss: 2.3124016285
Epoch:   500  |  train loss: 2.3246250629
Epoch:   600  |  train loss: 2.3876002789
Epoch:   700  |  train loss: 2.4078359604
Epoch:   800  |  train loss: 2.4486726761
Epoch:   900  |  train loss: 2.4959303856
Epoch:  1000  |  train loss: 2.5763031483
Processing class: 91
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 2.2492284298
Epoch:   200  |  train loss: 2.2894826889
Epoch:   300  |  train loss: 2.3917603493
Epoch:   400  |  train loss: 2.4654991150
Epoch:   500  |  train loss: 2.5564345837
Epoch:   600  |  train loss: 2.6074471474
Epoch:   700  |  train loss: 2.6593367100
Epoch:   800  |  train loss: 2.7219334602
Epoch:   900  |  train loss: 2.7905035973
Epoch:  1000  |  train loss: 2.8142930031
Processing class: 92
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 2.3251718044
Epoch:   200  |  train loss: 2.3416240215
Epoch:   300  |  train loss: 2.2999411106
Epoch:   400  |  train loss: 2.3688502789
Epoch:   500  |  train loss: 2.4374076366
Epoch:   600  |  train loss: 2.5451998234
Epoch:   700  |  train loss: 2.5731472969
Epoch:   800  |  train loss: 2.5234673023
Epoch:   900  |  train loss: 2.5630411148
Epoch:  1000  |  train loss: 2.5958594322
Processing class: 93
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 2.2447543144
Epoch:   200  |  train loss: 2.3468882084
Epoch:   300  |  train loss: 2.5329586506
Epoch:   400  |  train loss: 2.5817996502
Epoch:   500  |  train loss: 2.6086676598
Epoch:   600  |  train loss: 2.6023325443
Epoch:   700  |  train loss: 2.7419994831
Epoch:   800  |  train loss: 2.7595015049
Epoch:   900  |  train loss: 2.7929622173
Epoch:  1000  |  train loss: 2.8004916191
Processing class: 94
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 2.0923444748
Epoch:   200  |  train loss: 2.1226551056
Epoch:   300  |  train loss: 2.1103285551
Epoch:   400  |  train loss: 2.1894171715
Epoch:   500  |  train loss: 2.1200903893
Epoch:   600  |  train loss: 2.1956028461
Epoch:   700  |  train loss: 2.2665259361
Epoch:   800  |  train loss: 2.1695735931
Epoch:   900  |  train loss: 2.2663277149
Epoch:  1000  |  train loss: 2.2560840607
Processing class: 95
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 2.2122894764
Epoch:   200  |  train loss: 2.2887173176
Epoch:   300  |  train loss: 2.2779049873
Epoch:   400  |  train loss: 2.2888024807
Epoch:   500  |  train loss: 2.4060163021
Epoch:   600  |  train loss: 2.4200168133
Epoch:   700  |  train loss: 2.4759082794
Epoch:   800  |  train loss: 2.4690742493
Epoch:   900  |  train loss: 2.5429086685
Epoch:  1000  |  train loss: 2.6158667564
Processing class: 96
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 2.2822230339
Epoch:   200  |  train loss: 2.2334810257
Epoch:   300  |  train loss: 2.3311586380
Epoch:   400  |  train loss: 2.4562670231
Epoch:   500  |  train loss: 2.4541834831
Epoch:   600  |  train loss: 2.4636109829
Epoch:   700  |  train loss: 2.5035889149
Epoch:   800  |  train loss: 2.4876676083
Epoch:   900  |  train loss: 2.6127855301
Epoch:  1000  |  train loss: 2.6321755886
Processing class: 97
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 2.2819563389
Epoch:   200  |  train loss: 2.3534153938
Epoch:   300  |  train loss: 2.4248701572
Epoch:   400  |  train loss: 2.4789293766
Epoch:   500  |  train loss: 2.5774991035
Epoch:   600  |  train loss: 2.6460691929
Epoch:   700  |  train loss: 2.7101804733
Epoch:   800  |  train loss: 2.7529324055
Epoch:   900  |  train loss: 2.7968015194
Epoch:  1000  |  train loss: 2.8038507462
Processing class: 98
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 2.2390023708
Epoch:   200  |  train loss: 2.2826334000
Epoch:   300  |  train loss: 2.3595631123
Epoch:   400  |  train loss: 2.4346367836
Epoch:   500  |  train loss: 2.4656674862
Epoch:   600  |  train loss: 2.5337362766
Epoch:   700  |  train loss: 2.4960771084
Epoch:   800  |  train loss: 2.5745101452
Epoch:   900  |  train loss: 2.5924177647
Epoch:  1000  |  train loss: 2.5774447441
Processing class: 99
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 2.3236958981
Epoch:   200  |  train loss: 2.3059237480
Epoch:   300  |  train loss: 2.4672845840
Epoch:   400  |  train loss: 2.5774496078
Epoch:   500  |  train loss: 2.6093050480
Epoch:   600  |  train loss: 2.6321052551
Epoch:   700  |  train loss: 2.6323987007
Epoch:   800  |  train loss: 2.7170108318
Epoch:   900  |  train loss: 2.7170041561
Epoch:  1000  |  train loss: 2.7445470333
2024-03-06 05:38:23,188 [trainer.py] => CNN: {'total': 50.36, '00-09': 57.7, '10-19': 63.6, '20-29': 71.4, '30-39': 68.5, '40-49': 56.8, '50-59': 29.6, '60-69': 38.9, '70-79': 36.6, '80-89': 42.7, '90-99': 37.8, 'old': 51.76, 'new': 37.8}
2024-03-06 05:38:23,188 [trainer.py] => No NME accuracy
2024-03-06 05:38:23,188 [trainer.py] => FeCAM: {'total': 54.74, '00-09': 73.3, '10-19': 65.1, '20-29': 74.1, '30-39': 67.2, '40-49': 67.1, '50-59': 30.8, '60-69': 43.3, '70-79': 37.2, '80-89': 46.9, '90-99': 42.4, 'old': 56.11, 'new': 42.4}
2024-03-06 05:38:23,189 [trainer.py] => CNN top1 curve: [83.44, 71.75, 64.66, 59.18, 54.08, 50.36]
2024-03-06 05:38:23,189 [trainer.py] => CNN top5 curve: [96.5, 89.6, 86.54, 84.09, 81.74, 79.63]
2024-03-06 05:38:23,189 [trainer.py] => FeCAM top1 curve: [82.9, 72.78, 67.27, 61.72, 57.66, 54.74]
2024-03-06 05:38:23,189 [trainer.py] => FeCAM top5 curve: [95.84, 91.18, 87.21, 84.09, 81.63, 79.21]

Running FeCAM with params C=0.1, sigma=1
Running FeCAM with params C=0.1, sigma=5
Running FeCAM with params C=0.1, sigma=10
Running FeCAM with params C=0.1, sigma=20
Running FeCAM with params C=0.1, sigma=30
Running FeCAM with params C=0.1, sigma=40
Running FeCAM with params C=0.1, sigma=50
Running FeCAM with params C=0.5, sigma=1
Running FeCAM with params C=0.5, sigma=5
Running FeCAM with params C=0.5, sigma=10
Running FeCAM with params C=0.5, sigma=20
Running FeCAM with params C=0.5, sigma=30
Running FeCAM with params C=0.5, sigma=40
Running FeCAM with params C=0.5, sigma=50
Running FeCAM with params C=1, sigma=1
Running FeCAM with params C=1, sigma=5
Running FeCAM with params C=1, sigma=10
Running FeCAM with params C=1, sigma=20
Running FeCAM with params C=1, sigma=30
Running FeCAM with params C=1, sigma=40
Running FeCAM with params C=1, sigma=50
Running FeCAM with params C=5, sigma=1
Running FeCAM with params C=5, sigma=5
Running FeCAM with params C=5, sigma=10
Running FeCAM with params C=5, sigma=20
Running FeCAM with params C=5, sigma=30
Running FeCAM with params C=5, sigma=40
Running FeCAM with params C=5, sigma=50
Running FeCAM with params C=10, sigma=1
Running FeCAM with params C=10, sigma=5
Running FeCAM with params C=10, sigma=10
Running FeCAM with params C=10, sigma=20
Running FeCAM with params C=10, sigma=30
Running FeCAM with params C=10, sigma=40
Running FeCAM with params C=10, sigma=50
