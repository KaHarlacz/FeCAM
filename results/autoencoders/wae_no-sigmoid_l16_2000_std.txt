=========================================
2024-03-11 01:38:30,994 [trainer.py] => config: ./exps/FeCAM_cifar100.json
2024-03-11 01:38:30,994 [trainer.py] => prefix: train
2024-03-11 01:38:30,994 [trainer.py] => dataset: cifar100
2024-03-11 01:38:30,994 [trainer.py] => memory_size: 0
2024-03-11 01:38:30,994 [trainer.py] => shuffle: True
2024-03-11 01:38:30,994 [trainer.py] => init_cls: 50
2024-03-11 01:38:30,994 [trainer.py] => increment: 10
2024-03-11 01:38:30,994 [trainer.py] => model_name: fecam
2024-03-11 01:38:30,994 [trainer.py] => convnet_type: resnet18
2024-03-11 01:38:30,994 [trainer.py] => device: [device(type='cuda', index=0)]
2024-03-11 01:38:30,994 [trainer.py] => seed: 1993
2024-03-11 01:38:30,994 [trainer.py] => init_epochs: 200
2024-03-11 01:38:30,994 [trainer.py] => init_lr: 0.1
2024-03-11 01:38:30,994 [trainer.py] => init_weight_decay: 0.0005
2024-03-11 01:38:30,995 [trainer.py] => batch_size: 128
2024-03-11 01:38:30,995 [trainer.py] => num_workers: 8
2024-03-11 01:38:30,995 [trainer.py] => T: 5
2024-03-11 01:38:30,995 [trainer.py] => beta: 0.5
2024-03-11 01:38:30,995 [trainer.py] => alpha1: 1
2024-03-11 01:38:30,995 [trainer.py] => alpha2: 1
2024-03-11 01:38:30,995 [trainer.py] => ncm: False
2024-03-11 01:38:30,995 [trainer.py] => tukey: False
2024-03-11 01:38:30,995 [trainer.py] => diagonal: False
2024-03-11 01:38:30,995 [trainer.py] => per_class: True
2024-03-11 01:38:30,995 [trainer.py] => full_cov: True
2024-03-11 01:38:30,995 [trainer.py] => shrink: True
2024-03-11 01:38:30,995 [trainer.py] => norm_cov: False
2024-03-11 01:38:30,995 [trainer.py] => vecnorm: False
2024-03-11 01:38:30,995 [trainer.py] => ae_type: wae
2024-03-11 01:38:30,995 [trainer.py] => ae_standarization: True
2024-03-11 01:38:30,995 [trainer.py] => epochs: 2000
2024-03-11 01:38:30,995 [trainer.py] => ae_latent_dim: 16
2024-03-11 01:38:30,995 [trainer.py] => wae_sigma: 10
2024-03-11 01:38:30,995 [trainer.py] => wae_C: 0.1
Files already downloaded and verified
Files already downloaded and verified
2024-03-11 01:38:35,486 [data_manager.py] => [68, 56, 78, 8, 23, 84, 90, 65, 74, 76, 40, 89, 3, 92, 55, 9, 26, 80, 43, 38, 58, 70, 77, 1, 85, 19, 17, 50, 28, 53, 13, 81, 45, 82, 6, 59, 83, 16, 15, 44, 91, 41, 72, 60, 79, 52, 20, 10, 31, 54, 37, 95, 14, 71, 96, 98, 97, 2, 64, 66, 42, 22, 35, 86, 24, 34, 87, 21, 99, 0, 88, 27, 18, 94, 11, 12, 47, 25, 30, 46, 62, 69, 36, 61, 7, 63, 75, 5, 32, 4, 51, 48, 73, 93, 39, 67, 29, 49, 57, 33]
2024-03-11 01:38:35,752 [fecam.py] => Learning on 0-50
/home/z1165703/miniconda3/lib/python3.11/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
/home/z1165703/miniconda3/lib/python3.11/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
Processing class: 0
Standarizing class data
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.7741626978
Epoch:   200  |  train loss: 0.6552103877
Epoch:   300  |  train loss: 0.5810826182
Epoch:   400  |  train loss: 0.5287635803
Epoch:   500  |  train loss: 0.4878788412
Epoch:   600  |  train loss: 0.4521908700
Epoch:   700  |  train loss: 0.4221684694
Epoch:   800  |  train loss: 0.3982922435
Epoch:   900  |  train loss: 0.3781361222
Epoch:  1000  |  train loss: 0.3610966384
Epoch:  1100  |  train loss: 0.3458035946
Epoch:  1200  |  train loss: 0.3325443745
Epoch:  1300  |  train loss: 0.3213835716
Epoch:  1400  |  train loss: 0.3104466259
Epoch:  1500  |  train loss: 0.3011105180
Epoch:  1600  |  train loss: 0.2922830284
Epoch:  1700  |  train loss: 0.2843125224
Epoch:  1800  |  train loss: 0.2769998252
Epoch:  1900  |  train loss: 0.2707564712
Epoch:  2000  |  train loss: 0.2641366720
Processing class: 1
Standarizing class data
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.7786803603
Epoch:   200  |  train loss: 0.6466348410
Epoch:   300  |  train loss: 0.5699725509
Epoch:   400  |  train loss: 0.5159420013
Epoch:   500  |  train loss: 0.4757806599
Epoch:   600  |  train loss: 0.4419165909
Epoch:   700  |  train loss: 0.4154578805
Epoch:   800  |  train loss: 0.3937388599
Epoch:   900  |  train loss: 0.3752873719
Epoch:  1000  |  train loss: 0.3584257662
Epoch:  1100  |  train loss: 0.3435313463
Epoch:  1200  |  train loss: 0.3306031883
Epoch:  1300  |  train loss: 0.3187827945
Epoch:  1400  |  train loss: 0.3086449206
Epoch:  1500  |  train loss: 0.2988763571
Epoch:  1600  |  train loss: 0.2906284332
Epoch:  1700  |  train loss: 0.2830005050
Epoch:  1800  |  train loss: 0.2754275322
Epoch:  1900  |  train loss: 0.2684924483
Epoch:  2000  |  train loss: 0.2623171985
Processing class: 2
Standarizing class data
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.7883132577
Epoch:   200  |  train loss: 0.6688160539
Epoch:   300  |  train loss: 0.5890897632
Epoch:   400  |  train loss: 0.5340029716
Epoch:   500  |  train loss: 0.4917046964
Epoch:   600  |  train loss: 0.4579787791
Epoch:   700  |  train loss: 0.4291139901
Epoch:   800  |  train loss: 0.4054504812
Epoch:   900  |  train loss: 0.3843829274
Epoch:  1000  |  train loss: 0.3663117528
Epoch:  1100  |  train loss: 0.3511272907
Epoch:  1200  |  train loss: 0.3365839839
Epoch:  1300  |  train loss: 0.3248918772
Epoch:  1400  |  train loss: 0.3139262795
Epoch:  1500  |  train loss: 0.3044166386
Epoch:  1600  |  train loss: 0.2951940715
Epoch:  1700  |  train loss: 0.2869838715
Epoch:  1800  |  train loss: 0.2790572882
Epoch:  1900  |  train loss: 0.2723025501
Epoch:  2000  |  train loss: 0.2653655827
Processing class: 3
Standarizing class data
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.7706442833
Epoch:   200  |  train loss: 0.6283045530
Epoch:   300  |  train loss: 0.5444749355
Epoch:   400  |  train loss: 0.4886999249
Epoch:   500  |  train loss: 0.4485389531
Epoch:   600  |  train loss: 0.4164114356
Epoch:   700  |  train loss: 0.3903775811
Epoch:   800  |  train loss: 0.3678764880
Epoch:   900  |  train loss: 0.3498914778
Epoch:  1000  |  train loss: 0.3334782362
Epoch:  1100  |  train loss: 0.3191002429
Epoch:  1200  |  train loss: 0.3065325856
Epoch:  1300  |  train loss: 0.2960423291
Epoch:  1400  |  train loss: 0.2860527754
Epoch:  1500  |  train loss: 0.2768548846
Epoch:  1600  |  train loss: 0.2684311271
Epoch:  1700  |  train loss: 0.2607111454
Epoch:  1800  |  train loss: 0.2537716568
Epoch:  1900  |  train loss: 0.2475199282
Epoch:  2000  |  train loss: 0.2413316905
Processing class: 4
Standarizing class data
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.7679042697
Epoch:   200  |  train loss: 0.6286048770
Epoch:   300  |  train loss: 0.5545275092
Epoch:   400  |  train loss: 0.5021872282
Epoch:   500  |  train loss: 0.4608901441
Epoch:   600  |  train loss: 0.4288161278
Epoch:   700  |  train loss: 0.4020203888
Epoch:   800  |  train loss: 0.3801724195
Epoch:   900  |  train loss: 0.3614399552
Epoch:  1000  |  train loss: 0.3452156186
Epoch:  1100  |  train loss: 0.3313042939
Epoch:  1200  |  train loss: 0.3183788419
Epoch:  1300  |  train loss: 0.3059425414
Epoch:  1400  |  train loss: 0.2958754003
Epoch:  1500  |  train loss: 0.2865391374
Epoch:  1600  |  train loss: 0.2784781992
Epoch:  1700  |  train loss: 0.2703308642
Epoch:  1800  |  train loss: 0.2635526150
Epoch:  1900  |  train loss: 0.2566748559
Epoch:  2000  |  train loss: 0.2508170307
Processing class: 5
Standarizing class data
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.7711831093
Epoch:   200  |  train loss: 0.6367661119
Epoch:   300  |  train loss: 0.5619873643
Epoch:   400  |  train loss: 0.5115275323
Epoch:   500  |  train loss: 0.4733984351
Epoch:   600  |  train loss: 0.4410392344
Epoch:   700  |  train loss: 0.4140950680
Epoch:   800  |  train loss: 0.3931930304
Epoch:   900  |  train loss: 0.3749803066
Epoch:  1000  |  train loss: 0.3589696586
Epoch:  1100  |  train loss: 0.3456651688
Epoch:  1200  |  train loss: 0.3324901164
Epoch:  1300  |  train loss: 0.3219808817
Epoch:  1400  |  train loss: 0.3117186248
Epoch:  1500  |  train loss: 0.3025314867
Epoch:  1600  |  train loss: 0.2942677617
Epoch:  1700  |  train loss: 0.2860372543
Epoch:  1800  |  train loss: 0.2789354324
Epoch:  1900  |  train loss: 0.2724335909
Epoch:  2000  |  train loss: 0.2669095397
Processing class: 6
Standarizing class data
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.7839205503
Epoch:   200  |  train loss: 0.6580879211
Epoch:   300  |  train loss: 0.5853382349
Epoch:   400  |  train loss: 0.5365770578
Epoch:   500  |  train loss: 0.4984359741
Epoch:   600  |  train loss: 0.4669981480
Epoch:   700  |  train loss: 0.4408123493
Epoch:   800  |  train loss: 0.4190270066
Epoch:   900  |  train loss: 0.4000589430
Epoch:  1000  |  train loss: 0.3833675325
Epoch:  1100  |  train loss: 0.3701728165
Epoch:  1200  |  train loss: 0.3564527094
Epoch:  1300  |  train loss: 0.3450186670
Epoch:  1400  |  train loss: 0.3343379796
Epoch:  1500  |  train loss: 0.3253486633
Epoch:  1600  |  train loss: 0.3164674103
Epoch:  1700  |  train loss: 0.3092494011
Epoch:  1800  |  train loss: 0.3013321042
Epoch:  1900  |  train loss: 0.2946357548
Epoch:  2000  |  train loss: 0.2882768452
Processing class: 7
Standarizing class data
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.7422381520
Epoch:   200  |  train loss: 0.6211491704
Epoch:   300  |  train loss: 0.5527935505
Epoch:   400  |  train loss: 0.5046948135
Epoch:   500  |  train loss: 0.4672532320
Epoch:   600  |  train loss: 0.4369295239
Epoch:   700  |  train loss: 0.4115668952
Epoch:   800  |  train loss: 0.3900137961
Epoch:   900  |  train loss: 0.3732661307
Epoch:  1000  |  train loss: 0.3583372056
Epoch:  1100  |  train loss: 0.3450314224
Epoch:  1200  |  train loss: 0.3334491789
Epoch:  1300  |  train loss: 0.3232663333
Epoch:  1400  |  train loss: 0.3138561606
Epoch:  1500  |  train loss: 0.3043781161
Epoch:  1600  |  train loss: 0.2965445817
Epoch:  1700  |  train loss: 0.2894505024
Epoch:  1800  |  train loss: 0.2826541424
Epoch:  1900  |  train loss: 0.2760642886
Epoch:  2000  |  train loss: 0.2702692389
Processing class: 8
Standarizing class data
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.7557657599
Epoch:   200  |  train loss: 0.6276026964
Epoch:   300  |  train loss: 0.5475267649
Epoch:   400  |  train loss: 0.4943630695
Epoch:   500  |  train loss: 0.4554053724
Epoch:   600  |  train loss: 0.4234244585
Epoch:   700  |  train loss: 0.3981904030
Epoch:   800  |  train loss: 0.3769980550
Epoch:   900  |  train loss: 0.3582798481
Epoch:  1000  |  train loss: 0.3422821105
Epoch:  1100  |  train loss: 0.3287444711
Epoch:  1200  |  train loss: 0.3151659071
Epoch:  1300  |  train loss: 0.3033732772
Epoch:  1400  |  train loss: 0.2931361556
Epoch:  1500  |  train loss: 0.2851847410
Epoch:  1600  |  train loss: 0.2764559031
Epoch:  1700  |  train loss: 0.2688933492
Epoch:  1800  |  train loss: 0.2621131778
Epoch:  1900  |  train loss: 0.2555295616
Epoch:  2000  |  train loss: 0.2488299251
Processing class: 9
Standarizing class data
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.7685714245
Epoch:   200  |  train loss: 0.6407491922
Epoch:   300  |  train loss: 0.5580895305
Epoch:   400  |  train loss: 0.5003157973
Epoch:   500  |  train loss: 0.4564105511
Epoch:   600  |  train loss: 0.4230631173
Epoch:   700  |  train loss: 0.3956609547
Epoch:   800  |  train loss: 0.3721578896
Epoch:   900  |  train loss: 0.3532141387
Epoch:  1000  |  train loss: 0.3363045394
Epoch:  1100  |  train loss: 0.3222250998
Epoch:  1200  |  train loss: 0.3084178448
Epoch:  1300  |  train loss: 0.2965570271
Epoch:  1400  |  train loss: 0.2861863434
Epoch:  1500  |  train loss: 0.2773093283
Epoch:  1600  |  train loss: 0.2681930363
Epoch:  1700  |  train loss: 0.2598989695
Epoch:  1800  |  train loss: 0.2530521661
Epoch:  1900  |  train loss: 0.2461747289
Epoch:  2000  |  train loss: 0.2403495461
Processing class: 10
Standarizing class data
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.7713675261
Epoch:   200  |  train loss: 0.6387368679
Epoch:   300  |  train loss: 0.5617138863
Epoch:   400  |  train loss: 0.5090682387
Epoch:   500  |  train loss: 0.4713292062
Epoch:   600  |  train loss: 0.4417377710
Epoch:   700  |  train loss: 0.4161841571
Epoch:   800  |  train loss: 0.3944876671
Epoch:   900  |  train loss: 0.3767067432
Epoch:  1000  |  train loss: 0.3597971380
Epoch:  1100  |  train loss: 0.3450377584
Epoch:  1200  |  train loss: 0.3316210747
Epoch:  1300  |  train loss: 0.3201917171
Epoch:  1400  |  train loss: 0.3097407460
Epoch:  1500  |  train loss: 0.3003542304
Epoch:  1600  |  train loss: 0.2915125132
Epoch:  1700  |  train loss: 0.2834223747
Epoch:  1800  |  train loss: 0.2763857841
Epoch:  1900  |  train loss: 0.2699979663
Epoch:  2000  |  train loss: 0.2640758932
Processing class: 11
Standarizing class data
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.7416881323
Epoch:   200  |  train loss: 0.6159794569
Epoch:   300  |  train loss: 0.5438337445
Epoch:   400  |  train loss: 0.4919602692
Epoch:   500  |  train loss: 0.4537584484
Epoch:   600  |  train loss: 0.4231708288
Epoch:   700  |  train loss: 0.3983273745
Epoch:   800  |  train loss: 0.3778136790
Epoch:   900  |  train loss: 0.3595797420
Epoch:  1000  |  train loss: 0.3444358349
Epoch:  1100  |  train loss: 0.3300219834
Epoch:  1200  |  train loss: 0.3178384960
Epoch:  1300  |  train loss: 0.3060441375
Epoch:  1400  |  train loss: 0.2963607192
Epoch:  1500  |  train loss: 0.2874455690
Epoch:  1600  |  train loss: 0.2793123603
Epoch:  1700  |  train loss: 0.2711404204
Epoch:  1800  |  train loss: 0.2643606663
Epoch:  1900  |  train loss: 0.2572744220
Epoch:  2000  |  train loss: 0.2508185387
Processing class: 12
Standarizing class data
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.7588229775
Epoch:   200  |  train loss: 0.6371725678
Epoch:   300  |  train loss: 0.5648413658
Epoch:   400  |  train loss: 0.5084908605
Epoch:   500  |  train loss: 0.4667953610
Epoch:   600  |  train loss: 0.4344854534
Epoch:   700  |  train loss: 0.4090157092
Epoch:   800  |  train loss: 0.3871501803
Epoch:   900  |  train loss: 0.3684506118
Epoch:  1000  |  train loss: 0.3520435870
Epoch:  1100  |  train loss: 0.3374995828
Epoch:  1200  |  train loss: 0.3251499057
Epoch:  1300  |  train loss: 0.3136107087
Epoch:  1400  |  train loss: 0.3033718407
Epoch:  1500  |  train loss: 0.2944252372
Epoch:  1600  |  train loss: 0.2854239702
Epoch:  1700  |  train loss: 0.2780491650
Epoch:  1800  |  train loss: 0.2710107327
Epoch:  1900  |  train loss: 0.2649063945
Epoch:  2000  |  train loss: 0.2586216748
Processing class: 13
Standarizing class data
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.7629243851
Epoch:   200  |  train loss: 0.6289576530
Epoch:   300  |  train loss: 0.5553877950
Epoch:   400  |  train loss: 0.5041443229
Epoch:   500  |  train loss: 0.4661433101
Epoch:   600  |  train loss: 0.4365116119
Epoch:   700  |  train loss: 0.4105440557
Epoch:   800  |  train loss: 0.3896155536
Epoch:   900  |  train loss: 0.3711542726
Epoch:  1000  |  train loss: 0.3554462016
Epoch:  1100  |  train loss: 0.3421651244
Epoch:  1200  |  train loss: 0.3310729623
Epoch:  1300  |  train loss: 0.3200909436
Epoch:  1400  |  train loss: 0.3094526112
Epoch:  1500  |  train loss: 0.3013928652
Epoch:  1600  |  train loss: 0.2923607171
Epoch:  1700  |  train loss: 0.2846648097
Epoch:  1800  |  train loss: 0.2779165804
Epoch:  1900  |  train loss: 0.2720825672
Epoch:  2000  |  train loss: 0.2664939761
Processing class: 14
Standarizing class data
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.7711862206
Epoch:   200  |  train loss: 0.6462626457
Epoch:   300  |  train loss: 0.5723685503
Epoch:   400  |  train loss: 0.5198556900
Epoch:   500  |  train loss: 0.4807045460
Epoch:   600  |  train loss: 0.4505219936
Epoch:   700  |  train loss: 0.4263728678
Epoch:   800  |  train loss: 0.4047449172
Epoch:   900  |  train loss: 0.3865714908
Epoch:  1000  |  train loss: 0.3704059064
Epoch:  1100  |  train loss: 0.3566642821
Epoch:  1200  |  train loss: 0.3449348032
Epoch:  1300  |  train loss: 0.3341418028
Epoch:  1400  |  train loss: 0.3242933452
Epoch:  1500  |  train loss: 0.3146723270
Epoch:  1600  |  train loss: 0.3065176904
Epoch:  1700  |  train loss: 0.2988679826
Epoch:  1800  |  train loss: 0.2928290427
Epoch:  1900  |  train loss: 0.2856037498
Epoch:  2000  |  train loss: 0.2798724890
Processing class: 15
Standarizing class data
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.7407697320
Epoch:   200  |  train loss: 0.5962473392
Epoch:   300  |  train loss: 0.5173525751
Epoch:   400  |  train loss: 0.4641739666
Epoch:   500  |  train loss: 0.4256039262
Epoch:   600  |  train loss: 0.3958634377
Epoch:   700  |  train loss: 0.3713637531
Epoch:   800  |  train loss: 0.3515686512
Epoch:   900  |  train loss: 0.3340906799
Epoch:  1000  |  train loss: 0.3203007340
Epoch:  1100  |  train loss: 0.3071188271
Epoch:  1200  |  train loss: 0.2953284025
Epoch:  1300  |  train loss: 0.2857496977
Epoch:  1400  |  train loss: 0.2764194965
Epoch:  1500  |  train loss: 0.2678130507
Epoch:  1600  |  train loss: 0.2604573220
Epoch:  1700  |  train loss: 0.2527897120
Epoch:  1800  |  train loss: 0.2464523017
Epoch:  1900  |  train loss: 0.2402809441
Epoch:  2000  |  train loss: 0.2347394913
Processing class: 16
Standarizing class data
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.7579425693
Epoch:   200  |  train loss: 0.6328134298
Epoch:   300  |  train loss: 0.5605348825
Epoch:   400  |  train loss: 0.5126819611
Epoch:   500  |  train loss: 0.4742501318
Epoch:   600  |  train loss: 0.4439655244
Epoch:   700  |  train loss: 0.4176299036
Epoch:   800  |  train loss: 0.3966096163
Epoch:   900  |  train loss: 0.3782552183
Epoch:  1000  |  train loss: 0.3617931604
Epoch:  1100  |  train loss: 0.3481526554
Epoch:  1200  |  train loss: 0.3337044179
Epoch:  1300  |  train loss: 0.3231487155
Epoch:  1400  |  train loss: 0.3132073224
Epoch:  1500  |  train loss: 0.3039408088
Epoch:  1600  |  train loss: 0.2952225447
Epoch:  1700  |  train loss: 0.2877068043
Epoch:  1800  |  train loss: 0.2808350742
Epoch:  1900  |  train loss: 0.2743812025
Epoch:  2000  |  train loss: 0.2693172574
Processing class: 17
Standarizing class data
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.7730511546
Epoch:   200  |  train loss: 0.6467412114
Epoch:   300  |  train loss: 0.5686920404
Epoch:   400  |  train loss: 0.5143291295
Epoch:   500  |  train loss: 0.4722277999
Epoch:   600  |  train loss: 0.4397320509
Epoch:   700  |  train loss: 0.4136475265
Epoch:   800  |  train loss: 0.3917237759
Epoch:   900  |  train loss: 0.3729225874
Epoch:  1000  |  train loss: 0.3567635000
Epoch:  1100  |  train loss: 0.3427872539
Epoch:  1200  |  train loss: 0.3293185949
Epoch:  1300  |  train loss: 0.3172942102
Epoch:  1400  |  train loss: 0.3071634769
Epoch:  1500  |  train loss: 0.2984216511
Epoch:  1600  |  train loss: 0.2894846201
Epoch:  1700  |  train loss: 0.2824354231
Epoch:  1800  |  train loss: 0.2757136285
Epoch:  1900  |  train loss: 0.2690263629
Epoch:  2000  |  train loss: 0.2630482376
Processing class: 18
Standarizing class data
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.7264124155
Epoch:   200  |  train loss: 0.6047700167
Epoch:   300  |  train loss: 0.5339864850
Epoch:   400  |  train loss: 0.4858747959
Epoch:   500  |  train loss: 0.4477272928
Epoch:   600  |  train loss: 0.4189764082
Epoch:   700  |  train loss: 0.3941625118
Epoch:   800  |  train loss: 0.3733894169
Epoch:   900  |  train loss: 0.3563406467
Epoch:  1000  |  train loss: 0.3401032209
Epoch:  1100  |  train loss: 0.3269337833
Epoch:  1200  |  train loss: 0.3150210142
Epoch:  1300  |  train loss: 0.3044094741
Epoch:  1400  |  train loss: 0.2948406219
Epoch:  1500  |  train loss: 0.2858424187
Epoch:  1600  |  train loss: 0.2784632742
Epoch:  1700  |  train loss: 0.2712701440
Epoch:  1800  |  train loss: 0.2637953877
Epoch:  1900  |  train loss: 0.2585024655
Epoch:  2000  |  train loss: 0.2517362624
Processing class: 19
Standarizing class data
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.7484988093
Epoch:   200  |  train loss: 0.6180438161
Epoch:   300  |  train loss: 0.5426677167
Epoch:   400  |  train loss: 0.4878561556
Epoch:   500  |  train loss: 0.4470476568
Epoch:   600  |  train loss: 0.4152209461
Epoch:   700  |  train loss: 0.3892887533
Epoch:   800  |  train loss: 0.3672238588
Epoch:   900  |  train loss: 0.3501109779
Epoch:  1000  |  train loss: 0.3334370315
Epoch:  1100  |  train loss: 0.3196837068
Epoch:  1200  |  train loss: 0.3072735429
Epoch:  1300  |  train loss: 0.2962745309
Epoch:  1400  |  train loss: 0.2859813511
Epoch:  1500  |  train loss: 0.2774510026
Epoch:  1600  |  train loss: 0.2688485086
Epoch:  1700  |  train loss: 0.2615506113
Epoch:  1800  |  train loss: 0.2537279129
Epoch:  1900  |  train loss: 0.2477573663
Epoch:  2000  |  train loss: 0.2411560744
Processing class: 20
Standarizing class data
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.7480249405
Epoch:   200  |  train loss: 0.6442717910
Epoch:   300  |  train loss: 0.5700098872
Epoch:   400  |  train loss: 0.5178090036
Epoch:   500  |  train loss: 0.4789142907
Epoch:   600  |  train loss: 0.4481731355
Epoch:   700  |  train loss: 0.4229811907
Epoch:   800  |  train loss: 0.4016158342
Epoch:   900  |  train loss: 0.3821470737
Epoch:  1000  |  train loss: 0.3647111952
Epoch:  1100  |  train loss: 0.3500117898
Epoch:  1200  |  train loss: 0.3369267225
Epoch:  1300  |  train loss: 0.3259081423
Epoch:  1400  |  train loss: 0.3159370661
Epoch:  1500  |  train loss: 0.3065000653
Epoch:  1600  |  train loss: 0.2982399285
Epoch:  1700  |  train loss: 0.2898330688
Epoch:  1800  |  train loss: 0.2821667135
Epoch:  1900  |  train loss: 0.2753316939
Epoch:  2000  |  train loss: 0.2692632735
Processing class: 21
Standarizing class data
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.7629276037
Epoch:   200  |  train loss: 0.6311727405
Epoch:   300  |  train loss: 0.5495729804
Epoch:   400  |  train loss: 0.4968918622
Epoch:   500  |  train loss: 0.4579243958
Epoch:   600  |  train loss: 0.4251288056
Epoch:   700  |  train loss: 0.3982054353
Epoch:   800  |  train loss: 0.3768211544
Epoch:   900  |  train loss: 0.3582930326
Epoch:  1000  |  train loss: 0.3417082250
Epoch:  1100  |  train loss: 0.3272618234
Epoch:  1200  |  train loss: 0.3149856567
Epoch:  1300  |  train loss: 0.3040010870
Epoch:  1400  |  train loss: 0.2930803835
Epoch:  1500  |  train loss: 0.2841135502
Epoch:  1600  |  train loss: 0.2756256878
Epoch:  1700  |  train loss: 0.2683895409
Epoch:  1800  |  train loss: 0.2614255130
Epoch:  1900  |  train loss: 0.2549168617
Epoch:  2000  |  train loss: 0.2493615627
Processing class: 22
Standarizing class data
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.7771050215
Epoch:   200  |  train loss: 0.6512446523
Epoch:   300  |  train loss: 0.5697552204
Epoch:   400  |  train loss: 0.5161017418
Epoch:   500  |  train loss: 0.4754602492
Epoch:   600  |  train loss: 0.4461822569
Epoch:   700  |  train loss: 0.4201999485
Epoch:   800  |  train loss: 0.3998834074
Epoch:   900  |  train loss: 0.3821928501
Epoch:  1000  |  train loss: 0.3661731720
Epoch:  1100  |  train loss: 0.3518590093
Epoch:  1200  |  train loss: 0.3396338046
Epoch:  1300  |  train loss: 0.3290352881
Epoch:  1400  |  train loss: 0.3192265391
Epoch:  1500  |  train loss: 0.3099973500
Epoch:  1600  |  train loss: 0.3012428701
Epoch:  1700  |  train loss: 0.2933278263
Epoch:  1800  |  train loss: 0.2867889583
Epoch:  1900  |  train loss: 0.2805236042
Epoch:  2000  |  train loss: 0.2740225017
Processing class: 23
Standarizing class data
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.7783149004
Epoch:   200  |  train loss: 0.6550242424
Epoch:   300  |  train loss: 0.5771796227
Epoch:   400  |  train loss: 0.5228362799
Epoch:   500  |  train loss: 0.4826692343
Epoch:   600  |  train loss: 0.4505450785
Epoch:   700  |  train loss: 0.4246922255
Epoch:   800  |  train loss: 0.4020335495
Epoch:   900  |  train loss: 0.3836109459
Epoch:  1000  |  train loss: 0.3668957293
Epoch:  1100  |  train loss: 0.3517584324
Epoch:  1200  |  train loss: 0.3388473332
Epoch:  1300  |  train loss: 0.3275818467
Epoch:  1400  |  train loss: 0.3172536254
Epoch:  1500  |  train loss: 0.3073058844
Epoch:  1600  |  train loss: 0.2993467927
Epoch:  1700  |  train loss: 0.2924956024
Epoch:  1800  |  train loss: 0.2852801144
Epoch:  1900  |  train loss: 0.2788465023
Epoch:  2000  |  train loss: 0.2723620474
Processing class: 24
Standarizing class data
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.7716549516
Epoch:   200  |  train loss: 0.6418091893
Epoch:   300  |  train loss: 0.5621090531
Epoch:   400  |  train loss: 0.5106454432
Epoch:   500  |  train loss: 0.4719197631
Epoch:   600  |  train loss: 0.4406333208
Epoch:   700  |  train loss: 0.4145923734
Epoch:   800  |  train loss: 0.3926618695
Epoch:   900  |  train loss: 0.3731730282
Epoch:  1000  |  train loss: 0.3572433114
Epoch:  1100  |  train loss: 0.3424605489
Epoch:  1200  |  train loss: 0.3290180981
Epoch:  1300  |  train loss: 0.3176248670
Epoch:  1400  |  train loss: 0.3065826297
Epoch:  1500  |  train loss: 0.2975383222
Epoch:  1600  |  train loss: 0.2887784183
Epoch:  1700  |  train loss: 0.2800082147
Epoch:  1800  |  train loss: 0.2731446028
Epoch:  1900  |  train loss: 0.2663068175
Epoch:  2000  |  train loss: 0.2592482209
Processing class: 25
Standarizing class data
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.7743660569
Epoch:   200  |  train loss: 0.6339282155
Epoch:   300  |  train loss: 0.5541618466
Epoch:   400  |  train loss: 0.5003306687
Epoch:   500  |  train loss: 0.4605020583
Epoch:   600  |  train loss: 0.4290973723
Epoch:   700  |  train loss: 0.4036098301
Epoch:   800  |  train loss: 0.3817537487
Epoch:   900  |  train loss: 0.3622690022
Epoch:  1000  |  train loss: 0.3464222372
Epoch:  1100  |  train loss: 0.3317520022
Epoch:  1200  |  train loss: 0.3193530500
Epoch:  1300  |  train loss: 0.3078954220
Epoch:  1400  |  train loss: 0.2971894681
Epoch:  1500  |  train loss: 0.2882041633
Epoch:  1600  |  train loss: 0.2793413699
Epoch:  1700  |  train loss: 0.2713469565
Epoch:  1800  |  train loss: 0.2646349430
Epoch:  1900  |  train loss: 0.2577571154
Epoch:  2000  |  train loss: 0.2519753098
Processing class: 26
Standarizing class data
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.7817204714
Epoch:   200  |  train loss: 0.6485216141
Epoch:   300  |  train loss: 0.5700415134
Epoch:   400  |  train loss: 0.5166668475
Epoch:   500  |  train loss: 0.4767407894
Epoch:   600  |  train loss: 0.4455474138
Epoch:   700  |  train loss: 0.4188459516
Epoch:   800  |  train loss: 0.3971873760
Epoch:   900  |  train loss: 0.3779532731
Epoch:  1000  |  train loss: 0.3611046493
Epoch:  1100  |  train loss: 0.3455710113
Epoch:  1200  |  train loss: 0.3320625663
Epoch:  1300  |  train loss: 0.3205976427
Epoch:  1400  |  train loss: 0.3100803792
Epoch:  1500  |  train loss: 0.2996327579
Epoch:  1600  |  train loss: 0.2906942129
Epoch:  1700  |  train loss: 0.2826254070
Epoch:  1800  |  train loss: 0.2759191155
Epoch:  1900  |  train loss: 0.2686404645
Epoch:  2000  |  train loss: 0.2625406444
Processing class: 27
Standarizing class data
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.7633522630
Epoch:   200  |  train loss: 0.6479768753
Epoch:   300  |  train loss: 0.5732757926
Epoch:   400  |  train loss: 0.5190886378
Epoch:   500  |  train loss: 0.4783414006
Epoch:   600  |  train loss: 0.4475578666
Epoch:   700  |  train loss: 0.4215888083
Epoch:   800  |  train loss: 0.3998302579
Epoch:   900  |  train loss: 0.3810649157
Epoch:  1000  |  train loss: 0.3657886386
Epoch:  1100  |  train loss: 0.3514103413
Epoch:  1200  |  train loss: 0.3382452011
Epoch:  1300  |  train loss: 0.3272526801
Epoch:  1400  |  train loss: 0.3172236562
Epoch:  1500  |  train loss: 0.3079073548
Epoch:  1600  |  train loss: 0.2991535544
Epoch:  1700  |  train loss: 0.2917956114
Epoch:  1800  |  train loss: 0.2845647156
Epoch:  1900  |  train loss: 0.2788476646
Epoch:  2000  |  train loss: 0.2734002531
Processing class: 28
Standarizing class data
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.7735573292
Epoch:   200  |  train loss: 0.6490181923
Epoch:   300  |  train loss: 0.5728737593
Epoch:   400  |  train loss: 0.5182651341
Epoch:   500  |  train loss: 0.4786674500
Epoch:   600  |  train loss: 0.4483203828
Epoch:   700  |  train loss: 0.4225480735
Epoch:   800  |  train loss: 0.4007158160
Epoch:   900  |  train loss: 0.3816713512
Epoch:  1000  |  train loss: 0.3653492272
Epoch:  1100  |  train loss: 0.3522664666
Epoch:  1200  |  train loss: 0.3383647561
Epoch:  1300  |  train loss: 0.3274532378
Epoch:  1400  |  train loss: 0.3169286191
Epoch:  1500  |  train loss: 0.3074798763
Epoch:  1600  |  train loss: 0.2994880676
Epoch:  1700  |  train loss: 0.2907320678
Epoch:  1800  |  train loss: 0.2840357959
Epoch:  1900  |  train loss: 0.2770798564
Epoch:  2000  |  train loss: 0.2719755948
Processing class: 29
Standarizing class data
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.7608442068
Epoch:   200  |  train loss: 0.6232492805
Epoch:   300  |  train loss: 0.5505191088
Epoch:   400  |  train loss: 0.5003654718
Epoch:   500  |  train loss: 0.4618412375
Epoch:   600  |  train loss: 0.4300876856
Epoch:   700  |  train loss: 0.4050802410
Epoch:   800  |  train loss: 0.3842154682
Epoch:   900  |  train loss: 0.3658537805
Epoch:  1000  |  train loss: 0.3499210477
Epoch:  1100  |  train loss: 0.3353183150
Epoch:  1200  |  train loss: 0.3232013822
Epoch:  1300  |  train loss: 0.3123729885
Epoch:  1400  |  train loss: 0.3017658532
Epoch:  1500  |  train loss: 0.2931163251
Epoch:  1600  |  train loss: 0.2837288678
Epoch:  1700  |  train loss: 0.2761706650
Epoch:  1800  |  train loss: 0.2694139004
Epoch:  1900  |  train loss: 0.2629213691
Epoch:  2000  |  train loss: 0.2562292486
Processing class: 30
Standarizing class data
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.7765763283
Epoch:   200  |  train loss: 0.6445117354
Epoch:   300  |  train loss: 0.5695910454
Epoch:   400  |  train loss: 0.5149423420
Epoch:   500  |  train loss: 0.4741669476
Epoch:   600  |  train loss: 0.4422729373
Epoch:   700  |  train loss: 0.4165863216
Epoch:   800  |  train loss: 0.3947104871
Epoch:   900  |  train loss: 0.3758861303
Epoch:  1000  |  train loss: 0.3589741051
Epoch:  1100  |  train loss: 0.3447129250
Epoch:  1200  |  train loss: 0.3314891219
Epoch:  1300  |  train loss: 0.3202210844
Epoch:  1400  |  train loss: 0.3093431532
Epoch:  1500  |  train loss: 0.2991571307
Epoch:  1600  |  train loss: 0.2911161065
Epoch:  1700  |  train loss: 0.2825316966
Epoch:  1800  |  train loss: 0.2748140872
Epoch:  1900  |  train loss: 0.2686526656
Epoch:  2000  |  train loss: 0.2627442002
Processing class: 31
Standarizing class data
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.7956588626
Epoch:   200  |  train loss: 0.6744805574
Epoch:   300  |  train loss: 0.5980476499
Epoch:   400  |  train loss: 0.5457488298
Epoch:   500  |  train loss: 0.5060769677
Epoch:   600  |  train loss: 0.4737692833
Epoch:   700  |  train loss: 0.4466676056
Epoch:   800  |  train loss: 0.4246180415
Epoch:   900  |  train loss: 0.4063830078
Epoch:  1000  |  train loss: 0.3902135015
Epoch:  1100  |  train loss: 0.3764957547
Epoch:  1200  |  train loss: 0.3635304868
Epoch:  1300  |  train loss: 0.3507791936
Epoch:  1400  |  train loss: 0.3405014098
Epoch:  1500  |  train loss: 0.3308256269
Epoch:  1600  |  train loss: 0.3218662083
Epoch:  1700  |  train loss: 0.3133244574
Epoch:  1800  |  train loss: 0.3060455263
Epoch:  1900  |  train loss: 0.2987695158
Epoch:  2000  |  train loss: 0.2926006019
Processing class: 32
Standarizing class data
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.7581089139
Epoch:   200  |  train loss: 0.6282458901
Epoch:   300  |  train loss: 0.5562192678
Epoch:   400  |  train loss: 0.5064003527
Epoch:   500  |  train loss: 0.4688865662
Epoch:   600  |  train loss: 0.4374563515
Epoch:   700  |  train loss: 0.4118478417
Epoch:   800  |  train loss: 0.3892210841
Epoch:   900  |  train loss: 0.3706880748
Epoch:  1000  |  train loss: 0.3544589043
Epoch:  1100  |  train loss: 0.3394523382
Epoch:  1200  |  train loss: 0.3270965338
Epoch:  1300  |  train loss: 0.3153241634
Epoch:  1400  |  train loss: 0.3046747327
Epoch:  1500  |  train loss: 0.2953300595
Epoch:  1600  |  train loss: 0.2866562128
Epoch:  1700  |  train loss: 0.2791333318
Epoch:  1800  |  train loss: 0.2715980351
Epoch:  1900  |  train loss: 0.2654187799
Epoch:  2000  |  train loss: 0.2592364788
Processing class: 33
Standarizing class data
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.7337825418
Epoch:   200  |  train loss: 0.6186572433
Epoch:   300  |  train loss: 0.5447557211
Epoch:   400  |  train loss: 0.4908402979
Epoch:   500  |  train loss: 0.4512699783
Epoch:   600  |  train loss: 0.4201970696
Epoch:   700  |  train loss: 0.3948291302
Epoch:   800  |  train loss: 0.3739297748
Epoch:   900  |  train loss: 0.3564986646
Epoch:  1000  |  train loss: 0.3402469277
Epoch:  1100  |  train loss: 0.3270255625
Epoch:  1200  |  train loss: 0.3144998193
Epoch:  1300  |  train loss: 0.3027589858
Epoch:  1400  |  train loss: 0.2935512543
Epoch:  1500  |  train loss: 0.2850469530
Epoch:  1600  |  train loss: 0.2761523604
Epoch:  1700  |  train loss: 0.2691062868
Epoch:  1800  |  train loss: 0.2622505009
Epoch:  1900  |  train loss: 0.2549803585
Epoch:  2000  |  train loss: 0.2489987642
Processing class: 34
Standarizing class data
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.7618798733
Epoch:   200  |  train loss: 0.6303558946
Epoch:   300  |  train loss: 0.5601074219
Epoch:   400  |  train loss: 0.5103918195
Epoch:   500  |  train loss: 0.4729543269
Epoch:   600  |  train loss: 0.4422806144
Epoch:   700  |  train loss: 0.4172373593
Epoch:   800  |  train loss: 0.3966568887
Epoch:   900  |  train loss: 0.3791631877
Epoch:  1000  |  train loss: 0.3631408215
Epoch:  1100  |  train loss: 0.3494474769
Epoch:  1200  |  train loss: 0.3371465206
Epoch:  1300  |  train loss: 0.3265721500
Epoch:  1400  |  train loss: 0.3169307470
Epoch:  1500  |  train loss: 0.3081521094
Epoch:  1600  |  train loss: 0.3007870615
Epoch:  1700  |  train loss: 0.2935496330
Epoch:  1800  |  train loss: 0.2871860564
Epoch:  1900  |  train loss: 0.2803813159
Epoch:  2000  |  train loss: 0.2746264398
Processing class: 35
Standarizing class data
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.7698162675
Epoch:   200  |  train loss: 0.6535481691
Epoch:   300  |  train loss: 0.5740036607
Epoch:   400  |  train loss: 0.5183292210
Epoch:   500  |  train loss: 0.4752470851
Epoch:   600  |  train loss: 0.4419302940
Epoch:   700  |  train loss: 0.4148400784
Epoch:   800  |  train loss: 0.3938447058
Epoch:   900  |  train loss: 0.3745826423
Epoch:  1000  |  train loss: 0.3590107381
Epoch:  1100  |  train loss: 0.3443928003
Epoch:  1200  |  train loss: 0.3309591651
/home/z1165703/miniconda3/lib/python3.11/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
Epoch:  1300  |  train loss: 0.3202082217
Epoch:  1400  |  train loss: 0.3098056495
Epoch:  1500  |  train loss: 0.3002730787
Epoch:  1600  |  train loss: 0.2924431503
Epoch:  1700  |  train loss: 0.2839478076
Epoch:  1800  |  train loss: 0.2774295568
Epoch:  1900  |  train loss: 0.2700590909
Epoch:  2000  |  train loss: 0.2643652081
Processing class: 36
Standarizing class data
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.7629191995
Epoch:   200  |  train loss: 0.6416067719
Epoch:   300  |  train loss: 0.5647886395
Epoch:   400  |  train loss: 0.5105928481
Epoch:   500  |  train loss: 0.4704080343
Epoch:   600  |  train loss: 0.4392581344
Epoch:   700  |  train loss: 0.4126568913
Epoch:   800  |  train loss: 0.3916614413
Epoch:   900  |  train loss: 0.3722578585
Epoch:  1000  |  train loss: 0.3564286709
Epoch:  1100  |  train loss: 0.3420292735
Epoch:  1200  |  train loss: 0.3284282923
Epoch:  1300  |  train loss: 0.3178828716
Epoch:  1400  |  train loss: 0.3077553451
Epoch:  1500  |  train loss: 0.2974828660
Epoch:  1600  |  train loss: 0.2889682472
Epoch:  1700  |  train loss: 0.2821897388
Epoch:  1800  |  train loss: 0.2744628906
Epoch:  1900  |  train loss: 0.2675818026
Epoch:  2000  |  train loss: 0.2618321121
Processing class: 37
Standarizing class data
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.7471526861
Epoch:   200  |  train loss: 0.6145852327
Epoch:   300  |  train loss: 0.5408519149
Epoch:   400  |  train loss: 0.4891523242
Epoch:   500  |  train loss: 0.4503343940
Epoch:   600  |  train loss: 0.4204569638
Epoch:   700  |  train loss: 0.3955645084
Epoch:   800  |  train loss: 0.3757252276
Epoch:   900  |  train loss: 0.3579842985
Epoch:  1000  |  train loss: 0.3425291836
Epoch:  1100  |  train loss: 0.3297545373
Epoch:  1200  |  train loss: 0.3176304638
Epoch:  1300  |  train loss: 0.3064012408
Epoch:  1400  |  train loss: 0.2969251692
Epoch:  1500  |  train loss: 0.2880014360
Epoch:  1600  |  train loss: 0.2797827721
Epoch:  1700  |  train loss: 0.2729504466
Epoch:  1800  |  train loss: 0.2658366144
Epoch:  1900  |  train loss: 0.2590170145
Epoch:  2000  |  train loss: 0.2535139501
Processing class: 38
Standarizing class data
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.7774800062
Epoch:   200  |  train loss: 0.6478731275
Epoch:   300  |  train loss: 0.5737682104
Epoch:   400  |  train loss: 0.5215582728
Epoch:   500  |  train loss: 0.4822381139
Epoch:   600  |  train loss: 0.4505515516
Epoch:   700  |  train loss: 0.4255812705
Epoch:   800  |  train loss: 0.4028926611
Epoch:   900  |  train loss: 0.3830975771
Epoch:  1000  |  train loss: 0.3670889378
Epoch:  1100  |  train loss: 0.3531176805
Epoch:  1200  |  train loss: 0.3403277814
Epoch:  1300  |  train loss: 0.3289155841
Epoch:  1400  |  train loss: 0.3189159513
Epoch:  1500  |  train loss: 0.3095726848
Epoch:  1600  |  train loss: 0.3001708269
Epoch:  1700  |  train loss: 0.2926175416
Epoch:  1800  |  train loss: 0.2848773479
Epoch:  1900  |  train loss: 0.2784888923
Epoch:  2000  |  train loss: 0.2721524596
Processing class: 39
Standarizing class data
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.7720576882
Epoch:   200  |  train loss: 0.6467931628
Epoch:   300  |  train loss: 0.5732433558
Epoch:   400  |  train loss: 0.5212765098
Epoch:   500  |  train loss: 0.4819517434
Epoch:   600  |  train loss: 0.4497951329
Epoch:   700  |  train loss: 0.4242395937
Epoch:   800  |  train loss: 0.4016456902
Epoch:   900  |  train loss: 0.3820954978
Epoch:  1000  |  train loss: 0.3648616314
Epoch:  1100  |  train loss: 0.3505322278
Epoch:  1200  |  train loss: 0.3371675372
Epoch:  1300  |  train loss: 0.3247695804
Epoch:  1400  |  train loss: 0.3143567383
Epoch:  1500  |  train loss: 0.3048503935
Epoch:  1600  |  train loss: 0.2956655979
Epoch:  1700  |  train loss: 0.2878638804
Epoch:  1800  |  train loss: 0.2802342534
Epoch:  1900  |  train loss: 0.2729123712
Epoch:  2000  |  train loss: 0.2671036601
Processing class: 40
Standarizing class data
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.7589272141
Epoch:   200  |  train loss: 0.6263640761
Epoch:   300  |  train loss: 0.5523593783
Epoch:   400  |  train loss: 0.5006856203
Epoch:   500  |  train loss: 0.4615967214
Epoch:   600  |  train loss: 0.4312492430
Epoch:   700  |  train loss: 0.4054677010
Epoch:   800  |  train loss: 0.3833523750
Epoch:   900  |  train loss: 0.3644815564
Epoch:  1000  |  train loss: 0.3483124077
Epoch:  1100  |  train loss: 0.3333484173
Epoch:  1200  |  train loss: 0.3200475752
Epoch:  1300  |  train loss: 0.3092053175
Epoch:  1400  |  train loss: 0.2996247530
Epoch:  1500  |  train loss: 0.2897638440
Epoch:  1600  |  train loss: 0.2812656820
Epoch:  1700  |  train loss: 0.2738649905
Epoch:  1800  |  train loss: 0.2664189756
Epoch:  1900  |  train loss: 0.2607669652
Epoch:  2000  |  train loss: 0.2541157097
Processing class: 41
Standarizing class data
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.7418151140
Epoch:   200  |  train loss: 0.6228596687
Epoch:   300  |  train loss: 0.5487745881
Epoch:   400  |  train loss: 0.4977173388
Epoch:   500  |  train loss: 0.4625639021
Epoch:   600  |  train loss: 0.4348665237
Epoch:   700  |  train loss: 0.4122091472
Epoch:   800  |  train loss: 0.3919598639
Epoch:   900  |  train loss: 0.3751636744
Epoch:  1000  |  train loss: 0.3600252390
Epoch:  1100  |  train loss: 0.3471593857
Epoch:  1200  |  train loss: 0.3342691302
Epoch:  1300  |  train loss: 0.3245972633
Epoch:  1400  |  train loss: 0.3137222111
Epoch:  1500  |  train loss: 0.3043019772
Epoch:  1600  |  train loss: 0.2965684474
Epoch:  1700  |  train loss: 0.2888488531
Epoch:  1800  |  train loss: 0.2824418485
Epoch:  1900  |  train loss: 0.2757682204
Epoch:  2000  |  train loss: 0.2704877496
Processing class: 42
Standarizing class data
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.7783042908
Epoch:   200  |  train loss: 0.6408206582
Epoch:   300  |  train loss: 0.5536646366
Epoch:   400  |  train loss: 0.4978650212
Epoch:   500  |  train loss: 0.4589572966
Epoch:   600  |  train loss: 0.4282961845
Epoch:   700  |  train loss: 0.4030521870
Epoch:   800  |  train loss: 0.3814819157
Epoch:   900  |  train loss: 0.3631560683
Epoch:  1000  |  train loss: 0.3465256214
Epoch:  1100  |  train loss: 0.3321219206
Epoch:  1200  |  train loss: 0.3196503043
Epoch:  1300  |  train loss: 0.3078847408
Epoch:  1400  |  train loss: 0.2979563475
Epoch:  1500  |  train loss: 0.2888446808
Epoch:  1600  |  train loss: 0.2802480221
Epoch:  1700  |  train loss: 0.2732021749
Epoch:  1800  |  train loss: 0.2660229266
Epoch:  1900  |  train loss: 0.2599690735
Epoch:  2000  |  train loss: 0.2535322666
Processing class: 43
Standarizing class data
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.7554194570
Epoch:   200  |  train loss: 0.6115300059
Epoch:   300  |  train loss: 0.5257602334
Epoch:   400  |  train loss: 0.4691516221
Epoch:   500  |  train loss: 0.4268564343
Epoch:   600  |  train loss: 0.3944691896
Epoch:   700  |  train loss: 0.3677527487
Epoch:   800  |  train loss: 0.3460451245
Epoch:   900  |  train loss: 0.3275184333
Epoch:  1000  |  train loss: 0.3110149741
Epoch:  1100  |  train loss: 0.2967605591
Epoch:  1200  |  train loss: 0.2848165631
Epoch:  1300  |  train loss: 0.2744141102
Epoch:  1400  |  train loss: 0.2639259100
Epoch:  1500  |  train loss: 0.2548328668
Epoch:  1600  |  train loss: 0.2464780390
Epoch:  1700  |  train loss: 0.2395291001
Epoch:  1800  |  train loss: 0.2328513116
Epoch:  1900  |  train loss: 0.2270320773
Epoch:  2000  |  train loss: 0.2213744581
Processing class: 44
Standarizing class data
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.7462675452
Epoch:   200  |  train loss: 0.6392560244
Epoch:   300  |  train loss: 0.5660397410
Epoch:   400  |  train loss: 0.5144546628
Epoch:   500  |  train loss: 0.4759808719
Epoch:   600  |  train loss: 0.4461676180
Epoch:   700  |  train loss: 0.4216578424
Epoch:   800  |  train loss: 0.4010951519
Epoch:   900  |  train loss: 0.3833802521
Epoch:  1000  |  train loss: 0.3677928209
Epoch:  1100  |  train loss: 0.3535447478
Epoch:  1200  |  train loss: 0.3416160047
Epoch:  1300  |  train loss: 0.3304984391
Epoch:  1400  |  train loss: 0.3206348300
Epoch:  1500  |  train loss: 0.3109428644
Epoch:  1600  |  train loss: 0.3028118432
Epoch:  1700  |  train loss: 0.2945638239
Epoch:  1800  |  train loss: 0.2876647174
Epoch:  1900  |  train loss: 0.2810218573
Epoch:  2000  |  train loss: 0.2750161111
Processing class: 45
Standarizing class data
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.7588854432
Epoch:   200  |  train loss: 0.6245662451
Epoch:   300  |  train loss: 0.5457470298
Epoch:   400  |  train loss: 0.4903829634
Epoch:   500  |  train loss: 0.4482153893
Epoch:   600  |  train loss: 0.4151188910
Epoch:   700  |  train loss: 0.3888571084
Epoch:   800  |  train loss: 0.3661737323
Epoch:   900  |  train loss: 0.3480366528
Epoch:  1000  |  train loss: 0.3312973678
Epoch:  1100  |  train loss: 0.3169395268
Epoch:  1200  |  train loss: 0.3044827878
Epoch:  1300  |  train loss: 0.2933003902
Epoch:  1400  |  train loss: 0.2828222454
Epoch:  1500  |  train loss: 0.2738698542
Epoch:  1600  |  train loss: 0.2657308102
Epoch:  1700  |  train loss: 0.2584890962
Epoch:  1800  |  train loss: 0.2517331064
Epoch:  1900  |  train loss: 0.2456435472
Epoch:  2000  |  train loss: 0.2391466409
Processing class: 46
Standarizing class data
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.7653325200
Epoch:   200  |  train loss: 0.6288553715
Epoch:   300  |  train loss: 0.5509714007
Epoch:   400  |  train loss: 0.4970682383
Epoch:   500  |  train loss: 0.4575912535
Epoch:   600  |  train loss: 0.4268865585
Epoch:   700  |  train loss: 0.4021899164
Epoch:   800  |  train loss: 0.3817681789
Epoch:   900  |  train loss: 0.3631690025
Epoch:  1000  |  train loss: 0.3469875991
Epoch:  1100  |  train loss: 0.3325680614
Epoch:  1200  |  train loss: 0.3204691052
Epoch:  1300  |  train loss: 0.3090972960
Epoch:  1400  |  train loss: 0.2993839085
Epoch:  1500  |  train loss: 0.2893820405
Epoch:  1600  |  train loss: 0.2816780031
Epoch:  1700  |  train loss: 0.2733520269
Epoch:  1800  |  train loss: 0.2664684176
Epoch:  1900  |  train loss: 0.2598476946
Epoch:  2000  |  train loss: 0.2535433710
Processing class: 47
Standarizing class data
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.7652678609
Epoch:   200  |  train loss: 0.6279472589
Epoch:   300  |  train loss: 0.5482339263
Epoch:   400  |  train loss: 0.4945704162
Epoch:   500  |  train loss: 0.4540375650
Epoch:   600  |  train loss: 0.4214478493
Epoch:   700  |  train loss: 0.3943059802
Epoch:   800  |  train loss: 0.3723055303
Epoch:   900  |  train loss: 0.3528366268
Epoch:  1000  |  train loss: 0.3363042653
Epoch:  1100  |  train loss: 0.3220383525
Epoch:  1200  |  train loss: 0.3091771066
Epoch:  1300  |  train loss: 0.2976124942
Epoch:  1400  |  train loss: 0.2873792171
Epoch:  1500  |  train loss: 0.2774926007
Epoch:  1600  |  train loss: 0.2688087046
Epoch:  1700  |  train loss: 0.2597928107
Epoch:  1800  |  train loss: 0.2533836603
Epoch:  1900  |  train loss: 0.2459299505
Epoch:  2000  |  train loss: 0.2398716748
Processing class: 48
Standarizing class data
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.7629224777
Epoch:   200  |  train loss: 0.6336403489
Epoch:   300  |  train loss: 0.5566605091
Epoch:   400  |  train loss: 0.5038204253
Epoch:   500  |  train loss: 0.4635070324
Epoch:   600  |  train loss: 0.4308309138
Epoch:   700  |  train loss: 0.4046095431
Epoch:   800  |  train loss: 0.3825238585
Epoch:   900  |  train loss: 0.3630025029
Epoch:  1000  |  train loss: 0.3464478552
Epoch:  1100  |  train loss: 0.3320981681
Epoch:  1200  |  train loss: 0.3188107908
Epoch:  1300  |  train loss: 0.3076684594
Epoch:  1400  |  train loss: 0.2973625422
Epoch:  1500  |  train loss: 0.2873405337
Epoch:  1600  |  train loss: 0.2789169192
Epoch:  1700  |  train loss: 0.2715872109
Epoch:  1800  |  train loss: 0.2646636069
Epoch:  1900  |  train loss: 0.2573380023
Epoch:  2000  |  train loss: 0.2514954507
Processing class: 49
Standarizing class data
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.7701571345
Epoch:   200  |  train loss: 0.6444410443
Epoch:   300  |  train loss: 0.5678872228
Epoch:   400  |  train loss: 0.5136587083
Epoch:   500  |  train loss: 0.4743021727
Epoch:   600  |  train loss: 0.4429859698
Epoch:   700  |  train loss: 0.4181740642
Epoch:   800  |  train loss: 0.3969120443
Epoch:   900  |  train loss: 0.3790529788
Epoch:  1000  |  train loss: 0.3635489643
Epoch:  1100  |  train loss: 0.3481972694
Epoch:  1200  |  train loss: 0.3355571210
Epoch:  1300  |  train loss: 0.3250490785
Epoch:  1400  |  train loss: 0.3135548115
Epoch:  1500  |  train loss: 0.3046663165
Epoch:  1600  |  train loss: 0.2966512322
Epoch:  1700  |  train loss: 0.2882652164
Epoch:  1800  |  train loss: 0.2808676481
Epoch:  1900  |  train loss: 0.2752410471
Epoch:  2000  |  train loss: 0.2684236944
2024-03-11 02:13:09,268 [trainer.py] => CNN: {'total': 83.44, '00-09': 87.7, '10-19': 79.6, '20-29': 84.9, '30-39': 81.0, '40-49': 84.0, 'old': 0, 'new': 83.44}
2024-03-11 02:13:09,269 [trainer.py] => No NME accuracy
2024-03-11 02:13:09,269 [trainer.py] => FeCAM: {'total': 78.4, '00-09': 82.6, '10-19': 73.8, '20-29': 79.5, '30-39': 77.3, '40-49': 78.8, 'old': 0, 'new': 78.4}
2024-03-11 02:13:09,269 [trainer.py] => CNN top1 curve: [83.44]
2024-03-11 02:13:09,269 [trainer.py] => CNN top5 curve: [96.5]
2024-03-11 02:13:09,269 [trainer.py] => FeCAM top1 curve: [78.4]
2024-03-11 02:13:09,269 [trainer.py] => FeCAM top5 curve: [92.08]

2024-03-11 02:13:09,282 [fecam.py] => Learning on 50-60
/home/z1165703/miniconda3/lib/python3.11/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
/home/z1165703/miniconda3/lib/python3.11/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
/home/z1165703/miniconda3/lib/python3.11/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
Processing class: 50
Standarizing class data
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.7330908895
Epoch:   200  |  train loss: 0.6102489114
Epoch:   300  |  train loss: 0.5376966953
Epoch:   400  |  train loss: 0.4891969502
Epoch:   500  |  train loss: 0.4544686437
Epoch:   600  |  train loss: 0.4252240121
Epoch:   700  |  train loss: 0.4011302054
Epoch:   800  |  train loss: 0.3822137177
Epoch:   900  |  train loss: 0.3656598985
Epoch:  1000  |  train loss: 0.3500202954
Epoch:  1100  |  train loss: 0.3376202047
Epoch:  1200  |  train loss: 0.3258712232
Epoch:  1300  |  train loss: 0.3154263675
Epoch:  1400  |  train loss: 0.3064371943
Epoch:  1500  |  train loss: 0.2969226301
Epoch:  1600  |  train loss: 0.2895902097
Epoch:  1700  |  train loss: 0.2817189336
Epoch:  1800  |  train loss: 0.2749497294
Epoch:  1900  |  train loss: 0.2689018071
Epoch:  2000  |  train loss: 0.2636474639
Processing class: 51
Standarizing class data
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.7045344114
Epoch:   200  |  train loss: 0.5766471505
Epoch:   300  |  train loss: 0.5064595759
Epoch:   400  |  train loss: 0.4575559616
Epoch:   500  |  train loss: 0.4214659691
Epoch:   600  |  train loss: 0.3926753879
Epoch:   700  |  train loss: 0.3687909186
Epoch:   800  |  train loss: 0.3483455002
Epoch:   900  |  train loss: 0.3320073366
Epoch:  1000  |  train loss: 0.3172451258
Epoch:  1100  |  train loss: 0.3041618109
Epoch:  1200  |  train loss: 0.2925298691
Epoch:  1300  |  train loss: 0.2823448002
Epoch:  1400  |  train loss: 0.2726806164
Epoch:  1500  |  train loss: 0.2627774924
Epoch:  1600  |  train loss: 0.2556547523
Epoch:  1700  |  train loss: 0.2490678012
Epoch:  1800  |  train loss: 0.2423996389
Epoch:  1900  |  train loss: 0.2362804115
Epoch:  2000  |  train loss: 0.2303343147
Processing class: 52
Standarizing class data
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.7022073388
Epoch:   200  |  train loss: 0.5912817478
Epoch:   300  |  train loss: 0.5198572814
Epoch:   400  |  train loss: 0.4701308370
Epoch:   500  |  train loss: 0.4320905328
Epoch:   600  |  train loss: 0.4023704290
Epoch:   700  |  train loss: 0.3786758602
Epoch:   800  |  train loss: 0.3583295047
Epoch:   900  |  train loss: 0.3407628715
Epoch:  1000  |  train loss: 0.3252914548
Epoch:  1100  |  train loss: 0.3127883852
Epoch:  1200  |  train loss: 0.2999969006
Epoch:  1300  |  train loss: 0.2891186297
Epoch:  1400  |  train loss: 0.2798036456
Epoch:  1500  |  train loss: 0.2711906374
Epoch:  1600  |  train loss: 0.2630211055
Epoch:  1700  |  train loss: 0.2561136603
Epoch:  1800  |  train loss: 0.2488007069
Epoch:  1900  |  train loss: 0.2422496647
Epoch:  2000  |  train loss: 0.2369893044
Processing class: 53
Standarizing class data
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.7417331696
Epoch:   200  |  train loss: 0.6054279447
Epoch:   300  |  train loss: 0.5213988543
Epoch:   400  |  train loss: 0.4650276899
Epoch:   500  |  train loss: 0.4235472977
Epoch:   600  |  train loss: 0.3906476498
Epoch:   700  |  train loss: 0.3644920051
Epoch:   800  |  train loss: 0.3427147865
Epoch:   900  |  train loss: 0.3243866026
Epoch:  1000  |  train loss: 0.3089322448
Epoch:  1100  |  train loss: 0.2950752735
Epoch:  1200  |  train loss: 0.2831850648
Epoch:  1300  |  train loss: 0.2718640983
Epoch:  1400  |  train loss: 0.2629433215
Epoch:  1500  |  train loss: 0.2541473657
Epoch:  1600  |  train loss: 0.2468383700
Epoch:  1700  |  train loss: 0.2395187765
Epoch:  1800  |  train loss: 0.2329243034
Epoch:  1900  |  train loss: 0.2259568572
Epoch:  2000  |  train loss: 0.2202955931
Processing class: 54
Standarizing class data
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.7313699365
Epoch:   200  |  train loss: 0.5860566020
Epoch:   300  |  train loss: 0.5200007379
Epoch:   400  |  train loss: 0.4762993813
Epoch:   500  |  train loss: 0.4405782878
Epoch:   600  |  train loss: 0.4105075181
Epoch:   700  |  train loss: 0.3877685547
Epoch:   800  |  train loss: 0.3668190002
Epoch:   900  |  train loss: 0.3508085847
Epoch:  1000  |  train loss: 0.3352883101
Epoch:  1100  |  train loss: 0.3218942881
Epoch:  1200  |  train loss: 0.3112595260
Epoch:  1300  |  train loss: 0.2998866379
Epoch:  1400  |  train loss: 0.2907917023
Epoch:  1500  |  train loss: 0.2819468081
Epoch:  1600  |  train loss: 0.2735157132
Epoch:  1700  |  train loss: 0.2664406478
Epoch:  1800  |  train loss: 0.2602184117
Epoch:  1900  |  train loss: 0.2540419161
Epoch:  2000  |  train loss: 0.2480973989
Processing class: 55
Standarizing class data
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.7322879076
Epoch:   200  |  train loss: 0.6074781895
Epoch:   300  |  train loss: 0.5384437203
Epoch:   400  |  train loss: 0.4883778095
Epoch:   500  |  train loss: 0.4486715555
Epoch:   600  |  train loss: 0.4158251286
Epoch:   700  |  train loss: 0.3902578950
Epoch:   800  |  train loss: 0.3682445347
Epoch:   900  |  train loss: 0.3494626045
Epoch:  1000  |  train loss: 0.3336615145
Epoch:  1100  |  train loss: 0.3192212045
Epoch:  1200  |  train loss: 0.3065038979
Epoch:  1300  |  train loss: 0.2952746153
Epoch:  1400  |  train loss: 0.2852030337
Epoch:  1500  |  train loss: 0.2762781084
Epoch:  1600  |  train loss: 0.2684407592
Epoch:  1700  |  train loss: 0.2615227938
Epoch:  1800  |  train loss: 0.2544393599
Epoch:  1900  |  train loss: 0.2484529018
Epoch:  2000  |  train loss: 0.2430322081
Processing class: 56
Standarizing class data
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.7163051605
Epoch:   200  |  train loss: 0.5887799501
Epoch:   300  |  train loss: 0.5274764955
Epoch:   400  |  train loss: 0.4887819409
Epoch:   500  |  train loss: 0.4559952438
Epoch:   600  |  train loss: 0.4289223075
Epoch:   700  |  train loss: 0.4070956588
Epoch:   800  |  train loss: 0.3888658106
Epoch:   900  |  train loss: 0.3728234768
Epoch:  1000  |  train loss: 0.3587933838
Epoch:  1100  |  train loss: 0.3466517091
Epoch:  1200  |  train loss: 0.3359489679
Epoch:  1300  |  train loss: 0.3263637066
Epoch:  1400  |  train loss: 0.3169861495
Epoch:  1500  |  train loss: 0.3093253374
Epoch:  1600  |  train loss: 0.3012418389
Epoch:  1700  |  train loss: 0.2946301639
Epoch:  1800  |  train loss: 0.2879306018
Epoch:  1900  |  train loss: 0.2819676220
Epoch:  2000  |  train loss: 0.2764668524
Processing class: 57
Standarizing class data
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.7362050891
Epoch:   200  |  train loss: 0.5960230947
Epoch:   300  |  train loss: 0.5249065876
Epoch:   400  |  train loss: 0.4761581898
Epoch:   500  |  train loss: 0.4397452831
Epoch:   600  |  train loss: 0.4106100261
Epoch:   700  |  train loss: 0.3870482385
Epoch:   800  |  train loss: 0.3669356585
Epoch:   900  |  train loss: 0.3499554336
Epoch:  1000  |  train loss: 0.3361630023
Epoch:  1100  |  train loss: 0.3227073491
Epoch:  1200  |  train loss: 0.3122099161
Epoch:  1300  |  train loss: 0.3014495432
Epoch:  1400  |  train loss: 0.2921579659
Epoch:  1500  |  train loss: 0.2840849936
Epoch:  1600  |  train loss: 0.2769660830
Epoch:  1700  |  train loss: 0.2695713699
Epoch:  1800  |  train loss: 0.2632819176
Epoch:  1900  |  train loss: 0.2574491143
Epoch:  2000  |  train loss: 0.2511127263
Processing class: 58
Standarizing class data
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.6729288340
Epoch:   200  |  train loss: 0.5507356048
Epoch:   300  |  train loss: 0.4840478361
Epoch:   400  |  train loss: 0.4390432775
Epoch:   500  |  train loss: 0.4030527472
Epoch:   600  |  train loss: 0.3747067690
Epoch:   700  |  train loss: 0.3512059450
Epoch:   800  |  train loss: 0.3322219551
Epoch:   900  |  train loss: 0.3155695677
Epoch:  1000  |  train loss: 0.3009467542
Epoch:  1100  |  train loss: 0.2884878993
Epoch:  1200  |  train loss: 0.2776736319
Epoch:  1300  |  train loss: 0.2683760107
Epoch:  1400  |  train loss: 0.2592166245
Epoch:  1500  |  train loss: 0.2510876864
Epoch:  1600  |  train loss: 0.2430007696
Epoch:  1700  |  train loss: 0.2376674682
Epoch:  1800  |  train loss: 0.2313148707
Epoch:  1900  |  train loss: 0.2258028448
Epoch:  2000  |  train loss: 0.2207951605
Processing class: 59
Standarizing class data
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.7131235242
Epoch:   200  |  train loss: 0.5844910741
Epoch:   300  |  train loss: 0.5178011775
Epoch:   400  |  train loss: 0.4699761331
Epoch:   500  |  train loss: 0.4326694429
Epoch:   600  |  train loss: 0.4033162057
Epoch:   700  |  train loss: 0.3790810168
Epoch:   800  |  train loss: 0.3587364376
Epoch:   900  |  train loss: 0.3416745603
Epoch:  1000  |  train loss: 0.3252624929
Epoch:  1100  |  train loss: 0.3129897118
Epoch:  1200  |  train loss: 0.3024945617
Epoch:  1300  |  train loss: 0.2921331584
Epoch:  1400  |  train loss: 0.2822507441
Epoch:  1500  |  train loss: 0.2744803309
Epoch:  1600  |  train loss: 0.2670170605
Epoch:  1700  |  train loss: 0.2600021034
Epoch:  1800  |  train loss: 0.2541740119
Epoch:  1900  |  train loss: 0.2483982861
Epoch:  2000  |  train loss: 0.2430451453
2024-03-11 02:24:13,490 [trainer.py] => CNN: {'total': 71.75, '00-09': 82.2, '10-19': 72.8, '20-29': 78.5, '30-39': 74.9, '40-49': 68.4, '50-59': 53.7, 'old': 75.36, 'new': 53.7}
2024-03-11 02:24:13,512 [trainer.py] => No NME accuracy
2024-03-11 02:24:13,512 [trainer.py] => FeCAM: {'total': 60.8, '00-09': 68.9, '10-19': 55.4, '20-29': 63.8, '30-39': 60.5, '40-49': 63.2, '50-59': 53.0, 'old': 62.36, 'new': 53.0}
2024-03-11 02:24:13,512 [trainer.py] => CNN top1 curve: [83.44, 71.75]
2024-03-11 02:24:13,513 [trainer.py] => CNN top5 curve: [96.5, 89.6]
2024-03-11 02:24:13,513 [trainer.py] => FeCAM top1 curve: [78.4, 60.8]
2024-03-11 02:24:13,513 [trainer.py] => FeCAM top5 curve: [92.08, 84.62]

2024-03-11 02:24:13,528 [fecam.py] => Learning on 60-70
/home/z1165703/miniconda3/lib/python3.11/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
/home/z1165703/miniconda3/lib/python3.11/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
/home/z1165703/miniconda3/lib/python3.11/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
Processing class: 60
Standarizing class data
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.7313925743
Epoch:   200  |  train loss: 0.5835985422
Epoch:   300  |  train loss: 0.5133011997
Epoch:   400  |  train loss: 0.4642513931
Epoch:   500  |  train loss: 0.4271746695
Epoch:   600  |  train loss: 0.3984859705
Epoch:   700  |  train loss: 0.3746299148
Epoch:   800  |  train loss: 0.3542794228
Epoch:   900  |  train loss: 0.3371580839
Epoch:  1000  |  train loss: 0.3221925378
Epoch:  1100  |  train loss: 0.3092174351
Epoch:  1200  |  train loss: 0.2975359440
Epoch:  1300  |  train loss: 0.2866886497
Epoch:  1400  |  train loss: 0.2766967773
Epoch:  1500  |  train loss: 0.2683720529
Epoch:  1600  |  train loss: 0.2605138004
Epoch:  1700  |  train loss: 0.2536259860
Epoch:  1800  |  train loss: 0.2472674876
Epoch:  1900  |  train loss: 0.2409253240
Epoch:  2000  |  train loss: 0.2357810497
Processing class: 61
Standarizing class data
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.7024318695
Epoch:   200  |  train loss: 0.5735294700
Epoch:   300  |  train loss: 0.4984083116
Epoch:   400  |  train loss: 0.4471536517
Epoch:   500  |  train loss: 0.4097252250
Epoch:   600  |  train loss: 0.3794862270
Epoch:   700  |  train loss: 0.3542385697
Epoch:   800  |  train loss: 0.3341919422
Epoch:   900  |  train loss: 0.3157799065
Epoch:  1000  |  train loss: 0.3007232964
Epoch:  1100  |  train loss: 0.2870686293
Epoch:  1200  |  train loss: 0.2751606345
Epoch:  1300  |  train loss: 0.2642383099
Epoch:  1400  |  train loss: 0.2552140713
Epoch:  1500  |  train loss: 0.2462111354
Epoch:  1600  |  train loss: 0.2384996623
Epoch:  1700  |  train loss: 0.2318682879
Epoch:  1800  |  train loss: 0.2255694866
Epoch:  1900  |  train loss: 0.2195715368
Epoch:  2000  |  train loss: 0.2139901429
Processing class: 62
Standarizing class data
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.7450261354
Epoch:   200  |  train loss: 0.6214335680
Epoch:   300  |  train loss: 0.5480286956
Epoch:   400  |  train loss: 0.5000268459
Epoch:   500  |  train loss: 0.4649845958
Epoch:   600  |  train loss: 0.4381362498
Epoch:   700  |  train loss: 0.4146768212
Epoch:   800  |  train loss: 0.3947850347
Epoch:   900  |  train loss: 0.3783227265
Epoch:  1000  |  train loss: 0.3634270072
Epoch:  1100  |  train loss: 0.3503810763
Epoch:  1200  |  train loss: 0.3389233351
Epoch:  1300  |  train loss: 0.3287757635
Epoch:  1400  |  train loss: 0.3196308374
Epoch:  1500  |  train loss: 0.3108673096
Epoch:  1600  |  train loss: 0.3023811638
Epoch:  1700  |  train loss: 0.2960107327
Epoch:  1800  |  train loss: 0.2898329139
Epoch:  1900  |  train loss: 0.2838562012
Epoch:  2000  |  train loss: 0.2780948222
Processing class: 63
Standarizing class data
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.7207322240
Epoch:   200  |  train loss: 0.5866513014
Epoch:   300  |  train loss: 0.5171931446
Epoch:   400  |  train loss: 0.4662592113
Epoch:   500  |  train loss: 0.4288040459
Epoch:   600  |  train loss: 0.3997656941
Epoch:   700  |  train loss: 0.3764683902
Epoch:   800  |  train loss: 0.3569070220
Epoch:   900  |  train loss: 0.3400119662
Epoch:  1000  |  train loss: 0.3256002903
Epoch:  1100  |  train loss: 0.3131780386
Epoch:  1200  |  train loss: 0.3022277176
Epoch:  1300  |  train loss: 0.2924718022
Epoch:  1400  |  train loss: 0.2833023548
Epoch:  1500  |  train loss: 0.2754154921
Epoch:  1600  |  train loss: 0.2684945405
Epoch:  1700  |  train loss: 0.2614559114
Epoch:  1800  |  train loss: 0.2547997504
Epoch:  1900  |  train loss: 0.2489398152
Epoch:  2000  |  train loss: 0.2436828613
Processing class: 64
Standarizing class data
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.6761461496
Epoch:   200  |  train loss: 0.5402853489
Epoch:   300  |  train loss: 0.4704598904
Epoch:   400  |  train loss: 0.4270295382
Epoch:   500  |  train loss: 0.3929032624
Epoch:   600  |  train loss: 0.3658483207
Epoch:   700  |  train loss: 0.3440124989
Epoch:   800  |  train loss: 0.3257548451
Epoch:   900  |  train loss: 0.3102433443
Epoch:  1000  |  train loss: 0.2966550827
Epoch:  1100  |  train loss: 0.2843725681
Epoch:  1200  |  train loss: 0.2744019032
Epoch:  1300  |  train loss: 0.2649402201
Epoch:  1400  |  train loss: 0.2564616621
Epoch:  1500  |  train loss: 0.2484533221
Epoch:  1600  |  train loss: 0.2418707341
Epoch:  1700  |  train loss: 0.2350219756
Epoch:  1800  |  train loss: 0.2292042226
Epoch:  1900  |  train loss: 0.2234234899
Epoch:  2000  |  train loss: 0.2189260393
Processing class: 65
Standarizing class data
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.7163836479
Epoch:   200  |  train loss: 0.5784733891
Epoch:   300  |  train loss: 0.5096106648
Epoch:   400  |  train loss: 0.4633522689
Epoch:   500  |  train loss: 0.4263323307
Epoch:   600  |  train loss: 0.3972078443
Epoch:   700  |  train loss: 0.3747019470
Epoch:   800  |  train loss: 0.3541208267
Epoch:   900  |  train loss: 0.3369993985
Epoch:  1000  |  train loss: 0.3224267542
Epoch:  1100  |  train loss: 0.3098936498
Epoch:  1200  |  train loss: 0.2979738355
Epoch:  1300  |  train loss: 0.2877143919
Epoch:  1400  |  train loss: 0.2781734765
Epoch:  1500  |  train loss: 0.2694990158
Epoch:  1600  |  train loss: 0.2621503592
Epoch:  1700  |  train loss: 0.2547472209
Epoch:  1800  |  train loss: 0.2474012971
Epoch:  1900  |  train loss: 0.2415793806
Epoch:  2000  |  train loss: 0.2355831325
Processing class: 66
Standarizing class data
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.7131488323
Epoch:   200  |  train loss: 0.5846338630
Epoch:   300  |  train loss: 0.5064799488
Epoch:   400  |  train loss: 0.4525133967
Epoch:   500  |  train loss: 0.4129612625
Epoch:   600  |  train loss: 0.3819123626
Epoch:   700  |  train loss: 0.3567088008
Epoch:   800  |  train loss: 0.3361183167
Epoch:   900  |  train loss: 0.3185195386
Epoch:  1000  |  train loss: 0.3025895834
Epoch:  1100  |  train loss: 0.2891953349
Epoch:  1200  |  train loss: 0.2780788660
Epoch:  1300  |  train loss: 0.2673207164
Epoch:  1400  |  train loss: 0.2577963531
Epoch:  1500  |  train loss: 0.2493133724
Epoch:  1600  |  train loss: 0.2416502476
Epoch:  1700  |  train loss: 0.2341826767
Epoch:  1800  |  train loss: 0.2284691632
Epoch:  1900  |  train loss: 0.2225213379
Epoch:  2000  |  train loss: 0.2167163104
Processing class: 67
Standarizing class data
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.7224698901
Epoch:   200  |  train loss: 0.5973638296
Epoch:   300  |  train loss: 0.5200528502
Epoch:   400  |  train loss: 0.4704940617
Epoch:   500  |  train loss: 0.4327905893
Epoch:   600  |  train loss: 0.4030520797
Epoch:   700  |  train loss: 0.3786698997
Epoch:   800  |  train loss: 0.3585624337
Epoch:   900  |  train loss: 0.3408888161
Epoch:  1000  |  train loss: 0.3256724298
Epoch:  1100  |  train loss: 0.3120407820
Epoch:  1200  |  train loss: 0.3006696641
Epoch:  1300  |  train loss: 0.2903325260
Epoch:  1400  |  train loss: 0.2801761389
Epoch:  1500  |  train loss: 0.2707083106
Epoch:  1600  |  train loss: 0.2637844384
Epoch:  1700  |  train loss: 0.2570560455
Epoch:  1800  |  train loss: 0.2494815081
Epoch:  1900  |  train loss: 0.2434359282
Epoch:  2000  |  train loss: 0.2376011968
Processing class: 68
Standarizing class data
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.7353548765
Epoch:   200  |  train loss: 0.5977655649
Epoch:   300  |  train loss: 0.5255138338
Epoch:   400  |  train loss: 0.4757826805
Epoch:   500  |  train loss: 0.4385026157
Epoch:   600  |  train loss: 0.4082821310
Epoch:   700  |  train loss: 0.3841961801
Epoch:   800  |  train loss: 0.3638566077
Epoch:   900  |  train loss: 0.3453558683
Epoch:  1000  |  train loss: 0.3309986889
Epoch:  1100  |  train loss: 0.3169190645
Epoch:  1200  |  train loss: 0.3050108254
Epoch:  1300  |  train loss: 0.2948319852
Epoch:  1400  |  train loss: 0.2860128880
Epoch:  1500  |  train loss: 0.2770636618
Epoch:  1600  |  train loss: 0.2693744481
Epoch:  1700  |  train loss: 0.2620201707
Epoch:  1800  |  train loss: 0.2555807531
Epoch:  1900  |  train loss: 0.2498059154
Epoch:  2000  |  train loss: 0.2435830802
Processing class: 69
Standarizing class data
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.7072388530
Epoch:   200  |  train loss: 0.5867816091
Epoch:   300  |  train loss: 0.5116227031
Epoch:   400  |  train loss: 0.4606490850
Epoch:   500  |  train loss: 0.4228591561
Epoch:   600  |  train loss: 0.3920121551
Epoch:   700  |  train loss: 0.3676132560
Epoch:   800  |  train loss: 0.3475439370
Epoch:   900  |  train loss: 0.3301828384
Epoch:  1000  |  train loss: 0.3143761814
Epoch:  1100  |  train loss: 0.3011065722
Epoch:  1200  |  train loss: 0.2890158474
Epoch:  1300  |  train loss: 0.2790521979
Epoch:  1400  |  train loss: 0.2681789517
Epoch:  1500  |  train loss: 0.2604750454
Epoch:  1600  |  train loss: 0.2515873224
Epoch:  1700  |  train loss: 0.2446623594
Epoch:  1800  |  train loss: 0.2378630638
Epoch:  1900  |  train loss: 0.2316514909
Epoch:  2000  |  train loss: 0.2262171090
2024-03-11 02:37:08,830 [trainer.py] => CNN: {'total': 64.66, '00-09': 75.0, '10-19': 70.1, '20-29': 77.1, '30-39': 70.6, '40-49': 64.0, '50-59': 45.8, '60-69': 50.0, 'old': 67.1, 'new': 50.0}
2024-03-11 02:37:08,831 [trainer.py] => No NME accuracy
2024-03-11 02:37:08,831 [trainer.py] => FeCAM: {'total': 54.77, '00-09': 65.6, '10-19': 51.3, '20-29': 61.2, '30-39': 55.0, '40-49': 56.1, '50-59': 45.6, '60-69': 48.6, 'old': 55.8, 'new': 48.6}
2024-03-11 02:37:08,831 [trainer.py] => CNN top1 curve: [83.44, 71.75, 64.66]
2024-03-11 02:37:08,831 [trainer.py] => CNN top5 curve: [96.5, 89.6, 86.54]
2024-03-11 02:37:08,831 [trainer.py] => FeCAM top1 curve: [78.4, 60.8, 54.77]
2024-03-11 02:37:08,831 [trainer.py] => FeCAM top5 curve: [92.08, 84.62, 80.01]

2024-03-11 02:37:08,858 [fecam.py] => Learning on 70-80
/home/z1165703/miniconda3/lib/python3.11/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
/home/z1165703/miniconda3/lib/python3.11/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
/home/z1165703/miniconda3/lib/python3.11/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
Processing class: 70
Standarizing class data
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.7141964436
Epoch:   200  |  train loss: 0.5909485340
Epoch:   300  |  train loss: 0.5198692739
Epoch:   400  |  train loss: 0.4712014556
Epoch:   500  |  train loss: 0.4339576125
Epoch:   600  |  train loss: 0.4044787347
Epoch:   700  |  train loss: 0.3799385607
Epoch:   800  |  train loss: 0.3597043395
Epoch:   900  |  train loss: 0.3420934319
Epoch:  1000  |  train loss: 0.3268472850
Epoch:  1100  |  train loss: 0.3138570368
Epoch:  1200  |  train loss: 0.3009977102
Epoch:  1300  |  train loss: 0.2906542957
Epoch:  1400  |  train loss: 0.2804844916
Epoch:  1500  |  train loss: 0.2720002115
Epoch:  1600  |  train loss: 0.2636990905
Epoch:  1700  |  train loss: 0.2571686924
Epoch:  1800  |  train loss: 0.2496215165
Epoch:  1900  |  train loss: 0.2450989306
Epoch:  2000  |  train loss: 0.2375504553
Processing class: 71
Standarizing class data
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.7263481021
Epoch:   200  |  train loss: 0.6076431870
Epoch:   300  |  train loss: 0.5444203019
Epoch:   400  |  train loss: 0.4969829559
Epoch:   500  |  train loss: 0.4614869475
Epoch:   600  |  train loss: 0.4335617185
Epoch:   700  |  train loss: 0.4099128067
Epoch:   800  |  train loss: 0.3903189778
Epoch:   900  |  train loss: 0.3718462408
Epoch:  1000  |  train loss: 0.3567180693
Epoch:  1100  |  train loss: 0.3432998598
Epoch:  1200  |  train loss: 0.3317943156
Epoch:  1300  |  train loss: 0.3196715534
Epoch:  1400  |  train loss: 0.3099128485
Epoch:  1500  |  train loss: 0.3008886397
Epoch:  1600  |  train loss: 0.2928440273
Epoch:  1700  |  train loss: 0.2851165473
Epoch:  1800  |  train loss: 0.2784176946
Epoch:  1900  |  train loss: 0.2720017612
Epoch:  2000  |  train loss: 0.2665049732
Processing class: 72
Standarizing class data
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.7259518266
Epoch:   200  |  train loss: 0.5887284398
Epoch:   300  |  train loss: 0.5157646179
Epoch:   400  |  train loss: 0.4671311855
Epoch:   500  |  train loss: 0.4291686773
Epoch:   600  |  train loss: 0.3984529853
Epoch:   700  |  train loss: 0.3728197157
Epoch:   800  |  train loss: 0.3515284836
Epoch:   900  |  train loss: 0.3325105190
Epoch:  1000  |  train loss: 0.3163881004
Epoch:  1100  |  train loss: 0.3020950198
Epoch:  1200  |  train loss: 0.2904424667
Epoch:  1300  |  train loss: 0.2791678309
Epoch:  1400  |  train loss: 0.2699675441
Epoch:  1500  |  train loss: 0.2605395436
Epoch:  1600  |  train loss: 0.2527495235
Epoch:  1700  |  train loss: 0.2460065961
Epoch:  1800  |  train loss: 0.2386319578
Epoch:  1900  |  train loss: 0.2322269797
Epoch:  2000  |  train loss: 0.2265078306
Processing class: 73
Standarizing class data
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.7489223957
Epoch:   200  |  train loss: 0.6323925614
Epoch:   300  |  train loss: 0.5612984061
Epoch:   400  |  train loss: 0.5096283019
Epoch:   500  |  train loss: 0.4704315722
Epoch:   600  |  train loss: 0.4391905963
Epoch:   700  |  train loss: 0.4129083157
Epoch:   800  |  train loss: 0.3911245167
Epoch:   900  |  train loss: 0.3727050722
Epoch:  1000  |  train loss: 0.3553701937
Epoch:  1100  |  train loss: 0.3419734597
Epoch:  1200  |  train loss: 0.3285246134
Epoch:  1300  |  train loss: 0.3187075913
Epoch:  1400  |  train loss: 0.3083783150
Epoch:  1500  |  train loss: 0.2998072326
Epoch:  1600  |  train loss: 0.2915516376
Epoch:  1700  |  train loss: 0.2839850783
Epoch:  1800  |  train loss: 0.2768856645
Epoch:  1900  |  train loss: 0.2713107049
Epoch:  2000  |  train loss: 0.2653561890
Processing class: 74
Standarizing class data
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.7589339375
Epoch:   200  |  train loss: 0.6123193145
Epoch:   300  |  train loss: 0.5317855239
Epoch:   400  |  train loss: 0.4798202515
Epoch:   500  |  train loss: 0.4413433433
Epoch:   600  |  train loss: 0.4104549646
Epoch:   700  |  train loss: 0.3850083470
Epoch:   800  |  train loss: 0.3631312430
Epoch:   900  |  train loss: 0.3444486022
Epoch:  1000  |  train loss: 0.3284132361
Epoch:  1100  |  train loss: 0.3149025857
Epoch:  1200  |  train loss: 0.3025007188
Epoch:  1300  |  train loss: 0.2917003572
Epoch:  1400  |  train loss: 0.2814101219
Epoch:  1500  |  train loss: 0.2728340030
Epoch:  1600  |  train loss: 0.2645852625
Epoch:  1700  |  train loss: 0.2569782495
Epoch:  1800  |  train loss: 0.2500761092
Epoch:  1900  |  train loss: 0.2439651281
Epoch:  2000  |  train loss: 0.2387851238
Processing class: 75
Standarizing class data
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.7492130399
Epoch:   200  |  train loss: 0.6261508942
Epoch:   300  |  train loss: 0.5536471486
Epoch:   400  |  train loss: 0.5041801810
Epoch:   500  |  train loss: 0.4673766732
Epoch:   600  |  train loss: 0.4373103678
Epoch:   700  |  train loss: 0.4132632792
Epoch:   800  |  train loss: 0.3923179090
Epoch:   900  |  train loss: 0.3747237384
Epoch:  1000  |  train loss: 0.3599274457
Epoch:  1100  |  train loss: 0.3462316036
Epoch:  1200  |  train loss: 0.3338227749
Epoch:  1300  |  train loss: 0.3229806125
Epoch:  1400  |  train loss: 0.3135834396
Epoch:  1500  |  train loss: 0.3042255640
Epoch:  1600  |  train loss: 0.2956488729
Epoch:  1700  |  train loss: 0.2879636765
Epoch:  1800  |  train loss: 0.2803068221
Epoch:  1900  |  train loss: 0.2742185950
Epoch:  2000  |  train loss: 0.2682485044
Processing class: 76
Standarizing class data
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.7049533129
Epoch:   200  |  train loss: 0.5720290661
Epoch:   300  |  train loss: 0.4961059928
Epoch:   400  |  train loss: 0.4470800042
Epoch:   500  |  train loss: 0.4112237453
Epoch:   600  |  train loss: 0.3838005960
Epoch:   700  |  train loss: 0.3602153003
Epoch:   800  |  train loss: 0.3411876857
Epoch:   900  |  train loss: 0.3238938153
Epoch:  1000  |  train loss: 0.3088158786
Epoch:  1100  |  train loss: 0.2951179802
Epoch:  1200  |  train loss: 0.2830878973
Epoch:  1300  |  train loss: 0.2734496236
Epoch:  1400  |  train loss: 0.2639559865
Epoch:  1500  |  train loss: 0.2551648825
Epoch:  1600  |  train loss: 0.2469324946
Epoch:  1700  |  train loss: 0.2402517319
Epoch:  1800  |  train loss: 0.2338660598
Epoch:  1900  |  train loss: 0.2280505896
Epoch:  2000  |  train loss: 0.2220451504
Processing class: 77
Standarizing class data
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.7116950631
Epoch:   200  |  train loss: 0.5758083344
Epoch:   300  |  train loss: 0.5002714753
Epoch:   400  |  train loss: 0.4498389363
Epoch:   500  |  train loss: 0.4115607977
Epoch:   600  |  train loss: 0.3815021873
Epoch:   700  |  train loss: 0.3553246796
Epoch:   800  |  train loss: 0.3346753478
Epoch:   900  |  train loss: 0.3180743277
Epoch:  1000  |  train loss: 0.3022881508
Epoch:  1100  |  train loss: 0.2888676584
Epoch:  1200  |  train loss: 0.2776681900
Epoch:  1300  |  train loss: 0.2671335399
Epoch:  1400  |  train loss: 0.2579896122
Epoch:  1500  |  train loss: 0.2494154662
Epoch:  1600  |  train loss: 0.2416639805
Epoch:  1700  |  train loss: 0.2344218999
Epoch:  1800  |  train loss: 0.2277156770
Epoch:  1900  |  train loss: 0.2221758068
Epoch:  2000  |  train loss: 0.2165676981
Processing class: 78
Standarizing class data
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.7278653979
Epoch:   200  |  train loss: 0.6014875054
Epoch:   300  |  train loss: 0.5276743650
Epoch:   400  |  train loss: 0.4750403464
Epoch:   500  |  train loss: 0.4354435861
Epoch:   600  |  train loss: 0.4044689894
Epoch:   700  |  train loss: 0.3790729105
Epoch:   800  |  train loss: 0.3580630600
Epoch:   900  |  train loss: 0.3400863051
Epoch:  1000  |  train loss: 0.3249977827
Epoch:  1100  |  train loss: 0.3110942841
Epoch:  1200  |  train loss: 0.2987377524
Epoch:  1300  |  train loss: 0.2883527339
Epoch:  1400  |  train loss: 0.2788020849
Epoch:  1500  |  train loss: 0.2698028564
Epoch:  1600  |  train loss: 0.2614375532
Epoch:  1700  |  train loss: 0.2546425968
Epoch:  1800  |  train loss: 0.2482360452
Epoch:  1900  |  train loss: 0.2418627411
Epoch:  2000  |  train loss: 0.2354554474
Processing class: 79
Standarizing class data
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.7385523677
Epoch:   200  |  train loss: 0.6021988869
Epoch:   300  |  train loss: 0.5312849402
Epoch:   400  |  train loss: 0.4803422213
Epoch:   500  |  train loss: 0.4435607433
Epoch:   600  |  train loss: 0.4139829457
Epoch:   700  |  train loss: 0.3902432978
Epoch:   800  |  train loss: 0.3706473410
Epoch:   900  |  train loss: 0.3542622805
Epoch:  1000  |  train loss: 0.3393678963
Epoch:  1100  |  train loss: 0.3263946176
Epoch:  1200  |  train loss: 0.3156985641
Epoch:  1300  |  train loss: 0.3055773854
Epoch:  1400  |  train loss: 0.2969700933
Epoch:  1500  |  train loss: 0.2884472668
Epoch:  1600  |  train loss: 0.2801448226
Epoch:  1700  |  train loss: 0.2740207076
Epoch:  1800  |  train loss: 0.2674060225
Epoch:  1900  |  train loss: 0.2615514219
Epoch:  2000  |  train loss: 0.2566217989
2024-03-11 02:52:11,265 [trainer.py] => CNN: {'total': 59.18, '00-09': 73.6, '10-19': 68.4, '20-29': 76.9, '30-39': 69.1, '40-49': 60.6, '50-59': 37.2, '60-69': 45.7, '70-79': 41.9, 'old': 61.64, 'new': 41.9}
2024-03-11 02:52:11,266 [trainer.py] => No NME accuracy
2024-03-11 02:52:11,266 [trainer.py] => FeCAM: {'total': 49.54, '00-09': 61.2, '10-19': 49.9, '20-29': 60.1, '30-39': 52.9, '40-49': 53.5, '50-59': 37.7, '60-69': 42.7, '70-79': 38.3, 'old': 51.14, 'new': 38.3}
2024-03-11 02:52:11,266 [trainer.py] => CNN top1 curve: [83.44, 71.75, 64.66, 59.18]
2024-03-11 02:52:11,266 [trainer.py] => CNN top5 curve: [96.5, 89.6, 86.54, 84.09]
2024-03-11 02:52:11,266 [trainer.py] => FeCAM top1 curve: [78.4, 60.8, 54.77, 49.54]
2024-03-11 02:52:11,266 [trainer.py] => FeCAM top5 curve: [92.08, 84.62, 80.01, 76.4]

2024-03-11 02:52:11,272 [fecam.py] => Learning on 80-90
/home/z1165703/miniconda3/lib/python3.11/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
/home/z1165703/miniconda3/lib/python3.11/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
/home/z1165703/miniconda3/lib/python3.11/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
Processing class: 80
Standarizing class data
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.7069026589
Epoch:   200  |  train loss: 0.5991062880
Epoch:   300  |  train loss: 0.5286594629
Epoch:   400  |  train loss: 0.4732801855
Epoch:   500  |  train loss: 0.4322705388
Epoch:   600  |  train loss: 0.3994390666
Epoch:   700  |  train loss: 0.3727804840
Epoch:   800  |  train loss: 0.3507743359
Epoch:   900  |  train loss: 0.3320898414
Epoch:  1000  |  train loss: 0.3161208093
Epoch:  1100  |  train loss: 0.3017412364
Epoch:  1200  |  train loss: 0.2885129452
Epoch:  1300  |  train loss: 0.2778005302
Epoch:  1400  |  train loss: 0.2672224939
Epoch:  1500  |  train loss: 0.2581974536
Epoch:  1600  |  train loss: 0.2501701117
Epoch:  1700  |  train loss: 0.2427523494
Epoch:  1800  |  train loss: 0.2354061574
Epoch:  1900  |  train loss: 0.2287056208
Epoch:  2000  |  train loss: 0.2239476651
Processing class: 81
Standarizing class data
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.7089880109
Epoch:   200  |  train loss: 0.5867406607
Epoch:   300  |  train loss: 0.5141667962
Epoch:   400  |  train loss: 0.4629112601
Epoch:   500  |  train loss: 0.4253050268
Epoch:   600  |  train loss: 0.3962323308
Epoch:   700  |  train loss: 0.3715718627
Epoch:   800  |  train loss: 0.3506600261
Epoch:   900  |  train loss: 0.3326182306
Epoch:  1000  |  train loss: 0.3165336609
Epoch:  1100  |  train loss: 0.3033956289
Epoch:  1200  |  train loss: 0.2917410731
Epoch:  1300  |  train loss: 0.2802018583
Epoch:  1400  |  train loss: 0.2704343617
Epoch:  1500  |  train loss: 0.2620084226
Epoch:  1600  |  train loss: 0.2541030139
Epoch:  1700  |  train loss: 0.2468587816
Epoch:  1800  |  train loss: 0.2393074393
Epoch:  1900  |  train loss: 0.2339664370
Epoch:  2000  |  train loss: 0.2280050665
Processing class: 82
Standarizing class data
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.7239026308
Epoch:   200  |  train loss: 0.5949217081
Epoch:   300  |  train loss: 0.5256899834
Epoch:   400  |  train loss: 0.4769765556
Epoch:   500  |  train loss: 0.4390250683
Epoch:   600  |  train loss: 0.4091773748
Epoch:   700  |  train loss: 0.3842752218
Epoch:   800  |  train loss: 0.3628572702
Epoch:   900  |  train loss: 0.3455256581
Epoch:  1000  |  train loss: 0.3303551376
Epoch:  1100  |  train loss: 0.3166686177
Epoch:  1200  |  train loss: 0.3049768686
Epoch:  1300  |  train loss: 0.2940473914
Epoch:  1400  |  train loss: 0.2844767928
Epoch:  1500  |  train loss: 0.2757317722
Epoch:  1600  |  train loss: 0.2687254071
Epoch:  1700  |  train loss: 0.2622345924
Epoch:  1800  |  train loss: 0.2550568879
Epoch:  1900  |  train loss: 0.2496228516
Epoch:  2000  |  train loss: 0.2439430088
Processing class: 83
Standarizing class data
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.6781944275
Epoch:   200  |  train loss: 0.5560404301
Epoch:   300  |  train loss: 0.4859579206
Epoch:   400  |  train loss: 0.4393519461
Epoch:   500  |  train loss: 0.4053046525
Epoch:   600  |  train loss: 0.3783206463
Epoch:   700  |  train loss: 0.3555484653
Epoch:   800  |  train loss: 0.3367218614
Epoch:   900  |  train loss: 0.3203744233
Epoch:  1000  |  train loss: 0.3063934863
Epoch:  1100  |  train loss: 0.2949437737
Epoch:  1200  |  train loss: 0.2850641131
Epoch:  1300  |  train loss: 0.2743015945
Epoch:  1400  |  train loss: 0.2661449850
Epoch:  1500  |  train loss: 0.2588035285
Epoch:  1600  |  train loss: 0.2510355443
Epoch:  1700  |  train loss: 0.2445392013
Epoch:  1800  |  train loss: 0.2384812623
Epoch:  1900  |  train loss: 0.2321367860
Epoch:  2000  |  train loss: 0.2273445010
Processing class: 84
Standarizing class data
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.7262091398
Epoch:   200  |  train loss: 0.5919079900
Epoch:   300  |  train loss: 0.5136512280
Epoch:   400  |  train loss: 0.4623032510
Epoch:   500  |  train loss: 0.4242727995
Epoch:   600  |  train loss: 0.3957425773
Epoch:   700  |  train loss: 0.3723327219
Epoch:   800  |  train loss: 0.3525589347
Epoch:   900  |  train loss: 0.3358521640
Epoch:  1000  |  train loss: 0.3209098220
Epoch:  1100  |  train loss: 0.3079568982
Epoch:  1200  |  train loss: 0.2969506085
Epoch:  1300  |  train loss: 0.2871200562
Epoch:  1400  |  train loss: 0.2783672094
Epoch:  1500  |  train loss: 0.2700968504
Epoch:  1600  |  train loss: 0.2624385059
Epoch:  1700  |  train loss: 0.2555702597
Epoch:  1800  |  train loss: 0.2487653762
Epoch:  1900  |  train loss: 0.2439122021
Epoch:  2000  |  train loss: 0.2378865808
Processing class: 85
Standarizing class data
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.7158203959
Epoch:   200  |  train loss: 0.5868085861
Epoch:   300  |  train loss: 0.5091914177
Epoch:   400  |  train loss: 0.4605889082
Epoch:   500  |  train loss: 0.4223798990
Epoch:   600  |  train loss: 0.3913055003
Epoch:   700  |  train loss: 0.3663133740
Epoch:   800  |  train loss: 0.3469961226
Epoch:   900  |  train loss: 0.3296388030
Epoch:  1000  |  train loss: 0.3142744601
Epoch:  1100  |  train loss: 0.3016113460
Epoch:  1200  |  train loss: 0.2892402112
Epoch:  1300  |  train loss: 0.2789556146
Epoch:  1400  |  train loss: 0.2690897465
Epoch:  1500  |  train loss: 0.2610453665
Epoch:  1600  |  train loss: 0.2531586319
Epoch:  1700  |  train loss: 0.2460190862
Epoch:  1800  |  train loss: 0.2392161638
Epoch:  1900  |  train loss: 0.2335719436
Epoch:  2000  |  train loss: 0.2280464172
Processing class: 86
Standarizing class data
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.7344505310
Epoch:   200  |  train loss: 0.6089737535
Epoch:   300  |  train loss: 0.5321599126
Epoch:   400  |  train loss: 0.4827743292
Epoch:   500  |  train loss: 0.4443310440
Epoch:   600  |  train loss: 0.4145265698
Epoch:   700  |  train loss: 0.3898639143
Epoch:   800  |  train loss: 0.3691036582
Epoch:   900  |  train loss: 0.3512581348
Epoch:  1000  |  train loss: 0.3362736940
Epoch:  1100  |  train loss: 0.3225118876
Epoch:  1200  |  train loss: 0.3099084675
Epoch:  1300  |  train loss: 0.2996508539
Epoch:  1400  |  train loss: 0.2898019254
Epoch:  1500  |  train loss: 0.2815459132
Epoch:  1600  |  train loss: 0.2731897712
Epoch:  1700  |  train loss: 0.2664965272
Epoch:  1800  |  train loss: 0.2590031117
Epoch:  1900  |  train loss: 0.2532831252
Epoch:  2000  |  train loss: 0.2480438083
Processing class: 87
Standarizing class data
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.7411693335
Epoch:   200  |  train loss: 0.6260697246
Epoch:   300  |  train loss: 0.5541799307
Epoch:   400  |  train loss: 0.5072161674
Epoch:   500  |  train loss: 0.4702583969
Epoch:   600  |  train loss: 0.4412421107
Epoch:   700  |  train loss: 0.4169558942
Epoch:   800  |  train loss: 0.3978859067
Epoch:   900  |  train loss: 0.3806014776
Epoch:  1000  |  train loss: 0.3653215170
Epoch:  1100  |  train loss: 0.3520328104
Epoch:  1200  |  train loss: 0.3403388381
Epoch:  1300  |  train loss: 0.3305586219
Epoch:  1400  |  train loss: 0.3199387014
Epoch:  1500  |  train loss: 0.3116185009
Epoch:  1600  |  train loss: 0.3033906698
Epoch:  1700  |  train loss: 0.2966451764
Epoch:  1800  |  train loss: 0.2897786915
Epoch:  1900  |  train loss: 0.2832734704
Epoch:  2000  |  train loss: 0.2781991720
Processing class: 88
Standarizing class data
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.7142739534
Epoch:   200  |  train loss: 0.5707184076
Epoch:   300  |  train loss: 0.4988187373
Epoch:   400  |  train loss: 0.4514420092
Epoch:   500  |  train loss: 0.4156405389
Epoch:   600  |  train loss: 0.3863152862
Epoch:   700  |  train loss: 0.3628008544
Epoch:   800  |  train loss: 0.3437551022
Epoch:   900  |  train loss: 0.3268436372
Epoch:  1000  |  train loss: 0.3120439470
Epoch:  1100  |  train loss: 0.2999864817
Epoch:  1200  |  train loss: 0.2883292913
Epoch:  1300  |  train loss: 0.2786291540
Epoch:  1400  |  train loss: 0.2701270580
Epoch:  1500  |  train loss: 0.2614869714
Epoch:  1600  |  train loss: 0.2540121794
Epoch:  1700  |  train loss: 0.2478469014
Epoch:  1800  |  train loss: 0.2409972519
Epoch:  1900  |  train loss: 0.2351525337
Epoch:  2000  |  train loss: 0.2291403502
Processing class: 89
Standarizing class data
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.7003920555
Epoch:   200  |  train loss: 0.5766401172
Epoch:   300  |  train loss: 0.5043611228
Epoch:   400  |  train loss: 0.4557229340
Epoch:   500  |  train loss: 0.4202417374
Epoch:   600  |  train loss: 0.3911332488
Epoch:   700  |  train loss: 0.3663045466
Epoch:   800  |  train loss: 0.3457694650
Epoch:   900  |  train loss: 0.3294193804
Epoch:  1000  |  train loss: 0.3145193219
Epoch:  1100  |  train loss: 0.3013782680
Epoch:  1200  |  train loss: 0.2896908402
Epoch:  1300  |  train loss: 0.2795279205
Epoch:  1400  |  train loss: 0.2704947293
Epoch:  1500  |  train loss: 0.2628109723
Epoch:  1600  |  train loss: 0.2544176489
Epoch:  1700  |  train loss: 0.2473730981
Epoch:  1800  |  train loss: 0.2414526194
Epoch:  1900  |  train loss: 0.2351698041
Epoch:  2000  |  train loss: 0.2293349475
2024-03-11 03:09:38,851 [trainer.py] => CNN: {'total': 54.08, '00-09': 67.9, '10-19': 63.6, '20-29': 73.2, '30-39': 68.8, '40-49': 57.2, '50-59': 32.6, '60-69': 40.9, '70-79': 38.1, '80-89': 44.4, 'old': 55.29, 'new': 44.4}
2024-03-11 03:09:38,854 [trainer.py] => No NME accuracy
2024-03-11 03:09:38,854 [trainer.py] => FeCAM: {'total': 45.87, '00-09': 58.2, '10-19': 46.3, '20-29': 56.6, '30-39': 51.3, '40-49': 50.9, '50-59': 32.9, '60-69': 39.4, '70-79': 34.5, '80-89': 42.7, 'old': 46.26, 'new': 42.7}
2024-03-11 03:09:38,854 [trainer.py] => CNN top1 curve: [83.44, 71.75, 64.66, 59.18, 54.08]
2024-03-11 03:09:38,854 [trainer.py] => CNN top5 curve: [96.5, 89.6, 86.54, 84.09, 81.74]
2024-03-11 03:09:38,854 [trainer.py] => FeCAM top1 curve: [78.4, 60.8, 54.77, 49.54, 45.87]
2024-03-11 03:09:38,854 [trainer.py] => FeCAM top5 curve: [92.08, 84.62, 80.01, 76.4, 72.66]

2024-03-11 03:09:38,870 [fecam.py] => Learning on 90-100
/home/z1165703/miniconda3/lib/python3.11/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
/home/z1165703/miniconda3/lib/python3.11/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
/home/z1165703/miniconda3/lib/python3.11/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
Processing class: 90
Standarizing class data
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.7045740962
Epoch:   200  |  train loss: 0.5728198051
Epoch:   300  |  train loss: 0.4973219573
Epoch:   400  |  train loss: 0.4438643277
Epoch:   500  |  train loss: 0.4062562644
Epoch:   600  |  train loss: 0.3782705545
Epoch:   700  |  train loss: 0.3545415699
Epoch:   800  |  train loss: 0.3356435597
Epoch:   900  |  train loss: 0.3196395576
Epoch:  1000  |  train loss: 0.3058459580
Epoch:  1100  |  train loss: 0.2928102851
Epoch:  1200  |  train loss: 0.2814761281
Epoch:  1300  |  train loss: 0.2717109859
Epoch:  1400  |  train loss: 0.2630263984
Epoch:  1500  |  train loss: 0.2549760371
Epoch:  1600  |  train loss: 0.2469991386
Epoch:  1700  |  train loss: 0.2397685647
Epoch:  1800  |  train loss: 0.2350101709
Epoch:  1900  |  train loss: 0.2287013710
Epoch:  2000  |  train loss: 0.2228975683
Processing class: 91
Standarizing class data
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.7211765528
Epoch:   200  |  train loss: 0.6016370773
Epoch:   300  |  train loss: 0.5293217838
Epoch:   400  |  train loss: 0.4798758507
Epoch:   500  |  train loss: 0.4433599710
Epoch:   600  |  train loss: 0.4139230490
Epoch:   700  |  train loss: 0.3896868289
Epoch:   800  |  train loss: 0.3694793463
Epoch:   900  |  train loss: 0.3523648202
Epoch:  1000  |  train loss: 0.3368037939
Epoch:  1100  |  train loss: 0.3236958444
Epoch:  1200  |  train loss: 0.3114575148
Epoch:  1300  |  train loss: 0.3004428864
Epoch:  1400  |  train loss: 0.2909689665
Epoch:  1500  |  train loss: 0.2825720906
Epoch:  1600  |  train loss: 0.2744222581
Epoch:  1700  |  train loss: 0.2664993584
Epoch:  1800  |  train loss: 0.2597664177
Epoch:  1900  |  train loss: 0.2541450560
Epoch:  2000  |  train loss: 0.2480009854
Processing class: 92
Standarizing class data
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.7337788820
Epoch:   200  |  train loss: 0.6031686187
Epoch:   300  |  train loss: 0.5333067358
Epoch:   400  |  train loss: 0.4868396819
Epoch:   500  |  train loss: 0.4497971714
Epoch:   600  |  train loss: 0.4209765971
Epoch:   700  |  train loss: 0.3967231929
Epoch:   800  |  train loss: 0.3753592372
Epoch:   900  |  train loss: 0.3568300366
Epoch:  1000  |  train loss: 0.3401529551
Epoch:  1100  |  train loss: 0.3265173674
Epoch:  1200  |  train loss: 0.3138885856
Epoch:  1300  |  train loss: 0.3030852497
Epoch:  1400  |  train loss: 0.2924368978
Epoch:  1500  |  train loss: 0.2833310246
Epoch:  1600  |  train loss: 0.2758715451
Epoch:  1700  |  train loss: 0.2681507528
Epoch:  1800  |  train loss: 0.2614018798
Epoch:  1900  |  train loss: 0.2548530310
Epoch:  2000  |  train loss: 0.2481788963
Processing class: 93
Standarizing class data
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.7205142617
Epoch:   200  |  train loss: 0.5910017490
Epoch:   300  |  train loss: 0.5207135856
Epoch:   400  |  train loss: 0.4712747991
Epoch:   500  |  train loss: 0.4337078333
Epoch:   600  |  train loss: 0.4038831770
Epoch:   700  |  train loss: 0.3794566035
Epoch:   800  |  train loss: 0.3580077350
Epoch:   900  |  train loss: 0.3401318192
Epoch:  1000  |  train loss: 0.3237847388
Epoch:  1100  |  train loss: 0.3097355127
Epoch:  1200  |  train loss: 0.2972567856
Epoch:  1300  |  train loss: 0.2857924104
Epoch:  1400  |  train loss: 0.2760541975
Epoch:  1500  |  train loss: 0.2669968307
Epoch:  1600  |  train loss: 0.2590521455
Epoch:  1700  |  train loss: 0.2508935302
Epoch:  1800  |  train loss: 0.2441832185
Epoch:  1900  |  train loss: 0.2378071815
Epoch:  2000  |  train loss: 0.2323018819
Processing class: 94
Standarizing class data
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.7098526001
Epoch:   200  |  train loss: 0.5756072283
Epoch:   300  |  train loss: 0.4965324700
Epoch:   400  |  train loss: 0.4447668254
Epoch:   500  |  train loss: 0.4083999097
Epoch:   600  |  train loss: 0.3783431411
Epoch:   700  |  train loss: 0.3549965143
Epoch:   800  |  train loss: 0.3344938278
Epoch:   900  |  train loss: 0.3181620598
Epoch:  1000  |  train loss: 0.3034398675
Epoch:  1100  |  train loss: 0.2905900240
Epoch:  1200  |  train loss: 0.2789540470
Epoch:  1300  |  train loss: 0.2686846137
Epoch:  1400  |  train loss: 0.2601189613
Epoch:  1500  |  train loss: 0.2516908556
Epoch:  1600  |  train loss: 0.2442750454
Epoch:  1700  |  train loss: 0.2367024601
Epoch:  1800  |  train loss: 0.2313982844
Epoch:  1900  |  train loss: 0.2252758533
Epoch:  2000  |  train loss: 0.2203329802
Processing class: 95
Standarizing class data
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.7241982341
Epoch:   200  |  train loss: 0.5857979298
Epoch:   300  |  train loss: 0.5134055316
Epoch:   400  |  train loss: 0.4677478194
Epoch:   500  |  train loss: 0.4317304850
Epoch:   600  |  train loss: 0.4033799529
Epoch:   700  |  train loss: 0.3794356108
Epoch:   800  |  train loss: 0.3600273311
Epoch:   900  |  train loss: 0.3423675776
Epoch:  1000  |  train loss: 0.3288983166
Epoch:  1100  |  train loss: 0.3157935798
Epoch:  1200  |  train loss: 0.3045727074
Epoch:  1300  |  train loss: 0.2935347021
Epoch:  1400  |  train loss: 0.2843049109
Epoch:  1500  |  train loss: 0.2756116748
Epoch:  1600  |  train loss: 0.2682375073
Epoch:  1700  |  train loss: 0.2604330957
Epoch:  1800  |  train loss: 0.2537118256
Epoch:  1900  |  train loss: 0.2480800062
Epoch:  2000  |  train loss: 0.2425280422
Processing class: 96
Standarizing class data
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.7065470219
Epoch:   200  |  train loss: 0.5713476658
Epoch:   300  |  train loss: 0.4997043312
Epoch:   400  |  train loss: 0.4506254673
Epoch:   500  |  train loss: 0.4156376183
Epoch:   600  |  train loss: 0.3878388345
Epoch:   700  |  train loss: 0.3653249681
Epoch:   800  |  train loss: 0.3464239001
Epoch:   900  |  train loss: 0.3299812078
Epoch:  1000  |  train loss: 0.3151740909
Epoch:  1100  |  train loss: 0.3023312867
Epoch:  1200  |  train loss: 0.2903533995
Epoch:  1300  |  train loss: 0.2790835440
Epoch:  1400  |  train loss: 0.2694494665
Epoch:  1500  |  train loss: 0.2614130974
Epoch:  1600  |  train loss: 0.2533438861
Epoch:  1700  |  train loss: 0.2462604314
Epoch:  1800  |  train loss: 0.2397998810
Epoch:  1900  |  train loss: 0.2335268408
Epoch:  2000  |  train loss: 0.2289346337
Processing class: 97
Standarizing class data
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.7236375332
Epoch:   200  |  train loss: 0.5861809850
Epoch:   300  |  train loss: 0.5103459179
Epoch:   400  |  train loss: 0.4581469119
Epoch:   500  |  train loss: 0.4177370131
Epoch:   600  |  train loss: 0.3869305134
Epoch:   700  |  train loss: 0.3618087590
Epoch:   800  |  train loss: 0.3407694340
Epoch:   900  |  train loss: 0.3233703256
Epoch:  1000  |  train loss: 0.3081439793
Epoch:  1100  |  train loss: 0.2948068738
Epoch:  1200  |  train loss: 0.2830804944
Epoch:  1300  |  train loss: 0.2727696717
Epoch:  1400  |  train loss: 0.2631894827
Epoch:  1500  |  train loss: 0.2551274747
Epoch:  1600  |  train loss: 0.2471534520
Epoch:  1700  |  train loss: 0.2397795320
Epoch:  1800  |  train loss: 0.2343403935
Epoch:  1900  |  train loss: 0.2281547874
Epoch:  2000  |  train loss: 0.2233885169
Processing class: 98
Standarizing class data
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.6903159380
Epoch:   200  |  train loss: 0.5693277895
Epoch:   300  |  train loss: 0.5014281809
Epoch:   400  |  train loss: 0.4526794374
Epoch:   500  |  train loss: 0.4140102327
Epoch:   600  |  train loss: 0.3835551202
Epoch:   700  |  train loss: 0.3577826023
Epoch:   800  |  train loss: 0.3374005914
Epoch:   900  |  train loss: 0.3198128641
Epoch:  1000  |  train loss: 0.3051198840
Epoch:  1100  |  train loss: 0.2918048799
Epoch:  1200  |  train loss: 0.2803434253
Epoch:  1300  |  train loss: 0.2706993878
Epoch:  1400  |  train loss: 0.2618267715
Epoch:  1500  |  train loss: 0.2533160925
Epoch:  1600  |  train loss: 0.2461140990
Epoch:  1700  |  train loss: 0.2390028805
Epoch:  1800  |  train loss: 0.2324177802
Epoch:  1900  |  train loss: 0.2273647189
Epoch:  2000  |  train loss: 0.2218270302
Processing class: 99
Standarizing class data
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.7059562564
Epoch:   200  |  train loss: 0.5793950796
Epoch:   300  |  train loss: 0.5092658579
Epoch:   400  |  train loss: 0.4619143665
Epoch:   500  |  train loss: 0.4242065549
Epoch:   600  |  train loss: 0.3930001676
Epoch:   700  |  train loss: 0.3683631361
Epoch:   800  |  train loss: 0.3472021341
Epoch:   900  |  train loss: 0.3292673290
Epoch:  1000  |  train loss: 0.3144013286
Epoch:  1100  |  train loss: 0.3004557014
Epoch:  1200  |  train loss: 0.2883217514
Epoch:  1300  |  train loss: 0.2777390063
Epoch:  1400  |  train loss: 0.2686542213
Epoch:  1500  |  train loss: 0.2597396612
Epoch:  1600  |  train loss: 0.2514423817
Epoch:  1700  |  train loss: 0.2442447841
Epoch:  1800  |  train loss: 0.2383368522
Epoch:  1900  |  train loss: 0.2316937536
Epoch:  2000  |  train loss: 0.2256722927
2024-03-11 03:29:43,142 [trainer.py] => CNN: {'total': 50.36, '00-09': 57.7, '10-19': 63.6, '20-29': 71.4, '30-39': 68.5, '40-49': 56.8, '50-59': 29.6, '60-69': 38.9, '70-79': 36.6, '80-89': 42.7, '90-99': 37.8, 'old': 51.76, 'new': 37.8}
2024-03-11 03:29:43,142 [trainer.py] => No NME accuracy
2024-03-11 03:29:43,142 [trainer.py] => FeCAM: {'total': 43.12, '00-09': 54.5, '10-19': 44.7, '20-29': 54.1, '30-39': 49.8, '40-49': 47.4, '50-59': 29.5, '60-69': 37.4, '70-79': 31.2, '80-89': 40.1, '90-99': 42.5, 'old': 43.19, 'new': 42.5}
2024-03-11 03:29:43,142 [trainer.py] => CNN top1 curve: [83.44, 71.75, 64.66, 59.18, 54.08, 50.36]
2024-03-11 03:29:43,142 [trainer.py] => CNN top5 curve: [96.5, 89.6, 86.54, 84.09, 81.74, 79.63]
2024-03-11 03:29:43,143 [trainer.py] => FeCAM top1 curve: [78.4, 60.8, 54.77, 49.54, 45.87, 43.12]
2024-03-11 03:29:43,143 [trainer.py] => FeCAM top5 curve: [92.08, 84.62, 80.01, 76.4, 72.66, 70.28]

Running FeCAM with params C=False, sigma=32
Running FeCAM with params C=False, sigma=16
Running FeCAM with params C=True, sigma=32
Running FeCAM with params C=True, sigma=16
