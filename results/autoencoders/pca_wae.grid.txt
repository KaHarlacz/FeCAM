=========================================
2024-03-17 22:07:32,147 [trainer.py] => config: ./exps/FeCAM_cifar100.json
2024-03-17 22:07:32,147 [trainer.py] => prefix: train
2024-03-17 22:07:32,147 [trainer.py] => dataset: cifar100
2024-03-17 22:07:32,147 [trainer.py] => memory_size: 0
2024-03-17 22:07:32,147 [trainer.py] => shuffle: True
2024-03-17 22:07:32,147 [trainer.py] => init_cls: 50
2024-03-17 22:07:32,147 [trainer.py] => increment: 10
2024-03-17 22:07:32,147 [trainer.py] => model_name: fecam
2024-03-17 22:07:32,147 [trainer.py] => convnet_type: resnet18
2024-03-17 22:07:32,147 [trainer.py] => device: [device(type='cuda', index=0)]
2024-03-17 22:07:32,147 [trainer.py] => seed: 1993
2024-03-17 22:07:32,147 [trainer.py] => init_epochs: 200
2024-03-17 22:07:32,147 [trainer.py] => init_lr: 0.1
2024-03-17 22:07:32,147 [trainer.py] => init_weight_decay: 0.0005
2024-03-17 22:07:32,147 [trainer.py] => batch_size: 128
2024-03-17 22:07:32,147 [trainer.py] => num_workers: 8
2024-03-17 22:07:32,148 [trainer.py] => T: 5
2024-03-17 22:07:32,148 [trainer.py] => beta: 0.5
2024-03-17 22:07:32,148 [trainer.py] => alpha1: 1
2024-03-17 22:07:32,148 [trainer.py] => alpha2: 1
2024-03-17 22:07:32,148 [trainer.py] => ncm: False
2024-03-17 22:07:32,148 [trainer.py] => tukey: False
2024-03-17 22:07:32,148 [trainer.py] => diagonal: False
2024-03-17 22:07:32,148 [trainer.py] => per_class: True
2024-03-17 22:07:32,148 [trainer.py] => full_cov: True
2024-03-17 22:07:32,148 [trainer.py] => shrink: True
2024-03-17 22:07:32,148 [trainer.py] => norm_cov: False
2024-03-17 22:07:32,148 [trainer.py] => epochs: 2000
2024-03-17 22:07:32,148 [trainer.py] => vecnorm: False
2024-03-17 22:07:32,148 [trainer.py] => ae_type: wae
2024-03-17 22:07:32,148 [trainer.py] => ae_latent_dim: 32
2024-03-17 22:07:32,148 [trainer.py] => ae_n: 1
2024-03-17 22:07:32,148 [trainer.py] => wae_sigma: 10
2024-03-17 22:07:32,148 [trainer.py] => wae_C: 0.1
2024-03-17 22:07:32,148 [trainer.py] => ae_standarization: False
2024-03-17 22:07:32,148 [trainer.py] => ae_pca: True
2024-03-17 22:07:32,148 [trainer.py] => ae_pca_components: 50
2024-03-17 22:07:32,148 [trainer.py] => ae_clsf: recon-cost
Files already downloaded and verified
Files already downloaded and verified
2024-03-17 22:07:34,899 [data_manager.py] => [68, 56, 78, 8, 23, 84, 90, 65, 74, 76, 40, 89, 3, 92, 55, 9, 26, 80, 43, 38, 58, 70, 77, 1, 85, 19, 17, 50, 28, 53, 13, 81, 45, 82, 6, 59, 83, 16, 15, 44, 91, 41, 72, 60, 79, 52, 20, 10, 31, 54, 37, 95, 14, 71, 96, 98, 97, 2, 64, 66, 42, 22, 35, 86, 24, 34, 87, 21, 99, 0, 88, 27, 18, 94, 11, 12, 47, 25, 30, 46, 62, 69, 36, 61, 7, 63, 75, 5, 32, 4, 51, 48, 73, 93, 39, 67, 29, 49, 57, 33]
2024-03-17 22:07:35,212 [fecam.py] => Learning on 0-50
/home/z1165703/miniconda3/lib/python3.11/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
/home/z1165703/miniconda3/lib/python3.11/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
Processing class: 0
Using PCA transformation
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.7703406572
Epoch:   200  |  train loss: 0.5930846214
Epoch:   300  |  train loss: 0.4669096887
Epoch:   400  |  train loss: 0.3788503706
Epoch:   500  |  train loss: 0.3169933856
Epoch:   600  |  train loss: 0.2697352529
Epoch:   700  |  train loss: 0.2328078508
Epoch:   800  |  train loss: 0.2033261299
Epoch:   900  |  train loss: 0.1795828044
Epoch:  1000  |  train loss: 0.1614964366
Epoch:  1100  |  train loss: 0.1460692048
Epoch:  1200  |  train loss: 0.1329549611
Epoch:  1300  |  train loss: 0.1219951347
Epoch:  1400  |  train loss: 0.1117218792
Epoch:  1500  |  train loss: 0.1029066801
Epoch:  1600  |  train loss: 0.0950538516
Epoch:  1700  |  train loss: 0.0887104258
Epoch:  1800  |  train loss: 0.0824464604
Epoch:  1900  |  train loss: 0.0772546381
Epoch:  2000  |  train loss: 0.0729010433
Processing class: 1
Using PCA transformation
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.7633968115
Epoch:   200  |  train loss: 0.5697662592
Epoch:   300  |  train loss: 0.4297527790
Epoch:   400  |  train loss: 0.3458094537
Epoch:   500  |  train loss: 0.2839916825
Epoch:   600  |  train loss: 0.2360120535
Epoch:   700  |  train loss: 0.1998384863
Epoch:   800  |  train loss: 0.1710853577
Epoch:   900  |  train loss: 0.1486657917
Epoch:  1000  |  train loss: 0.1311188906
Epoch:  1100  |  train loss: 0.1167498186
Epoch:  1200  |  train loss: 0.1049055338
Epoch:  1300  |  train loss: 0.0945028156
Epoch:  1400  |  train loss: 0.0858135045
Epoch:  1500  |  train loss: 0.0786809772
Epoch:  1600  |  train loss: 0.0725242332
Epoch:  1700  |  train loss: 0.0675916135
Epoch:  1800  |  train loss: 0.0626461603
Epoch:  1900  |  train loss: 0.0583334155
Epoch:  2000  |  train loss: 0.0546832994
Processing class: 2
Using PCA transformation
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.7649511576
Epoch:   200  |  train loss: 0.5575283766
Epoch:   300  |  train loss: 0.4286174715
Epoch:   400  |  train loss: 0.3377557218
Epoch:   500  |  train loss: 0.2762476444
Epoch:   600  |  train loss: 0.2334307939
Epoch:   700  |  train loss: 0.1989804596
Epoch:   800  |  train loss: 0.1719714701
Epoch:   900  |  train loss: 0.1505372077
Epoch:  1000  |  train loss: 0.1328328401
Epoch:  1100  |  train loss: 0.1191477105
Epoch:  1200  |  train loss: 0.1078242838
Epoch:  1300  |  train loss: 0.0980141014
Epoch:  1400  |  train loss: 0.0903802752
Epoch:  1500  |  train loss: 0.0835471466
Epoch:  1600  |  train loss: 0.0772994444
Epoch:  1700  |  train loss: 0.0723889306
Epoch:  1800  |  train loss: 0.0678978220
Epoch:  1900  |  train loss: 0.0639304034
Epoch:  2000  |  train loss: 0.0599253066
Processing class: 3
Using PCA transformation
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.7813402176
Epoch:   200  |  train loss: 0.5886410952
Epoch:   300  |  train loss: 0.4515209198
Epoch:   400  |  train loss: 0.3632444561
Epoch:   500  |  train loss: 0.3003961027
Epoch:   600  |  train loss: 0.2538406342
Epoch:   700  |  train loss: 0.2183130592
Epoch:   800  |  train loss: 0.1899879545
Epoch:   900  |  train loss: 0.1664344966
Epoch:  1000  |  train loss: 0.1476437181
Epoch:  1100  |  train loss: 0.1314420670
Epoch:  1200  |  train loss: 0.1187276721
Epoch:  1300  |  train loss: 0.1083160892
Epoch:  1400  |  train loss: 0.0991428047
Epoch:  1500  |  train loss: 0.0909196839
Epoch:  1600  |  train loss: 0.0840265930
Epoch:  1700  |  train loss: 0.0778637409
Epoch:  1800  |  train loss: 0.0725645125
Epoch:  1900  |  train loss: 0.0681327194
Epoch:  2000  |  train loss: 0.0645224780
Processing class: 4
Using PCA transformation
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.7792046547
Epoch:   200  |  train loss: 0.5705586433
Epoch:   300  |  train loss: 0.4408112526
Epoch:   400  |  train loss: 0.3598691761
Epoch:   500  |  train loss: 0.2992492974
Epoch:   600  |  train loss: 0.2548372507
Epoch:   700  |  train loss: 0.2196351886
Epoch:   800  |  train loss: 0.1921104223
Epoch:   900  |  train loss: 0.1693837047
Epoch:  1000  |  train loss: 0.1511414021
Epoch:  1100  |  train loss: 0.1362074614
Epoch:  1200  |  train loss: 0.1239193484
Epoch:  1300  |  train loss: 0.1133836553
Epoch:  1400  |  train loss: 0.1038104475
Epoch:  1500  |  train loss: 0.0957761928
Epoch:  1600  |  train loss: 0.0894420698
Epoch:  1700  |  train loss: 0.0829161793
Epoch:  1800  |  train loss: 0.0778436750
Epoch:  1900  |  train loss: 0.0727802560
Epoch:  2000  |  train loss: 0.0692639276
Processing class: 5
Using PCA transformation
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.7598159671
Epoch:   200  |  train loss: 0.5657392740
Epoch:   300  |  train loss: 0.4292524576
Epoch:   400  |  train loss: 0.3345878720
Epoch:   500  |  train loss: 0.2722357810
Epoch:   600  |  train loss: 0.2270728260
Epoch:   700  |  train loss: 0.1943187386
Epoch:   800  |  train loss: 0.1693946958
Epoch:   900  |  train loss: 0.1495087683
Epoch:  1000  |  train loss: 0.1327845693
Epoch:  1100  |  train loss: 0.1191853017
Epoch:  1200  |  train loss: 0.1075649306
Epoch:  1300  |  train loss: 0.0979531571
Epoch:  1400  |  train loss: 0.0891118154
Epoch:  1500  |  train loss: 0.0830169663
Epoch:  1600  |  train loss: 0.0766746998
Epoch:  1700  |  train loss: 0.0710584894
Epoch:  1800  |  train loss: 0.0665060073
Epoch:  1900  |  train loss: 0.0619545408
Epoch:  2000  |  train loss: 0.0583144993
Processing class: 6
Using PCA transformation
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.7612579107
Epoch:   200  |  train loss: 0.5663681865
Epoch:   300  |  train loss: 0.4348536015
Epoch:   400  |  train loss: 0.3426723897
Epoch:   500  |  train loss: 0.2795618713
Epoch:   600  |  train loss: 0.2346334994
Epoch:   700  |  train loss: 0.1999830246
Epoch:   800  |  train loss: 0.1736437559
Epoch:   900  |  train loss: 0.1516794264
Epoch:  1000  |  train loss: 0.1346971929
Epoch:  1100  |  train loss: 0.1211409763
Epoch:  1200  |  train loss: 0.1094573751
Epoch:  1300  |  train loss: 0.0995065004
Epoch:  1400  |  train loss: 0.0909499332
Epoch:  1500  |  train loss: 0.0840608507
Epoch:  1600  |  train loss: 0.0779855952
Epoch:  1700  |  train loss: 0.0721682057
Epoch:  1800  |  train loss: 0.0672466874
Epoch:  1900  |  train loss: 0.0627274722
Epoch:  2000  |  train loss: 0.0588863768
Processing class: 7
Using PCA transformation
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.7667552710
Epoch:   200  |  train loss: 0.5840196490
Epoch:   300  |  train loss: 0.4385226548
Epoch:   400  |  train loss: 0.3467006922
Epoch:   500  |  train loss: 0.2862214088
Epoch:   600  |  train loss: 0.2408628196
Epoch:   700  |  train loss: 0.2057926416
Epoch:   800  |  train loss: 0.1782918125
Epoch:   900  |  train loss: 0.1564533561
Epoch:  1000  |  train loss: 0.1386399120
Epoch:  1100  |  train loss: 0.1237837180
Epoch:  1200  |  train loss: 0.1122925997
Epoch:  1300  |  train loss: 0.1021456793
Epoch:  1400  |  train loss: 0.0934346423
Epoch:  1500  |  train loss: 0.0860399172
Epoch:  1600  |  train loss: 0.0799423859
Epoch:  1700  |  train loss: 0.0741446257
Epoch:  1800  |  train loss: 0.0691634789
Epoch:  1900  |  train loss: 0.0646100387
Epoch:  2000  |  train loss: 0.0603531368
Processing class: 8
Using PCA transformation
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.7622557282
Epoch:   200  |  train loss: 0.5628552437
Epoch:   300  |  train loss: 0.4225490749
Epoch:   400  |  train loss: 0.3336238265
Epoch:   500  |  train loss: 0.2735884786
Epoch:   600  |  train loss: 0.2302980036
Epoch:   700  |  train loss: 0.1980956286
Epoch:   800  |  train loss: 0.1725175738
Epoch:   900  |  train loss: 0.1526719987
Epoch:  1000  |  train loss: 0.1361487657
Epoch:  1100  |  train loss: 0.1226188943
Epoch:  1200  |  train loss: 0.1110331640
Epoch:  1300  |  train loss: 0.1006689399
Epoch:  1400  |  train loss: 0.0915990993
Epoch:  1500  |  train loss: 0.0849227786
Epoch:  1600  |  train loss: 0.0778660297
Epoch:  1700  |  train loss: 0.0729453072
Epoch:  1800  |  train loss: 0.0677164271
Epoch:  1900  |  train loss: 0.0639512867
Epoch:  2000  |  train loss: 0.0594805285
Processing class: 9
Using PCA transformation
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.7749117970
Epoch:   200  |  train loss: 0.5637297153
Epoch:   300  |  train loss: 0.4368847907
Epoch:   400  |  train loss: 0.3495346785
Epoch:   500  |  train loss: 0.2869544744
Epoch:   600  |  train loss: 0.2411218792
Epoch:   700  |  train loss: 0.2052504718
Epoch:   800  |  train loss: 0.1773117393
Epoch:   900  |  train loss: 0.1562640369
Epoch:  1000  |  train loss: 0.1387247533
Epoch:  1100  |  train loss: 0.1247224942
Epoch:  1200  |  train loss: 0.1128761396
Epoch:  1300  |  train loss: 0.1033812433
Epoch:  1400  |  train loss: 0.0948453397
Epoch:  1500  |  train loss: 0.0876921386
Epoch:  1600  |  train loss: 0.0810852513
Epoch:  1700  |  train loss: 0.0753429800
Epoch:  1800  |  train loss: 0.0705557466
Epoch:  1900  |  train loss: 0.0663874313
Epoch:  2000  |  train loss: 0.0625826210
Processing class: 10
Using PCA transformation
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.7686532021
Epoch:   200  |  train loss: 0.5575416088
Epoch:   300  |  train loss: 0.4131398380
Epoch:   400  |  train loss: 0.3212761998
Epoch:   500  |  train loss: 0.2599972576
Epoch:   600  |  train loss: 0.2150331527
Epoch:   700  |  train loss: 0.1831384748
Epoch:   800  |  train loss: 0.1579046279
Epoch:   900  |  train loss: 0.1393281013
Epoch:  1000  |  train loss: 0.1239029154
Epoch:  1100  |  train loss: 0.1119319022
Epoch:  1200  |  train loss: 0.1013761804
Epoch:  1300  |  train loss: 0.0920616880
Epoch:  1400  |  train loss: 0.0848000556
Epoch:  1500  |  train loss: 0.0789906099
Epoch:  1600  |  train loss: 0.0729073495
Epoch:  1700  |  train loss: 0.0681395859
Epoch:  1800  |  train loss: 0.0637450501
Epoch:  1900  |  train loss: 0.0597099580
Epoch:  2000  |  train loss: 0.0566977426
Processing class: 11
Using PCA transformation
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.7827246070
Epoch:   200  |  train loss: 0.5894247651
Epoch:   300  |  train loss: 0.4606842816
Epoch:   400  |  train loss: 0.3716929853
Epoch:   500  |  train loss: 0.3096414864
Epoch:   600  |  train loss: 0.2629003704
Epoch:   700  |  train loss: 0.2268959314
Epoch:   800  |  train loss: 0.1975790262
Epoch:   900  |  train loss: 0.1735734731
Epoch:  1000  |  train loss: 0.1538591057
Epoch:  1100  |  train loss: 0.1369538069
Epoch:  1200  |  train loss: 0.1238954917
Epoch:  1300  |  train loss: 0.1123418823
Epoch:  1400  |  train loss: 0.1034484267
Epoch:  1500  |  train loss: 0.0952152148
Epoch:  1600  |  train loss: 0.0886608094
Epoch:  1700  |  train loss: 0.0818246141
Epoch:  1800  |  train loss: 0.0763590410
Epoch:  1900  |  train loss: 0.0714820281
Epoch:  2000  |  train loss: 0.0673322603
Processing class: 12
Using PCA transformation
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.7767100692
Epoch:   200  |  train loss: 0.5885374904
Epoch:   300  |  train loss: 0.4495126188
Epoch:   400  |  train loss: 0.3597562850
Epoch:   500  |  train loss: 0.2952970743
Epoch:   600  |  train loss: 0.2477897257
Epoch:   700  |  train loss: 0.2123715103
Epoch:   800  |  train loss: 0.1860069633
Epoch:   900  |  train loss: 0.1628454298
Epoch:  1000  |  train loss: 0.1443759620
Epoch:  1100  |  train loss: 0.1298057839
Epoch:  1200  |  train loss: 0.1166137412
Epoch:  1300  |  train loss: 0.1055940852
Epoch:  1400  |  train loss: 0.0956744909
Epoch:  1500  |  train loss: 0.0880964831
Epoch:  1600  |  train loss: 0.0806993082
Epoch:  1700  |  train loss: 0.0746048212
Epoch:  1800  |  train loss: 0.0693941027
Epoch:  1900  |  train loss: 0.0651348203
Epoch:  2000  |  train loss: 0.0611345179
Processing class: 13
Using PCA transformation
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.7743755579
Epoch:   200  |  train loss: 0.5756524324
Epoch:   300  |  train loss: 0.4475771546
Epoch:   400  |  train loss: 0.3579927802
Epoch:   500  |  train loss: 0.2982039869
Epoch:   600  |  train loss: 0.2541641295
Epoch:   700  |  train loss: 0.2193694770
Epoch:   800  |  train loss: 0.1922418624
Epoch:   900  |  train loss: 0.1701471657
Epoch:  1000  |  train loss: 0.1524702489
Epoch:  1100  |  train loss: 0.1375457108
Epoch:  1200  |  train loss: 0.1254109323
Epoch:  1300  |  train loss: 0.1148883328
Epoch:  1400  |  train loss: 0.1050848290
Epoch:  1500  |  train loss: 0.0972011596
Epoch:  1600  |  train loss: 0.0904595375
Epoch:  1700  |  train loss: 0.0851382613
Epoch:  1800  |  train loss: 0.0796830878
Epoch:  1900  |  train loss: 0.0747401103
Epoch:  2000  |  train loss: 0.0710214317
Processing class: 14
Using PCA transformation
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.7744246960
Epoch:   200  |  train loss: 0.5677601457
Epoch:   300  |  train loss: 0.4408464491
Epoch:   400  |  train loss: 0.3574901283
Epoch:   500  |  train loss: 0.2982654095
Epoch:   600  |  train loss: 0.2492750168
Epoch:   700  |  train loss: 0.2112615734
Epoch:   800  |  train loss: 0.1828886092
Epoch:   900  |  train loss: 0.1596284539
Epoch:  1000  |  train loss: 0.1410687298
Epoch:  1100  |  train loss: 0.1257279962
Epoch:  1200  |  train loss: 0.1135492489
Epoch:  1300  |  train loss: 0.1038894296
Epoch:  1400  |  train loss: 0.0943801135
Epoch:  1500  |  train loss: 0.0864437342
Epoch:  1600  |  train loss: 0.0800416663
Epoch:  1700  |  train loss: 0.0743744329
Epoch:  1800  |  train loss: 0.0692734346
Epoch:  1900  |  train loss: 0.0652320817
Epoch:  2000  |  train loss: 0.0609486677
Processing class: 15
Using PCA transformation
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.7674628019
Epoch:   200  |  train loss: 0.5626160026
Epoch:   300  |  train loss: 0.4220264852
Epoch:   400  |  train loss: 0.3343762815
Epoch:   500  |  train loss: 0.2729057014
Epoch:   600  |  train loss: 0.2288396746
Epoch:   700  |  train loss: 0.1946906030
Epoch:   800  |  train loss: 0.1685954809
Epoch:   900  |  train loss: 0.1469131142
Epoch:  1000  |  train loss: 0.1305226028
Epoch:  1100  |  train loss: 0.1161055282
Epoch:  1200  |  train loss: 0.1041741773
Epoch:  1300  |  train loss: 0.0952125505
Epoch:  1400  |  train loss: 0.0867819890
Epoch:  1500  |  train loss: 0.0794888631
Epoch:  1600  |  train loss: 0.0736523226
Epoch:  1700  |  train loss: 0.0687601566
Epoch:  1800  |  train loss: 0.0638982430
Epoch:  1900  |  train loss: 0.0601525821
Epoch:  2000  |  train loss: 0.0565736309
Processing class: 16
Using PCA transformation
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.7948268890
Epoch:   200  |  train loss: 0.5725375175
Epoch:   300  |  train loss: 0.4364545226
Epoch:   400  |  train loss: 0.3464055538
Epoch:   500  |  train loss: 0.2857783020
Epoch:   600  |  train loss: 0.2432337046
Epoch:   700  |  train loss: 0.2107584476
Epoch:   800  |  train loss: 0.1839254260
Epoch:   900  |  train loss: 0.1615753740
Epoch:  1000  |  train loss: 0.1438466728
Epoch:  1100  |  train loss: 0.1300014317
Epoch:  1200  |  train loss: 0.1168142647
Epoch:  1300  |  train loss: 0.1068574399
Epoch:  1400  |  train loss: 0.0986498475
Epoch:  1500  |  train loss: 0.0907434046
Epoch:  1600  |  train loss: 0.0832626417
Epoch:  1700  |  train loss: 0.0775532037
Epoch:  1800  |  train loss: 0.0727309406
Epoch:  1900  |  train loss: 0.0679323390
Epoch:  2000  |  train loss: 0.0637995675
Processing class: 17
Using PCA transformation
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.7656934261
Epoch:   200  |  train loss: 0.5577900648
Epoch:   300  |  train loss: 0.4367767334
Epoch:   400  |  train loss: 0.3539472997
Epoch:   500  |  train loss: 0.2932453692
Epoch:   600  |  train loss: 0.2461949468
Epoch:   700  |  train loss: 0.2094451457
Epoch:   800  |  train loss: 0.1814399272
Epoch:   900  |  train loss: 0.1590170711
Epoch:  1000  |  train loss: 0.1406101257
Epoch:  1100  |  train loss: 0.1259352177
Epoch:  1200  |  train loss: 0.1129744112
Epoch:  1300  |  train loss: 0.1017931193
Epoch:  1400  |  train loss: 0.0934620187
Epoch:  1500  |  train loss: 0.0853117198
Epoch:  1600  |  train loss: 0.0779696256
Epoch:  1700  |  train loss: 0.0724321112
Epoch:  1800  |  train loss: 0.0675261050
Epoch:  1900  |  train loss: 0.0627237193
Epoch:  2000  |  train loss: 0.0591438279
Processing class: 18
Using PCA transformation
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.7859213948
Epoch:   200  |  train loss: 0.5940025926
Epoch:   300  |  train loss: 0.4611965954
Epoch:   400  |  train loss: 0.3722887337
Epoch:   500  |  train loss: 0.3086270690
Epoch:   600  |  train loss: 0.2604923010
Epoch:   700  |  train loss: 0.2228576660
Epoch:   800  |  train loss: 0.1939742178
Epoch:   900  |  train loss: 0.1718500197
Epoch:  1000  |  train loss: 0.1533259243
Epoch:  1100  |  train loss: 0.1383090973
Epoch:  1200  |  train loss: 0.1253225967
Epoch:  1300  |  train loss: 0.1151121661
Epoch:  1400  |  train loss: 0.1056689799
Epoch:  1500  |  train loss: 0.0970922649
Epoch:  1600  |  train loss: 0.0896705240
Epoch:  1700  |  train loss: 0.0833513036
Epoch:  1800  |  train loss: 0.0778080702
Epoch:  1900  |  train loss: 0.0732059956
Epoch:  2000  |  train loss: 0.0686436325
Processing class: 19
Using PCA transformation
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.7708938360
Epoch:   200  |  train loss: 0.5767195702
Epoch:   300  |  train loss: 0.4522761822
Epoch:   400  |  train loss: 0.3655206800
Epoch:   500  |  train loss: 0.3019822001
Epoch:   600  |  train loss: 0.2544832736
Epoch:   700  |  train loss: 0.2177497208
Epoch:   800  |  train loss: 0.1892094523
Epoch:   900  |  train loss: 0.1659996033
Epoch:  1000  |  train loss: 0.1476068884
Epoch:  1100  |  train loss: 0.1324567854
Epoch:  1200  |  train loss: 0.1194991916
Epoch:  1300  |  train loss: 0.1081441596
Epoch:  1400  |  train loss: 0.0985273302
Epoch:  1500  |  train loss: 0.0911093235
Epoch:  1600  |  train loss: 0.0838348046
Epoch:  1700  |  train loss: 0.0773053735
Epoch:  1800  |  train loss: 0.0717809886
Epoch:  1900  |  train loss: 0.0675193876
Epoch:  2000  |  train loss: 0.0634455368
Processing class: 20
Using PCA transformation
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.7748797774
Epoch:   200  |  train loss: 0.6016511440
Epoch:   300  |  train loss: 0.4678770483
Epoch:   400  |  train loss: 0.3755337417
Epoch:   500  |  train loss: 0.3091559947
Epoch:   600  |  train loss: 0.2619549811
Epoch:   700  |  train loss: 0.2266839504
Epoch:   800  |  train loss: 0.1986575812
Epoch:   900  |  train loss: 0.1762080312
Epoch:  1000  |  train loss: 0.1571137726
Epoch:  1100  |  train loss: 0.1413406402
Epoch:  1200  |  train loss: 0.1285479948
Epoch:  1300  |  train loss: 0.1169139713
Epoch:  1400  |  train loss: 0.1074972913
Epoch:  1500  |  train loss: 0.0992507398
Epoch:  1600  |  train loss: 0.0911752641
Epoch:  1700  |  train loss: 0.0840871811
Epoch:  1800  |  train loss: 0.0785442337
Epoch:  1900  |  train loss: 0.0741875395
Epoch:  2000  |  train loss: 0.0687714577
Processing class: 21
Using PCA transformation
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.7541587472
Epoch:   200  |  train loss: 0.5408115983
Epoch:   300  |  train loss: 0.4064750671
Epoch:   400  |  train loss: 0.3206637084
Epoch:   500  |  train loss: 0.2589003384
Epoch:   600  |  train loss: 0.2157019824
Epoch:   700  |  train loss: 0.1847472012
Epoch:   800  |  train loss: 0.1607441336
Epoch:   900  |  train loss: 0.1416505069
Epoch:  1000  |  train loss: 0.1258983627
Epoch:  1100  |  train loss: 0.1134251639
Epoch:  1200  |  train loss: 0.1029069692
Epoch:  1300  |  train loss: 0.0935299233
Epoch:  1400  |  train loss: 0.0851730600
Epoch:  1500  |  train loss: 0.0786234483
Epoch:  1600  |  train loss: 0.0723910287
Epoch:  1700  |  train loss: 0.0674726218
Epoch:  1800  |  train loss: 0.0628404431
Epoch:  1900  |  train loss: 0.0591921218
Epoch:  2000  |  train loss: 0.0557579234
Processing class: 22
Using PCA transformation
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.7806610703
Epoch:   200  |  train loss: 0.5724260688
Epoch:   300  |  train loss: 0.4268799067
Epoch:   400  |  train loss: 0.3433864176
Epoch:   500  |  train loss: 0.2822170258
Epoch:   600  |  train loss: 0.2379666269
Epoch:   700  |  train loss: 0.2045818716
Epoch:   800  |  train loss: 0.1783436954
Epoch:   900  |  train loss: 0.1574418873
Epoch:  1000  |  train loss: 0.1401282817
Epoch:  1100  |  train loss: 0.1246654704
Epoch:  1200  |  train loss: 0.1124411970
Epoch:  1300  |  train loss: 0.1022354186
Epoch:  1400  |  train loss: 0.0932678819
Epoch:  1500  |  train loss: 0.0858766079
Epoch:  1600  |  train loss: 0.0792758435
Epoch:  1700  |  train loss: 0.0731886625
Epoch:  1800  |  train loss: 0.0685760632
Epoch:  1900  |  train loss: 0.0644158110
Epoch:  2000  |  train loss: 0.0605033182
Processing class: 23
Using PCA transformation
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.7750525713
Epoch:   200  |  train loss: 0.5772937298
Epoch:   300  |  train loss: 0.4443297803
Epoch:   400  |  train loss: 0.3603146732
Epoch:   500  |  train loss: 0.2989632130
Epoch:   600  |  train loss: 0.2532806784
Epoch:   700  |  train loss: 0.2181096941
Epoch:   800  |  train loss: 0.1904651880
Epoch:   900  |  train loss: 0.1680487335
Epoch:  1000  |  train loss: 0.1498487175
Epoch:  1100  |  train loss: 0.1338787168
Epoch:  1200  |  train loss: 0.1205874026
Epoch:  1300  |  train loss: 0.1095182016
Epoch:  1400  |  train loss: 0.0996947587
Epoch:  1500  |  train loss: 0.0914486140
Epoch:  1600  |  train loss: 0.0844291449
Epoch:  1700  |  train loss: 0.0783118844
Epoch:  1800  |  train loss: 0.0731920466
Epoch:  1900  |  train loss: 0.0685231134
Epoch:  2000  |  train loss: 0.0643142343
Processing class: 24
Using PCA transformation
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.7643740535
Epoch:   200  |  train loss: 0.5925769448
Epoch:   300  |  train loss: 0.4519224882
Epoch:   400  |  train loss: 0.3534440041
Epoch:   500  |  train loss: 0.2849372864
Epoch:   600  |  train loss: 0.2356812418
Epoch:   700  |  train loss: 0.2004194319
Epoch:   800  |  train loss: 0.1723973155
Epoch:   900  |  train loss: 0.1503573298
Epoch:  1000  |  train loss: 0.1330533117
Epoch:  1100  |  train loss: 0.1187679678
Epoch:  1200  |  train loss: 0.1069699913
Epoch:  1300  |  train loss: 0.0965928555
Epoch:  1400  |  train loss: 0.0875394702
Epoch:  1500  |  train loss: 0.0801092997
Epoch:  1600  |  train loss: 0.0738578737
Epoch:  1700  |  train loss: 0.0684826389
Epoch:  1800  |  train loss: 0.0632943876
Epoch:  1900  |  train loss: 0.0597705491
Epoch:  2000  |  train loss: 0.0555831201
Processing class: 25
Using PCA transformation
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.7757604718
Epoch:   200  |  train loss: 0.5747726798
Epoch:   300  |  train loss: 0.4374197781
Epoch:   400  |  train loss: 0.3491423607
Epoch:   500  |  train loss: 0.2866019130
Epoch:   600  |  train loss: 0.2415368259
Epoch:   700  |  train loss: 0.2058186620
Epoch:   800  |  train loss: 0.1779486477
Epoch:   900  |  train loss: 0.1553672761
Epoch:  1000  |  train loss: 0.1370081484
Epoch:  1100  |  train loss: 0.1221216649
Epoch:  1200  |  train loss: 0.1091686606
Epoch:  1300  |  train loss: 0.0986545667
Epoch:  1400  |  train loss: 0.0900771707
Epoch:  1500  |  train loss: 0.0823690176
Epoch:  1600  |  train loss: 0.0760097235
Epoch:  1700  |  train loss: 0.0706138685
Epoch:  1800  |  train loss: 0.0655543327
Epoch:  1900  |  train loss: 0.0607671380
Epoch:  2000  |  train loss: 0.0574343741
Processing class: 26
Using PCA transformation
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.7732722878
Epoch:   200  |  train loss: 0.5892129660
Epoch:   300  |  train loss: 0.4507432580
Epoch:   400  |  train loss: 0.3589650631
Epoch:   500  |  train loss: 0.2965568781
Epoch:   600  |  train loss: 0.2511156112
Epoch:   700  |  train loss: 0.2156534046
Epoch:   800  |  train loss: 0.1879587263
Epoch:   900  |  train loss: 0.1664169341
Epoch:  1000  |  train loss: 0.1471706629
Epoch:  1100  |  train loss: 0.1314303219
Epoch:  1200  |  train loss: 0.1181986019
Epoch:  1300  |  train loss: 0.1072841823
Epoch:  1400  |  train loss: 0.0977422237
Epoch:  1500  |  train loss: 0.0897772983
Epoch:  1600  |  train loss: 0.0824643984
Epoch:  1700  |  train loss: 0.0764531359
Epoch:  1800  |  train loss: 0.0717476979
Epoch:  1900  |  train loss: 0.0666408911
Epoch:  2000  |  train loss: 0.0629735589
Processing class: 27
Using PCA transformation
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.7719328403
Epoch:   200  |  train loss: 0.5886369109
Epoch:   300  |  train loss: 0.4451665163
Epoch:   400  |  train loss: 0.3463828266
Epoch:   500  |  train loss: 0.2812208474
Epoch:   600  |  train loss: 0.2350878179
Epoch:   700  |  train loss: 0.2004707038
Epoch:   800  |  train loss: 0.1754676640
Epoch:   900  |  train loss: 0.1544589162
Epoch:  1000  |  train loss: 0.1380989432
Epoch:  1100  |  train loss: 0.1241806522
Epoch:  1200  |  train loss: 0.1125338987
Epoch:  1300  |  train loss: 0.1029013827
Epoch:  1400  |  train loss: 0.0939290717
Epoch:  1500  |  train loss: 0.0861531422
Epoch:  1600  |  train loss: 0.0797103405
Epoch:  1700  |  train loss: 0.0743397549
Epoch:  1800  |  train loss: 0.0691330284
Epoch:  1900  |  train loss: 0.0650154680
Epoch:  2000  |  train loss: 0.0610733315
Processing class: 28
Using PCA transformation
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.7670533538
Epoch:   200  |  train loss: 0.5904192567
Epoch:   300  |  train loss: 0.4395833135
Epoch:   400  |  train loss: 0.3491648853
Epoch:   500  |  train loss: 0.2866355181
Epoch:   600  |  train loss: 0.2403939694
Epoch:   700  |  train loss: 0.2042650580
Epoch:   800  |  train loss: 0.1762314975
Epoch:   900  |  train loss: 0.1538492948
Epoch:  1000  |  train loss: 0.1349504709
Epoch:  1100  |  train loss: 0.1206718668
Epoch:  1200  |  train loss: 0.1075637668
Epoch:  1300  |  train loss: 0.0973086536
Epoch:  1400  |  train loss: 0.0883910000
Epoch:  1500  |  train loss: 0.0806970909
Epoch:  1600  |  train loss: 0.0738585010
Epoch:  1700  |  train loss: 0.0679952860
Epoch:  1800  |  train loss: 0.0628587879
Epoch:  1900  |  train loss: 0.0584034912
Epoch:  2000  |  train loss: 0.0548908457
Processing class: 29
Using PCA transformation
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.7699818492
Epoch:   200  |  train loss: 0.5407741666
Epoch:   300  |  train loss: 0.4078249097
Epoch:   400  |  train loss: 0.3217741191
Epoch:   500  |  train loss: 0.2630612314
Epoch:   600  |  train loss: 0.2215826303
Epoch:   700  |  train loss: 0.1891369611
Epoch:   800  |  train loss: 0.1634999812
Epoch:   900  |  train loss: 0.1430941433
Epoch:  1000  |  train loss: 0.1271839678
Epoch:  1100  |  train loss: 0.1136559978
Epoch:  1200  |  train loss: 0.1034428164
Epoch:  1300  |  train loss: 0.0947523624
Epoch:  1400  |  train loss: 0.0870207325
Epoch:  1500  |  train loss: 0.0802305102
Epoch:  1600  |  train loss: 0.0749393448
Epoch:  1700  |  train loss: 0.0698001713
Epoch:  1800  |  train loss: 0.0652405024
Epoch:  1900  |  train loss: 0.0615777120
Epoch:  2000  |  train loss: 0.0577168860
Processing class: 30
Using PCA transformation
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.7796433926
Epoch:   200  |  train loss: 0.5966509938
Epoch:   300  |  train loss: 0.4660413802
Epoch:   400  |  train loss: 0.3792577147
Epoch:   500  |  train loss: 0.3172877431
Epoch:   600  |  train loss: 0.2696761012
Epoch:   700  |  train loss: 0.2329825580
Epoch:   800  |  train loss: 0.2043776333
Epoch:   900  |  train loss: 0.1812756002
Epoch:  1000  |  train loss: 0.1620175272
Epoch:  1100  |  train loss: 0.1464248329
Epoch:  1200  |  train loss: 0.1323160946
Epoch:  1300  |  train loss: 0.1205277458
Epoch:  1400  |  train loss: 0.1112503737
Epoch:  1500  |  train loss: 0.1022584349
Epoch:  1600  |  train loss: 0.0946536243
Epoch:  1700  |  train loss: 0.0885545537
Epoch:  1800  |  train loss: 0.0827312455
Epoch:  1900  |  train loss: 0.0777918532
Epoch:  2000  |  train loss: 0.0730905712
Processing class: 31
Using PCA transformation
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.7836290836
Epoch:   200  |  train loss: 0.6066511750
Epoch:   300  |  train loss: 0.4765351653
Epoch:   400  |  train loss: 0.3844188631
Epoch:   500  |  train loss: 0.3234590292
Epoch:   600  |  train loss: 0.2769747734
Epoch:   700  |  train loss: 0.2397134155
Epoch:   800  |  train loss: 0.2085828811
Epoch:   900  |  train loss: 0.1832028836
Epoch:  1000  |  train loss: 0.1635870844
Epoch:  1100  |  train loss: 0.1473159373
Epoch:  1200  |  train loss: 0.1324917942
Epoch:  1300  |  train loss: 0.1208266661
Epoch:  1400  |  train loss: 0.1107112661
Epoch:  1500  |  train loss: 0.1017761931
Epoch:  1600  |  train loss: 0.0935502827
Epoch:  1700  |  train loss: 0.0874320492
Epoch:  1800  |  train loss: 0.0809562415
Epoch:  1900  |  train loss: 0.0761668354
Epoch:  2000  |  train loss: 0.0711393014
Processing class: 32
Using PCA transformation
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.7520861030
Epoch:   200  |  train loss: 0.5735795617
Epoch:   300  |  train loss: 0.4409233153
Epoch:   400  |  train loss: 0.3573048890
Epoch:   500  |  train loss: 0.2946301997
Epoch:   600  |  train loss: 0.2456321061
Epoch:   700  |  train loss: 0.2086943924
Epoch:   800  |  train loss: 0.1803834528
Epoch:   900  |  train loss: 0.1583717287
Epoch:  1000  |  train loss: 0.1407120109
Epoch:  1100  |  train loss: 0.1258401513
Epoch:  1200  |  train loss: 0.1137515277
Epoch:  1300  |  train loss: 0.1028476492
Epoch:  1400  |  train loss: 0.0941229418
Epoch:  1500  |  train loss: 0.0860230684
Epoch:  1600  |  train loss: 0.0795803502
Epoch:  1700  |  train loss: 0.0736265391
Epoch:  1800  |  train loss: 0.0682886198
Epoch:  1900  |  train loss: 0.0645167880
Epoch:  2000  |  train loss: 0.0598847106
Processing class: 33
Using PCA transformation
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.7659833789
Epoch:   200  |  train loss: 0.5457681417
Epoch:   300  |  train loss: 0.4104697406
Epoch:   400  |  train loss: 0.3232960165
Epoch:   500  |  train loss: 0.2643753469
Epoch:   600  |  train loss: 0.2203979343
Epoch:   700  |  train loss: 0.1876437038
Epoch:   800  |  train loss: 0.1624130547
Epoch:   900  |  train loss: 0.1425290197
Epoch:  1000  |  train loss: 0.1267698750
Epoch:  1100  |  train loss: 0.1146008149
Epoch:  1200  |  train loss: 0.1033151597
Epoch:  1300  |  train loss: 0.0944624618
Epoch:  1400  |  train loss: 0.0865605876
Epoch:  1500  |  train loss: 0.0799043924
Epoch:  1600  |  train loss: 0.0740535393
Epoch:  1700  |  train loss: 0.0687125534
Epoch:  1800  |  train loss: 0.0643358678
Epoch:  1900  |  train loss: 0.0602187410
Epoch:  2000  |  train loss: 0.0565756015
Processing class: 34
Using PCA transformation
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.7726967454
Epoch:   200  |  train loss: 0.5717608690
Epoch:   300  |  train loss: 0.4359838188
Epoch:   400  |  train loss: 0.3438960671
Epoch:   500  |  train loss: 0.2811064661
Epoch:   600  |  train loss: 0.2354092568
Epoch:   700  |  train loss: 0.2002321273
Epoch:   800  |  train loss: 0.1737851024
Epoch:   900  |  train loss: 0.1530574292
Epoch:  1000  |  train loss: 0.1361048311
Epoch:  1100  |  train loss: 0.1221871719
Epoch:  1200  |  train loss: 0.1103185043
Epoch:  1300  |  train loss: 0.0995409265
Epoch:  1400  |  train loss: 0.0912955612
Epoch:  1500  |  train loss: 0.0840247840
Epoch:  1600  |  train loss: 0.0779073685
Epoch:  1700  |  train loss: 0.0725200310
Epoch:  1800  |  train loss: 0.0680366009
Epoch:  1900  |  train loss: 0.0637601495
Epoch:  2000  |  train loss: 0.0603994928
Processing class: 35
Using PCA transformation
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.7619546175
Epoch:   200  |  train loss: 0.5611445904
Epoch:   300  |  train loss: 0.4270395517
Epoch:   400  |  train loss: 0.3388737798
Epoch:   500  |  train loss: 0.2785912812
Epoch:   600  |  train loss: 0.2357993990
Epoch:   700  |  train loss: 0.2035338521
Epoch:   800  |  train loss: 0.1787967503
Epoch:   900  |  train loss: 0.1566368341
/home/z1165703/miniconda3/lib/python3.11/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
Epoch:  1000  |  train loss: 0.1395772487
Epoch:  1100  |  train loss: 0.1255541027
Epoch:  1200  |  train loss: 0.1127565756
Epoch:  1300  |  train loss: 0.1027343899
Epoch:  1400  |  train loss: 0.0935827672
Epoch:  1500  |  train loss: 0.0862894014
Epoch:  1600  |  train loss: 0.0798725590
Epoch:  1700  |  train loss: 0.0733882695
Epoch:  1800  |  train loss: 0.0690434307
Epoch:  1900  |  train loss: 0.0640802860
Epoch:  2000  |  train loss: 0.0601392537
Processing class: 36
Using PCA transformation
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.7853812337
Epoch:   200  |  train loss: 0.5854105353
Epoch:   300  |  train loss: 0.4442807972
Epoch:   400  |  train loss: 0.3472615421
Epoch:   500  |  train loss: 0.2788514853
Epoch:   600  |  train loss: 0.2329196692
Epoch:   700  |  train loss: 0.1986690104
Epoch:   800  |  train loss: 0.1730123967
Epoch:   900  |  train loss: 0.1518483102
Epoch:  1000  |  train loss: 0.1354760319
Epoch:  1100  |  train loss: 0.1211128563
Epoch:  1200  |  train loss: 0.1092945531
Epoch:  1300  |  train loss: 0.0989368081
Epoch:  1400  |  train loss: 0.0900868565
Epoch:  1500  |  train loss: 0.0820303068
Epoch:  1600  |  train loss: 0.0757027596
Epoch:  1700  |  train loss: 0.0707051143
Epoch:  1800  |  train loss: 0.0656412855
Epoch:  1900  |  train loss: 0.0608540781
Epoch:  2000  |  train loss: 0.0573904596
Processing class: 37
Using PCA transformation
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.7878363848
Epoch:   200  |  train loss: 0.5574716687
Epoch:   300  |  train loss: 0.4188121319
Epoch:   400  |  train loss: 0.3325215042
Epoch:   500  |  train loss: 0.2716162324
Epoch:   600  |  train loss: 0.2273471743
Epoch:   700  |  train loss: 0.1947059333
Epoch:   800  |  train loss: 0.1694184631
Epoch:   900  |  train loss: 0.1487422258
Epoch:  1000  |  train loss: 0.1312474459
Epoch:  1100  |  train loss: 0.1173733652
Epoch:  1200  |  train loss: 0.1061520308
Epoch:  1300  |  train loss: 0.0960223287
Epoch:  1400  |  train loss: 0.0887089521
Epoch:  1500  |  train loss: 0.0808382392
Epoch:  1600  |  train loss: 0.0750822768
Epoch:  1700  |  train loss: 0.0703458294
Epoch:  1800  |  train loss: 0.0651175618
Epoch:  1900  |  train loss: 0.0614685409
Epoch:  2000  |  train loss: 0.0578088775
Processing class: 38
Using PCA transformation
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.7712665439
Epoch:   200  |  train loss: 0.5820465684
Epoch:   300  |  train loss: 0.4452302516
Epoch:   400  |  train loss: 0.3523817480
Epoch:   500  |  train loss: 0.2878395140
Epoch:   600  |  train loss: 0.2399503559
Epoch:   700  |  train loss: 0.2040106624
Epoch:   800  |  train loss: 0.1764781266
Epoch:   900  |  train loss: 0.1544726253
Epoch:  1000  |  train loss: 0.1367358327
Epoch:  1100  |  train loss: 0.1227025375
Epoch:  1200  |  train loss: 0.1102791294
Epoch:  1300  |  train loss: 0.1000030935
Epoch:  1400  |  train loss: 0.0911083639
Epoch:  1500  |  train loss: 0.0832130581
Epoch:  1600  |  train loss: 0.0765560448
Epoch:  1700  |  train loss: 0.0712258711
Epoch:  1800  |  train loss: 0.0658228129
Epoch:  1900  |  train loss: 0.0618916206
Epoch:  2000  |  train loss: 0.0580723263
Processing class: 39
Using PCA transformation
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.7739563107
Epoch:   200  |  train loss: 0.5867113829
Epoch:   300  |  train loss: 0.4473101795
Epoch:   400  |  train loss: 0.3548842907
Epoch:   500  |  train loss: 0.2944080472
Epoch:   600  |  train loss: 0.2500643939
Epoch:   700  |  train loss: 0.2163428396
Epoch:   800  |  train loss: 0.1895100325
Epoch:   900  |  train loss: 0.1686935693
Epoch:  1000  |  train loss: 0.1509888768
Epoch:  1100  |  train loss: 0.1356243163
Epoch:  1200  |  train loss: 0.1228164792
Epoch:  1300  |  train loss: 0.1123082846
Epoch:  1400  |  train loss: 0.1018372700
Epoch:  1500  |  train loss: 0.0934210747
Epoch:  1600  |  train loss: 0.0865616098
Epoch:  1700  |  train loss: 0.0802945867
Epoch:  1800  |  train loss: 0.0745992184
Epoch:  1900  |  train loss: 0.0694186792
Epoch:  2000  |  train loss: 0.0659077778
Processing class: 40
Using PCA transformation
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.7653714180
Epoch:   200  |  train loss: 0.5611187220
Epoch:   300  |  train loss: 0.4296545863
Epoch:   400  |  train loss: 0.3415520489
Epoch:   500  |  train loss: 0.2773873508
Epoch:   600  |  train loss: 0.2290533036
Epoch:   700  |  train loss: 0.1941475660
Epoch:   800  |  train loss: 0.1665831000
Epoch:   900  |  train loss: 0.1456878126
Epoch:  1000  |  train loss: 0.1281295314
Epoch:  1100  |  train loss: 0.1137835860
Epoch:  1200  |  train loss: 0.1022651911
Epoch:  1300  |  train loss: 0.0929296017
Epoch:  1400  |  train loss: 0.0849297807
Epoch:  1500  |  train loss: 0.0778149754
Epoch:  1600  |  train loss: 0.0715812907
Epoch:  1700  |  train loss: 0.0662261143
Epoch:  1800  |  train loss: 0.0617140681
Epoch:  1900  |  train loss: 0.0579201870
Epoch:  2000  |  train loss: 0.0539715677
Processing class: 41
Using PCA transformation
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.7674195290
Epoch:   200  |  train loss: 0.5631959200
Epoch:   300  |  train loss: 0.4291965425
Epoch:   400  |  train loss: 0.3400004447
Epoch:   500  |  train loss: 0.2766505897
Epoch:   600  |  train loss: 0.2320201695
Epoch:   700  |  train loss: 0.1959346473
Epoch:   800  |  train loss: 0.1679721087
Epoch:   900  |  train loss: 0.1469273806
Epoch:  1000  |  train loss: 0.1295521647
Epoch:  1100  |  train loss: 0.1158503383
Epoch:  1200  |  train loss: 0.1035599098
Epoch:  1300  |  train loss: 0.0948273852
Epoch:  1400  |  train loss: 0.0863550037
Epoch:  1500  |  train loss: 0.0794581413
Epoch:  1600  |  train loss: 0.0734965324
Epoch:  1700  |  train loss: 0.0679797217
Epoch:  1800  |  train loss: 0.0638857335
Epoch:  1900  |  train loss: 0.0595629111
Epoch:  2000  |  train loss: 0.0561958060
Processing class: 42
Using PCA transformation
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.7794677377
Epoch:   200  |  train loss: 0.5836548090
Epoch:   300  |  train loss: 0.4543820083
Epoch:   400  |  train loss: 0.3689877987
Epoch:   500  |  train loss: 0.3070038676
Epoch:   600  |  train loss: 0.2619261742
Epoch:   700  |  train loss: 0.2263385981
Epoch:   800  |  train loss: 0.1992294490
Epoch:   900  |  train loss: 0.1769188941
Epoch:  1000  |  train loss: 0.1584922671
Epoch:  1100  |  train loss: 0.1438371688
Epoch:  1200  |  train loss: 0.1319349021
Epoch:  1300  |  train loss: 0.1202929646
Epoch:  1400  |  train loss: 0.1116519138
Epoch:  1500  |  train loss: 0.1034869671
Epoch:  1600  |  train loss: 0.0957308620
Epoch:  1700  |  train loss: 0.0897870868
Epoch:  1800  |  train loss: 0.0840042695
Epoch:  1900  |  train loss: 0.0789765760
Epoch:  2000  |  train loss: 0.0743005261
Processing class: 43
Using PCA transformation
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.7755126953
Epoch:   200  |  train loss: 0.5986796618
Epoch:   300  |  train loss: 0.4450136006
Epoch:   400  |  train loss: 0.3489641249
Epoch:   500  |  train loss: 0.2838346601
Epoch:   600  |  train loss: 0.2401264131
Epoch:   700  |  train loss: 0.2062150210
Epoch:   800  |  train loss: 0.1802408010
Epoch:   900  |  train loss: 0.1590921581
Epoch:  1000  |  train loss: 0.1414795637
Epoch:  1100  |  train loss: 0.1276278213
Epoch:  1200  |  train loss: 0.1154539526
Epoch:  1300  |  train loss: 0.1051931009
Epoch:  1400  |  train loss: 0.0962679848
Epoch:  1500  |  train loss: 0.0891270354
Epoch:  1600  |  train loss: 0.0820458278
Epoch:  1700  |  train loss: 0.0766274616
Epoch:  1800  |  train loss: 0.0714368150
Epoch:  1900  |  train loss: 0.0675022438
Epoch:  2000  |  train loss: 0.0632625714
Processing class: 44
Using PCA transformation
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.7700443387
Epoch:   200  |  train loss: 0.5680511713
Epoch:   300  |  train loss: 0.4306738853
Epoch:   400  |  train loss: 0.3452757955
Epoch:   500  |  train loss: 0.2854067206
Epoch:   600  |  train loss: 0.2386133552
Epoch:   700  |  train loss: 0.2028365344
Epoch:   800  |  train loss: 0.1747831017
Epoch:   900  |  train loss: 0.1532577902
Epoch:  1000  |  train loss: 0.1346442908
Epoch:  1100  |  train loss: 0.1206268162
Epoch:  1200  |  train loss: 0.1091148347
Epoch:  1300  |  train loss: 0.0990665212
Epoch:  1400  |  train loss: 0.0907120019
Epoch:  1500  |  train loss: 0.0831233785
Epoch:  1600  |  train loss: 0.0773828194
Epoch:  1700  |  train loss: 0.0712473527
Epoch:  1800  |  train loss: 0.0667435452
Epoch:  1900  |  train loss: 0.0628041364
Epoch:  2000  |  train loss: 0.0588867739
Processing class: 45
Using PCA transformation
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.7693042159
Epoch:   200  |  train loss: 0.5972024679
Epoch:   300  |  train loss: 0.4550503552
Epoch:   400  |  train loss: 0.3644428372
Epoch:   500  |  train loss: 0.3011284888
Epoch:   600  |  train loss: 0.2505515307
Epoch:   700  |  train loss: 0.2144685864
Epoch:   800  |  train loss: 0.1846704572
Epoch:   900  |  train loss: 0.1612289727
Epoch:  1000  |  train loss: 0.1425693065
Epoch:  1100  |  train loss: 0.1274162263
Epoch:  1200  |  train loss: 0.1147710681
Epoch:  1300  |  train loss: 0.1039406627
Epoch:  1400  |  train loss: 0.0946621433
Epoch:  1500  |  train loss: 0.0875519857
Epoch:  1600  |  train loss: 0.0810888499
Epoch:  1700  |  train loss: 0.0748907849
Epoch:  1800  |  train loss: 0.0703556746
Epoch:  1900  |  train loss: 0.0653025791
Epoch:  2000  |  train loss: 0.0615225166
Processing class: 46
Using PCA transformation
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.7696297884
Epoch:   200  |  train loss: 0.5768107176
Epoch:   300  |  train loss: 0.4515120089
Epoch:   400  |  train loss: 0.3654848099
Epoch:   500  |  train loss: 0.3059492707
Epoch:   600  |  train loss: 0.2594122231
Epoch:   700  |  train loss: 0.2226115137
Epoch:   800  |  train loss: 0.1935234904
Epoch:   900  |  train loss: 0.1686087400
Epoch:  1000  |  train loss: 0.1491854668
Epoch:  1100  |  train loss: 0.1325776398
Epoch:  1200  |  train loss: 0.1198323280
Epoch:  1300  |  train loss: 0.1083571300
Epoch:  1400  |  train loss: 0.0987454548
Epoch:  1500  |  train loss: 0.0903911293
Epoch:  1600  |  train loss: 0.0840241760
Epoch:  1700  |  train loss: 0.0775077611
Epoch:  1800  |  train loss: 0.0721623302
Epoch:  1900  |  train loss: 0.0678556696
Epoch:  2000  |  train loss: 0.0638887376
Processing class: 47
Using PCA transformation
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.7789519429
Epoch:   200  |  train loss: 0.5744650722
Epoch:   300  |  train loss: 0.4452386737
Epoch:   400  |  train loss: 0.3603341818
Epoch:   500  |  train loss: 0.2997532785
Epoch:   600  |  train loss: 0.2545071989
Epoch:   700  |  train loss: 0.2181782275
Epoch:   800  |  train loss: 0.1914976686
Epoch:   900  |  train loss: 0.1693947554
Epoch:  1000  |  train loss: 0.1510320276
Epoch:  1100  |  train loss: 0.1370455503
Epoch:  1200  |  train loss: 0.1247065246
Epoch:  1300  |  train loss: 0.1148122400
Epoch:  1400  |  train loss: 0.1061753735
Epoch:  1500  |  train loss: 0.0981266618
Epoch:  1600  |  train loss: 0.0912712038
Epoch:  1700  |  train loss: 0.0852969930
Epoch:  1800  |  train loss: 0.0801693380
Epoch:  1900  |  train loss: 0.0755140424
Epoch:  2000  |  train loss: 0.0709340513
Processing class: 48
Using PCA transformation
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.7616931796
Epoch:   200  |  train loss: 0.5426036954
Epoch:   300  |  train loss: 0.4097345829
Epoch:   400  |  train loss: 0.3257841587
Epoch:   500  |  train loss: 0.2662924320
Epoch:   600  |  train loss: 0.2224978238
Epoch:   700  |  train loss: 0.1896353364
Epoch:   800  |  train loss: 0.1644050002
Epoch:   900  |  train loss: 0.1438736439
Epoch:  1000  |  train loss: 0.1272395134
Epoch:  1100  |  train loss: 0.1137395322
Epoch:  1200  |  train loss: 0.1022686139
Epoch:  1300  |  train loss: 0.0924714580
Epoch:  1400  |  train loss: 0.0847237304
Epoch:  1500  |  train loss: 0.0774280429
Epoch:  1600  |  train loss: 0.0713524967
Epoch:  1700  |  train loss: 0.0664673626
Epoch:  1800  |  train loss: 0.0621694334
Epoch:  1900  |  train loss: 0.0585778989
Epoch:  2000  |  train loss: 0.0549244195
Processing class: 49
Using PCA transformation
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.7767934322
Epoch:   200  |  train loss: 0.5813546300
Epoch:   300  |  train loss: 0.4515888333
Epoch:   400  |  train loss: 0.3592783451
Epoch:   500  |  train loss: 0.2995808423
Epoch:   600  |  train loss: 0.2531935871
Epoch:   700  |  train loss: 0.2164384723
Epoch:   800  |  train loss: 0.1881306976
Epoch:   900  |  train loss: 0.1656378478
Epoch:  1000  |  train loss: 0.1473231852
Epoch:  1100  |  train loss: 0.1320984751
Epoch:  1200  |  train loss: 0.1188697845
Epoch:  1300  |  train loss: 0.1083588302
Epoch:  1400  |  train loss: 0.0985148937
Epoch:  1500  |  train loss: 0.0909105688
Epoch:  1600  |  train loss: 0.0841702759
Epoch:  1700  |  train loss: 0.0780236989
Epoch:  1800  |  train loss: 0.0725945488
Epoch:  1900  |  train loss: 0.0681880489
Epoch:  2000  |  train loss: 0.0636627130
Clasifying using reconstruction function cost
2024-03-17 22:41:10,383 [trainer.py] => CNN: {'total': 83.44, '00-09': 87.7, '10-19': 79.6, '20-29': 84.9, '30-39': 81.0, '40-49': 84.0, 'old': 0, 'new': 83.44}
2024-03-17 22:41:10,387 [trainer.py] => No NME accuracy
2024-03-17 22:41:10,387 [trainer.py] => FeCAM: {'total': 57.54, '00-09': 59.8, '10-19': 57.3, '20-29': 55.9, '30-39': 53.3, '40-49': 61.4, 'old': 0, 'new': 57.54}
2024-03-17 22:41:10,387 [trainer.py] => CNN top1 curve: [83.44]
2024-03-17 22:41:10,387 [trainer.py] => CNN top5 curve: [96.5]
2024-03-17 22:41:10,388 [trainer.py] => FeCAM top1 curve: [57.54]
2024-03-17 22:41:10,388 [trainer.py] => FeCAM top5 curve: [75.48]

2024-03-17 22:41:10,407 [fecam.py] => Learning on 50-60
/home/z1165703/miniconda3/lib/python3.11/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
/home/z1165703/miniconda3/lib/python3.11/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
/home/z1165703/miniconda3/lib/python3.11/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
Processing class: 50
Using PCA transformation
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.7576901317
Epoch:   200  |  train loss: 0.5276906610
Epoch:   300  |  train loss: 0.4017543733
Epoch:   400  |  train loss: 0.3129103720
Epoch:   500  |  train loss: 0.2536303610
Epoch:   600  |  train loss: 0.2120598793
Epoch:   700  |  train loss: 0.1808947086
Epoch:   800  |  train loss: 0.1571134269
Epoch:   900  |  train loss: 0.1380389720
Epoch:  1000  |  train loss: 0.1226932049
Epoch:  1100  |  train loss: 0.1097492635
Epoch:  1200  |  train loss: 0.0991178423
Epoch:  1300  |  train loss: 0.0902477965
Epoch:  1400  |  train loss: 0.0826833859
Epoch:  1500  |  train loss: 0.0762186363
Epoch:  1600  |  train loss: 0.0708400130
Epoch:  1700  |  train loss: 0.0662003964
Epoch:  1800  |  train loss: 0.0618179671
Epoch:  1900  |  train loss: 0.0582347587
Epoch:  2000  |  train loss: 0.0547857210
Processing class: 51
Using PCA transformation
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.7733748436
Epoch:   200  |  train loss: 0.5325865507
Epoch:   300  |  train loss: 0.3887789488
Epoch:   400  |  train loss: 0.3025853395
Epoch:   500  |  train loss: 0.2430961788
Epoch:   600  |  train loss: 0.2032083690
Epoch:   700  |  train loss: 0.1744598657
Epoch:   800  |  train loss: 0.1523440123
Epoch:   900  |  train loss: 0.1353892684
Epoch:  1000  |  train loss: 0.1210295022
Epoch:  1100  |  train loss: 0.1089405909
Epoch:  1200  |  train loss: 0.0985012248
Epoch:  1300  |  train loss: 0.0901552886
Epoch:  1400  |  train loss: 0.0829715133
Epoch:  1500  |  train loss: 0.0764800027
Epoch:  1600  |  train loss: 0.0710428387
Epoch:  1700  |  train loss: 0.0667173281
Epoch:  1800  |  train loss: 0.0622147292
Epoch:  1900  |  train loss: 0.0587696314
Epoch:  2000  |  train loss: 0.0558679841
Processing class: 52
Using PCA transformation
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.7322975636
Epoch:   200  |  train loss: 0.5339284539
Epoch:   300  |  train loss: 0.3915265203
Epoch:   400  |  train loss: 0.2940867603
Epoch:   500  |  train loss: 0.2280679971
Epoch:   600  |  train loss: 0.1867509276
Epoch:   700  |  train loss: 0.1584862858
Epoch:   800  |  train loss: 0.1366073191
Epoch:   900  |  train loss: 0.1207396835
Epoch:  1000  |  train loss: 0.1075732529
Epoch:  1100  |  train loss: 0.0965985805
Epoch:  1200  |  train loss: 0.0884605020
Epoch:  1300  |  train loss: 0.0812736824
Epoch:  1400  |  train loss: 0.0745306313
Epoch:  1500  |  train loss: 0.0689994603
Epoch:  1600  |  train loss: 0.0647126466
Epoch:  1700  |  train loss: 0.0604163401
Epoch:  1800  |  train loss: 0.0566002622
Epoch:  1900  |  train loss: 0.0534801044
Epoch:  2000  |  train loss: 0.0504021086
Processing class: 53
Using PCA transformation
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.7599822521
Epoch:   200  |  train loss: 0.5357607841
Epoch:   300  |  train loss: 0.3976725817
Epoch:   400  |  train loss: 0.3074499965
Epoch:   500  |  train loss: 0.2478059828
Epoch:   600  |  train loss: 0.2042676359
Epoch:   700  |  train loss: 0.1723484129
Epoch:   800  |  train loss: 0.1479653120
Epoch:   900  |  train loss: 0.1298082903
Epoch:  1000  |  train loss: 0.1148104236
Epoch:  1100  |  train loss: 0.1034787104
Epoch:  1200  |  train loss: 0.0934002995
Epoch:  1300  |  train loss: 0.0851521999
Epoch:  1400  |  train loss: 0.0785223603
Epoch:  1500  |  train loss: 0.0726607963
Epoch:  1600  |  train loss: 0.0681382596
Epoch:  1700  |  train loss: 0.0636049822
Epoch:  1800  |  train loss: 0.0598223716
Epoch:  1900  |  train loss: 0.0565404475
Epoch:  2000  |  train loss: 0.0534910657
Processing class: 54
Using PCA transformation
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.7492120147
Epoch:   200  |  train loss: 0.5405066252
Epoch:   300  |  train loss: 0.4024833083
Epoch:   400  |  train loss: 0.3127527833
Epoch:   500  |  train loss: 0.2527015865
Epoch:   600  |  train loss: 0.2118357003
Epoch:   700  |  train loss: 0.1809255868
Epoch:   800  |  train loss: 0.1561389446
Epoch:   900  |  train loss: 0.1363777280
Epoch:  1000  |  train loss: 0.1211007506
Epoch:  1100  |  train loss: 0.1087115765
Epoch:  1200  |  train loss: 0.0986932844
Epoch:  1300  |  train loss: 0.0896833673
Epoch:  1400  |  train loss: 0.0828224197
Epoch:  1500  |  train loss: 0.0766484886
Epoch:  1600  |  train loss: 0.0708479241
Epoch:  1700  |  train loss: 0.0661096141
Epoch:  1800  |  train loss: 0.0628592089
Epoch:  1900  |  train loss: 0.0594452783
Epoch:  2000  |  train loss: 0.0559303924
Processing class: 55
Using PCA transformation
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.7685135603
Epoch:   200  |  train loss: 0.5859150410
Epoch:   300  |  train loss: 0.4351044178
Epoch:   400  |  train loss: 0.3374319255
Epoch:   500  |  train loss: 0.2724450588
Epoch:   600  |  train loss: 0.2254314721
Epoch:   700  |  train loss: 0.1925814033
Epoch:   800  |  train loss: 0.1685461342
Epoch:   900  |  train loss: 0.1488434106
Epoch:  1000  |  train loss: 0.1331700951
Epoch:  1100  |  train loss: 0.1205698654
Epoch:  1200  |  train loss: 0.1103125751
Epoch:  1300  |  train loss: 0.1013615698
Epoch:  1400  |  train loss: 0.0936295226
Epoch:  1500  |  train loss: 0.0869152576
Epoch:  1600  |  train loss: 0.0813388556
Epoch:  1700  |  train loss: 0.0759093687
Epoch:  1800  |  train loss: 0.0718782529
Epoch:  1900  |  train loss: 0.0676344067
Epoch:  2000  |  train loss: 0.0643177688
Processing class: 56
Using PCA transformation
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.7686565995
Epoch:   200  |  train loss: 0.5485621214
Epoch:   300  |  train loss: 0.4123266518
Epoch:   400  |  train loss: 0.3218849480
Epoch:   500  |  train loss: 0.2620077968
Epoch:   600  |  train loss: 0.2167402178
Epoch:   700  |  train loss: 0.1835567415
Epoch:   800  |  train loss: 0.1581909597
Epoch:   900  |  train loss: 0.1370023340
Epoch:  1000  |  train loss: 0.1214101613
Epoch:  1100  |  train loss: 0.1090951160
Epoch:  1200  |  train loss: 0.0985274062
Epoch:  1300  |  train loss: 0.0900822461
Epoch:  1400  |  train loss: 0.0826552495
Epoch:  1500  |  train loss: 0.0760161787
Epoch:  1600  |  train loss: 0.0707740158
Epoch:  1700  |  train loss: 0.0657305866
Epoch:  1800  |  train loss: 0.0613388740
Epoch:  1900  |  train loss: 0.0573732041
Epoch:  2000  |  train loss: 0.0544659771
Processing class: 57
Using PCA transformation
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.7462819815
Epoch:   200  |  train loss: 0.5146417201
Epoch:   300  |  train loss: 0.3549200475
Epoch:   400  |  train loss: 0.2695041776
Epoch:   500  |  train loss: 0.2195040226
Epoch:   600  |  train loss: 0.1846174568
Epoch:   700  |  train loss: 0.1597677380
Epoch:   800  |  train loss: 0.1403640717
Epoch:   900  |  train loss: 0.1241828591
Epoch:  1000  |  train loss: 0.1111079663
Epoch:  1100  |  train loss: 0.1005382255
Epoch:  1200  |  train loss: 0.0913779408
Epoch:  1300  |  train loss: 0.0838319898
Epoch:  1400  |  train loss: 0.0771511301
Epoch:  1500  |  train loss: 0.0719308794
Epoch:  1600  |  train loss: 0.0671171054
Epoch:  1700  |  train loss: 0.0629214577
Epoch:  1800  |  train loss: 0.0595499597
Epoch:  1900  |  train loss: 0.0555760637
Epoch:  2000  |  train loss: 0.0529028721
Processing class: 58
Using PCA transformation
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.7623500705
Epoch:   200  |  train loss: 0.5252538502
Epoch:   300  |  train loss: 0.3778505445
Epoch:   400  |  train loss: 0.2924190700
Epoch:   500  |  train loss: 0.2340981096
Epoch:   600  |  train loss: 0.1937804043
Epoch:   700  |  train loss: 0.1645627648
Epoch:   800  |  train loss: 0.1435265124
Epoch:   900  |  train loss: 0.1264621928
Epoch:  1000  |  train loss: 0.1131483361
Epoch:  1100  |  train loss: 0.1020220354
Epoch:  1200  |  train loss: 0.0926868126
Epoch:  1300  |  train loss: 0.0848180935
Epoch:  1400  |  train loss: 0.0777506113
Epoch:  1500  |  train loss: 0.0725354180
Epoch:  1600  |  train loss: 0.0670886859
Epoch:  1700  |  train loss: 0.0632650033
Epoch:  1800  |  train loss: 0.0592877761
Epoch:  1900  |  train loss: 0.0563126646
Epoch:  2000  |  train loss: 0.0528229475
Processing class: 59
Using PCA transformation
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.7680909991
Epoch:   200  |  train loss: 0.5612986445
Epoch:   300  |  train loss: 0.4151114643
Epoch:   400  |  train loss: 0.3176375091
Epoch:   500  |  train loss: 0.2581971169
Epoch:   600  |  train loss: 0.2184964836
Epoch:   700  |  train loss: 0.1892812699
Epoch:   800  |  train loss: 0.1656097144
Epoch:   900  |  train loss: 0.1465974033
Epoch:  1000  |  train loss: 0.1306007832
Epoch:  1100  |  train loss: 0.1180007011
Epoch:  1200  |  train loss: 0.1071030796
Epoch:  1300  |  train loss: 0.0979216650
Epoch:  1400  |  train loss: 0.0896776274
Epoch:  1500  |  train loss: 0.0827489600
Epoch:  1600  |  train loss: 0.0773193344
Epoch:  1700  |  train loss: 0.0718877509
Epoch:  1800  |  train loss: 0.0672267750
Epoch:  1900  |  train loss: 0.0632232189
Epoch:  2000  |  train loss: 0.0596237496
Clasifying using reconstruction function cost
2024-03-17 22:54:15,677 [trainer.py] => CNN: {'total': 71.75, '00-09': 82.2, '10-19': 72.8, '20-29': 78.5, '30-39': 74.9, '40-49': 68.4, '50-59': 53.7, 'old': 75.36, 'new': 53.7}
2024-03-17 22:54:15,679 [trainer.py] => No NME accuracy
2024-03-17 22:54:15,679 [trainer.py] => FeCAM: {'total': 47.05, '00-09': 57.3, '10-19': 53.6, '20-29': 55.1, '30-39': 50.8, '40-49': 55.0, '50-59': 10.5, 'old': 54.36, 'new': 10.5}
2024-03-17 22:54:15,679 [trainer.py] => CNN top1 curve: [83.44, 71.75]
2024-03-17 22:54:15,679 [trainer.py] => CNN top5 curve: [96.5, 89.6]
2024-03-17 22:54:15,679 [trainer.py] => FeCAM top1 curve: [57.54, 47.05]
2024-03-17 22:54:15,679 [trainer.py] => FeCAM top5 curve: [75.48, 66.38]

2024-03-17 22:54:15,693 [fecam.py] => Learning on 60-70
/home/z1165703/miniconda3/lib/python3.11/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
/home/z1165703/miniconda3/lib/python3.11/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
/home/z1165703/miniconda3/lib/python3.11/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
Processing class: 60
Using PCA transformation
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.7608264327
Epoch:   200  |  train loss: 0.5222487509
Epoch:   300  |  train loss: 0.3807627022
Epoch:   400  |  train loss: 0.2960004807
Epoch:   500  |  train loss: 0.2434253097
Epoch:   600  |  train loss: 0.2040584862
Epoch:   700  |  train loss: 0.1761764199
Epoch:   800  |  train loss: 0.1538375914
Epoch:   900  |  train loss: 0.1358652353
Epoch:  1000  |  train loss: 0.1213951230
Epoch:  1100  |  train loss: 0.1096168295
Epoch:  1200  |  train loss: 0.0991826639
Epoch:  1300  |  train loss: 0.0909494072
Epoch:  1400  |  train loss: 0.0833347812
Epoch:  1500  |  train loss: 0.0768368140
Epoch:  1600  |  train loss: 0.0714756638
Epoch:  1700  |  train loss: 0.0661842227
Epoch:  1800  |  train loss: 0.0622807376
Epoch:  1900  |  train loss: 0.0588333979
Epoch:  2000  |  train loss: 0.0554180525
Processing class: 61
Using PCA transformation
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.7363371968
Epoch:   200  |  train loss: 0.5113711715
Epoch:   300  |  train loss: 0.3692496061
Epoch:   400  |  train loss: 0.2859895587
Epoch:   500  |  train loss: 0.2319649220
Epoch:   600  |  train loss: 0.1922566444
Epoch:   700  |  train loss: 0.1621449381
Epoch:   800  |  train loss: 0.1395384103
Epoch:   900  |  train loss: 0.1222298577
Epoch:  1000  |  train loss: 0.1087036252
Epoch:  1100  |  train loss: 0.0979596645
Epoch:  1200  |  train loss: 0.0887933195
Epoch:  1300  |  train loss: 0.0815175861
Epoch:  1400  |  train loss: 0.0755048752
Epoch:  1500  |  train loss: 0.0689499021
Epoch:  1600  |  train loss: 0.0645961747
Epoch:  1700  |  train loss: 0.0606870808
Epoch:  1800  |  train loss: 0.0569613688
Epoch:  1900  |  train loss: 0.0536836736
Epoch:  2000  |  train loss: 0.0506303616
Processing class: 62
Using PCA transformation
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.7575670600
Epoch:   200  |  train loss: 0.5230550766
Epoch:   300  |  train loss: 0.3943999648
Epoch:   400  |  train loss: 0.3104058802
Epoch:   500  |  train loss: 0.2518676639
Epoch:   600  |  train loss: 0.2112806469
Epoch:   700  |  train loss: 0.1810885996
Epoch:   800  |  train loss: 0.1577689379
Epoch:   900  |  train loss: 0.1394075841
Epoch:  1000  |  train loss: 0.1243688375
Epoch:  1100  |  train loss: 0.1126735970
Epoch:  1200  |  train loss: 0.1022921860
Epoch:  1300  |  train loss: 0.0943029433
Epoch:  1400  |  train loss: 0.0865768179
Epoch:  1500  |  train loss: 0.0799664855
Epoch:  1600  |  train loss: 0.0746304229
Epoch:  1700  |  train loss: 0.0695270509
Epoch:  1800  |  train loss: 0.0658851102
Epoch:  1900  |  train loss: 0.0612953067
Epoch:  2000  |  train loss: 0.0580189079
Processing class: 63
Using PCA transformation
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.7620588899
Epoch:   200  |  train loss: 0.5209289253
Epoch:   300  |  train loss: 0.3931753695
Epoch:   400  |  train loss: 0.3146589577
Epoch:   500  |  train loss: 0.2591449767
Epoch:   600  |  train loss: 0.2166372031
Epoch:   700  |  train loss: 0.1857585311
Epoch:   800  |  train loss: 0.1611323446
Epoch:   900  |  train loss: 0.1426418394
Epoch:  1000  |  train loss: 0.1266668111
Epoch:  1100  |  train loss: 0.1140377268
Epoch:  1200  |  train loss: 0.1028192163
Epoch:  1300  |  train loss: 0.0938944831
Epoch:  1400  |  train loss: 0.0862632275
Epoch:  1500  |  train loss: 0.0788929209
Epoch:  1600  |  train loss: 0.0735719010
Epoch:  1700  |  train loss: 0.0687785521
Epoch:  1800  |  train loss: 0.0639624462
Epoch:  1900  |  train loss: 0.0602349758
Epoch:  2000  |  train loss: 0.0563035563
Processing class: 64
Using PCA transformation
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.7510892272
Epoch:   200  |  train loss: 0.5092358947
Epoch:   300  |  train loss: 0.3662194014
Epoch:   400  |  train loss: 0.2827759862
Epoch:   500  |  train loss: 0.2264962733
Epoch:   600  |  train loss: 0.1893992603
Epoch:   700  |  train loss: 0.1628968269
Epoch:   800  |  train loss: 0.1411517501
Epoch:   900  |  train loss: 0.1252181858
Epoch:  1000  |  train loss: 0.1119923607
Epoch:  1100  |  train loss: 0.1009442985
Epoch:  1200  |  train loss: 0.0917588338
Epoch:  1300  |  train loss: 0.0834118739
Epoch:  1400  |  train loss: 0.0767774388
Epoch:  1500  |  train loss: 0.0702902928
Epoch:  1600  |  train loss: 0.0655365750
Epoch:  1700  |  train loss: 0.0612646736
Epoch:  1800  |  train loss: 0.0571269095
Epoch:  1900  |  train loss: 0.0538230091
Epoch:  2000  |  train loss: 0.0507916033
Processing class: 65
Using PCA transformation
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.7604705572
Epoch:   200  |  train loss: 0.5290841103
Epoch:   300  |  train loss: 0.3912329018
Epoch:   400  |  train loss: 0.3059847951
Epoch:   500  |  train loss: 0.2475983530
Epoch:   600  |  train loss: 0.2064180434
Epoch:   700  |  train loss: 0.1768871874
Epoch:   800  |  train loss: 0.1551732421
Epoch:   900  |  train loss: 0.1369690597
Epoch:  1000  |  train loss: 0.1223801628
Epoch:  1100  |  train loss: 0.1102729440
Epoch:  1200  |  train loss: 0.1011523858
Epoch:  1300  |  train loss: 0.0920314044
Epoch:  1400  |  train loss: 0.0847789362
Epoch:  1500  |  train loss: 0.0778754905
Epoch:  1600  |  train loss: 0.0727887735
Epoch:  1700  |  train loss: 0.0678988814
Epoch:  1800  |  train loss: 0.0634830713
Epoch:  1900  |  train loss: 0.0596113518
Epoch:  2000  |  train loss: 0.0563293353
Processing class: 66
Using PCA transformation
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.7653007746
Epoch:   200  |  train loss: 0.5402688503
Epoch:   300  |  train loss: 0.3831352532
Epoch:   400  |  train loss: 0.2941662192
Epoch:   500  |  train loss: 0.2359874606
Epoch:   600  |  train loss: 0.1969608516
Epoch:   700  |  train loss: 0.1683714360
Epoch:   800  |  train loss: 0.1473433226
Epoch:   900  |  train loss: 0.1300249755
Epoch:  1000  |  train loss: 0.1156392321
Epoch:  1100  |  train loss: 0.1041755766
Epoch:  1200  |  train loss: 0.0947649732
Epoch:  1300  |  train loss: 0.0865377426
Epoch:  1400  |  train loss: 0.0796565816
Epoch:  1500  |  train loss: 0.0735737681
Epoch:  1600  |  train loss: 0.0688100219
Epoch:  1700  |  train loss: 0.0642728336
Epoch:  1800  |  train loss: 0.0605356894
Epoch:  1900  |  train loss: 0.0573751956
Epoch:  2000  |  train loss: 0.0538385637
Processing class: 67
Using PCA transformation
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.7697475910
Epoch:   200  |  train loss: 0.5328053594
Epoch:   300  |  train loss: 0.3951984525
Epoch:   400  |  train loss: 0.3082493603
Epoch:   500  |  train loss: 0.2527616173
Epoch:   600  |  train loss: 0.2130146861
Epoch:   700  |  train loss: 0.1832596540
Epoch:   800  |  train loss: 0.1603593469
Epoch:   900  |  train loss: 0.1410779595
Epoch:  1000  |  train loss: 0.1264068618
Epoch:  1100  |  train loss: 0.1142096549
Epoch:  1200  |  train loss: 0.1038997412
Epoch:  1300  |  train loss: 0.0948301479
Epoch:  1400  |  train loss: 0.0872000650
Epoch:  1500  |  train loss: 0.0806741640
Epoch:  1600  |  train loss: 0.0753634527
Epoch:  1700  |  train loss: 0.0705471501
Epoch:  1800  |  train loss: 0.0659297273
Epoch:  1900  |  train loss: 0.0618278243
Epoch:  2000  |  train loss: 0.0587870739
Processing class: 68
Using PCA transformation
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.7528054476
Epoch:   200  |  train loss: 0.5117937207
Epoch:   300  |  train loss: 0.3685896814
Epoch:   400  |  train loss: 0.2812529564
Epoch:   500  |  train loss: 0.2270684987
Epoch:   600  |  train loss: 0.1891150743
Epoch:   700  |  train loss: 0.1609528005
Epoch:   800  |  train loss: 0.1397866398
Epoch:   900  |  train loss: 0.1225050032
Epoch:  1000  |  train loss: 0.1085211813
Epoch:  1100  |  train loss: 0.0980524704
Epoch:  1200  |  train loss: 0.0878710076
Epoch:  1300  |  train loss: 0.0806103721
Epoch:  1400  |  train loss: 0.0741718680
Epoch:  1500  |  train loss: 0.0685234025
Epoch:  1600  |  train loss: 0.0634499565
Epoch:  1700  |  train loss: 0.0596563995
Epoch:  1800  |  train loss: 0.0558043398
Epoch:  1900  |  train loss: 0.0523720860
Epoch:  2000  |  train loss: 0.0492689751
Processing class: 69
Using PCA transformation
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.7579915404
Epoch:   200  |  train loss: 0.5416738510
Epoch:   300  |  train loss: 0.4010807455
Epoch:   400  |  train loss: 0.3140977323
Epoch:   500  |  train loss: 0.2554739565
Epoch:   600  |  train loss: 0.2138429582
Epoch:   700  |  train loss: 0.1808143109
Epoch:   800  |  train loss: 0.1560857445
Epoch:   900  |  train loss: 0.1360860437
Epoch:  1000  |  train loss: 0.1204522640
Epoch:  1100  |  train loss: 0.1079405591
Epoch:  1200  |  train loss: 0.0967519149
Epoch:  1300  |  train loss: 0.0876146287
Epoch:  1400  |  train loss: 0.0799255595
Epoch:  1500  |  train loss: 0.0738134712
Epoch:  1600  |  train loss: 0.0681695327
Epoch:  1700  |  train loss: 0.0635940075
Epoch:  1800  |  train loss: 0.0593191601
Epoch:  1900  |  train loss: 0.0556209289
Epoch:  2000  |  train loss: 0.0526041105
Clasifying using reconstruction function cost
2024-03-17 23:10:02,911 [trainer.py] => CNN: {'total': 64.66, '00-09': 75.0, '10-19': 70.1, '20-29': 77.1, '30-39': 70.6, '40-49': 64.0, '50-59': 45.8, '60-69': 50.0, 'old': 67.1, 'new': 50.0}
2024-03-17 23:10:02,912 [trainer.py] => No NME accuracy
2024-03-17 23:10:02,912 [trainer.py] => FeCAM: {'total': 40.66, '00-09': 56.1, '10-19': 50.6, '20-29': 54.2, '30-39': 47.6, '40-49': 52.3, '50-59': 9.3, '60-69': 14.5, 'old': 45.02, 'new': 14.5}
2024-03-17 23:10:02,912 [trainer.py] => CNN top1 curve: [83.44, 71.75, 64.66]
2024-03-17 23:10:02,912 [trainer.py] => CNN top5 curve: [96.5, 89.6, 86.54]
2024-03-17 23:10:02,912 [trainer.py] => FeCAM top1 curve: [57.54, 47.05, 40.66]
2024-03-17 23:10:02,912 [trainer.py] => FeCAM top5 curve: [75.48, 66.38, 60.07]

2024-03-17 23:10:02,923 [fecam.py] => Learning on 70-80
/home/z1165703/miniconda3/lib/python3.11/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
/home/z1165703/miniconda3/lib/python3.11/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
/home/z1165703/miniconda3/lib/python3.11/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
Processing class: 70
Using PCA transformation
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.7737862468
Epoch:   200  |  train loss: 0.5381145835
Epoch:   300  |  train loss: 0.3880296469
Epoch:   400  |  train loss: 0.3017739832
Epoch:   500  |  train loss: 0.2438982189
Epoch:   600  |  train loss: 0.2049215138
Epoch:   700  |  train loss: 0.1766318500
Epoch:   800  |  train loss: 0.1531145424
Epoch:   900  |  train loss: 0.1343650669
Epoch:  1000  |  train loss: 0.1192987129
Epoch:  1100  |  train loss: 0.1073827937
Epoch:  1200  |  train loss: 0.0969862357
Epoch:  1300  |  train loss: 0.0891927034
Epoch:  1400  |  train loss: 0.0815369144
Epoch:  1500  |  train loss: 0.0754793271
Epoch:  1600  |  train loss: 0.0697340488
Epoch:  1700  |  train loss: 0.0655511171
Epoch:  1800  |  train loss: 0.0609164886
Epoch:  1900  |  train loss: 0.0580566511
Epoch:  2000  |  train loss: 0.0541618489
Processing class: 71
Using PCA transformation
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.7732504010
Epoch:   200  |  train loss: 0.5570557833
Epoch:   300  |  train loss: 0.4136679769
Epoch:   400  |  train loss: 0.3197493970
Epoch:   500  |  train loss: 0.2576573193
Epoch:   600  |  train loss: 0.2130774349
Epoch:   700  |  train loss: 0.1801357001
Epoch:   800  |  train loss: 0.1554583818
Epoch:   900  |  train loss: 0.1359073490
Epoch:  1000  |  train loss: 0.1205666631
Epoch:  1100  |  train loss: 0.1081877664
Epoch:  1200  |  train loss: 0.0983899713
Epoch:  1300  |  train loss: 0.0889325783
Epoch:  1400  |  train loss: 0.0820389122
Epoch:  1500  |  train loss: 0.0756479636
Epoch:  1600  |  train loss: 0.0700797409
Epoch:  1700  |  train loss: 0.0653262377
Epoch:  1800  |  train loss: 0.0611733489
Epoch:  1900  |  train loss: 0.0573714875
Epoch:  2000  |  train loss: 0.0544191726
Processing class: 72
Using PCA transformation
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.7532978415
Epoch:   200  |  train loss: 0.5148942351
Epoch:   300  |  train loss: 0.3745057225
Epoch:   400  |  train loss: 0.2824518979
Epoch:   500  |  train loss: 0.2239486098
Epoch:   600  |  train loss: 0.1850542337
Epoch:   700  |  train loss: 0.1574600995
Epoch:   800  |  train loss: 0.1367084593
Epoch:   900  |  train loss: 0.1206691936
Epoch:  1000  |  train loss: 0.1080138564
Epoch:  1100  |  train loss: 0.0980752662
Epoch:  1200  |  train loss: 0.0893836975
Epoch:  1300  |  train loss: 0.0811977297
Epoch:  1400  |  train loss: 0.0750727192
Epoch:  1500  |  train loss: 0.0696126640
Epoch:  1600  |  train loss: 0.0646861508
Epoch:  1700  |  train loss: 0.0605566435
Epoch:  1800  |  train loss: 0.0568683356
Epoch:  1900  |  train loss: 0.0535326645
Epoch:  2000  |  train loss: 0.0504020505
Processing class: 73
Using PCA transformation
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.7728606939
Epoch:   200  |  train loss: 0.5701983094
Epoch:   300  |  train loss: 0.4286655664
Epoch:   400  |  train loss: 0.3388652325
Epoch:   500  |  train loss: 0.2787985384
Epoch:   600  |  train loss: 0.2350021064
Epoch:   700  |  train loss: 0.1980471998
Epoch:   800  |  train loss: 0.1713402867
Epoch:   900  |  train loss: 0.1493082911
Epoch:  1000  |  train loss: 0.1316347003
Epoch:  1100  |  train loss: 0.1180129632
Epoch:  1200  |  train loss: 0.1054997638
Epoch:  1300  |  train loss: 0.0966219589
Epoch:  1400  |  train loss: 0.0878578842
Epoch:  1500  |  train loss: 0.0812519133
Epoch:  1600  |  train loss: 0.0749241948
Epoch:  1700  |  train loss: 0.0692678526
Epoch:  1800  |  train loss: 0.0650750384
Epoch:  1900  |  train loss: 0.0614062443
Epoch:  2000  |  train loss: 0.0572963439
Processing class: 74
Using PCA transformation
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.7475804806
Epoch:   200  |  train loss: 0.5123149157
Epoch:   300  |  train loss: 0.3747063696
Epoch:   400  |  train loss: 0.2870065510
Epoch:   500  |  train loss: 0.2289309531
Epoch:   600  |  train loss: 0.1899638981
Epoch:   700  |  train loss: 0.1607190162
Epoch:   800  |  train loss: 0.1392868310
Epoch:   900  |  train loss: 0.1217352033
Epoch:  1000  |  train loss: 0.1083125398
Epoch:  1100  |  train loss: 0.0976553425
Epoch:  1200  |  train loss: 0.0884402543
Epoch:  1300  |  train loss: 0.0801658764
Epoch:  1400  |  train loss: 0.0734323740
Epoch:  1500  |  train loss: 0.0683198929
Epoch:  1600  |  train loss: 0.0632791877
Epoch:  1700  |  train loss: 0.0587523215
Epoch:  1800  |  train loss: 0.0547842309
Epoch:  1900  |  train loss: 0.0516575381
Epoch:  2000  |  train loss: 0.0487743586
Processing class: 75
Using PCA transformation
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.7626043797
Epoch:   200  |  train loss: 0.5318584561
Epoch:   300  |  train loss: 0.3893406630
Epoch:   400  |  train loss: 0.3018374324
Epoch:   500  |  train loss: 0.2424028367
Epoch:   600  |  train loss: 0.2027467936
Epoch:   700  |  train loss: 0.1740091264
Epoch:   800  |  train loss: 0.1518582612
Epoch:   900  |  train loss: 0.1347451627
Epoch:  1000  |  train loss: 0.1203659162
Epoch:  1100  |  train loss: 0.1092303663
Epoch:  1200  |  train loss: 0.0981849551
Epoch:  1300  |  train loss: 0.0899320424
Epoch:  1400  |  train loss: 0.0826846257
Epoch:  1500  |  train loss: 0.0763833165
Epoch:  1600  |  train loss: 0.0710079476
Epoch:  1700  |  train loss: 0.0666354567
Epoch:  1800  |  train loss: 0.0620156966
Epoch:  1900  |  train loss: 0.0584392451
Epoch:  2000  |  train loss: 0.0553656206
Processing class: 76
Using PCA transformation
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.7558115482
Epoch:   200  |  train loss: 0.5521490335
Epoch:   300  |  train loss: 0.4342634380
Epoch:   400  |  train loss: 0.3513179183
Epoch:   500  |  train loss: 0.2893878818
Epoch:   600  |  train loss: 0.2448717624
Epoch:   700  |  train loss: 0.2093231708
Epoch:   800  |  train loss: 0.1821623087
Epoch:   900  |  train loss: 0.1610544950
Epoch:  1000  |  train loss: 0.1430319875
Epoch:  1100  |  train loss: 0.1277889118
Epoch:  1200  |  train loss: 0.1153639153
Epoch:  1300  |  train loss: 0.1049673587
Epoch:  1400  |  train loss: 0.0962882474
Epoch:  1500  |  train loss: 0.0890124619
Epoch:  1600  |  train loss: 0.0823827490
Epoch:  1700  |  train loss: 0.0766022801
Epoch:  1800  |  train loss: 0.0717847317
Epoch:  1900  |  train loss: 0.0671305731
Epoch:  2000  |  train loss: 0.0632142536
Processing class: 77
Using PCA transformation
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.7438832641
Epoch:   200  |  train loss: 0.5377368033
Epoch:   300  |  train loss: 0.3910683274
Epoch:   400  |  train loss: 0.2951152265
Epoch:   500  |  train loss: 0.2332998514
Epoch:   600  |  train loss: 0.1914614022
Epoch:   700  |  train loss: 0.1623272538
Epoch:   800  |  train loss: 0.1415630847
Epoch:   900  |  train loss: 0.1253642842
Epoch:  1000  |  train loss: 0.1121020555
Epoch:  1100  |  train loss: 0.1017488942
Epoch:  1200  |  train loss: 0.0925070941
Epoch:  1300  |  train loss: 0.0853621274
Epoch:  1400  |  train loss: 0.0782264665
Epoch:  1500  |  train loss: 0.0730621114
Epoch:  1600  |  train loss: 0.0679042459
Epoch:  1700  |  train loss: 0.0634453431
Epoch:  1800  |  train loss: 0.0593416795
Epoch:  1900  |  train loss: 0.0564161472
Epoch:  2000  |  train loss: 0.0532489486
Processing class: 78
Using PCA transformation
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.7520203471
Epoch:   200  |  train loss: 0.5485576391
Epoch:   300  |  train loss: 0.3988763809
Epoch:   400  |  train loss: 0.3118532896
Epoch:   500  |  train loss: 0.2552386820
Epoch:   600  |  train loss: 0.2144060642
Epoch:   700  |  train loss: 0.1845067173
Epoch:   800  |  train loss: 0.1600604296
Epoch:   900  |  train loss: 0.1417942494
Epoch:  1000  |  train loss: 0.1265359610
Epoch:  1100  |  train loss: 0.1140313074
Epoch:  1200  |  train loss: 0.1039429143
Epoch:  1300  |  train loss: 0.0958281890
Epoch:  1400  |  train loss: 0.0887442112
Epoch:  1500  |  train loss: 0.0820855692
Epoch:  1600  |  train loss: 0.0761214137
Epoch:  1700  |  train loss: 0.0718679696
Epoch:  1800  |  train loss: 0.0672680765
Epoch:  1900  |  train loss: 0.0637873180
Epoch:  2000  |  train loss: 0.0598897442
Processing class: 79
Using PCA transformation
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.7576365352
Epoch:   200  |  train loss: 0.5271332860
Epoch:   300  |  train loss: 0.3806736469
Epoch:   400  |  train loss: 0.2901502132
Epoch:   500  |  train loss: 0.2332997501
Epoch:   600  |  train loss: 0.1953967959
Epoch:   700  |  train loss: 0.1668320239
Epoch:   800  |  train loss: 0.1453456461
Epoch:   900  |  train loss: 0.1283023521
Epoch:  1000  |  train loss: 0.1143526644
Epoch:  1100  |  train loss: 0.1031743899
Epoch:  1200  |  train loss: 0.0937512144
Epoch:  1300  |  train loss: 0.0858464569
Epoch:  1400  |  train loss: 0.0788002297
Epoch:  1500  |  train loss: 0.0726502895
Epoch:  1600  |  train loss: 0.0674999863
Epoch:  1700  |  train loss: 0.0635303974
Epoch:  1800  |  train loss: 0.0594933949
Epoch:  1900  |  train loss: 0.0560111195
Epoch:  2000  |  train loss: 0.0535677314
Clasifying using reconstruction function cost
2024-03-17 23:28:53,005 [trainer.py] => CNN: {'total': 59.18, '00-09': 73.6, '10-19': 68.4, '20-29': 76.9, '30-39': 69.1, '40-49': 60.6, '50-59': 37.2, '60-69': 45.7, '70-79': 41.9, 'old': 61.64, 'new': 41.9}
2024-03-17 23:28:53,006 [trainer.py] => No NME accuracy
2024-03-17 23:28:53,006 [trainer.py] => FeCAM: {'total': 35.56, '00-09': 55.0, '10-19': 49.9, '20-29': 53.8, '30-39': 46.5, '40-49': 49.7, '50-59': 8.4, '60-69': 14.0, '70-79': 7.2, 'old': 39.61, 'new': 7.2}
2024-03-17 23:28:53,006 [trainer.py] => CNN top1 curve: [83.44, 71.75, 64.66, 59.18]
2024-03-17 23:28:53,006 [trainer.py] => CNN top5 curve: [96.5, 89.6, 86.54, 84.09]
2024-03-17 23:28:53,006 [trainer.py] => FeCAM top1 curve: [57.54, 47.05, 40.66, 35.56]
2024-03-17 23:28:53,007 [trainer.py] => FeCAM top5 curve: [75.48, 66.38, 60.07, 54.35]

2024-03-17 23:28:53,019 [fecam.py] => Learning on 80-90
/home/z1165703/miniconda3/lib/python3.11/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
/home/z1165703/miniconda3/lib/python3.11/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
/home/z1165703/miniconda3/lib/python3.11/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
Processing class: 80
Using PCA transformation
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.7620175123
Epoch:   200  |  train loss: 0.5470153570
Epoch:   300  |  train loss: 0.4072300017
Epoch:   400  |  train loss: 0.3216915250
Epoch:   500  |  train loss: 0.2634471118
Epoch:   600  |  train loss: 0.2190971375
Epoch:   700  |  train loss: 0.1864046276
Epoch:   800  |  train loss: 0.1618389040
Epoch:   900  |  train loss: 0.1422639638
Epoch:  1000  |  train loss: 0.1257831231
Epoch:  1100  |  train loss: 0.1126459718
Epoch:  1200  |  train loss: 0.1024495751
Epoch:  1300  |  train loss: 0.0935790330
Epoch:  1400  |  train loss: 0.0850789830
Epoch:  1500  |  train loss: 0.0789035305
Epoch:  1600  |  train loss: 0.0735784888
Epoch:  1700  |  train loss: 0.0683233514
Epoch:  1800  |  train loss: 0.0640596658
Epoch:  1900  |  train loss: 0.0602374017
Epoch:  2000  |  train loss: 0.0571558841
Processing class: 81
Using PCA transformation
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.7633841395
Epoch:   200  |  train loss: 0.5218143642
Epoch:   300  |  train loss: 0.3822989762
Epoch:   400  |  train loss: 0.2929593444
Epoch:   500  |  train loss: 0.2356742501
Epoch:   600  |  train loss: 0.1956433654
Epoch:   700  |  train loss: 0.1666857034
Epoch:   800  |  train loss: 0.1447165906
Epoch:   900  |  train loss: 0.1273620591
Epoch:  1000  |  train loss: 0.1135217562
Epoch:  1100  |  train loss: 0.1016687497
Epoch:  1200  |  train loss: 0.0926367760
Epoch:  1300  |  train loss: 0.0843850404
Epoch:  1400  |  train loss: 0.0775466904
Epoch:  1500  |  train loss: 0.0718297884
Epoch:  1600  |  train loss: 0.0664419994
Epoch:  1700  |  train loss: 0.0622764744
Epoch:  1800  |  train loss: 0.0577936068
Epoch:  1900  |  train loss: 0.0546440415
Epoch:  2000  |  train loss: 0.0516519114
Processing class: 82
Using PCA transformation
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.7802703977
Epoch:   200  |  train loss: 0.5766767144
Epoch:   300  |  train loss: 0.4272707105
Epoch:   400  |  train loss: 0.3295984089
Epoch:   500  |  train loss: 0.2672443628
Epoch:   600  |  train loss: 0.2227502465
Epoch:   700  |  train loss: 0.1901414514
Epoch:   800  |  train loss: 0.1663195997
Epoch:   900  |  train loss: 0.1467921764
Epoch:  1000  |  train loss: 0.1306068897
Epoch:  1100  |  train loss: 0.1168984830
Epoch:  1200  |  train loss: 0.1052998036
Epoch:  1300  |  train loss: 0.0959904671
Epoch:  1400  |  train loss: 0.0881684065
Epoch:  1500  |  train loss: 0.0819022715
Epoch:  1600  |  train loss: 0.0761494517
Epoch:  1700  |  train loss: 0.0712230355
Epoch:  1800  |  train loss: 0.0658976600
Epoch:  1900  |  train loss: 0.0621088117
Epoch:  2000  |  train loss: 0.0582632005
Processing class: 83
Using PCA transformation
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.7467379928
Epoch:   200  |  train loss: 0.5416984439
Epoch:   300  |  train loss: 0.4072314143
Epoch:   400  |  train loss: 0.3181319892
Epoch:   500  |  train loss: 0.2558526367
Epoch:   600  |  train loss: 0.2131317049
Epoch:   700  |  train loss: 0.1812170118
Epoch:   800  |  train loss: 0.1579036117
Epoch:   900  |  train loss: 0.1401690692
Epoch:  1000  |  train loss: 0.1247606620
Epoch:  1100  |  train loss: 0.1134639710
Epoch:  1200  |  train loss: 0.1032223508
Epoch:  1300  |  train loss: 0.0947061181
Epoch:  1400  |  train loss: 0.0869731218
Epoch:  1500  |  train loss: 0.0810548559
Epoch:  1600  |  train loss: 0.0752486110
Epoch:  1700  |  train loss: 0.0699624002
Epoch:  1800  |  train loss: 0.0661686838
Epoch:  1900  |  train loss: 0.0618674248
Epoch:  2000  |  train loss: 0.0585878439
Processing class: 84
Using PCA transformation
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.7437714458
Epoch:   200  |  train loss: 0.5181290925
Epoch:   300  |  train loss: 0.3904519260
Epoch:   400  |  train loss: 0.3069560409
Epoch:   500  |  train loss: 0.2487933218
Epoch:   600  |  train loss: 0.2064226449
Epoch:   700  |  train loss: 0.1753372073
Epoch:   800  |  train loss: 0.1514176011
Epoch:   900  |  train loss: 0.1335490227
Epoch:  1000  |  train loss: 0.1187910914
Epoch:  1100  |  train loss: 0.1071171343
Epoch:  1200  |  train loss: 0.0971312180
Epoch:  1300  |  train loss: 0.0891038954
Epoch:  1400  |  train loss: 0.0825989664
Epoch:  1500  |  train loss: 0.0758778334
Epoch:  1600  |  train loss: 0.0703234553
Epoch:  1700  |  train loss: 0.0656358123
Epoch:  1800  |  train loss: 0.0612857722
Epoch:  1900  |  train loss: 0.0584191225
Epoch:  2000  |  train loss: 0.0543161891
Processing class: 85
Using PCA transformation
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.7421430707
Epoch:   200  |  train loss: 0.5067787647
Epoch:   300  |  train loss: 0.3649357855
Epoch:   400  |  train loss: 0.2813336670
Epoch:   500  |  train loss: 0.2262468934
Epoch:   600  |  train loss: 0.1880315453
Epoch:   700  |  train loss: 0.1593700051
Epoch:   800  |  train loss: 0.1381550133
Epoch:   900  |  train loss: 0.1218029261
Epoch:  1000  |  train loss: 0.1083902419
Epoch:  1100  |  train loss: 0.0976209626
Epoch:  1200  |  train loss: 0.0880972058
Epoch:  1300  |  train loss: 0.0803089693
Epoch:  1400  |  train loss: 0.0734268144
Epoch:  1500  |  train loss: 0.0686506853
Epoch:  1600  |  train loss: 0.0639082208
Epoch:  1700  |  train loss: 0.0594772458
Epoch:  1800  |  train loss: 0.0556950867
Epoch:  1900  |  train loss: 0.0524041243
Epoch:  2000  |  train loss: 0.0494598567
Processing class: 86
Using PCA transformation
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.7601421356
Epoch:   200  |  train loss: 0.5275958657
Epoch:   300  |  train loss: 0.3963344634
Epoch:   400  |  train loss: 0.3091362119
Epoch:   500  |  train loss: 0.2529181004
Epoch:   600  |  train loss: 0.2134719759
Epoch:   700  |  train loss: 0.1836240977
Epoch:   800  |  train loss: 0.1605604082
Epoch:   900  |  train loss: 0.1424680322
Epoch:  1000  |  train loss: 0.1278283685
Epoch:  1100  |  train loss: 0.1151852101
Epoch:  1200  |  train loss: 0.1047545359
Epoch:  1300  |  train loss: 0.0957370281
Epoch:  1400  |  train loss: 0.0882075667
Epoch:  1500  |  train loss: 0.0818319991
Epoch:  1600  |  train loss: 0.0768428907
Epoch:  1700  |  train loss: 0.0718044043
Epoch:  1800  |  train loss: 0.0673348695
Epoch:  1900  |  train loss: 0.0631329991
Epoch:  2000  |  train loss: 0.0601279952
Processing class: 87
Using PCA transformation
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.7465665340
Epoch:   200  |  train loss: 0.5432332277
Epoch:   300  |  train loss: 0.3978201210
Epoch:   400  |  train loss: 0.3112612128
Epoch:   500  |  train loss: 0.2536633253
Epoch:   600  |  train loss: 0.2118934691
Epoch:   700  |  train loss: 0.1811496884
Epoch:   800  |  train loss: 0.1573522925
Epoch:   900  |  train loss: 0.1385833710
Epoch:  1000  |  train loss: 0.1237221390
Epoch:  1100  |  train loss: 0.1106948465
Epoch:  1200  |  train loss: 0.1006469503
Epoch:  1300  |  train loss: 0.0924327478
Epoch:  1400  |  train loss: 0.0844426796
Epoch:  1500  |  train loss: 0.0783250377
Epoch:  1600  |  train loss: 0.0731708735
Epoch:  1700  |  train loss: 0.0683446616
Epoch:  1800  |  train loss: 0.0639414147
Epoch:  1900  |  train loss: 0.0603031322
Epoch:  2000  |  train loss: 0.0568952456
Processing class: 88
Using PCA transformation
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.7500548005
Epoch:   200  |  train loss: 0.5084904552
Epoch:   300  |  train loss: 0.3599461198
Epoch:   400  |  train loss: 0.2750924528
Epoch:   500  |  train loss: 0.2200359583
Epoch:   600  |  train loss: 0.1821369112
Epoch:   700  |  train loss: 0.1546809494
Epoch:   800  |  train loss: 0.1352192402
Epoch:   900  |  train loss: 0.1186883777
Epoch:  1000  |  train loss: 0.1057168886
Epoch:  1100  |  train loss: 0.0956365585
Epoch:  1200  |  train loss: 0.0875018269
Epoch:  1300  |  train loss: 0.0801271707
Epoch:  1400  |  train loss: 0.0742596313
Epoch:  1500  |  train loss: 0.0686846912
Epoch:  1600  |  train loss: 0.0639284238
Epoch:  1700  |  train loss: 0.0599150963
Epoch:  1800  |  train loss: 0.0560985178
Epoch:  1900  |  train loss: 0.0527430415
Epoch:  2000  |  train loss: 0.0498631418
Processing class: 89
Using PCA transformation
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.7568038940
Epoch:   200  |  train loss: 0.5138800204
Epoch:   300  |  train loss: 0.3687736750
Epoch:   400  |  train loss: 0.2863846004
Epoch:   500  |  train loss: 0.2317312419
Epoch:   600  |  train loss: 0.1912873149
Epoch:   700  |  train loss: 0.1619565874
Epoch:   800  |  train loss: 0.1394611061
Epoch:   900  |  train loss: 0.1226016164
Epoch:  1000  |  train loss: 0.1093009174
Epoch:  1100  |  train loss: 0.0979391590
Epoch:  1200  |  train loss: 0.0887602270
Epoch:  1300  |  train loss: 0.0811739385
Epoch:  1400  |  train loss: 0.0749452263
Epoch:  1500  |  train loss: 0.0695100754
Epoch:  1600  |  train loss: 0.0644457251
Epoch:  1700  |  train loss: 0.0602232173
Epoch:  1800  |  train loss: 0.0569802076
Epoch:  1900  |  train loss: 0.0536123373
Epoch:  2000  |  train loss: 0.0508023165
Clasifying using reconstruction function cost
2024-03-17 23:51:14,743 [trainer.py] => CNN: {'total': 54.08, '00-09': 67.9, '10-19': 63.6, '20-29': 73.2, '30-39': 68.8, '40-49': 57.2, '50-59': 32.6, '60-69': 40.9, '70-79': 38.1, '80-89': 44.4, 'old': 55.29, 'new': 44.4}
2024-03-17 23:51:14,748 [trainer.py] => No NME accuracy
2024-03-17 23:51:14,748 [trainer.py] => FeCAM: {'total': 31.58, '00-09': 54.2, '10-19': 46.7, '20-29': 52.0, '30-39': 44.8, '40-49': 47.0, '50-59': 7.9, '60-69': 13.0, '70-79': 6.9, '80-89': 11.7, 'old': 34.06, 'new': 11.7}
2024-03-17 23:51:14,748 [trainer.py] => CNN top1 curve: [83.44, 71.75, 64.66, 59.18, 54.08]
2024-03-17 23:51:14,748 [trainer.py] => CNN top5 curve: [96.5, 89.6, 86.54, 84.09, 81.74]
2024-03-17 23:51:14,748 [trainer.py] => FeCAM top1 curve: [57.54, 47.05, 40.66, 35.56, 31.58]
2024-03-17 23:51:14,748 [trainer.py] => FeCAM top5 curve: [75.48, 66.38, 60.07, 54.35, 50.82]

2024-03-17 23:51:14,768 [fecam.py] => Learning on 90-100
/home/z1165703/miniconda3/lib/python3.11/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
/home/z1165703/miniconda3/lib/python3.11/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
/home/z1165703/miniconda3/lib/python3.11/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
Processing class: 90
Using PCA transformation
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.7518951774
Epoch:   200  |  train loss: 0.4926886320
Epoch:   300  |  train loss: 0.3575196922
Epoch:   400  |  train loss: 0.2775981367
Epoch:   500  |  train loss: 0.2240178168
Epoch:   600  |  train loss: 0.1870671332
Epoch:   700  |  train loss: 0.1602170378
Epoch:   800  |  train loss: 0.1390687317
Epoch:   900  |  train loss: 0.1225460842
Epoch:  1000  |  train loss: 0.1097064063
Epoch:  1100  |  train loss: 0.0985564724
Epoch:  1200  |  train loss: 0.0896280482
Epoch:  1300  |  train loss: 0.0823165476
Epoch:  1400  |  train loss: 0.0763327271
Epoch:  1500  |  train loss: 0.0704829365
Epoch:  1600  |  train loss: 0.0660918698
Epoch:  1700  |  train loss: 0.0620570853
Epoch:  1800  |  train loss: 0.0585116424
Epoch:  1900  |  train loss: 0.0556507334
Epoch:  2000  |  train loss: 0.0524653353
Processing class: 91
Using PCA transformation
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.7640858889
Epoch:   200  |  train loss: 0.5655638814
Epoch:   300  |  train loss: 0.4412864983
Epoch:   400  |  train loss: 0.3576711595
Epoch:   500  |  train loss: 0.2936487973
Epoch:   600  |  train loss: 0.2452295870
Epoch:   700  |  train loss: 0.2095091522
Epoch:   800  |  train loss: 0.1809514284
Epoch:   900  |  train loss: 0.1589826196
Epoch:  1000  |  train loss: 0.1418498456
Epoch:  1100  |  train loss: 0.1273247078
Epoch:  1200  |  train loss: 0.1157379225
Epoch:  1300  |  train loss: 0.1058837131
Epoch:  1400  |  train loss: 0.0976638302
Epoch:  1500  |  train loss: 0.0902373984
Epoch:  1600  |  train loss: 0.0835686862
Epoch:  1700  |  train loss: 0.0781459585
Epoch:  1800  |  train loss: 0.0727585226
Epoch:  1900  |  train loss: 0.0686915278
Epoch:  2000  |  train loss: 0.0646692090
Processing class: 92
Using PCA transformation
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.7526637912
Epoch:   200  |  train loss: 0.5301409006
Epoch:   300  |  train loss: 0.3866048157
Epoch:   400  |  train loss: 0.2991921306
Epoch:   500  |  train loss: 0.2396540761
Epoch:   600  |  train loss: 0.1984793156
Epoch:   700  |  train loss: 0.1677236915
Epoch:   800  |  train loss: 0.1454608381
Epoch:   900  |  train loss: 0.1277874887
Epoch:  1000  |  train loss: 0.1148075432
Epoch:  1100  |  train loss: 0.1031104103
Epoch:  1200  |  train loss: 0.0933005005
Epoch:  1300  |  train loss: 0.0852848530
Epoch:  1400  |  train loss: 0.0779303521
Epoch:  1500  |  train loss: 0.0721285194
Epoch:  1600  |  train loss: 0.0672000065
Epoch:  1700  |  train loss: 0.0625222422
Epoch:  1800  |  train loss: 0.0583067268
Epoch:  1900  |  train loss: 0.0546234280
Epoch:  2000  |  train loss: 0.0511848390
Processing class: 93
Using PCA transformation
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.7753093362
Epoch:   200  |  train loss: 0.5374630213
Epoch:   300  |  train loss: 0.4021302640
Epoch:   400  |  train loss: 0.3174575925
Epoch:   500  |  train loss: 0.2594764113
Epoch:   600  |  train loss: 0.2176689863
Epoch:   700  |  train loss: 0.1865693539
Epoch:   800  |  train loss: 0.1624086767
Epoch:   900  |  train loss: 0.1432517737
Epoch:  1000  |  train loss: 0.1278228909
Epoch:  1100  |  train loss: 0.1150681123
Epoch:  1200  |  train loss: 0.1039905086
Epoch:  1300  |  train loss: 0.0955355465
Epoch:  1400  |  train loss: 0.0880696788
Epoch:  1500  |  train loss: 0.0809751019
Epoch:  1600  |  train loss: 0.0755490765
Epoch:  1700  |  train loss: 0.0702682689
Epoch:  1800  |  train loss: 0.0658130944
Epoch:  1900  |  train loss: 0.0616890743
Epoch:  2000  |  train loss: 0.0582281023
Processing class: 94
Using PCA transformation
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.7512194633
Epoch:   200  |  train loss: 0.4905497491
Epoch:   300  |  train loss: 0.3490919530
Epoch:   400  |  train loss: 0.2647865832
Epoch:   500  |  train loss: 0.2116149902
Epoch:   600  |  train loss: 0.1740419567
Epoch:   700  |  train loss: 0.1477451026
Epoch:   800  |  train loss: 0.1283270478
Epoch:   900  |  train loss: 0.1142966107
Epoch:  1000  |  train loss: 0.1023250535
Epoch:  1100  |  train loss: 0.0929588497
Epoch:  1200  |  train loss: 0.0854437813
Epoch:  1300  |  train loss: 0.0786697254
Epoch:  1400  |  train loss: 0.0734608218
Epoch:  1500  |  train loss: 0.0685412705
Epoch:  1600  |  train loss: 0.0643578969
Epoch:  1700  |  train loss: 0.0607572667
Epoch:  1800  |  train loss: 0.0577688046
Epoch:  1900  |  train loss: 0.0553684607
Epoch:  2000  |  train loss: 0.0522032693
Processing class: 95
Using PCA transformation
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.7569862008
Epoch:   200  |  train loss: 0.5215794563
Epoch:   300  |  train loss: 0.3792745113
Epoch:   400  |  train loss: 0.2953878164
Epoch:   500  |  train loss: 0.2378668368
Epoch:   600  |  train loss: 0.1961719036
Epoch:   700  |  train loss: 0.1655311227
Epoch:   800  |  train loss: 0.1429214597
Epoch:   900  |  train loss: 0.1249760166
Epoch:  1000  |  train loss: 0.1107910857
Epoch:  1100  |  train loss: 0.0992477685
Epoch:  1200  |  train loss: 0.0898816764
Epoch:  1300  |  train loss: 0.0813140750
Epoch:  1400  |  train loss: 0.0745214522
Epoch:  1500  |  train loss: 0.0687149897
Epoch:  1600  |  train loss: 0.0639503695
Epoch:  1700  |  train loss: 0.0596605159
Epoch:  1800  |  train loss: 0.0556168154
Epoch:  1900  |  train loss: 0.0530726604
Epoch:  2000  |  train loss: 0.0500082090
Processing class: 96
Using PCA transformation
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.7341457963
Epoch:   200  |  train loss: 0.5061794281
Epoch:   300  |  train loss: 0.3579470754
Epoch:   400  |  train loss: 0.2726601720
Epoch:   500  |  train loss: 0.2178318501
Epoch:   600  |  train loss: 0.1803745210
Epoch:   700  |  train loss: 0.1529659152
Epoch:   800  |  train loss: 0.1327182680
Epoch:   900  |  train loss: 0.1178152218
Epoch:  1000  |  train loss: 0.1053057417
Epoch:  1100  |  train loss: 0.0957736909
Epoch:  1200  |  train loss: 0.0874075517
Epoch:  1300  |  train loss: 0.0801576301
Epoch:  1400  |  train loss: 0.0738795370
Epoch:  1500  |  train loss: 0.0691199183
Epoch:  1600  |  train loss: 0.0647159219
Epoch:  1700  |  train loss: 0.0605420910
Epoch:  1800  |  train loss: 0.0568717018
Epoch:  1900  |  train loss: 0.0535680749
Epoch:  2000  |  train loss: 0.0513046965
Processing class: 97
Using PCA transformation
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.7615675807
Epoch:   200  |  train loss: 0.5489406407
Epoch:   300  |  train loss: 0.3984909475
Epoch:   400  |  train loss: 0.3094316959
Epoch:   500  |  train loss: 0.2501952678
Epoch:   600  |  train loss: 0.2060518295
Epoch:   700  |  train loss: 0.1743625343
Epoch:   800  |  train loss: 0.1492560506
Epoch:   900  |  train loss: 0.1306496382
Epoch:  1000  |  train loss: 0.1153282732
Epoch:  1100  |  train loss: 0.1040234029
Epoch:  1200  |  train loss: 0.0938775823
Epoch:  1300  |  train loss: 0.0856811732
Epoch:  1400  |  train loss: 0.0786288127
Epoch:  1500  |  train loss: 0.0723773241
Epoch:  1600  |  train loss: 0.0668617979
Epoch:  1700  |  train loss: 0.0625629202
Epoch:  1800  |  train loss: 0.0585549824
Epoch:  1900  |  train loss: 0.0545124352
Epoch:  2000  |  train loss: 0.0519438624
Processing class: 98
Using PCA transformation
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.7520056725
Epoch:   200  |  train loss: 0.5038916409
Epoch:   300  |  train loss: 0.3672269285
Epoch:   400  |  train loss: 0.2869548857
Epoch:   500  |  train loss: 0.2324002117
Epoch:   600  |  train loss: 0.1952853262
Epoch:   700  |  train loss: 0.1674254388
Epoch:   800  |  train loss: 0.1456618398
Epoch:   900  |  train loss: 0.1285598606
Epoch:  1000  |  train loss: 0.1142694309
Epoch:  1100  |  train loss: 0.1025316298
Epoch:  1200  |  train loss: 0.0931793526
Epoch:  1300  |  train loss: 0.0854111061
Epoch:  1400  |  train loss: 0.0788858548
Epoch:  1500  |  train loss: 0.0732072204
Epoch:  1600  |  train loss: 0.0677607849
Epoch:  1700  |  train loss: 0.0639507949
Epoch:  1800  |  train loss: 0.0595241643
Epoch:  1900  |  train loss: 0.0566602267
Epoch:  2000  |  train loss: 0.0536876000
Processing class: 99
Using PCA transformation
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.7574390411
Epoch:   200  |  train loss: 0.5164549589
Epoch:   300  |  train loss: 0.3657248378
Epoch:   400  |  train loss: 0.2852657139
Epoch:   500  |  train loss: 0.2317986608
Epoch:   600  |  train loss: 0.1936215460
Epoch:   700  |  train loss: 0.1671146512
Epoch:   800  |  train loss: 0.1457902819
Epoch:   900  |  train loss: 0.1283571541
Epoch:  1000  |  train loss: 0.1147555843
Epoch:  1100  |  train loss: 0.1034726992
Epoch:  1200  |  train loss: 0.0943415478
Epoch:  1300  |  train loss: 0.0861872748
Epoch:  1400  |  train loss: 0.0799784899
Epoch:  1500  |  train loss: 0.0741869718
Epoch:  1600  |  train loss: 0.0694939218
Epoch:  1700  |  train loss: 0.0647721410
Epoch:  1800  |  train loss: 0.0612138912
Epoch:  1900  |  train loss: 0.0576859556
Epoch:  2000  |  train loss: 0.0547209851
Clasifying using reconstruction function cost
2024-03-18 00:17:43,038 [trainer.py] => CNN: {'total': 50.36, '00-09': 57.7, '10-19': 63.6, '20-29': 71.4, '30-39': 68.5, '40-49': 56.8, '50-59': 29.6, '60-69': 38.9, '70-79': 36.6, '80-89': 42.7, '90-99': 37.8, 'old': 51.76, 'new': 37.8}
2024-03-18 00:17:43,041 [trainer.py] => No NME accuracy
2024-03-18 00:17:43,041 [trainer.py] => FeCAM: {'total': 28.24, '00-09': 51.2, '10-19': 44.4, '20-29': 51.1, '30-39': 43.9, '40-49': 45.6, '50-59': 6.7, '60-69': 12.5, '70-79': 6.5, '80-89': 11.2, '90-99': 9.3, 'old': 30.34, 'new': 9.3}
2024-03-18 00:17:43,041 [trainer.py] => CNN top1 curve: [83.44, 71.75, 64.66, 59.18, 54.08, 50.36]
2024-03-18 00:17:43,041 [trainer.py] => CNN top5 curve: [96.5, 89.6, 86.54, 84.09, 81.74, 79.63]
2024-03-18 00:17:43,041 [trainer.py] => FeCAM top1 curve: [57.54, 47.05, 40.66, 35.56, 31.58, 28.24]
2024-03-18 00:17:43,042 [trainer.py] => FeCAM top5 curve: [75.48, 66.38, 60.07, 54.35, 50.82, 47.17]

=========================================
2024-03-18 00:17:55,272 [trainer.py] => config: ./exps/FeCAM_cifar100.json
2024-03-18 00:17:55,272 [trainer.py] => prefix: train
2024-03-18 00:17:55,273 [trainer.py] => dataset: cifar100
2024-03-18 00:17:55,273 [trainer.py] => memory_size: 0
2024-03-18 00:17:55,273 [trainer.py] => shuffle: True
2024-03-18 00:17:55,273 [trainer.py] => init_cls: 50
2024-03-18 00:17:55,273 [trainer.py] => increment: 10
2024-03-18 00:17:55,273 [trainer.py] => model_name: fecam
2024-03-18 00:17:55,273 [trainer.py] => convnet_type: resnet18
2024-03-18 00:17:55,273 [trainer.py] => device: [device(type='cuda', index=0)]
2024-03-18 00:17:55,273 [trainer.py] => seed: 1993
2024-03-18 00:17:55,273 [trainer.py] => init_epochs: 200
2024-03-18 00:17:55,273 [trainer.py] => init_lr: 0.1
2024-03-18 00:17:55,273 [trainer.py] => init_weight_decay: 0.0005
2024-03-18 00:17:55,273 [trainer.py] => batch_size: 128
2024-03-18 00:17:55,273 [trainer.py] => num_workers: 8
2024-03-18 00:17:55,273 [trainer.py] => T: 5
2024-03-18 00:17:55,273 [trainer.py] => beta: 0.5
2024-03-18 00:17:55,273 [trainer.py] => alpha1: 1
2024-03-18 00:17:55,273 [trainer.py] => alpha2: 1
2024-03-18 00:17:55,273 [trainer.py] => ncm: False
2024-03-18 00:17:55,273 [trainer.py] => tukey: False
2024-03-18 00:17:55,273 [trainer.py] => diagonal: False
2024-03-18 00:17:55,273 [trainer.py] => per_class: True
2024-03-18 00:17:55,273 [trainer.py] => full_cov: True
2024-03-18 00:17:55,273 [trainer.py] => shrink: True
2024-03-18 00:17:55,273 [trainer.py] => norm_cov: False
2024-03-18 00:17:55,273 [trainer.py] => epochs: 2000
2024-03-18 00:17:55,273 [trainer.py] => vecnorm: False
2024-03-18 00:17:55,273 [trainer.py] => ae_type: wae
2024-03-18 00:17:55,273 [trainer.py] => ae_latent_dim: 32
2024-03-18 00:17:55,273 [trainer.py] => ae_n: 1
2024-03-18 00:17:55,273 [trainer.py] => wae_sigma: 10
2024-03-18 00:17:55,273 [trainer.py] => wae_C: 0.1
2024-03-18 00:17:55,273 [trainer.py] => ae_standarization: False
2024-03-18 00:17:55,273 [trainer.py] => ae_pca: True
2024-03-18 00:17:55,273 [trainer.py] => ae_pca_components: 100
2024-03-18 00:17:55,273 [trainer.py] => ae_clsf: recon-cost
Files already downloaded and verified
Files already downloaded and verified
2024-03-18 00:17:59,356 [data_manager.py] => [68, 56, 78, 8, 23, 84, 90, 65, 74, 76, 40, 89, 3, 92, 55, 9, 26, 80, 43, 38, 58, 70, 77, 1, 85, 19, 17, 50, 28, 53, 13, 81, 45, 82, 6, 59, 83, 16, 15, 44, 91, 41, 72, 60, 79, 52, 20, 10, 31, 54, 37, 95, 14, 71, 96, 98, 97, 2, 64, 66, 42, 22, 35, 86, 24, 34, 87, 21, 99, 0, 88, 27, 18, 94, 11, 12, 47, 25, 30, 46, 62, 69, 36, 61, 7, 63, 75, 5, 32, 4, 51, 48, 73, 93, 39, 67, 29, 49, 57, 33]
2024-03-18 00:17:59,638 [fecam.py] => Learning on 0-50
/home/z1165703/miniconda3/lib/python3.11/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
/home/z1165703/miniconda3/lib/python3.11/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
Processing class: 0
Using PCA transformation
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.8831527948
Epoch:   200  |  train loss: 0.7123578310
Epoch:   300  |  train loss: 0.5844454169
Epoch:   400  |  train loss: 0.4914560676
Epoch:   500  |  train loss: 0.4250111997
Epoch:   600  |  train loss: 0.3753196657
Epoch:   700  |  train loss: 0.3350973189
Epoch:   800  |  train loss: 0.3025693953
Epoch:   900  |  train loss: 0.2749578714
Epoch:  1000  |  train loss: 0.2514648408
Epoch:  1100  |  train loss: 0.2320889890
Epoch:  1200  |  train loss: 0.2138680756
Epoch:  1300  |  train loss: 0.1990542918
Epoch:  1400  |  train loss: 0.1851340503
Epoch:  1500  |  train loss: 0.1739753157
Epoch:  1600  |  train loss: 0.1630876362
Epoch:  1700  |  train loss: 0.1535440922
Epoch:  1800  |  train loss: 0.1452252895
Epoch:  1900  |  train loss: 0.1369204819
Epoch:  2000  |  train loss: 0.1307444513
Processing class: 1
Using PCA transformation
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.8729763865
Epoch:   200  |  train loss: 0.6966092110
Epoch:   300  |  train loss: 0.5683640003
Epoch:   400  |  train loss: 0.4773638248
Epoch:   500  |  train loss: 0.4111948013
Epoch:   600  |  train loss: 0.3591765463
Epoch:   700  |  train loss: 0.3189553976
Epoch:   800  |  train loss: 0.2850428581
Epoch:   900  |  train loss: 0.2573632538
Epoch:  1000  |  train loss: 0.2340078413
Epoch:  1100  |  train loss: 0.2152359039
Epoch:  1200  |  train loss: 0.1986443102
Epoch:  1300  |  train loss: 0.1840792716
Epoch:  1400  |  train loss: 0.1719391167
Epoch:  1500  |  train loss: 0.1613843918
Epoch:  1600  |  train loss: 0.1519971400
Epoch:  1700  |  train loss: 0.1431019932
Epoch:  1800  |  train loss: 0.1356702685
Epoch:  1900  |  train loss: 0.1287511975
Epoch:  2000  |  train loss: 0.1230243906
Processing class: 2
Using PCA transformation
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.8721906185
Epoch:   200  |  train loss: 0.6885045767
Epoch:   300  |  train loss: 0.5647605181
Epoch:   400  |  train loss: 0.4763088405
Epoch:   500  |  train loss: 0.4089264929
Epoch:   600  |  train loss: 0.3587587833
Epoch:   700  |  train loss: 0.3195828915
Epoch:   800  |  train loss: 0.2891468167
Epoch:   900  |  train loss: 0.2630840242
Epoch:  1000  |  train loss: 0.2410223246
Epoch:  1100  |  train loss: 0.2225167394
Epoch:  1200  |  train loss: 0.2070249647
Epoch:  1300  |  train loss: 0.1928787678
Epoch:  1400  |  train loss: 0.1805403680
Epoch:  1500  |  train loss: 0.1693097204
Epoch:  1600  |  train loss: 0.1602219045
Epoch:  1700  |  train loss: 0.1504713356
Epoch:  1800  |  train loss: 0.1427421004
Epoch:  1900  |  train loss: 0.1352848262
Epoch:  2000  |  train loss: 0.1293213159
Processing class: 3
Using PCA transformation
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.8760523558
Epoch:   200  |  train loss: 0.7149909735
Epoch:   300  |  train loss: 0.5946252584
Epoch:   400  |  train loss: 0.5087323368
Epoch:   500  |  train loss: 0.4434216380
Epoch:   600  |  train loss: 0.3930402577
Epoch:   700  |  train loss: 0.3510540962
Epoch:   800  |  train loss: 0.3171764851
Epoch:   900  |  train loss: 0.2894114792
Epoch:  1000  |  train loss: 0.2670806289
Epoch:  1100  |  train loss: 0.2468940556
Epoch:  1200  |  train loss: 0.2304335594
Epoch:  1300  |  train loss: 0.2154451251
Epoch:  1400  |  train loss: 0.2020883322
Epoch:  1500  |  train loss: 0.1910902470
Epoch:  1600  |  train loss: 0.1808141351
Epoch:  1700  |  train loss: 0.1711202353
Epoch:  1800  |  train loss: 0.1622322530
Epoch:  1900  |  train loss: 0.1541988164
Epoch:  2000  |  train loss: 0.1475300968
Processing class: 4
Using PCA transformation
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.8729732871
Epoch:   200  |  train loss: 0.6958812475
Epoch:   300  |  train loss: 0.5810667038
Epoch:   400  |  train loss: 0.4939554274
Epoch:   500  |  train loss: 0.4268404901
Epoch:   600  |  train loss: 0.3755262077
Epoch:   700  |  train loss: 0.3359880745
Epoch:   800  |  train loss: 0.3053537726
Epoch:   900  |  train loss: 0.2786699355
Epoch:  1000  |  train loss: 0.2580600768
Epoch:  1100  |  train loss: 0.2388668537
Epoch:  1200  |  train loss: 0.2225757003
Epoch:  1300  |  train loss: 0.2085321486
Epoch:  1400  |  train loss: 0.1960426718
Epoch:  1500  |  train loss: 0.1857539952
Epoch:  1600  |  train loss: 0.1757169306
Epoch:  1700  |  train loss: 0.1666550457
Epoch:  1800  |  train loss: 0.1578055114
Epoch:  1900  |  train loss: 0.1506040514
Epoch:  2000  |  train loss: 0.1433153749
Processing class: 5
Using PCA transformation
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.8716620922
Epoch:   200  |  train loss: 0.6995337129
Epoch:   300  |  train loss: 0.5754934788
Epoch:   400  |  train loss: 0.4891589820
Epoch:   500  |  train loss: 0.4255511940
Epoch:   600  |  train loss: 0.3749384284
Epoch:   700  |  train loss: 0.3342252553
Epoch:   800  |  train loss: 0.3032981455
Epoch:   900  |  train loss: 0.2765233934
Epoch:  1000  |  train loss: 0.2538571239
Epoch:  1100  |  train loss: 0.2339236170
Epoch:  1200  |  train loss: 0.2170383841
Epoch:  1300  |  train loss: 0.2030519992
Epoch:  1400  |  train loss: 0.1902222455
Epoch:  1500  |  train loss: 0.1790761501
Epoch:  1600  |  train loss: 0.1693989098
Epoch:  1700  |  train loss: 0.1604318291
Epoch:  1800  |  train loss: 0.1522426933
Epoch:  1900  |  train loss: 0.1450834721
Epoch:  2000  |  train loss: 0.1390134096
Processing class: 6
Using PCA transformation
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.8795667768
Epoch:   200  |  train loss: 0.7031129718
Epoch:   300  |  train loss: 0.5749952197
Epoch:   400  |  train loss: 0.4853831112
Epoch:   500  |  train loss: 0.4182017863
Epoch:   600  |  train loss: 0.3677904308
Epoch:   700  |  train loss: 0.3283109725
Epoch:   800  |  train loss: 0.2959003150
Epoch:   900  |  train loss: 0.2689770281
Epoch:  1000  |  train loss: 0.2465390056
Epoch:  1100  |  train loss: 0.2274913311
Epoch:  1200  |  train loss: 0.2097041786
Epoch:  1300  |  train loss: 0.1948371410
Epoch:  1400  |  train loss: 0.1812960893
Epoch:  1500  |  train loss: 0.1701672047
Epoch:  1600  |  train loss: 0.1591189533
Epoch:  1700  |  train loss: 0.1507984519
Epoch:  1800  |  train loss: 0.1415523618
Epoch:  1900  |  train loss: 0.1340095103
Epoch:  2000  |  train loss: 0.1274505407
Processing class: 7
Using PCA transformation
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.8722031474
Epoch:   200  |  train loss: 0.6954158306
Epoch:   300  |  train loss: 0.5738655567
Epoch:   400  |  train loss: 0.4896015167
Epoch:   500  |  train loss: 0.4257928848
Epoch:   600  |  train loss: 0.3749525487
Epoch:   700  |  train loss: 0.3336254776
Epoch:   800  |  train loss: 0.3004520774
Epoch:   900  |  train loss: 0.2736797869
Epoch:  1000  |  train loss: 0.2494888067
Epoch:  1100  |  train loss: 0.2298470199
Epoch:  1200  |  train loss: 0.2138251007
Epoch:  1300  |  train loss: 0.1990031242
Epoch:  1400  |  train loss: 0.1873559892
Epoch:  1500  |  train loss: 0.1754570603
Epoch:  1600  |  train loss: 0.1655024141
Epoch:  1700  |  train loss: 0.1566602170
Epoch:  1800  |  train loss: 0.1479213983
Epoch:  1900  |  train loss: 0.1405283839
Epoch:  2000  |  train loss: 0.1337752730
Processing class: 8
Using PCA transformation
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.8783944964
Epoch:   200  |  train loss: 0.7117766976
Epoch:   300  |  train loss: 0.5944403172
Epoch:   400  |  train loss: 0.5087540150
Epoch:   500  |  train loss: 0.4441345215
Epoch:   600  |  train loss: 0.3959877551
Epoch:   700  |  train loss: 0.3579287469
Epoch:   800  |  train loss: 0.3262379229
Epoch:   900  |  train loss: 0.2993778646
Epoch:  1000  |  train loss: 0.2753878295
Epoch:  1100  |  train loss: 0.2569024742
Epoch:  1200  |  train loss: 0.2392187923
Epoch:  1300  |  train loss: 0.2240033239
Epoch:  1400  |  train loss: 0.2102724791
Epoch:  1500  |  train loss: 0.1989947379
Epoch:  1600  |  train loss: 0.1877622277
Epoch:  1700  |  train loss: 0.1781816244
Epoch:  1800  |  train loss: 0.1689057291
Epoch:  1900  |  train loss: 0.1602546751
Epoch:  2000  |  train loss: 0.1526721328
Processing class: 9
Using PCA transformation
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.8741317153
Epoch:   200  |  train loss: 0.6954243183
Epoch:   300  |  train loss: 0.5694479465
Epoch:   400  |  train loss: 0.4777829528
Epoch:   500  |  train loss: 0.4113836884
Epoch:   600  |  train loss: 0.3623186886
Epoch:   700  |  train loss: 0.3247831881
Epoch:   800  |  train loss: 0.2935553014
Epoch:   900  |  train loss: 0.2685815513
Epoch:  1000  |  train loss: 0.2469632149
Epoch:  1100  |  train loss: 0.2284927785
Epoch:  1200  |  train loss: 0.2124872088
Epoch:  1300  |  train loss: 0.1987792999
Epoch:  1400  |  train loss: 0.1865330905
Epoch:  1500  |  train loss: 0.1755963951
Epoch:  1600  |  train loss: 0.1649664849
Epoch:  1700  |  train loss: 0.1558187515
Epoch:  1800  |  train loss: 0.1487426579
Epoch:  1900  |  train loss: 0.1405752450
Epoch:  2000  |  train loss: 0.1340666056
Processing class: 10
Using PCA transformation
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.8720535517
Epoch:   200  |  train loss: 0.7041668773
Epoch:   300  |  train loss: 0.5898717880
Epoch:   400  |  train loss: 0.5056448936
Epoch:   500  |  train loss: 0.4413769662
Epoch:   600  |  train loss: 0.3932212651
Epoch:   700  |  train loss: 0.3538976908
Epoch:   800  |  train loss: 0.3213825285
Epoch:   900  |  train loss: 0.2944842875
Epoch:  1000  |  train loss: 0.2719558895
Epoch:  1100  |  train loss: 0.2511223555
Epoch:  1200  |  train loss: 0.2336002409
Epoch:  1300  |  train loss: 0.2189601570
Epoch:  1400  |  train loss: 0.2057423413
Epoch:  1500  |  train loss: 0.1936536551
Epoch:  1600  |  train loss: 0.1832524359
Epoch:  1700  |  train loss: 0.1726527959
Epoch:  1800  |  train loss: 0.1642740697
Epoch:  1900  |  train loss: 0.1573187828
Epoch:  2000  |  train loss: 0.1491069138
Processing class: 11
Using PCA transformation
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.8721050978
Epoch:   200  |  train loss: 0.7085285306
Epoch:   300  |  train loss: 0.5942967176
Epoch:   400  |  train loss: 0.5095351398
Epoch:   500  |  train loss: 0.4458129287
Epoch:   600  |  train loss: 0.3968217015
Epoch:   700  |  train loss: 0.3580520988
Epoch:   800  |  train loss: 0.3252385139
Epoch:   900  |  train loss: 0.2983102262
Epoch:  1000  |  train loss: 0.2748559117
Epoch:  1100  |  train loss: 0.2555873096
Epoch:  1200  |  train loss: 0.2384036630
Epoch:  1300  |  train loss: 0.2226284385
Epoch:  1400  |  train loss: 0.2093734920
Epoch:  1500  |  train loss: 0.1976527721
Epoch:  1600  |  train loss: 0.1876704663
Epoch:  1700  |  train loss: 0.1768881768
Epoch:  1800  |  train loss: 0.1686466336
Epoch:  1900  |  train loss: 0.1601505876
Epoch:  2000  |  train loss: 0.1531255722
Processing class: 12
Using PCA transformation
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.8762317657
Epoch:   200  |  train loss: 0.7156214714
Epoch:   300  |  train loss: 0.5999039769
Epoch:   400  |  train loss: 0.5157125950
Epoch:   500  |  train loss: 0.4523949504
Epoch:   600  |  train loss: 0.4035175145
Epoch:   700  |  train loss: 0.3639887691
Epoch:   800  |  train loss: 0.3301283598
Epoch:   900  |  train loss: 0.3023692906
Epoch:  1000  |  train loss: 0.2786134005
Epoch:  1100  |  train loss: 0.2583755434
Epoch:  1200  |  train loss: 0.2406478971
Epoch:  1300  |  train loss: 0.2261635572
Epoch:  1400  |  train loss: 0.2116355181
Epoch:  1500  |  train loss: 0.1999234498
Epoch:  1600  |  train loss: 0.1897988260
Epoch:  1700  |  train loss: 0.1799527407
Epoch:  1800  |  train loss: 0.1710569501
Epoch:  1900  |  train loss: 0.1635320902
Epoch:  2000  |  train loss: 0.1561889172
Processing class: 13
Using PCA transformation
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.8707642674
Epoch:   200  |  train loss: 0.6936926246
Epoch:   300  |  train loss: 0.5723116875
Epoch:   400  |  train loss: 0.4814380288
Epoch:   500  |  train loss: 0.4166485429
Epoch:   600  |  train loss: 0.3683818161
Epoch:   700  |  train loss: 0.3303525329
Epoch:   800  |  train loss: 0.2997369647
Epoch:   900  |  train loss: 0.2752046585
Epoch:  1000  |  train loss: 0.2531699091
Epoch:  1100  |  train loss: 0.2340816170
Epoch:  1200  |  train loss: 0.2185883671
Epoch:  1300  |  train loss: 0.2043129444
Epoch:  1400  |  train loss: 0.1915988415
Epoch:  1500  |  train loss: 0.1800751120
Epoch:  1600  |  train loss: 0.1701855510
Epoch:  1700  |  train loss: 0.1616323084
Epoch:  1800  |  train loss: 0.1531297386
Epoch:  1900  |  train loss: 0.1453280598
Epoch:  2000  |  train loss: 0.1391720593
Processing class: 14
Using PCA transformation
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.8758303165
Epoch:   200  |  train loss: 0.7040242195
Epoch:   300  |  train loss: 0.5876583219
Epoch:   400  |  train loss: 0.5036688387
Epoch:   500  |  train loss: 0.4387108386
Epoch:   600  |  train loss: 0.3873964131
Epoch:   700  |  train loss: 0.3463553846
Epoch:   800  |  train loss: 0.3125038385
Epoch:   900  |  train loss: 0.2855498791
Epoch:  1000  |  train loss: 0.2612936020
Epoch:  1100  |  train loss: 0.2415331632
Epoch:  1200  |  train loss: 0.2240858942
Epoch:  1300  |  train loss: 0.2101774693
Epoch:  1400  |  train loss: 0.1966058940
Epoch:  1500  |  train loss: 0.1851323873
Epoch:  1600  |  train loss: 0.1745767206
Epoch:  1700  |  train loss: 0.1662484854
Epoch:  1800  |  train loss: 0.1581288517
Epoch:  1900  |  train loss: 0.1497451752
Epoch:  2000  |  train loss: 0.1428667009
Processing class: 15
Using PCA transformation
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.8723885417
Epoch:   200  |  train loss: 0.6791096807
Epoch:   300  |  train loss: 0.5571780920
Epoch:   400  |  train loss: 0.4713294923
Epoch:   500  |  train loss: 0.4076985002
Epoch:   600  |  train loss: 0.3600335658
Epoch:   700  |  train loss: 0.3217920899
Epoch:   800  |  train loss: 0.2905700564
Epoch:   900  |  train loss: 0.2648868322
Epoch:  1000  |  train loss: 0.2428543240
Epoch:  1100  |  train loss: 0.2229606628
Epoch:  1200  |  train loss: 0.2055873513
Epoch:  1300  |  train loss: 0.1916470349
Epoch:  1400  |  train loss: 0.1789522558
Epoch:  1500  |  train loss: 0.1679885805
Epoch:  1600  |  train loss: 0.1581713200
Epoch:  1700  |  train loss: 0.1495590359
Epoch:  1800  |  train loss: 0.1416719198
Epoch:  1900  |  train loss: 0.1343733341
Epoch:  2000  |  train loss: 0.1283069104
Processing class: 16
Using PCA transformation
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.8767697215
Epoch:   200  |  train loss: 0.7047158241
Epoch:   300  |  train loss: 0.5844485760
Epoch:   400  |  train loss: 0.4985907495
Epoch:   500  |  train loss: 0.4348290563
Epoch:   600  |  train loss: 0.3864942312
Epoch:   700  |  train loss: 0.3465945363
Epoch:   800  |  train loss: 0.3148968279
Epoch:   900  |  train loss: 0.2881690085
Epoch:  1000  |  train loss: 0.2652961373
Epoch:  1100  |  train loss: 0.2458012074
Epoch:  1200  |  train loss: 0.2274322629
Epoch:  1300  |  train loss: 0.2121226490
Epoch:  1400  |  train loss: 0.1997861952
Epoch:  1500  |  train loss: 0.1877044946
Epoch:  1600  |  train loss: 0.1769325912
Epoch:  1700  |  train loss: 0.1671774954
Epoch:  1800  |  train loss: 0.1593643457
Epoch:  1900  |  train loss: 0.1513881117
Epoch:  2000  |  train loss: 0.1440620482
Processing class: 17
Using PCA transformation
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.8748355627
Epoch:   200  |  train loss: 0.7039757490
Epoch:   300  |  train loss: 0.5829310894
Epoch:   400  |  train loss: 0.4971489549
Epoch:   500  |  train loss: 0.4329605937
Epoch:   600  |  train loss: 0.3829488695
Epoch:   700  |  train loss: 0.3433669865
Epoch:   800  |  train loss: 0.3095736980
Epoch:   900  |  train loss: 0.2822362781
Epoch:  1000  |  train loss: 0.2575294375
Epoch:  1100  |  train loss: 0.2385010362
Epoch:  1200  |  train loss: 0.2211920142
Epoch:  1300  |  train loss: 0.2061497509
Epoch:  1400  |  train loss: 0.1933323771
Epoch:  1500  |  train loss: 0.1823281556
Epoch:  1600  |  train loss: 0.1705861330
Epoch:  1700  |  train loss: 0.1618985385
Epoch:  1800  |  train loss: 0.1529015690
Epoch:  1900  |  train loss: 0.1450195253
Epoch:  2000  |  train loss: 0.1383117169
Processing class: 18
Using PCA transformation
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.8721272588
Epoch:   200  |  train loss: 0.6921214581
Epoch:   300  |  train loss: 0.5745969415
Epoch:   400  |  train loss: 0.4892448962
Epoch:   500  |  train loss: 0.4255936503
Epoch:   600  |  train loss: 0.3772571027
Epoch:   700  |  train loss: 0.3383632541
Epoch:   800  |  train loss: 0.3071085691
Epoch:   900  |  train loss: 0.2811375022
Epoch:  1000  |  train loss: 0.2588062614
Epoch:  1100  |  train loss: 0.2395739436
Epoch:  1200  |  train loss: 0.2224797517
Epoch:  1300  |  train loss: 0.2078755051
Epoch:  1400  |  train loss: 0.1949677587
Epoch:  1500  |  train loss: 0.1832489640
Epoch:  1600  |  train loss: 0.1726127326
Epoch:  1700  |  train loss: 0.1634482771
Epoch:  1800  |  train loss: 0.1546604842
Epoch:  1900  |  train loss: 0.1480388373
Epoch:  2000  |  train loss: 0.1401265025
Processing class: 19
Using PCA transformation
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.8741276503
Epoch:   200  |  train loss: 0.6932500005
Epoch:   300  |  train loss: 0.5703482389
Epoch:   400  |  train loss: 0.4823158205
Epoch:   500  |  train loss: 0.4170576572
Epoch:   600  |  train loss: 0.3663435102
Epoch:   700  |  train loss: 0.3256740153
Epoch:   800  |  train loss: 0.2934227109
Epoch:   900  |  train loss: 0.2666782677
Epoch:  1000  |  train loss: 0.2438109070
Epoch:  1100  |  train loss: 0.2246209145
Epoch:  1200  |  train loss: 0.2082611442
Epoch:  1300  |  train loss: 0.1942039162
Epoch:  1400  |  train loss: 0.1816646338
Epoch:  1500  |  train loss: 0.1706336349
Epoch:  1600  |  train loss: 0.1601959348
Epoch:  1700  |  train loss: 0.1511886925
Epoch:  1800  |  train loss: 0.1432603925
Epoch:  1900  |  train loss: 0.1358107641
Epoch:  2000  |  train loss: 0.1289725974
Processing class: 20
Using PCA transformation
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.8718542576
Epoch:   200  |  train loss: 0.7033112407
Epoch:   300  |  train loss: 0.5872549295
Epoch:   400  |  train loss: 0.5053966463
Epoch:   500  |  train loss: 0.4437301576
Epoch:   600  |  train loss: 0.3937886238
Epoch:   700  |  train loss: 0.3527181268
Epoch:   800  |  train loss: 0.3203420997
Epoch:   900  |  train loss: 0.2922585368
Epoch:  1000  |  train loss: 0.2689414501
Epoch:  1100  |  train loss: 0.2491898984
Epoch:  1200  |  train loss: 0.2325011373
Epoch:  1300  |  train loss: 0.2167643905
Epoch:  1400  |  train loss: 0.2040681064
Epoch:  1500  |  train loss: 0.1919385284
Epoch:  1600  |  train loss: 0.1810112178
Epoch:  1700  |  train loss: 0.1720961690
Epoch:  1800  |  train loss: 0.1639776587
Epoch:  1900  |  train loss: 0.1567241520
Epoch:  2000  |  train loss: 0.1495570898
Processing class: 21
Using PCA transformation
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.8696928740
Epoch:   200  |  train loss: 0.7022004128
Epoch:   300  |  train loss: 0.5775885344
Epoch:   400  |  train loss: 0.4918475389
Epoch:   500  |  train loss: 0.4291101873
Epoch:   600  |  train loss: 0.3808719516
Epoch:   700  |  train loss: 0.3435136378
Epoch:   800  |  train loss: 0.3126460373
Epoch:   900  |  train loss: 0.2864327729
Epoch:  1000  |  train loss: 0.2649065733
Epoch:  1100  |  train loss: 0.2459237784
Epoch:  1200  |  train loss: 0.2298978984
Epoch:  1300  |  train loss: 0.2157191366
Epoch:  1400  |  train loss: 0.2026491106
Epoch:  1500  |  train loss: 0.1914320767
Epoch:  1600  |  train loss: 0.1811223477
Epoch:  1700  |  train loss: 0.1722294271
Epoch:  1800  |  train loss: 0.1632469237
Epoch:  1900  |  train loss: 0.1557235926
Epoch:  2000  |  train loss: 0.1490071744
Processing class: 22
Using PCA transformation
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.8576337337
Epoch:   200  |  train loss: 0.6774653316
Epoch:   300  |  train loss: 0.5511562228
Epoch:   400  |  train loss: 0.4663086772
Epoch:   500  |  train loss: 0.4052574754
Epoch:   600  |  train loss: 0.3579859316
Epoch:   700  |  train loss: 0.3205278277
Epoch:   800  |  train loss: 0.2898357570
Epoch:   900  |  train loss: 0.2655838251
Epoch:  1000  |  train loss: 0.2443933696
Epoch:  1100  |  train loss: 0.2271904349
Epoch:  1200  |  train loss: 0.2116113842
Epoch:  1300  |  train loss: 0.1984790981
Epoch:  1400  |  train loss: 0.1861666322
Epoch:  1500  |  train loss: 0.1759605318
Epoch:  1600  |  train loss: 0.1656662643
Epoch:  1700  |  train loss: 0.1563109398
Epoch:  1800  |  train loss: 0.1496520698
Epoch:  1900  |  train loss: 0.1409035385
Epoch:  2000  |  train loss: 0.1343552500
Processing class: 23
Using PCA transformation
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.8782633543
Epoch:   200  |  train loss: 0.7076161742
Epoch:   300  |  train loss: 0.5813336134
Epoch:   400  |  train loss: 0.4943066299
Epoch:   500  |  train loss: 0.4311240196
Epoch:   600  |  train loss: 0.3830124140
Epoch:   700  |  train loss: 0.3435971141
Epoch:   800  |  train loss: 0.3131997705
Epoch:   900  |  train loss: 0.2873330057
Epoch:  1000  |  train loss: 0.2648093998
Epoch:  1100  |  train loss: 0.2451990873
Epoch:  1200  |  train loss: 0.2283291310
Epoch:  1300  |  train loss: 0.2137642115
Epoch:  1400  |  train loss: 0.1998336673
Epoch:  1500  |  train loss: 0.1887873381
Epoch:  1600  |  train loss: 0.1775499880
Epoch:  1700  |  train loss: 0.1690306723
Epoch:  1800  |  train loss: 0.1597962946
Epoch:  1900  |  train loss: 0.1521317393
Epoch:  2000  |  train loss: 0.1452029854
Processing class: 24
Using PCA transformation
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.8724153399
Epoch:   200  |  train loss: 0.6995219707
Epoch:   300  |  train loss: 0.5752000809
Epoch:   400  |  train loss: 0.4886818290
Epoch:   500  |  train loss: 0.4247762740
Epoch:   600  |  train loss: 0.3752659380
Epoch:   700  |  train loss: 0.3375573277
Epoch:   800  |  train loss: 0.3063046396
Epoch:   900  |  train loss: 0.2794181466
Epoch:  1000  |  train loss: 0.2576075673
Epoch:  1100  |  train loss: 0.2385637313
Epoch:  1200  |  train loss: 0.2214250684
Epoch:  1300  |  train loss: 0.2063260198
Epoch:  1400  |  train loss: 0.1941019356
Epoch:  1500  |  train loss: 0.1826994807
Epoch:  1600  |  train loss: 0.1716553211
Epoch:  1700  |  train loss: 0.1631407976
Epoch:  1800  |  train loss: 0.1543651491
Epoch:  1900  |  train loss: 0.1473396689
Epoch:  2000  |  train loss: 0.1399643868
Processing class: 25
Using PCA transformation
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.8700234652
Epoch:   200  |  train loss: 0.6952592254
Epoch:   300  |  train loss: 0.5752812505
Epoch:   400  |  train loss: 0.4884362519
Epoch:   500  |  train loss: 0.4247728467
Epoch:   600  |  train loss: 0.3752695203
Epoch:   700  |  train loss: 0.3350048602
Epoch:   800  |  train loss: 0.3025189936
Epoch:   900  |  train loss: 0.2748367608
Epoch:  1000  |  train loss: 0.2512468427
Epoch:  1100  |  train loss: 0.2322019637
Epoch:  1200  |  train loss: 0.2153358549
Epoch:  1300  |  train loss: 0.2002558529
Epoch:  1400  |  train loss: 0.1879908174
Epoch:  1500  |  train loss: 0.1768878013
Epoch:  1600  |  train loss: 0.1668944508
Epoch:  1700  |  train loss: 0.1579998255
Epoch:  1800  |  train loss: 0.1497307152
Epoch:  1900  |  train loss: 0.1422197133
Epoch:  2000  |  train loss: 0.1359203175
Processing class: 26
Using PCA transformation
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.8807309389
Epoch:   200  |  train loss: 0.7205811858
Epoch:   300  |  train loss: 0.5953346014
Epoch:   400  |  train loss: 0.5094305217
Epoch:   500  |  train loss: 0.4464459419
Epoch:   600  |  train loss: 0.3970814764
Epoch:   700  |  train loss: 0.3573234916
Epoch:   800  |  train loss: 0.3257124186
Epoch:   900  |  train loss: 0.2990815401
Epoch:  1000  |  train loss: 0.2761169076
Epoch:  1100  |  train loss: 0.2562288582
Epoch:  1200  |  train loss: 0.2386658221
Epoch:  1300  |  train loss: 0.2244065881
Epoch:  1400  |  train loss: 0.2112941027
Epoch:  1500  |  train loss: 0.1994419456
Epoch:  1600  |  train loss: 0.1882796496
Epoch:  1700  |  train loss: 0.1783908516
Epoch:  1800  |  train loss: 0.1703669578
Epoch:  1900  |  train loss: 0.1621844918
Epoch:  2000  |  train loss: 0.1553042829
Processing class: 27
Using PCA transformation
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.8831390738
Epoch:   200  |  train loss: 0.7076841593
Epoch:   300  |  train loss: 0.5833327770
Epoch:   400  |  train loss: 0.4942974508
Epoch:   500  |  train loss: 0.4278514683
Epoch:   600  |  train loss: 0.3765281618
Epoch:   700  |  train loss: 0.3349206746
Epoch:   800  |  train loss: 0.3018177807
Epoch:   900  |  train loss: 0.2745583475
Epoch:  1000  |  train loss: 0.2520266265
Epoch:  1100  |  train loss: 0.2324502230
Epoch:  1200  |  train loss: 0.2152692199
Epoch:  1300  |  train loss: 0.2008115619
Epoch:  1400  |  train loss: 0.1879642159
Epoch:  1500  |  train loss: 0.1761914253
Epoch:  1600  |  train loss: 0.1661523342
Epoch:  1700  |  train loss: 0.1561950058
Epoch:  1800  |  train loss: 0.1479890555
Epoch:  1900  |  train loss: 0.1403073937
Epoch:  2000  |  train loss: 0.1337001085
Processing class: 28
Using PCA transformation
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.8752213836
Epoch:   200  |  train loss: 0.7017221808
Epoch:   300  |  train loss: 0.5845541835
Epoch:   400  |  train loss: 0.4990692914
Epoch:   500  |  train loss: 0.4325360596
Epoch:   600  |  train loss: 0.3811475635
Epoch:   700  |  train loss: 0.3423855424
Epoch:   800  |  train loss: 0.3105844975
Epoch:   900  |  train loss: 0.2835323393
Epoch:  1000  |  train loss: 0.2612901032
Epoch:  1100  |  train loss: 0.2425145954
Epoch:  1200  |  train loss: 0.2250964284
Epoch:  1300  |  train loss: 0.2105893165
Epoch:  1400  |  train loss: 0.1970170289
Epoch:  1500  |  train loss: 0.1852266967
Epoch:  1600  |  train loss: 0.1752773821
Epoch:  1700  |  train loss: 0.1653957188
Epoch:  1800  |  train loss: 0.1567318678
Epoch:  1900  |  train loss: 0.1489006042
Epoch:  2000  |  train loss: 0.1418852746
Processing class: 29
Using PCA transformation
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.8743855476
Epoch:   200  |  train loss: 0.6958223581
Epoch:   300  |  train loss: 0.5682225943
Epoch:   400  |  train loss: 0.4812266111
Epoch:   500  |  train loss: 0.4178608000
Epoch:   600  |  train loss: 0.3683052003
Epoch:   700  |  train loss: 0.3294545949
Epoch:   800  |  train loss: 0.2978451133
Epoch:   900  |  train loss: 0.2707699895
Epoch:  1000  |  train loss: 0.2477414519
Epoch:  1100  |  train loss: 0.2290163487
Epoch:  1200  |  train loss: 0.2117332458
Epoch:  1300  |  train loss: 0.1979217947
Epoch:  1400  |  train loss: 0.1862435937
Epoch:  1500  |  train loss: 0.1754976928
Epoch:  1600  |  train loss: 0.1648483366
Epoch:  1700  |  train loss: 0.1567703187
Epoch:  1800  |  train loss: 0.1481538504
Epoch:  1900  |  train loss: 0.1415661365
Epoch:  2000  |  train loss: 0.1356501698
Processing class: 30
Using PCA transformation
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.8815039635
Epoch:   200  |  train loss: 0.7095090389
Epoch:   300  |  train loss: 0.5946099639
Epoch:   400  |  train loss: 0.5119776189
Epoch:   500  |  train loss: 0.4501299262
Epoch:   600  |  train loss: 0.4016316712
Epoch:   700  |  train loss: 0.3624738395
Epoch:   800  |  train loss: 0.3306101024
Epoch:   900  |  train loss: 0.3024623930
Epoch:  1000  |  train loss: 0.2784748614
Epoch:  1100  |  train loss: 0.2583648801
Epoch:  1200  |  train loss: 0.2407985538
Epoch:  1300  |  train loss: 0.2249001861
Epoch:  1400  |  train loss: 0.2100386351
Epoch:  1500  |  train loss: 0.1968009502
Epoch:  1600  |  train loss: 0.1866518676
Epoch:  1700  |  train loss: 0.1758317411
Epoch:  1800  |  train loss: 0.1665231347
Epoch:  1900  |  train loss: 0.1580597252
Epoch:  2000  |  train loss: 0.1502674550
Processing class: 31
Using PCA transformation
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.8695841551
Epoch:   200  |  train loss: 0.6944349170
Epoch:   300  |  train loss: 0.5686124563
Epoch:   400  |  train loss: 0.4812766910
Epoch:   500  |  train loss: 0.4155270576
Epoch:   600  |  train loss: 0.3646094441
Epoch:   700  |  train loss: 0.3247874737
Epoch:   800  |  train loss: 0.2929542243
Epoch:   900  |  train loss: 0.2653815269
Epoch:  1000  |  train loss: 0.2429923475
Epoch:  1100  |  train loss: 0.2236182451
Epoch:  1200  |  train loss: 0.2069970042
Epoch:  1300  |  train loss: 0.1931463540
Epoch:  1400  |  train loss: 0.1798947871
Epoch:  1500  |  train loss: 0.1689875633
Epoch:  1600  |  train loss: 0.1586310118
Epoch:  1700  |  train loss: 0.1495333433
Epoch:  1800  |  train loss: 0.1413239866
Epoch:  1900  |  train loss: 0.1339079469
Epoch:  2000  |  train loss: 0.1275189102
Processing class: 32
Using PCA transformation
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.8792888165
Epoch:   200  |  train loss: 0.7013047099
Epoch:   300  |  train loss: 0.5841279149
Epoch:   400  |  train loss: 0.5020245612
Epoch:   500  |  train loss: 0.4421015978
Epoch:   600  |  train loss: 0.3947323799
Epoch:   700  |  train loss: 0.3568576396
Epoch:   800  |  train loss: 0.3261538148
Epoch:   900  |  train loss: 0.3004455566
Epoch:  1000  |  train loss: 0.2789031744
Epoch:  1100  |  train loss: 0.2597179711
Epoch:  1200  |  train loss: 0.2425477266
Epoch:  1300  |  train loss: 0.2275321394
Epoch:  1400  |  train loss: 0.2137938052
Epoch:  1500  |  train loss: 0.2018435925
Epoch:  1600  |  train loss: 0.1914552867
Epoch:  1700  |  train loss: 0.1817443728
Epoch:  1800  |  train loss: 0.1735166937
Epoch:  1900  |  train loss: 0.1656750649
Epoch:  2000  |  train loss: 0.1581704736
Processing class: 33
Using PCA transformation
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.8718560576
Epoch:   200  |  train loss: 0.6910466075
Epoch:   300  |  train loss: 0.5685664654
Epoch:   400  |  train loss: 0.4819427133
Epoch:   500  |  train loss: 0.4177569389
Epoch:   600  |  train loss: 0.3686545134
Epoch:   700  |  train loss: 0.3295112312
Epoch:   800  |  train loss: 0.2976046443
Epoch:   900  |  train loss: 0.2717429042
Epoch:  1000  |  train loss: 0.2491406888
Epoch:  1100  |  train loss: 0.2308841228
Epoch:  1200  |  train loss: 0.2138871580
Epoch:  1300  |  train loss: 0.1992568046
Epoch:  1400  |  train loss: 0.1871511698
Epoch:  1500  |  train loss: 0.1762877345
Epoch:  1600  |  train loss: 0.1663909703
Epoch:  1700  |  train loss: 0.1571133971
Epoch:  1800  |  train loss: 0.1493103981
Epoch:  1900  |  train loss: 0.1416798532
Epoch:  2000  |  train loss: 0.1350769490
Processing class: 34
Using PCA transformation
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.8769242048
Epoch:   200  |  train loss: 0.7226778865
Epoch:   300  |  train loss: 0.6035730958
Epoch:   400  |  train loss: 0.5139990449
Epoch:   500  |  train loss: 0.4481328666
Epoch:   600  |  train loss: 0.3988433897
Epoch:   700  |  train loss: 0.3590012610
Epoch:   800  |  train loss: 0.3267533243
Epoch:   900  |  train loss: 0.2996020675
Epoch:  1000  |  train loss: 0.2766224861
Epoch:  1100  |  train loss: 0.2568412393
Epoch:  1200  |  train loss: 0.2396416694
Epoch:  1300  |  train loss: 0.2240349323
Epoch:  1400  |  train loss: 0.2100415707
Epoch:  1500  |  train loss: 0.1977646112
Epoch:  1600  |  train loss: 0.1868295342
Epoch:  1700  |  train loss: 0.1770656586
Epoch:  1800  |  train loss: 0.1674142063
Epoch:  1900  |  train loss: 0.1582958877
Epoch:  2000  |  train loss: 0.1517241061
Processing class: 35
Using PCA transformation
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.8696092844
Epoch:   200  |  train loss: 0.6739889741
Epoch:   300  |  train loss: 0.5496561646
Epoch:   400  |  train loss: 0.4648405969
Epoch:   500  |  train loss: 0.4028574646
Epoch:   600  |  train loss: 0.3541001856
Epoch:   700  |  train loss: 0.3147639394
Epoch:   800  |  train loss: 0.2828514755
Epoch:   900  |  train loss: 0.2566529691
/home/z1165703/miniconda3/lib/python3.11/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
Epoch:  1000  |  train loss: 0.2345489681
Epoch:  1100  |  train loss: 0.2162741393
Epoch:  1200  |  train loss: 0.2005956799
Epoch:  1300  |  train loss: 0.1862104297
Epoch:  1400  |  train loss: 0.1744276464
Epoch:  1500  |  train loss: 0.1635446489
Epoch:  1600  |  train loss: 0.1541993439
Epoch:  1700  |  train loss: 0.1457958758
Epoch:  1800  |  train loss: 0.1382778674
Epoch:  1900  |  train loss: 0.1311295658
Epoch:  2000  |  train loss: 0.1249879956
Processing class: 36
Using PCA transformation
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.8729763627
Epoch:   200  |  train loss: 0.6963256836
Epoch:   300  |  train loss: 0.5787266493
Epoch:   400  |  train loss: 0.4976482749
Epoch:   500  |  train loss: 0.4348315418
Epoch:   600  |  train loss: 0.3851251304
Epoch:   700  |  train loss: 0.3442574918
Epoch:   800  |  train loss: 0.3132843852
Epoch:   900  |  train loss: 0.2864675343
Epoch:  1000  |  train loss: 0.2639429927
Epoch:  1100  |  train loss: 0.2449136108
Epoch:  1200  |  train loss: 0.2279815823
Epoch:  1300  |  train loss: 0.2136956453
Epoch:  1400  |  train loss: 0.2001944214
Epoch:  1500  |  train loss: 0.1888126910
Epoch:  1600  |  train loss: 0.1785753399
Epoch:  1700  |  train loss: 0.1694851220
Epoch:  1800  |  train loss: 0.1609845102
Epoch:  1900  |  train loss: 0.1529905349
Epoch:  2000  |  train loss: 0.1460662931
Processing class: 37
Using PCA transformation
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.8789359212
Epoch:   200  |  train loss: 0.6992387891
Epoch:   300  |  train loss: 0.5802255273
Epoch:   400  |  train loss: 0.4883281767
Epoch:   500  |  train loss: 0.4205775797
Epoch:   600  |  train loss: 0.3688498437
Epoch:   700  |  train loss: 0.3278478682
Epoch:   800  |  train loss: 0.2963109016
Epoch:   900  |  train loss: 0.2694835544
Epoch:  1000  |  train loss: 0.2474185288
Epoch:  1100  |  train loss: 0.2288274407
Epoch:  1200  |  train loss: 0.2119559467
Epoch:  1300  |  train loss: 0.1977806568
Epoch:  1400  |  train loss: 0.1859692901
Epoch:  1500  |  train loss: 0.1748227298
Epoch:  1600  |  train loss: 0.1646552593
Epoch:  1700  |  train loss: 0.1564662337
Epoch:  1800  |  train loss: 0.1485502392
Epoch:  1900  |  train loss: 0.1410855919
Epoch:  2000  |  train loss: 0.1342672497
Processing class: 38
Using PCA transformation
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.8714117765
Epoch:   200  |  train loss: 0.6995300889
Epoch:   300  |  train loss: 0.5755914211
Epoch:   400  |  train loss: 0.4862407207
Epoch:   500  |  train loss: 0.4199603319
Epoch:   600  |  train loss: 0.3687849879
Epoch:   700  |  train loss: 0.3283295751
Epoch:   800  |  train loss: 0.2957687378
Epoch:   900  |  train loss: 0.2675484240
Epoch:  1000  |  train loss: 0.2450912148
Epoch:  1100  |  train loss: 0.2250765651
Epoch:  1200  |  train loss: 0.2089772910
Epoch:  1300  |  train loss: 0.1946321160
Epoch:  1400  |  train loss: 0.1824471176
Epoch:  1500  |  train loss: 0.1719218016
Epoch:  1600  |  train loss: 0.1628332406
Epoch:  1700  |  train loss: 0.1541714907
Epoch:  1800  |  train loss: 0.1460396439
Epoch:  1900  |  train loss: 0.1391182572
Epoch:  2000  |  train loss: 0.1325046808
Processing class: 39
Using PCA transformation
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.8654176474
Epoch:   200  |  train loss: 0.6961493492
Epoch:   300  |  train loss: 0.5763846278
Epoch:   400  |  train loss: 0.4900264621
Epoch:   500  |  train loss: 0.4240266621
Epoch:   600  |  train loss: 0.3737178147
Epoch:   700  |  train loss: 0.3326509416
Epoch:   800  |  train loss: 0.3008021533
Epoch:   900  |  train loss: 0.2742371500
Epoch:  1000  |  train loss: 0.2526829511
Epoch:  1100  |  train loss: 0.2335585505
Epoch:  1200  |  train loss: 0.2174895734
Epoch:  1300  |  train loss: 0.2018564045
Epoch:  1400  |  train loss: 0.1892975569
Epoch:  1500  |  train loss: 0.1778539628
Epoch:  1600  |  train loss: 0.1671911180
Epoch:  1700  |  train loss: 0.1584288120
Epoch:  1800  |  train loss: 0.1497814357
Epoch:  1900  |  train loss: 0.1420877665
Epoch:  2000  |  train loss: 0.1347401500
Processing class: 40
Using PCA transformation
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.8807289720
Epoch:   200  |  train loss: 0.6998900294
Epoch:   300  |  train loss: 0.5844264627
Epoch:   400  |  train loss: 0.5018160462
Epoch:   500  |  train loss: 0.4394744754
Epoch:   600  |  train loss: 0.3912853420
Epoch:   700  |  train loss: 0.3518242121
Epoch:   800  |  train loss: 0.3205480933
Epoch:   900  |  train loss: 0.2943775892
Epoch:  1000  |  train loss: 0.2717547059
Epoch:  1100  |  train loss: 0.2514885187
Epoch:  1200  |  train loss: 0.2345808059
Epoch:  1300  |  train loss: 0.2204091936
Epoch:  1400  |  train loss: 0.2064838439
Epoch:  1500  |  train loss: 0.1946511686
Epoch:  1600  |  train loss: 0.1839817584
Epoch:  1700  |  train loss: 0.1739913821
Epoch:  1800  |  train loss: 0.1655480355
Epoch:  1900  |  train loss: 0.1575109273
Epoch:  2000  |  train loss: 0.1502426803
Processing class: 41
Using PCA transformation
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.8785236716
Epoch:   200  |  train loss: 0.7030991554
Epoch:   300  |  train loss: 0.5901410103
Epoch:   400  |  train loss: 0.5060970783
Epoch:   500  |  train loss: 0.4398756623
Epoch:   600  |  train loss: 0.3890082002
Epoch:   700  |  train loss: 0.3494204462
Epoch:   800  |  train loss: 0.3155835569
Epoch:   900  |  train loss: 0.2876457274
Epoch:  1000  |  train loss: 0.2645754039
Epoch:  1100  |  train loss: 0.2450494558
Epoch:  1200  |  train loss: 0.2277276576
Epoch:  1300  |  train loss: 0.2130753756
Epoch:  1400  |  train loss: 0.1991620958
Epoch:  1500  |  train loss: 0.1873750389
Epoch:  1600  |  train loss: 0.1776587754
Epoch:  1700  |  train loss: 0.1679895550
Epoch:  1800  |  train loss: 0.1599435717
Epoch:  1900  |  train loss: 0.1515908033
Epoch:  2000  |  train loss: 0.1454112500
Processing class: 42
Using PCA transformation
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.8806903005
Epoch:   200  |  train loss: 0.7023625016
Epoch:   300  |  train loss: 0.5779845357
Epoch:   400  |  train loss: 0.4892804325
Epoch:   500  |  train loss: 0.4231166363
Epoch:   600  |  train loss: 0.3729728460
Epoch:   700  |  train loss: 0.3326009572
Epoch:   800  |  train loss: 0.2997888744
Epoch:   900  |  train loss: 0.2735888124
Epoch:  1000  |  train loss: 0.2501682460
Epoch:  1100  |  train loss: 0.2312458545
Epoch:  1200  |  train loss: 0.2145320922
Epoch:  1300  |  train loss: 0.2008125752
Epoch:  1400  |  train loss: 0.1872383982
Epoch:  1500  |  train loss: 0.1762841731
Epoch:  1600  |  train loss: 0.1660512090
Epoch:  1700  |  train loss: 0.1572180688
Epoch:  1800  |  train loss: 0.1492305487
Epoch:  1900  |  train loss: 0.1417954564
Epoch:  2000  |  train loss: 0.1349966228
Processing class: 43
Using PCA transformation
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.8654311657
Epoch:   200  |  train loss: 0.6907936811
Epoch:   300  |  train loss: 0.5668780088
Epoch:   400  |  train loss: 0.4796725035
Epoch:   500  |  train loss: 0.4158692777
Epoch:   600  |  train loss: 0.3672314167
Epoch:   700  |  train loss: 0.3273387909
Epoch:   800  |  train loss: 0.2959694862
Epoch:   900  |  train loss: 0.2695545077
Epoch:  1000  |  train loss: 0.2473617077
Epoch:  1100  |  train loss: 0.2278114021
Epoch:  1200  |  train loss: 0.2113334060
Epoch:  1300  |  train loss: 0.1967638522
Epoch:  1400  |  train loss: 0.1848847568
Epoch:  1500  |  train loss: 0.1732719511
Epoch:  1600  |  train loss: 0.1635044962
Epoch:  1700  |  train loss: 0.1545257002
Epoch:  1800  |  train loss: 0.1468073905
Epoch:  1900  |  train loss: 0.1395670086
Epoch:  2000  |  train loss: 0.1325198144
Processing class: 44
Using PCA transformation
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.8724111676
Epoch:   200  |  train loss: 0.6874848962
Epoch:   300  |  train loss: 0.5676036954
Epoch:   400  |  train loss: 0.4837825894
Epoch:   500  |  train loss: 0.4210640013
Epoch:   600  |  train loss: 0.3722209871
Epoch:   700  |  train loss: 0.3341899633
Epoch:   800  |  train loss: 0.3025921822
Epoch:   900  |  train loss: 0.2755939364
Epoch:  1000  |  train loss: 0.2529391825
Epoch:  1100  |  train loss: 0.2338958740
Epoch:  1200  |  train loss: 0.2167491287
Epoch:  1300  |  train loss: 0.2021430135
Epoch:  1400  |  train loss: 0.1900414318
Epoch:  1500  |  train loss: 0.1782083720
Epoch:  1600  |  train loss: 0.1683298767
Epoch:  1700  |  train loss: 0.1596852303
Epoch:  1800  |  train loss: 0.1510365278
Epoch:  1900  |  train loss: 0.1440239310
Epoch:  2000  |  train loss: 0.1373997688
Processing class: 45
Using PCA transformation
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.8769763708
Epoch:   200  |  train loss: 0.7163331270
Epoch:   300  |  train loss: 0.6093497038
Epoch:   400  |  train loss: 0.5295995474
Epoch:   500  |  train loss: 0.4697216153
Epoch:   600  |  train loss: 0.4236419022
Epoch:   700  |  train loss: 0.3850065768
Epoch:   800  |  train loss: 0.3520342767
Epoch:   900  |  train loss: 0.3254371345
Epoch:  1000  |  train loss: 0.3009591997
Epoch:  1100  |  train loss: 0.2802162230
Epoch:  1200  |  train loss: 0.2623170257
Epoch:  1300  |  train loss: 0.2467177659
Epoch:  1400  |  train loss: 0.2319525152
Epoch:  1500  |  train loss: 0.2193894416
Epoch:  1600  |  train loss: 0.2080540597
Epoch:  1700  |  train loss: 0.1972131550
Epoch:  1800  |  train loss: 0.1876077145
Epoch:  1900  |  train loss: 0.1785847515
Epoch:  2000  |  train loss: 0.1708066583
Processing class: 46
Using PCA transformation
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.8762476206
Epoch:   200  |  train loss: 0.6969938993
Epoch:   300  |  train loss: 0.5780373812
Epoch:   400  |  train loss: 0.4931205213
Epoch:   500  |  train loss: 0.4310998678
Epoch:   600  |  train loss: 0.3831400931
Epoch:   700  |  train loss: 0.3441289246
Epoch:   800  |  train loss: 0.3117439210
Epoch:   900  |  train loss: 0.2842025578
Epoch:  1000  |  train loss: 0.2610336959
Epoch:  1100  |  train loss: 0.2417519182
Epoch:  1200  |  train loss: 0.2241801292
Epoch:  1300  |  train loss: 0.2083610326
Epoch:  1400  |  train loss: 0.1947419763
Epoch:  1500  |  train loss: 0.1831597656
Epoch:  1600  |  train loss: 0.1729089558
Epoch:  1700  |  train loss: 0.1632040679
Epoch:  1800  |  train loss: 0.1545241594
Epoch:  1900  |  train loss: 0.1478223592
Epoch:  2000  |  train loss: 0.1402479887
Processing class: 47
Using PCA transformation
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.8718513012
Epoch:   200  |  train loss: 0.6922311425
Epoch:   300  |  train loss: 0.5768693089
Epoch:   400  |  train loss: 0.4925539672
Epoch:   500  |  train loss: 0.4279718220
Epoch:   600  |  train loss: 0.3785281479
Epoch:   700  |  train loss: 0.3392198145
Epoch:   800  |  train loss: 0.3073028266
Epoch:   900  |  train loss: 0.2818125606
Epoch:  1000  |  train loss: 0.2600247979
Epoch:  1100  |  train loss: 0.2406791925
Epoch:  1200  |  train loss: 0.2239154369
Epoch:  1300  |  train loss: 0.2100282282
Epoch:  1400  |  train loss: 0.1972878456
Epoch:  1500  |  train loss: 0.1858612388
Epoch:  1600  |  train loss: 0.1756098062
Epoch:  1700  |  train loss: 0.1667585015
Epoch:  1800  |  train loss: 0.1583321780
Epoch:  1900  |  train loss: 0.1503988802
Epoch:  2000  |  train loss: 0.1440822184
Processing class: 48
Using PCA transformation
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.8744526505
Epoch:   200  |  train loss: 0.7188599467
Epoch:   300  |  train loss: 0.6031975627
Epoch:   400  |  train loss: 0.5196282148
Epoch:   500  |  train loss: 0.4561305940
Epoch:   600  |  train loss: 0.4061880589
Epoch:   700  |  train loss: 0.3663440168
Epoch:   800  |  train loss: 0.3314851165
Epoch:   900  |  train loss: 0.3028397620
Epoch:  1000  |  train loss: 0.2803041458
Epoch:  1100  |  train loss: 0.2600497901
Epoch:  1200  |  train loss: 0.2427636921
Epoch:  1300  |  train loss: 0.2279521197
Epoch:  1400  |  train loss: 0.2143121928
Epoch:  1500  |  train loss: 0.2025215209
Epoch:  1600  |  train loss: 0.1919146419
Epoch:  1700  |  train loss: 0.1817978770
Epoch:  1800  |  train loss: 0.1729517490
Epoch:  1900  |  train loss: 0.1650804132
Epoch:  2000  |  train loss: 0.1572809964
Processing class: 49
Using PCA transformation
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.8679663658
Epoch:   200  |  train loss: 0.7040764689
Epoch:   300  |  train loss: 0.5823567986
Epoch:   400  |  train loss: 0.4948579550
Epoch:   500  |  train loss: 0.4305677474
Epoch:   600  |  train loss: 0.3814669907
Epoch:   700  |  train loss: 0.3412856042
Epoch:   800  |  train loss: 0.3080614328
Epoch:   900  |  train loss: 0.2812698305
Epoch:  1000  |  train loss: 0.2584570885
Epoch:  1100  |  train loss: 0.2378309935
Epoch:  1200  |  train loss: 0.2207569271
Epoch:  1300  |  train loss: 0.2052235752
Epoch:  1400  |  train loss: 0.1914248616
Epoch:  1500  |  train loss: 0.1800444663
Epoch:  1600  |  train loss: 0.1698736221
Epoch:  1700  |  train loss: 0.1603824914
Epoch:  1800  |  train loss: 0.1517030597
Epoch:  1900  |  train loss: 0.1445227385
Epoch:  2000  |  train loss: 0.1375006735
Clasifying using reconstruction function cost
2024-03-18 00:52:02,571 [trainer.py] => CNN: {'total': 83.44, '00-09': 87.7, '10-19': 79.6, '20-29': 84.9, '30-39': 81.0, '40-49': 84.0, 'old': 0, 'new': 83.44}
2024-03-18 00:52:02,575 [trainer.py] => No NME accuracy
2024-03-18 00:52:02,575 [trainer.py] => FeCAM: {'total': 78.74, '00-09': 85.0, '10-19': 74.4, '20-29': 79.3, '30-39': 75.8, '40-49': 79.2, 'old': 0, 'new': 78.74}
2024-03-18 00:52:02,575 [trainer.py] => CNN top1 curve: [83.44]
2024-03-18 00:52:02,575 [trainer.py] => CNN top5 curve: [96.5]
2024-03-18 00:52:02,575 [trainer.py] => FeCAM top1 curve: [78.74]
2024-03-18 00:52:02,575 [trainer.py] => FeCAM top5 curve: [89.46]

2024-03-18 00:52:02,595 [fecam.py] => Learning on 50-60
/home/z1165703/miniconda3/lib/python3.11/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
/home/z1165703/miniconda3/lib/python3.11/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
/home/z1165703/miniconda3/lib/python3.11/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
Processing class: 50
Using PCA transformation
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.8706378579
Epoch:   200  |  train loss: 0.6983090997
Epoch:   300  |  train loss: 0.5757930398
Epoch:   400  |  train loss: 0.4932202637
Epoch:   500  |  train loss: 0.4316518247
Epoch:   600  |  train loss: 0.3848236442
Epoch:   700  |  train loss: 0.3451603532
Epoch:   800  |  train loss: 0.3133824468
Epoch:   900  |  train loss: 0.2874857783
Epoch:  1000  |  train loss: 0.2653489172
Epoch:  1100  |  train loss: 0.2461238772
Epoch:  1200  |  train loss: 0.2297998250
Epoch:  1300  |  train loss: 0.2147862643
Epoch:  1400  |  train loss: 0.2020659089
Epoch:  1500  |  train loss: 0.1911438644
Epoch:  1600  |  train loss: 0.1803548157
Epoch:  1700  |  train loss: 0.1715999752
Epoch:  1800  |  train loss: 0.1633783430
Epoch:  1900  |  train loss: 0.1555494606
Epoch:  2000  |  train loss: 0.1492826313
Processing class: 51
Using PCA transformation
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.8685785770
Epoch:   200  |  train loss: 0.6872519732
Epoch:   300  |  train loss: 0.5591900349
Epoch:   400  |  train loss: 0.4697337151
Epoch:   500  |  train loss: 0.4026439786
Epoch:   600  |  train loss: 0.3538511753
Epoch:   700  |  train loss: 0.3159310997
Epoch:   800  |  train loss: 0.2857551873
Epoch:   900  |  train loss: 0.2598952949
Epoch:  1000  |  train loss: 0.2387904972
Epoch:  1100  |  train loss: 0.2200537503
Epoch:  1200  |  train loss: 0.2042869061
Epoch:  1300  |  train loss: 0.1901717156
Epoch:  1400  |  train loss: 0.1776138425
Epoch:  1500  |  train loss: 0.1662147790
Epoch:  1600  |  train loss: 0.1566024601
Epoch:  1700  |  train loss: 0.1476989299
Epoch:  1800  |  train loss: 0.1396664798
Epoch:  1900  |  train loss: 0.1326042920
Epoch:  2000  |  train loss: 0.1260150164
Processing class: 52
Using PCA transformation
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.8697251797
Epoch:   200  |  train loss: 0.6923413873
Epoch:   300  |  train loss: 0.5719937444
Epoch:   400  |  train loss: 0.4843420267
Epoch:   500  |  train loss: 0.4173785269
Epoch:   600  |  train loss: 0.3659683645
Epoch:   700  |  train loss: 0.3256051719
Epoch:   800  |  train loss: 0.2929465294
Epoch:   900  |  train loss: 0.2655204475
Epoch:  1000  |  train loss: 0.2434900224
Epoch:  1100  |  train loss: 0.2245993465
Epoch:  1200  |  train loss: 0.2089165807
Epoch:  1300  |  train loss: 0.1953351945
Epoch:  1400  |  train loss: 0.1826861352
Epoch:  1500  |  train loss: 0.1717378139
Epoch:  1600  |  train loss: 0.1619789779
Epoch:  1700  |  train loss: 0.1528518856
Epoch:  1800  |  train loss: 0.1451954544
Epoch:  1900  |  train loss: 0.1381345779
Epoch:  2000  |  train loss: 0.1314289808
Processing class: 53
Using PCA transformation
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.8721013308
Epoch:   200  |  train loss: 0.6848534703
Epoch:   300  |  train loss: 0.5678422451
Epoch:   400  |  train loss: 0.4823937416
Epoch:   500  |  train loss: 0.4196739435
Epoch:   600  |  train loss: 0.3705160618
Epoch:   700  |  train loss: 0.3330765605
Epoch:   800  |  train loss: 0.3015183508
Epoch:   900  |  train loss: 0.2762389243
Epoch:  1000  |  train loss: 0.2539687485
Epoch:  1100  |  train loss: 0.2353884041
Epoch:  1200  |  train loss: 0.2185201526
Epoch:  1300  |  train loss: 0.2047774464
Epoch:  1400  |  train loss: 0.1931742162
Epoch:  1500  |  train loss: 0.1815654069
Epoch:  1600  |  train loss: 0.1717716575
Epoch:  1700  |  train loss: 0.1625819117
Epoch:  1800  |  train loss: 0.1551819801
Epoch:  1900  |  train loss: 0.1473564386
Epoch:  2000  |  train loss: 0.1407893091
Processing class: 54
Using PCA transformation
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.8637581944
Epoch:   200  |  train loss: 0.6686877966
Epoch:   300  |  train loss: 0.5465153933
Epoch:   400  |  train loss: 0.4614490926
Epoch:   500  |  train loss: 0.3972371638
Epoch:   600  |  train loss: 0.3481279314
Epoch:   700  |  train loss: 0.3098488867
Epoch:   800  |  train loss: 0.2789854348
Epoch:   900  |  train loss: 0.2537235439
Epoch:  1000  |  train loss: 0.2327417165
Epoch:  1100  |  train loss: 0.2145439982
Epoch:  1200  |  train loss: 0.1990205646
Epoch:  1300  |  train loss: 0.1855708331
Epoch:  1400  |  train loss: 0.1738602281
Epoch:  1500  |  train loss: 0.1638944238
Epoch:  1600  |  train loss: 0.1543932468
Epoch:  1700  |  train loss: 0.1463377565
Epoch:  1800  |  train loss: 0.1391259164
Epoch:  1900  |  train loss: 0.1321519136
Epoch:  2000  |  train loss: 0.1263116375
Processing class: 55
Using PCA transformation
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.8638359666
Epoch:   200  |  train loss: 0.6801215649
Epoch:   300  |  train loss: 0.5494686604
Epoch:   400  |  train loss: 0.4603086650
Epoch:   500  |  train loss: 0.3951392949
Epoch:   600  |  train loss: 0.3460308969
Epoch:   700  |  train loss: 0.3085077345
Epoch:   800  |  train loss: 0.2778171539
Epoch:   900  |  train loss: 0.2533282280
Epoch:  1000  |  train loss: 0.2333910614
Epoch:  1100  |  train loss: 0.2161253691
Epoch:  1200  |  train loss: 0.2009955138
Epoch:  1300  |  train loss: 0.1885695577
Epoch:  1400  |  train loss: 0.1763987124
Epoch:  1500  |  train loss: 0.1659732014
Epoch:  1600  |  train loss: 0.1572848052
Epoch:  1700  |  train loss: 0.1486941516
Epoch:  1800  |  train loss: 0.1413884073
Epoch:  1900  |  train loss: 0.1347061068
Epoch:  2000  |  train loss: 0.1288151294
Processing class: 56
Using PCA transformation
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.8764316916
Epoch:   200  |  train loss: 0.6883930802
Epoch:   300  |  train loss: 0.5718730927
Epoch:   400  |  train loss: 0.4888229966
Epoch:   500  |  train loss: 0.4283211112
Epoch:   600  |  train loss: 0.3802074194
Epoch:   700  |  train loss: 0.3422981739
Epoch:   800  |  train loss: 0.3116533875
Epoch:   900  |  train loss: 0.2848221838
Epoch:  1000  |  train loss: 0.2626700997
Epoch:  1100  |  train loss: 0.2445594102
Epoch:  1200  |  train loss: 0.2281416446
Epoch:  1300  |  train loss: 0.2134891003
Epoch:  1400  |  train loss: 0.2009979635
Epoch:  1500  |  train loss: 0.1894160122
Epoch:  1600  |  train loss: 0.1797735244
Epoch:  1700  |  train loss: 0.1710594416
Epoch:  1800  |  train loss: 0.1624347895
Epoch:  1900  |  train loss: 0.1547441959
Epoch:  2000  |  train loss: 0.1481296450
Processing class: 57
Using PCA transformation
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.8689483643
Epoch:   200  |  train loss: 0.6700203180
Epoch:   300  |  train loss: 0.5378965855
Epoch:   400  |  train loss: 0.4503146589
Epoch:   500  |  train loss: 0.3875656366
Epoch:   600  |  train loss: 0.3384483337
Epoch:   700  |  train loss: 0.3007308125
Epoch:   800  |  train loss: 0.2690110683
Epoch:   900  |  train loss: 0.2434257001
Epoch:  1000  |  train loss: 0.2228807956
Epoch:  1100  |  train loss: 0.2051583827
Epoch:  1200  |  train loss: 0.1904518813
Epoch:  1300  |  train loss: 0.1771368682
Epoch:  1400  |  train loss: 0.1655676991
Epoch:  1500  |  train loss: 0.1561075330
Epoch:  1600  |  train loss: 0.1476335734
Epoch:  1700  |  train loss: 0.1395332843
Epoch:  1800  |  train loss: 0.1323514998
Epoch:  1900  |  train loss: 0.1252287015
Epoch:  2000  |  train loss: 0.1198001057
Processing class: 58
Using PCA transformation
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.8731383204
Epoch:   200  |  train loss: 0.6874068975
Epoch:   300  |  train loss: 0.5616874099
Epoch:   400  |  train loss: 0.4729695201
Epoch:   500  |  train loss: 0.4069596708
Epoch:   600  |  train loss: 0.3577881634
Epoch:   700  |  train loss: 0.3193388760
Epoch:   800  |  train loss: 0.2871771634
Epoch:   900  |  train loss: 0.2619406819
Epoch:  1000  |  train loss: 0.2407602429
Epoch:  1100  |  train loss: 0.2220977098
Epoch:  1200  |  train loss: 0.2066778243
Epoch:  1300  |  train loss: 0.1929868132
Epoch:  1400  |  train loss: 0.1807599783
Epoch:  1500  |  train loss: 0.1704479575
Epoch:  1600  |  train loss: 0.1604357123
Epoch:  1700  |  train loss: 0.1522438496
Epoch:  1800  |  train loss: 0.1441985458
Epoch:  1900  |  train loss: 0.1383207440
Epoch:  2000  |  train loss: 0.1319623441
Processing class: 59
Using PCA transformation
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.8745211005
Epoch:   200  |  train loss: 0.6911945343
Epoch:   300  |  train loss: 0.5684303761
Epoch:   400  |  train loss: 0.4827251494
Epoch:   500  |  train loss: 0.4173846424
Epoch:   600  |  train loss: 0.3673945785
Epoch:   700  |  train loss: 0.3275219142
Epoch:   800  |  train loss: 0.2955808699
Epoch:   900  |  train loss: 0.2696160495
Epoch:  1000  |  train loss: 0.2469013721
Epoch:  1100  |  train loss: 0.2284235090
Epoch:  1200  |  train loss: 0.2128422737
Epoch:  1300  |  train loss: 0.1984649569
Epoch:  1400  |  train loss: 0.1858709335
Epoch:  1500  |  train loss: 0.1750683784
Epoch:  1600  |  train loss: 0.1644883841
Epoch:  1700  |  train loss: 0.1561172515
Epoch:  1800  |  train loss: 0.1478267401
Epoch:  1900  |  train loss: 0.1408942670
Epoch:  2000  |  train loss: 0.1348800540
Clasifying using reconstruction function cost
2024-03-18 01:05:21,013 [trainer.py] => CNN: {'total': 71.75, '00-09': 82.2, '10-19': 72.8, '20-29': 78.5, '30-39': 74.9, '40-49': 68.4, '50-59': 53.7, 'old': 75.36, 'new': 53.7}
2024-03-18 01:05:21,014 [trainer.py] => No NME accuracy
2024-03-18 01:05:21,015 [trainer.py] => FeCAM: {'total': 66.0, '00-09': 81.2, '10-19': 68.0, '20-29': 75.4, '30-39': 71.2, '40-49': 72.3, '50-59': 27.9, 'old': 73.62, 'new': 27.9}
2024-03-18 01:05:21,015 [trainer.py] => CNN top1 curve: [83.44, 71.75]
2024-03-18 01:05:21,015 [trainer.py] => CNN top5 curve: [96.5, 89.6]
2024-03-18 01:05:21,015 [trainer.py] => FeCAM top1 curve: [78.74, 66.0]
2024-03-18 01:05:21,015 [trainer.py] => FeCAM top5 curve: [89.46, 83.07]

2024-03-18 01:05:21,027 [fecam.py] => Learning on 60-70
/home/z1165703/miniconda3/lib/python3.11/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
/home/z1165703/miniconda3/lib/python3.11/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
/home/z1165703/miniconda3/lib/python3.11/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
Processing class: 60
Using PCA transformation
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.8683891058
Epoch:   200  |  train loss: 0.6832673430
Epoch:   300  |  train loss: 0.5493858218
Epoch:   400  |  train loss: 0.4601538599
Epoch:   500  |  train loss: 0.3976491749
Epoch:   600  |  train loss: 0.3506795287
Epoch:   700  |  train loss: 0.3141622603
Epoch:   800  |  train loss: 0.2846474349
Epoch:   900  |  train loss: 0.2593467593
Epoch:  1000  |  train loss: 0.2388394982
Epoch:  1100  |  train loss: 0.2203602374
Epoch:  1200  |  train loss: 0.2043045908
Epoch:  1300  |  train loss: 0.1907854199
Epoch:  1400  |  train loss: 0.1796945959
Epoch:  1500  |  train loss: 0.1693417042
Epoch:  1600  |  train loss: 0.1596027493
Epoch:  1700  |  train loss: 0.1514903486
Epoch:  1800  |  train loss: 0.1436947823
Epoch:  1900  |  train loss: 0.1369531691
Epoch:  2000  |  train loss: 0.1307819158
Processing class: 61
Using PCA transformation
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.8693600178
Epoch:   200  |  train loss: 0.6857934237
Epoch:   300  |  train loss: 0.5618498564
Epoch:   400  |  train loss: 0.4718256235
Epoch:   500  |  train loss: 0.4048140585
Epoch:   600  |  train loss: 0.3545152843
Epoch:   700  |  train loss: 0.3144535780
Epoch:   800  |  train loss: 0.2828489959
Epoch:   900  |  train loss: 0.2575634211
Epoch:  1000  |  train loss: 0.2360330015
Epoch:  1100  |  train loss: 0.2179005772
Epoch:  1200  |  train loss: 0.2020631284
Epoch:  1300  |  train loss: 0.1884403557
Epoch:  1400  |  train loss: 0.1767425805
Epoch:  1500  |  train loss: 0.1663583457
Epoch:  1600  |  train loss: 0.1574445337
Epoch:  1700  |  train loss: 0.1490289927
Epoch:  1800  |  train loss: 0.1422306985
Epoch:  1900  |  train loss: 0.1352223456
Epoch:  2000  |  train loss: 0.1287605479
Processing class: 62
Using PCA transformation
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.8649304509
Epoch:   200  |  train loss: 0.6858607650
Epoch:   300  |  train loss: 0.5575789690
Epoch:   400  |  train loss: 0.4683689535
Epoch:   500  |  train loss: 0.4046857774
Epoch:   600  |  train loss: 0.3563031852
Epoch:   700  |  train loss: 0.3172767520
Epoch:   800  |  train loss: 0.2865345538
Epoch:   900  |  train loss: 0.2608023375
Epoch:  1000  |  train loss: 0.2397135466
Epoch:  1100  |  train loss: 0.2215022922
Epoch:  1200  |  train loss: 0.2056607038
Epoch:  1300  |  train loss: 0.1920236379
Epoch:  1400  |  train loss: 0.1801712185
Epoch:  1500  |  train loss: 0.1692634881
Epoch:  1600  |  train loss: 0.1601687610
Epoch:  1700  |  train loss: 0.1510604173
Epoch:  1800  |  train loss: 0.1442183077
Epoch:  1900  |  train loss: 0.1368541330
Epoch:  2000  |  train loss: 0.1309424758
Processing class: 63
Using PCA transformation
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.8695315242
Epoch:   200  |  train loss: 0.6919764042
Epoch:   300  |  train loss: 0.5752645135
Epoch:   400  |  train loss: 0.4932940066
Epoch:   500  |  train loss: 0.4288358033
Epoch:   600  |  train loss: 0.3791601658
Epoch:   700  |  train loss: 0.3395603657
Epoch:   800  |  train loss: 0.3077706456
Epoch:   900  |  train loss: 0.2807696760
Epoch:  1000  |  train loss: 0.2581780463
Epoch:  1100  |  train loss: 0.2393547833
Epoch:  1200  |  train loss: 0.2217894614
Epoch:  1300  |  train loss: 0.2076487452
Epoch:  1400  |  train loss: 0.1943746984
Epoch:  1500  |  train loss: 0.1829559267
Epoch:  1600  |  train loss: 0.1727922946
Epoch:  1700  |  train loss: 0.1635976583
Epoch:  1800  |  train loss: 0.1551298946
Epoch:  1900  |  train loss: 0.1481161952
Epoch:  2000  |  train loss: 0.1407374620
Processing class: 64
Using PCA transformation
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.8656257391
Epoch:   200  |  train loss: 0.6706821203
Epoch:   300  |  train loss: 0.5431110620
Epoch:   400  |  train loss: 0.4534468949
Epoch:   500  |  train loss: 0.3875979364
Epoch:   600  |  train loss: 0.3381583154
Epoch:   700  |  train loss: 0.2974527419
Epoch:   800  |  train loss: 0.2648299515
Epoch:   900  |  train loss: 0.2394029588
Epoch:  1000  |  train loss: 0.2181898952
Epoch:  1100  |  train loss: 0.2005942374
Epoch:  1200  |  train loss: 0.1851736546
Epoch:  1300  |  train loss: 0.1725077391
Epoch:  1400  |  train loss: 0.1612249970
Epoch:  1500  |  train loss: 0.1509391040
Epoch:  1600  |  train loss: 0.1417882591
Epoch:  1700  |  train loss: 0.1336601183
Epoch:  1800  |  train loss: 0.1263206959
Epoch:  1900  |  train loss: 0.1203813881
Epoch:  2000  |  train loss: 0.1143502057
Processing class: 65
Using PCA transformation
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.8676118493
Epoch:   200  |  train loss: 0.6901636481
Epoch:   300  |  train loss: 0.5716422558
Epoch:   400  |  train loss: 0.4856083155
Epoch:   500  |  train loss: 0.4228744149
Epoch:   600  |  train loss: 0.3747504950
Epoch:   700  |  train loss: 0.3374236763
Epoch:   800  |  train loss: 0.3064365804
Epoch:   900  |  train loss: 0.2800811768
Epoch:  1000  |  train loss: 0.2587731689
Epoch:  1100  |  train loss: 0.2393563807
Epoch:  1200  |  train loss: 0.2237270623
Epoch:  1300  |  train loss: 0.2094704390
Epoch:  1400  |  train loss: 0.1960371017
Epoch:  1500  |  train loss: 0.1855073065
Epoch:  1600  |  train loss: 0.1750101715
Epoch:  1700  |  train loss: 0.1661373377
Epoch:  1800  |  train loss: 0.1578378946
Epoch:  1900  |  train loss: 0.1500703841
Epoch:  2000  |  train loss: 0.1434171557
Processing class: 66
Using PCA transformation
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.8636506438
Epoch:   200  |  train loss: 0.6707529068
Epoch:   300  |  train loss: 0.5413276792
Epoch:   400  |  train loss: 0.4524351358
Epoch:   500  |  train loss: 0.3876561463
Epoch:   600  |  train loss: 0.3374422550
Epoch:   700  |  train loss: 0.2981579602
Epoch:   800  |  train loss: 0.2668683022
Epoch:   900  |  train loss: 0.2418216795
Epoch:  1000  |  train loss: 0.2206528127
Epoch:  1100  |  train loss: 0.2031126708
Epoch:  1200  |  train loss: 0.1878598720
Epoch:  1300  |  train loss: 0.1744771808
Epoch:  1400  |  train loss: 0.1627627373
Epoch:  1500  |  train loss: 0.1524934471
Epoch:  1600  |  train loss: 0.1435276210
Epoch:  1700  |  train loss: 0.1352506429
Epoch:  1800  |  train loss: 0.1290790141
Epoch:  1900  |  train loss: 0.1219626859
Epoch:  2000  |  train loss: 0.1161157131
Processing class: 67
Using PCA transformation
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.8749798536
Epoch:   200  |  train loss: 0.7060107470
Epoch:   300  |  train loss: 0.5764164925
Epoch:   400  |  train loss: 0.4818083346
Epoch:   500  |  train loss: 0.4133274019
Epoch:   600  |  train loss: 0.3618006527
Epoch:   700  |  train loss: 0.3219017029
Epoch:   800  |  train loss: 0.2903828681
Epoch:   900  |  train loss: 0.2637684703
Epoch:  1000  |  train loss: 0.2423966765
Epoch:  1100  |  train loss: 0.2240304381
Epoch:  1200  |  train loss: 0.2079524189
Epoch:  1300  |  train loss: 0.1941654652
Epoch:  1400  |  train loss: 0.1819613367
Epoch:  1500  |  train loss: 0.1703027815
Epoch:  1600  |  train loss: 0.1610370129
Epoch:  1700  |  train loss: 0.1525459379
Epoch:  1800  |  train loss: 0.1450100988
Epoch:  1900  |  train loss: 0.1373656362
Epoch:  2000  |  train loss: 0.1315102249
Processing class: 68
Using PCA transformation
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.8726259589
Epoch:   200  |  train loss: 0.6820007682
Epoch:   300  |  train loss: 0.5571364999
Epoch:   400  |  train loss: 0.4721812308
Epoch:   500  |  train loss: 0.4103846014
Epoch:   600  |  train loss: 0.3619212449
Epoch:   700  |  train loss: 0.3231396019
Epoch:   800  |  train loss: 0.2922297955
Epoch:   900  |  train loss: 0.2664343327
Epoch:  1000  |  train loss: 0.2453346193
Epoch:  1100  |  train loss: 0.2278149843
Epoch:  1200  |  train loss: 0.2122040004
Epoch:  1300  |  train loss: 0.2000948638
Epoch:  1400  |  train loss: 0.1883199275
Epoch:  1500  |  train loss: 0.1785018682
Epoch:  1600  |  train loss: 0.1688720077
Epoch:  1700  |  train loss: 0.1604722053
Epoch:  1800  |  train loss: 0.1533295602
Epoch:  1900  |  train loss: 0.1463636935
Epoch:  2000  |  train loss: 0.1403338253
Processing class: 69
Using PCA transformation
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.8629595041
Epoch:   200  |  train loss: 0.6840413451
Epoch:   300  |  train loss: 0.5493151188
Epoch:   400  |  train loss: 0.4582186878
Epoch:   500  |  train loss: 0.3918192089
Epoch:   600  |  train loss: 0.3428276896
Epoch:   700  |  train loss: 0.3043183208
Epoch:   800  |  train loss: 0.2737384975
Epoch:   900  |  train loss: 0.2482172251
Epoch:  1000  |  train loss: 0.2267368197
Epoch:  1100  |  train loss: 0.2092077821
Epoch:  1200  |  train loss: 0.1928725123
Epoch:  1300  |  train loss: 0.1808199495
Epoch:  1400  |  train loss: 0.1674830496
Epoch:  1500  |  train loss: 0.1573834687
Epoch:  1600  |  train loss: 0.1481627375
Epoch:  1700  |  train loss: 0.1395010769
Epoch:  1800  |  train loss: 0.1325815707
Epoch:  1900  |  train loss: 0.1252639860
Epoch:  2000  |  train loss: 0.1190224454
Clasifying using reconstruction function cost
2024-03-18 01:21:29,948 [trainer.py] => CNN: {'total': 64.66, '00-09': 75.0, '10-19': 70.1, '20-29': 77.1, '30-39': 70.6, '40-49': 64.0, '50-59': 45.8, '60-69': 50.0, 'old': 67.1, 'new': 50.0}
2024-03-18 01:21:29,949 [trainer.py] => No NME accuracy
2024-03-18 01:21:29,950 [trainer.py] => FeCAM: {'total': 58.77, '00-09': 79.5, '10-19': 65.3, '20-29': 73.6, '30-39': 69.0, '40-49': 70.2, '50-59': 24.4, '60-69': 29.4, 'old': 63.67, 'new': 29.4}
2024-03-18 01:21:29,950 [trainer.py] => CNN top1 curve: [83.44, 71.75, 64.66]
2024-03-18 01:21:29,950 [trainer.py] => CNN top5 curve: [96.5, 89.6, 86.54]
2024-03-18 01:21:29,950 [trainer.py] => FeCAM top1 curve: [78.74, 66.0, 58.77]
2024-03-18 01:21:29,950 [trainer.py] => FeCAM top5 curve: [89.46, 83.07, 77.74]

2024-03-18 01:21:29,962 [fecam.py] => Learning on 70-80
/home/z1165703/miniconda3/lib/python3.11/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
/home/z1165703/miniconda3/lib/python3.11/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
/home/z1165703/miniconda3/lib/python3.11/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
Processing class: 70
Using PCA transformation
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.8798394918
Epoch:   200  |  train loss: 0.7118332386
Epoch:   300  |  train loss: 0.5865490913
Epoch:   400  |  train loss: 0.5008310318
Epoch:   500  |  train loss: 0.4356097102
Epoch:   600  |  train loss: 0.3821296155
Epoch:   700  |  train loss: 0.3422778964
Epoch:   800  |  train loss: 0.3082128108
Epoch:   900  |  train loss: 0.2825722158
Epoch:  1000  |  train loss: 0.2607371032
Epoch:  1100  |  train loss: 0.2412215233
Epoch:  1200  |  train loss: 0.2253710121
Epoch:  1300  |  train loss: 0.2110716790
Epoch:  1400  |  train loss: 0.1985684812
Epoch:  1500  |  train loss: 0.1871851414
Epoch:  1600  |  train loss: 0.1778587908
Epoch:  1700  |  train loss: 0.1691647440
Epoch:  1800  |  train loss: 0.1599135756
Epoch:  1900  |  train loss: 0.1536609530
Epoch:  2000  |  train loss: 0.1460934818
Processing class: 71
Using PCA transformation
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.8739220738
Epoch:   200  |  train loss: 0.6964484572
Epoch:   300  |  train loss: 0.5654349804
Epoch:   400  |  train loss: 0.4772260487
Epoch:   500  |  train loss: 0.4144566298
Epoch:   600  |  train loss: 0.3671869576
Epoch:   700  |  train loss: 0.3288520873
Epoch:   800  |  train loss: 0.2982094109
Epoch:   900  |  train loss: 0.2712194383
Epoch:  1000  |  train loss: 0.2490672737
Epoch:  1100  |  train loss: 0.2308402330
Epoch:  1200  |  train loss: 0.2155174196
Epoch:  1300  |  train loss: 0.2004379928
Epoch:  1400  |  train loss: 0.1884637624
Epoch:  1500  |  train loss: 0.1774726987
Epoch:  1600  |  train loss: 0.1687260747
Epoch:  1700  |  train loss: 0.1589087069
Epoch:  1800  |  train loss: 0.1512363762
Epoch:  1900  |  train loss: 0.1432595789
Epoch:  2000  |  train loss: 0.1375878066
Processing class: 72
Using PCA transformation
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.8713768005
Epoch:   200  |  train loss: 0.6884346008
Epoch:   300  |  train loss: 0.5650029659
Epoch:   400  |  train loss: 0.4776360571
Epoch:   500  |  train loss: 0.4137754679
Epoch:   600  |  train loss: 0.3653703690
Epoch:   700  |  train loss: 0.3260169208
Epoch:   800  |  train loss: 0.2947141290
Epoch:   900  |  train loss: 0.2686472237
Epoch:  1000  |  train loss: 0.2463685334
Epoch:  1100  |  train loss: 0.2277612984
Epoch:  1200  |  train loss: 0.2121172726
Epoch:  1300  |  train loss: 0.1976418108
Epoch:  1400  |  train loss: 0.1861555487
Epoch:  1500  |  train loss: 0.1747397989
Epoch:  1600  |  train loss: 0.1649369985
Epoch:  1700  |  train loss: 0.1567131042
Epoch:  1800  |  train loss: 0.1478742748
Epoch:  1900  |  train loss: 0.1415384352
Epoch:  2000  |  train loss: 0.1341403991
Processing class: 73
Using PCA transformation
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.8744585276
Epoch:   200  |  train loss: 0.7120101690
Epoch:   300  |  train loss: 0.5988285184
Epoch:   400  |  train loss: 0.5144456506
Epoch:   500  |  train loss: 0.4511243284
Epoch:   600  |  train loss: 0.4030178189
Epoch:   700  |  train loss: 0.3636584699
Epoch:   800  |  train loss: 0.3314647317
Epoch:   900  |  train loss: 0.3042902827
Epoch:  1000  |  train loss: 0.2807879329
Epoch:  1100  |  train loss: 0.2607657373
Epoch:  1200  |  train loss: 0.2432311505
Epoch:  1300  |  train loss: 0.2285365015
Epoch:  1400  |  train loss: 0.2148277849
Epoch:  1500  |  train loss: 0.2023536056
Epoch:  1600  |  train loss: 0.1920569569
Epoch:  1700  |  train loss: 0.1816098541
Epoch:  1800  |  train loss: 0.1728978634
Epoch:  1900  |  train loss: 0.1650661439
Epoch:  2000  |  train loss: 0.1564184666
Processing class: 74
Using PCA transformation
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.8652302742
Epoch:   200  |  train loss: 0.6760863662
Epoch:   300  |  train loss: 0.5452920318
Epoch:   400  |  train loss: 0.4573332191
Epoch:   500  |  train loss: 0.3926541030
Epoch:   600  |  train loss: 0.3447554231
Epoch:   700  |  train loss: 0.3071149886
Epoch:   800  |  train loss: 0.2773561180
Epoch:   900  |  train loss: 0.2528972119
Epoch:  1000  |  train loss: 0.2326812327
Epoch:  1100  |  train loss: 0.2153173804
Epoch:  1200  |  train loss: 0.1994656891
Epoch:  1300  |  train loss: 0.1865674853
Epoch:  1400  |  train loss: 0.1747290522
Epoch:  1500  |  train loss: 0.1648268670
Epoch:  1600  |  train loss: 0.1564223617
Epoch:  1700  |  train loss: 0.1481140494
Epoch:  1800  |  train loss: 0.1403617084
Epoch:  1900  |  train loss: 0.1338969469
Epoch:  2000  |  train loss: 0.1281883895
Processing class: 75
Using PCA transformation
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.8677292228
Epoch:   200  |  train loss: 0.6843446732
Epoch:   300  |  train loss: 0.5689921379
Epoch:   400  |  train loss: 0.4882903516
Epoch:   500  |  train loss: 0.4278127313
Epoch:   600  |  train loss: 0.3786516249
Epoch:   700  |  train loss: 0.3399374425
Epoch:   800  |  train loss: 0.3077947259
Epoch:   900  |  train loss: 0.2814588249
Epoch:  1000  |  train loss: 0.2589779913
Epoch:  1100  |  train loss: 0.2397399515
Epoch:  1200  |  train loss: 0.2226314902
Epoch:  1300  |  train loss: 0.2086002499
Epoch:  1400  |  train loss: 0.1962926656
Epoch:  1500  |  train loss: 0.1848140597
Epoch:  1600  |  train loss: 0.1748815447
Epoch:  1700  |  train loss: 0.1662437737
Epoch:  1800  |  train loss: 0.1575833201
Epoch:  1900  |  train loss: 0.1500666171
Epoch:  2000  |  train loss: 0.1434598029
Processing class: 76
Using PCA transformation
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.8639999986
Epoch:   200  |  train loss: 0.6675464511
Epoch:   300  |  train loss: 0.5445894420
Epoch:   400  |  train loss: 0.4575959802
Epoch:   500  |  train loss: 0.3942480981
Epoch:   600  |  train loss: 0.3449038267
Epoch:   700  |  train loss: 0.3061472237
Epoch:   800  |  train loss: 0.2756572127
Epoch:   900  |  train loss: 0.2517225027
Epoch:  1000  |  train loss: 0.2307036847
Epoch:  1100  |  train loss: 0.2120405287
Epoch:  1200  |  train loss: 0.1967481077
Epoch:  1300  |  train loss: 0.1830609202
Epoch:  1400  |  train loss: 0.1718830049
Epoch:  1500  |  train loss: 0.1610006660
Epoch:  1600  |  train loss: 0.1516227126
Epoch:  1700  |  train loss: 0.1439980984
Epoch:  1800  |  train loss: 0.1362671614
Epoch:  1900  |  train loss: 0.1292598054
Epoch:  2000  |  train loss: 0.1236482233
Processing class: 77
Using PCA transformation
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.8711867690
Epoch:   200  |  train loss: 0.6869225860
Epoch:   300  |  train loss: 0.5573502898
Epoch:   400  |  train loss: 0.4679834247
Epoch:   500  |  train loss: 0.4026174426
Epoch:   600  |  train loss: 0.3535493970
Epoch:   700  |  train loss: 0.3148027182
Epoch:   800  |  train loss: 0.2840615630
Epoch:   900  |  train loss: 0.2582648575
Epoch:  1000  |  train loss: 0.2356889039
Epoch:  1100  |  train loss: 0.2174885541
Epoch:  1200  |  train loss: 0.2016591519
Epoch:  1300  |  train loss: 0.1884312809
Epoch:  1400  |  train loss: 0.1760306478
Epoch:  1500  |  train loss: 0.1656065106
Epoch:  1600  |  train loss: 0.1566344917
Epoch:  1700  |  train loss: 0.1477874845
Epoch:  1800  |  train loss: 0.1399203658
Epoch:  1900  |  train loss: 0.1339162976
Epoch:  2000  |  train loss: 0.1273851499
Processing class: 78
Using PCA transformation
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.8685787082
Epoch:   200  |  train loss: 0.6911749244
Epoch:   300  |  train loss: 0.5686872363
Epoch:   400  |  train loss: 0.4798563957
Epoch:   500  |  train loss: 0.4148306310
Epoch:   600  |  train loss: 0.3654214978
Epoch:   700  |  train loss: 0.3266446948
Epoch:   800  |  train loss: 0.2956523895
Epoch:   900  |  train loss: 0.2692755878
Epoch:  1000  |  train loss: 0.2478052497
Epoch:  1100  |  train loss: 0.2285241485
Epoch:  1200  |  train loss: 0.2120625913
Epoch:  1300  |  train loss: 0.1977726758
Epoch:  1400  |  train loss: 0.1859870642
Epoch:  1500  |  train loss: 0.1742858410
Epoch:  1600  |  train loss: 0.1644827783
Epoch:  1700  |  train loss: 0.1561724067
Epoch:  1800  |  train loss: 0.1480007410
Epoch:  1900  |  train loss: 0.1404135346
Epoch:  2000  |  train loss: 0.1338285595
Processing class: 79
Using PCA transformation
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.8663405180
Epoch:   200  |  train loss: 0.6715524435
Epoch:   300  |  train loss: 0.5437264562
Epoch:   400  |  train loss: 0.4544707596
Epoch:   500  |  train loss: 0.3893287897
Epoch:   600  |  train loss: 0.3394340694
Epoch:   700  |  train loss: 0.3014624059
Epoch:   800  |  train loss: 0.2706473768
Epoch:   900  |  train loss: 0.2459875196
Epoch:  1000  |  train loss: 0.2252313495
Epoch:  1100  |  train loss: 0.2076552957
Epoch:  1200  |  train loss: 0.1924776405
Epoch:  1300  |  train loss: 0.1790754944
Epoch:  1400  |  train loss: 0.1674658597
Epoch:  1500  |  train loss: 0.1566697627
Epoch:  1600  |  train loss: 0.1475877583
Epoch:  1700  |  train loss: 0.1394635171
Epoch:  1800  |  train loss: 0.1326864094
Epoch:  1900  |  train loss: 0.1250144929
Epoch:  2000  |  train loss: 0.1192181110
Clasifying using reconstruction function cost
2024-03-18 01:40:53,799 [trainer.py] => CNN: {'total': 59.18, '00-09': 73.6, '10-19': 68.4, '20-29': 76.9, '30-39': 69.1, '40-49': 60.6, '50-59': 37.2, '60-69': 45.7, '70-79': 41.9, 'old': 61.64, 'new': 41.9}
2024-03-18 01:40:53,801 [trainer.py] => No NME accuracy
2024-03-18 01:40:53,801 [trainer.py] => FeCAM: {'total': 52.46, '00-09': 78.0, '10-19': 64.4, '20-29': 73.2, '30-39': 66.6, '40-49': 65.2, '50-59': 21.2, '60-69': 28.0, '70-79': 23.1, 'old': 56.66, 'new': 23.1}
2024-03-18 01:40:53,801 [trainer.py] => CNN top1 curve: [83.44, 71.75, 64.66, 59.18]
2024-03-18 01:40:53,801 [trainer.py] => CNN top5 curve: [96.5, 89.6, 86.54, 84.09]
2024-03-18 01:40:53,801 [trainer.py] => FeCAM top1 curve: [78.74, 66.0, 58.77, 52.46]
2024-03-18 01:40:53,801 [trainer.py] => FeCAM top5 curve: [89.46, 83.07, 77.74, 73.53]

2024-03-18 01:40:53,813 [fecam.py] => Learning on 80-90
/home/z1165703/miniconda3/lib/python3.11/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
/home/z1165703/miniconda3/lib/python3.11/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
/home/z1165703/miniconda3/lib/python3.11/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
Processing class: 80
Using PCA transformation
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.8590599060
Epoch:   200  |  train loss: 0.6814599991
Epoch:   300  |  train loss: 0.5551922560
Epoch:   400  |  train loss: 0.4659046113
Epoch:   500  |  train loss: 0.4023487926
Epoch:   600  |  train loss: 0.3540833235
Epoch:   700  |  train loss: 0.3145683944
Epoch:   800  |  train loss: 0.2837117732
Epoch:   900  |  train loss: 0.2577491999
Epoch:  1000  |  train loss: 0.2363198102
Epoch:  1100  |  train loss: 0.2183555514
Epoch:  1200  |  train loss: 0.2024820298
Epoch:  1300  |  train loss: 0.1889154375
Epoch:  1400  |  train loss: 0.1765862823
Epoch:  1500  |  train loss: 0.1663312346
Epoch:  1600  |  train loss: 0.1568357229
Epoch:  1700  |  train loss: 0.1481263995
Epoch:  1800  |  train loss: 0.1405319035
Epoch:  1900  |  train loss: 0.1332812935
Epoch:  2000  |  train loss: 0.1273190781
Processing class: 81
Using PCA transformation
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.8579580545
Epoch:   200  |  train loss: 0.6484923959
Epoch:   300  |  train loss: 0.5162776828
Epoch:   400  |  train loss: 0.4286645293
Epoch:   500  |  train loss: 0.3658638179
Epoch:   600  |  train loss: 0.3184339702
Epoch:   700  |  train loss: 0.2821124792
Epoch:   800  |  train loss: 0.2524030298
Epoch:   900  |  train loss: 0.2285439759
Epoch:  1000  |  train loss: 0.2085643947
Epoch:  1100  |  train loss: 0.1921244532
Epoch:  1200  |  train loss: 0.1778850138
Epoch:  1300  |  train loss: 0.1652114004
Epoch:  1400  |  train loss: 0.1543096840
Epoch:  1500  |  train loss: 0.1447677761
Epoch:  1600  |  train loss: 0.1356850177
Epoch:  1700  |  train loss: 0.1285403475
Epoch:  1800  |  train loss: 0.1219745949
Epoch:  1900  |  train loss: 0.1158142507
Epoch:  2000  |  train loss: 0.1103063792
Processing class: 82
Using PCA transformation
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.8707581639
Epoch:   200  |  train loss: 0.6957526088
Epoch:   300  |  train loss: 0.5706396341
Epoch:   400  |  train loss: 0.4806083798
Epoch:   500  |  train loss: 0.4170110643
Epoch:   600  |  train loss: 0.3675325930
Epoch:   700  |  train loss: 0.3277453184
Epoch:   800  |  train loss: 0.2960558832
Epoch:   900  |  train loss: 0.2684995890
Epoch:  1000  |  train loss: 0.2448306978
Epoch:  1100  |  train loss: 0.2256293446
Epoch:  1200  |  train loss: 0.2081764907
Epoch:  1300  |  train loss: 0.1944689900
Epoch:  1400  |  train loss: 0.1813925564
Epoch:  1500  |  train loss: 0.1707426727
Epoch:  1600  |  train loss: 0.1609753430
Epoch:  1700  |  train loss: 0.1523609459
Epoch:  1800  |  train loss: 0.1445715815
Epoch:  1900  |  train loss: 0.1369996905
Epoch:  2000  |  train loss: 0.1307299390
Processing class: 83
Using PCA transformation
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.8735377789
Epoch:   200  |  train loss: 0.6814432859
Epoch:   300  |  train loss: 0.5584556520
Epoch:   400  |  train loss: 0.4750154376
Epoch:   500  |  train loss: 0.4142572939
Epoch:   600  |  train loss: 0.3662043273
Epoch:   700  |  train loss: 0.3274816155
Epoch:   800  |  train loss: 0.2957264483
Epoch:   900  |  train loss: 0.2690812767
Epoch:  1000  |  train loss: 0.2466795146
Epoch:  1100  |  train loss: 0.2290220886
Epoch:  1200  |  train loss: 0.2133919269
Epoch:  1300  |  train loss: 0.1986328959
Epoch:  1400  |  train loss: 0.1868048489
Epoch:  1500  |  train loss: 0.1759351671
Epoch:  1600  |  train loss: 0.1665498406
Epoch:  1700  |  train loss: 0.1585318655
Epoch:  1800  |  train loss: 0.1508686960
Epoch:  1900  |  train loss: 0.1439548999
Epoch:  2000  |  train loss: 0.1370681673
Processing class: 84
Using PCA transformation
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.8639036655
Epoch:   200  |  train loss: 0.6789894700
Epoch:   300  |  train loss: 0.5523986578
Epoch:   400  |  train loss: 0.4611178875
Epoch:   500  |  train loss: 0.3951008379
Epoch:   600  |  train loss: 0.3455897570
Epoch:   700  |  train loss: 0.3071405888
Epoch:   800  |  train loss: 0.2764552414
Epoch:   900  |  train loss: 0.2509367615
Epoch:  1000  |  train loss: 0.2296792716
Epoch:  1100  |  train loss: 0.2128934264
Epoch:  1200  |  train loss: 0.1974357426
Epoch:  1300  |  train loss: 0.1839827836
Epoch:  1400  |  train loss: 0.1720498860
Epoch:  1500  |  train loss: 0.1620422125
Epoch:  1600  |  train loss: 0.1530371875
Epoch:  1700  |  train loss: 0.1450964600
Epoch:  1800  |  train loss: 0.1378259361
Epoch:  1900  |  train loss: 0.1319443494
Epoch:  2000  |  train loss: 0.1249408141
Processing class: 85
Using PCA transformation
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.8630886674
Epoch:   200  |  train loss: 0.6647784829
Epoch:   300  |  train loss: 0.5434405446
Epoch:   400  |  train loss: 0.4586354554
Epoch:   500  |  train loss: 0.3945874214
Epoch:   600  |  train loss: 0.3455930769
Epoch:   700  |  train loss: 0.3071492076
Epoch:   800  |  train loss: 0.2759873033
Epoch:   900  |  train loss: 0.2509955168
Epoch:  1000  |  train loss: 0.2307806522
Epoch:  1100  |  train loss: 0.2125624567
Epoch:  1200  |  train loss: 0.1962438971
Epoch:  1300  |  train loss: 0.1826178223
Epoch:  1400  |  train loss: 0.1713035911
Epoch:  1500  |  train loss: 0.1612556189
Epoch:  1600  |  train loss: 0.1520436525
Epoch:  1700  |  train loss: 0.1440491021
Epoch:  1800  |  train loss: 0.1361662090
Epoch:  1900  |  train loss: 0.1297946095
Epoch:  2000  |  train loss: 0.1241223365
Processing class: 86
Using PCA transformation
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.8708819866
Epoch:   200  |  train loss: 0.7007939816
Epoch:   300  |  train loss: 0.5637745261
Epoch:   400  |  train loss: 0.4690025032
Epoch:   500  |  train loss: 0.4015479207
Epoch:   600  |  train loss: 0.3523429096
Epoch:   700  |  train loss: 0.3125180721
Epoch:   800  |  train loss: 0.2807346880
Epoch:   900  |  train loss: 0.2545507729
Epoch:  1000  |  train loss: 0.2329771072
Epoch:  1100  |  train loss: 0.2141353637
Epoch:  1200  |  train loss: 0.1984883398
Epoch:  1300  |  train loss: 0.1843198061
Epoch:  1400  |  train loss: 0.1722254813
Epoch:  1500  |  train loss: 0.1615725935
Epoch:  1600  |  train loss: 0.1525550544
Epoch:  1700  |  train loss: 0.1441818804
Epoch:  1800  |  train loss: 0.1365981579
Epoch:  1900  |  train loss: 0.1293744177
Epoch:  2000  |  train loss: 0.1234698817
Processing class: 87
Using PCA transformation
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.8776753902
Epoch:   200  |  train loss: 0.6968975782
Epoch:   300  |  train loss: 0.5778791785
Epoch:   400  |  train loss: 0.4955923975
Epoch:   500  |  train loss: 0.4328237593
Epoch:   600  |  train loss: 0.3837261915
Epoch:   700  |  train loss: 0.3435872674
Epoch:   800  |  train loss: 0.3111156464
Epoch:   900  |  train loss: 0.2841840148
Epoch:  1000  |  train loss: 0.2613145530
Epoch:  1100  |  train loss: 0.2424520999
Epoch:  1200  |  train loss: 0.2254690707
Epoch:  1300  |  train loss: 0.2108746201
Epoch:  1400  |  train loss: 0.1979037523
Epoch:  1500  |  train loss: 0.1866627157
Epoch:  1600  |  train loss: 0.1761001319
Epoch:  1700  |  train loss: 0.1670534074
Epoch:  1800  |  train loss: 0.1585907936
Epoch:  1900  |  train loss: 0.1505558103
Epoch:  2000  |  train loss: 0.1443356484
Processing class: 88
Using PCA transformation
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.8729698062
Epoch:   200  |  train loss: 0.6785952926
Epoch:   300  |  train loss: 0.5466636777
Epoch:   400  |  train loss: 0.4559933186
Epoch:   500  |  train loss: 0.3920708477
Epoch:   600  |  train loss: 0.3440647185
Epoch:   700  |  train loss: 0.3062859058
Epoch:   800  |  train loss: 0.2754889369
Epoch:   900  |  train loss: 0.2503417104
Epoch:  1000  |  train loss: 0.2298284262
Epoch:  1100  |  train loss: 0.2125506461
Epoch:  1200  |  train loss: 0.1981851995
Epoch:  1300  |  train loss: 0.1848459572
Epoch:  1400  |  train loss: 0.1734416246
Epoch:  1500  |  train loss: 0.1629093826
Epoch:  1600  |  train loss: 0.1541472197
Epoch:  1700  |  train loss: 0.1461623162
Epoch:  1800  |  train loss: 0.1385360777
Epoch:  1900  |  train loss: 0.1326061249
Epoch:  2000  |  train loss: 0.1266639382
Processing class: 89
Using PCA transformation
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.8687078118
Epoch:   200  |  train loss: 0.6924355149
Epoch:   300  |  train loss: 0.5719779015
Epoch:   400  |  train loss: 0.4852901578
Epoch:   500  |  train loss: 0.4213664055
Epoch:   600  |  train loss: 0.3713458419
Epoch:   700  |  train loss: 0.3309654713
Epoch:   800  |  train loss: 0.2993677497
Epoch:   900  |  train loss: 0.2733500421
Epoch:  1000  |  train loss: 0.2511729538
Epoch:  1100  |  train loss: 0.2322022766
Epoch:  1200  |  train loss: 0.2157608062
Epoch:  1300  |  train loss: 0.2021798134
Epoch:  1400  |  train loss: 0.1897147387
Epoch:  1500  |  train loss: 0.1787705600
Epoch:  1600  |  train loss: 0.1690629929
Epoch:  1700  |  train loss: 0.1601031482
Epoch:  1800  |  train loss: 0.1526079029
Epoch:  1900  |  train loss: 0.1453498453
Epoch:  2000  |  train loss: 0.1399998605
Clasifying using reconstruction function cost
2024-03-18 02:03:58,790 [trainer.py] => CNN: {'total': 54.08, '00-09': 67.9, '10-19': 63.6, '20-29': 73.2, '30-39': 68.8, '40-49': 57.2, '50-59': 32.6, '60-69': 40.9, '70-79': 38.1, '80-89': 44.4, 'old': 55.29, 'new': 44.4}
2024-03-18 02:03:58,792 [trainer.py] => No NME accuracy
2024-03-18 02:03:58,792 [trainer.py] => FeCAM: {'total': 47.48, '00-09': 76.4, '10-19': 62.2, '20-29': 70.8, '30-39': 65.2, '40-49': 62.4, '50-59': 18.5, '60-69': 24.8, '70-79': 21.0, '80-89': 26.0, 'old': 50.16, 'new': 26.0}
2024-03-18 02:03:58,792 [trainer.py] => CNN top1 curve: [83.44, 71.75, 64.66, 59.18, 54.08]
2024-03-18 02:03:58,792 [trainer.py] => CNN top5 curve: [96.5, 89.6, 86.54, 84.09, 81.74]
2024-03-18 02:03:58,792 [trainer.py] => FeCAM top1 curve: [78.74, 66.0, 58.77, 52.46, 47.48]
2024-03-18 02:03:58,792 [trainer.py] => FeCAM top5 curve: [89.46, 83.07, 77.74, 73.53, 70.39]

2024-03-18 02:03:58,804 [fecam.py] => Learning on 90-100
/home/z1165703/miniconda3/lib/python3.11/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
/home/z1165703/miniconda3/lib/python3.11/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
/home/z1165703/miniconda3/lib/python3.11/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
Processing class: 90
Using PCA transformation
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.8672947049
Epoch:   200  |  train loss: 0.6652899504
Epoch:   300  |  train loss: 0.5354278684
Epoch:   400  |  train loss: 0.4466717362
Epoch:   500  |  train loss: 0.3820314407
Epoch:   600  |  train loss: 0.3326793432
Epoch:   700  |  train loss: 0.2927414000
Epoch:   800  |  train loss: 0.2623228014
Epoch:   900  |  train loss: 0.2372139186
Epoch:  1000  |  train loss: 0.2164460182
Epoch:  1100  |  train loss: 0.1980660409
Epoch:  1200  |  train loss: 0.1825521320
Epoch:  1300  |  train loss: 0.1700557411
Epoch:  1400  |  train loss: 0.1581205219
Epoch:  1500  |  train loss: 0.1486057788
Epoch:  1600  |  train loss: 0.1393183172
Epoch:  1700  |  train loss: 0.1316002518
Epoch:  1800  |  train loss: 0.1249031454
Epoch:  1900  |  train loss: 0.1186711714
Epoch:  2000  |  train loss: 0.1126924098
Processing class: 91
Using PCA transformation
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.8676517010
Epoch:   200  |  train loss: 0.7029061079
Epoch:   300  |  train loss: 0.5827896595
Epoch:   400  |  train loss: 0.4965211809
Epoch:   500  |  train loss: 0.4304193079
Epoch:   600  |  train loss: 0.3803659618
Epoch:   700  |  train loss: 0.3410992980
Epoch:   800  |  train loss: 0.3095957637
Epoch:   900  |  train loss: 0.2838364244
Epoch:  1000  |  train loss: 0.2620805144
Epoch:  1100  |  train loss: 0.2426864088
Epoch:  1200  |  train loss: 0.2257016331
Epoch:  1300  |  train loss: 0.2112895280
Epoch:  1400  |  train loss: 0.1983175635
Epoch:  1500  |  train loss: 0.1869335681
Epoch:  1600  |  train loss: 0.1770640314
Epoch:  1700  |  train loss: 0.1677369535
Epoch:  1800  |  train loss: 0.1592444807
Epoch:  1900  |  train loss: 0.1517079949
Epoch:  2000  |  train loss: 0.1450311333
Processing class: 92
Using PCA transformation
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.8719141960
Epoch:   200  |  train loss: 0.6752314925
Epoch:   300  |  train loss: 0.5499098778
Epoch:   400  |  train loss: 0.4599171221
Epoch:   500  |  train loss: 0.3934525192
Epoch:   600  |  train loss: 0.3426340699
Epoch:   700  |  train loss: 0.3034334064
Epoch:   800  |  train loss: 0.2728344202
Epoch:   900  |  train loss: 0.2475221723
Epoch:  1000  |  train loss: 0.2263994694
Epoch:  1100  |  train loss: 0.2085550934
Epoch:  1200  |  train loss: 0.1930319935
Epoch:  1300  |  train loss: 0.1797438949
Epoch:  1400  |  train loss: 0.1671805203
Epoch:  1500  |  train loss: 0.1568512172
Epoch:  1600  |  train loss: 0.1481514156
Epoch:  1700  |  train loss: 0.1393088549
Epoch:  1800  |  train loss: 0.1322642595
Epoch:  1900  |  train loss: 0.1247923270
Epoch:  2000  |  train loss: 0.1186881423
Processing class: 93
Using PCA transformation
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.8729464889
Epoch:   200  |  train loss: 0.6840378881
Epoch:   300  |  train loss: 0.5506991625
Epoch:   400  |  train loss: 0.4619485080
Epoch:   500  |  train loss: 0.3967484236
Epoch:   600  |  train loss: 0.3468205750
Epoch:   700  |  train loss: 0.3073745549
Epoch:   800  |  train loss: 0.2745958924
Epoch:   900  |  train loss: 0.2479958504
Epoch:  1000  |  train loss: 0.2266979277
Epoch:  1100  |  train loss: 0.2084622473
Epoch:  1200  |  train loss: 0.1928135127
Epoch:  1300  |  train loss: 0.1784073740
Epoch:  1400  |  train loss: 0.1662202209
Epoch:  1500  |  train loss: 0.1564502716
Epoch:  1600  |  train loss: 0.1471242964
Epoch:  1700  |  train loss: 0.1389883339
Epoch:  1800  |  train loss: 0.1315498799
Epoch:  1900  |  train loss: 0.1249717340
Epoch:  2000  |  train loss: 0.1186009452
Processing class: 94
Using PCA transformation
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.8608115554
Epoch:   200  |  train loss: 0.6548019052
Epoch:   300  |  train loss: 0.5171479344
Epoch:   400  |  train loss: 0.4222273350
Epoch:   500  |  train loss: 0.3577418268
Epoch:   600  |  train loss: 0.3107507646
Epoch:   700  |  train loss: 0.2755386591
Epoch:   800  |  train loss: 0.2468033731
Epoch:   900  |  train loss: 0.2243879706
Epoch:  1000  |  train loss: 0.2050385088
Epoch:  1100  |  train loss: 0.1889971197
Epoch:  1200  |  train loss: 0.1754574567
Epoch:  1300  |  train loss: 0.1627743036
Epoch:  1400  |  train loss: 0.1526256055
Epoch:  1500  |  train loss: 0.1439620316
Epoch:  1600  |  train loss: 0.1355398819
Epoch:  1700  |  train loss: 0.1282052293
Epoch:  1800  |  train loss: 0.1223398238
Epoch:  1900  |  train loss: 0.1160344258
Epoch:  2000  |  train loss: 0.1113165915
Processing class: 95
Using PCA transformation
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.8727458954
Epoch:   200  |  train loss: 0.6752374053
Epoch:   300  |  train loss: 0.5447221518
Epoch:   400  |  train loss: 0.4585061729
Epoch:   500  |  train loss: 0.3950021148
Epoch:   600  |  train loss: 0.3460428834
Epoch:   700  |  train loss: 0.3071733773
Epoch:   800  |  train loss: 0.2758969486
Epoch:   900  |  train loss: 0.2498687565
Epoch:  1000  |  train loss: 0.2286253095
Epoch:  1100  |  train loss: 0.2109831572
Epoch:  1200  |  train loss: 0.1955643564
Epoch:  1300  |  train loss: 0.1819654673
Epoch:  1400  |  train loss: 0.1710218191
Epoch:  1500  |  train loss: 0.1609185666
Epoch:  1600  |  train loss: 0.1514085710
Epoch:  1700  |  train loss: 0.1434395403
Epoch:  1800  |  train loss: 0.1359002262
Epoch:  1900  |  train loss: 0.1298340663
Epoch:  2000  |  train loss: 0.1237916648
Processing class: 96
Using PCA transformation
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.8687777519
Epoch:   200  |  train loss: 0.6870681405
Epoch:   300  |  train loss: 0.5590544701
Epoch:   400  |  train loss: 0.4685982108
Epoch:   500  |  train loss: 0.4041246295
Epoch:   600  |  train loss: 0.3550391853
Epoch:   700  |  train loss: 0.3163045228
Epoch:   800  |  train loss: 0.2860570490
Epoch:   900  |  train loss: 0.2607492924
Epoch:  1000  |  train loss: 0.2394806623
Epoch:  1100  |  train loss: 0.2219472706
Epoch:  1200  |  train loss: 0.2063040555
Epoch:  1300  |  train loss: 0.1928685308
Epoch:  1400  |  train loss: 0.1809525192
Epoch:  1500  |  train loss: 0.1701483697
Epoch:  1600  |  train loss: 0.1609789550
Epoch:  1700  |  train loss: 0.1523320973
Epoch:  1800  |  train loss: 0.1445647657
Epoch:  1900  |  train loss: 0.1373180211
Epoch:  2000  |  train loss: 0.1317379579
Processing class: 97
Using PCA transformation
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.8758688688
Epoch:   200  |  train loss: 0.7016190529
Epoch:   300  |  train loss: 0.5877974868
Epoch:   400  |  train loss: 0.5063124537
Epoch:   500  |  train loss: 0.4411700845
Epoch:   600  |  train loss: 0.3909304261
Epoch:   700  |  train loss: 0.3514667213
Epoch:   800  |  train loss: 0.3178577840
Epoch:   900  |  train loss: 0.2915524185
Epoch:  1000  |  train loss: 0.2680894732
Epoch:  1100  |  train loss: 0.2484440327
Epoch:  1200  |  train loss: 0.2317609787
Epoch:  1300  |  train loss: 0.2162528723
Epoch:  1400  |  train loss: 0.2029789865
Epoch:  1500  |  train loss: 0.1907476962
Epoch:  1600  |  train loss: 0.1803892434
Epoch:  1700  |  train loss: 0.1701016784
Epoch:  1800  |  train loss: 0.1627020031
Epoch:  1900  |  train loss: 0.1545846969
Epoch:  2000  |  train loss: 0.1472705185
Processing class: 98
Using PCA transformation
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.8780052900
Epoch:   200  |  train loss: 0.6821423531
Epoch:   300  |  train loss: 0.5597554922
Epoch:   400  |  train loss: 0.4703903735
Epoch:   500  |  train loss: 0.4041609108
Epoch:   600  |  train loss: 0.3552281857
Epoch:   700  |  train loss: 0.3179759622
Epoch:   800  |  train loss: 0.2872307122
Epoch:   900  |  train loss: 0.2631280303
Epoch:  1000  |  train loss: 0.2416261226
Epoch:  1100  |  train loss: 0.2236616910
Epoch:  1200  |  train loss: 0.2086626351
Epoch:  1300  |  train loss: 0.1951912224
Epoch:  1400  |  train loss: 0.1837993711
Epoch:  1500  |  train loss: 0.1733409435
Epoch:  1600  |  train loss: 0.1643096089
Epoch:  1700  |  train loss: 0.1550593048
Epoch:  1800  |  train loss: 0.1473877430
Epoch:  1900  |  train loss: 0.1403679878
Epoch:  2000  |  train loss: 0.1339392170
Processing class: 99
Using PCA transformation
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.8760889888
Epoch:   200  |  train loss: 0.6972054958
Epoch:   300  |  train loss: 0.5725423574
Epoch:   400  |  train loss: 0.4893983006
Epoch:   500  |  train loss: 0.4269776225
Epoch:   600  |  train loss: 0.3776136100
Epoch:   700  |  train loss: 0.3395356655
Epoch:   800  |  train loss: 0.3078203082
Epoch:   900  |  train loss: 0.2812873244
Epoch:  1000  |  train loss: 0.2584461689
Epoch:  1100  |  train loss: 0.2400227338
Epoch:  1200  |  train loss: 0.2224795312
Epoch:  1300  |  train loss: 0.2079447240
Epoch:  1400  |  train loss: 0.1958135605
Epoch:  1500  |  train loss: 0.1845325142
Epoch:  1600  |  train loss: 0.1744541615
Epoch:  1700  |  train loss: 0.1648904324
Epoch:  1800  |  train loss: 0.1572194159
Epoch:  1900  |  train loss: 0.1496203065
Epoch:  2000  |  train loss: 0.1423626423
Clasifying using reconstruction function cost
2024-03-18 02:31:07,570 [trainer.py] => CNN: {'total': 50.36, '00-09': 57.7, '10-19': 63.6, '20-29': 71.4, '30-39': 68.5, '40-49': 56.8, '50-59': 29.6, '60-69': 38.9, '70-79': 36.6, '80-89': 42.7, '90-99': 37.8, 'old': 51.76, 'new': 37.8}
2024-03-18 02:31:07,571 [trainer.py] => No NME accuracy
2024-03-18 02:31:07,571 [trainer.py] => FeCAM: {'total': 42.81, '00-09': 72.2, '10-19': 60.8, '20-29': 68.7, '30-39': 63.3, '40-49': 60.3, '50-59': 16.1, '60-69': 20.6, '70-79': 17.1, '80-89': 22.7, '90-99': 26.3, 'old': 44.64, 'new': 26.3}
2024-03-18 02:31:07,572 [trainer.py] => CNN top1 curve: [83.44, 71.75, 64.66, 59.18, 54.08, 50.36]
2024-03-18 02:31:07,572 [trainer.py] => CNN top5 curve: [96.5, 89.6, 86.54, 84.09, 81.74, 79.63]
2024-03-18 02:31:07,572 [trainer.py] => FeCAM top1 curve: [78.74, 66.0, 58.77, 52.46, 47.48, 42.81]
2024-03-18 02:31:07,572 [trainer.py] => FeCAM top5 curve: [89.46, 83.07, 77.74, 73.53, 70.39, 66.24]

=========================================
2024-03-18 02:31:19,323 [trainer.py] => config: ./exps/FeCAM_cifar100.json
2024-03-18 02:31:19,323 [trainer.py] => prefix: train
2024-03-18 02:31:19,323 [trainer.py] => dataset: cifar100
2024-03-18 02:31:19,323 [trainer.py] => memory_size: 0
2024-03-18 02:31:19,323 [trainer.py] => shuffle: True
2024-03-18 02:31:19,323 [trainer.py] => init_cls: 50
2024-03-18 02:31:19,323 [trainer.py] => increment: 10
2024-03-18 02:31:19,323 [trainer.py] => model_name: fecam
2024-03-18 02:31:19,323 [trainer.py] => convnet_type: resnet18
2024-03-18 02:31:19,323 [trainer.py] => device: [device(type='cuda', index=0)]
2024-03-18 02:31:19,323 [trainer.py] => seed: 1993
2024-03-18 02:31:19,323 [trainer.py] => init_epochs: 200
2024-03-18 02:31:19,323 [trainer.py] => init_lr: 0.1
2024-03-18 02:31:19,324 [trainer.py] => init_weight_decay: 0.0005
2024-03-18 02:31:19,324 [trainer.py] => batch_size: 128
2024-03-18 02:31:19,324 [trainer.py] => num_workers: 8
2024-03-18 02:31:19,324 [trainer.py] => T: 5
2024-03-18 02:31:19,324 [trainer.py] => beta: 0.5
2024-03-18 02:31:19,324 [trainer.py] => alpha1: 1
2024-03-18 02:31:19,324 [trainer.py] => alpha2: 1
2024-03-18 02:31:19,324 [trainer.py] => ncm: False
2024-03-18 02:31:19,324 [trainer.py] => tukey: False
2024-03-18 02:31:19,324 [trainer.py] => diagonal: False
2024-03-18 02:31:19,324 [trainer.py] => per_class: True
2024-03-18 02:31:19,324 [trainer.py] => full_cov: True
2024-03-18 02:31:19,324 [trainer.py] => shrink: True
2024-03-18 02:31:19,324 [trainer.py] => norm_cov: False
2024-03-18 02:31:19,324 [trainer.py] => epochs: 2000
2024-03-18 02:31:19,324 [trainer.py] => vecnorm: False
2024-03-18 02:31:19,324 [trainer.py] => ae_type: wae
2024-03-18 02:31:19,324 [trainer.py] => ae_latent_dim: 32
2024-03-18 02:31:19,324 [trainer.py] => ae_n: 1
2024-03-18 02:31:19,324 [trainer.py] => wae_sigma: 10
2024-03-18 02:31:19,324 [trainer.py] => wae_C: 0.1
2024-03-18 02:31:19,324 [trainer.py] => ae_standarization: False
2024-03-18 02:31:19,324 [trainer.py] => ae_pca: True
2024-03-18 02:31:19,324 [trainer.py] => ae_pca_components: 200
2024-03-18 02:31:19,324 [trainer.py] => ae_clsf: recon-cost
Files already downloaded and verified
Files already downloaded and verified
2024-03-18 02:31:22,065 [data_manager.py] => [68, 56, 78, 8, 23, 84, 90, 65, 74, 76, 40, 89, 3, 92, 55, 9, 26, 80, 43, 38, 58, 70, 77, 1, 85, 19, 17, 50, 28, 53, 13, 81, 45, 82, 6, 59, 83, 16, 15, 44, 91, 41, 72, 60, 79, 52, 20, 10, 31, 54, 37, 95, 14, 71, 96, 98, 97, 2, 64, 66, 42, 22, 35, 86, 24, 34, 87, 21, 99, 0, 88, 27, 18, 94, 11, 12, 47, 25, 30, 46, 62, 69, 36, 61, 7, 63, 75, 5, 32, 4, 51, 48, 73, 93, 39, 67, 29, 49, 57, 33]
2024-03-18 02:31:22,369 [fecam.py] => Learning on 0-50
/home/z1165703/miniconda3/lib/python3.11/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
/home/z1165703/miniconda3/lib/python3.11/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
Processing class: 0
Using PCA transformation
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.9132790685
Epoch:   200  |  train loss: 0.7677484751
Epoch:   300  |  train loss: 0.6648443580
Epoch:   400  |  train loss: 0.5899171352
Epoch:   500  |  train loss: 0.5344149590
Epoch:   600  |  train loss: 0.4906498671
Epoch:   700  |  train loss: 0.4549303591
Epoch:   800  |  train loss: 0.4256176710
Epoch:   900  |  train loss: 0.4002131224
Epoch:  1000  |  train loss: 0.3781160772
Epoch:  1100  |  train loss: 0.3596866727
Epoch:  1200  |  train loss: 0.3424739897
Epoch:  1300  |  train loss: 0.3273726583
Epoch:  1400  |  train loss: 0.3138912082
Epoch:  1500  |  train loss: 0.3018726587
Epoch:  1600  |  train loss: 0.2899172664
Epoch:  1700  |  train loss: 0.2799355090
Epoch:  1800  |  train loss: 0.2703020990
Epoch:  1900  |  train loss: 0.2614007771
Epoch:  2000  |  train loss: 0.2533849269
Processing class: 1
Using PCA transformation
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.9167945743
Epoch:   200  |  train loss: 0.7664003372
Epoch:   300  |  train loss: 0.6583073378
Epoch:   400  |  train loss: 0.5811638951
Epoch:   500  |  train loss: 0.5255919456
Epoch:   600  |  train loss: 0.4813760221
Epoch:   700  |  train loss: 0.4460716426
Epoch:   800  |  train loss: 0.4164743483
Epoch:   900  |  train loss: 0.3911030889
Epoch:  1000  |  train loss: 0.3684199095
Epoch:  1100  |  train loss: 0.3484610856
Epoch:  1200  |  train loss: 0.3310538113
Epoch:  1300  |  train loss: 0.3153539717
Epoch:  1400  |  train loss: 0.3008882940
Epoch:  1500  |  train loss: 0.2889897764
Epoch:  1600  |  train loss: 0.2776758671
Epoch:  1700  |  train loss: 0.2666959256
Epoch:  1800  |  train loss: 0.2573963910
Epoch:  1900  |  train loss: 0.2483763009
Epoch:  2000  |  train loss: 0.2402941644
Processing class: 2
Using PCA transformation
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.9136903524
Epoch:   200  |  train loss: 0.7599364281
Epoch:   300  |  train loss: 0.6536008954
Epoch:   400  |  train loss: 0.5757930160
Epoch:   500  |  train loss: 0.5174374461
Epoch:   600  |  train loss: 0.4727737546
Epoch:   700  |  train loss: 0.4370374084
Epoch:   800  |  train loss: 0.4074226499
Epoch:   900  |  train loss: 0.3825229347
Epoch:  1000  |  train loss: 0.3607848883
Epoch:  1100  |  train loss: 0.3415338039
Epoch:  1200  |  train loss: 0.3245104015
Epoch:  1300  |  train loss: 0.3097183526
Epoch:  1400  |  train loss: 0.2964364171
Epoch:  1500  |  train loss: 0.2840825200
Epoch:  1600  |  train loss: 0.2729736209
Epoch:  1700  |  train loss: 0.2626876771
Epoch:  1800  |  train loss: 0.2535041362
Epoch:  1900  |  train loss: 0.2451126456
Epoch:  2000  |  train loss: 0.2371329248
Processing class: 3
Using PCA transformation
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.9205707312
Epoch:   200  |  train loss: 0.7739108205
Epoch:   300  |  train loss: 0.6724644303
Epoch:   400  |  train loss: 0.5991768718
Epoch:   500  |  train loss: 0.5444816947
Epoch:   600  |  train loss: 0.5022766709
Epoch:   700  |  train loss: 0.4676113904
Epoch:   800  |  train loss: 0.4381331861
Epoch:   900  |  train loss: 0.4129147470
Epoch:  1000  |  train loss: 0.3901488304
Epoch:  1100  |  train loss: 0.3702796221
Epoch:  1200  |  train loss: 0.3528162181
Epoch:  1300  |  train loss: 0.3370804191
Epoch:  1400  |  train loss: 0.3238813639
Epoch:  1500  |  train loss: 0.3108768821
Epoch:  1600  |  train loss: 0.2994497359
Epoch:  1700  |  train loss: 0.2882887363
Epoch:  1800  |  train loss: 0.2781049728
Epoch:  1900  |  train loss: 0.2693965197
Epoch:  2000  |  train loss: 0.2612901807
Processing class: 4
Using PCA transformation
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.9157640100
Epoch:   200  |  train loss: 0.7597153664
Epoch:   300  |  train loss: 0.6558300495
Epoch:   400  |  train loss: 0.5814110994
Epoch:   500  |  train loss: 0.5263832211
Epoch:   600  |  train loss: 0.4844538212
Epoch:   700  |  train loss: 0.4501878619
Epoch:   800  |  train loss: 0.4225419223
Epoch:   900  |  train loss: 0.3981094778
Epoch:  1000  |  train loss: 0.3776654840
Epoch:  1100  |  train loss: 0.3596892059
Epoch:  1200  |  train loss: 0.3438036621
Epoch:  1300  |  train loss: 0.3283261716
Epoch:  1400  |  train loss: 0.3153906047
Epoch:  1500  |  train loss: 0.3031838000
Epoch:  1600  |  train loss: 0.2921037912
Epoch:  1700  |  train loss: 0.2826979756
Epoch:  1800  |  train loss: 0.2725569308
Epoch:  1900  |  train loss: 0.2638653994
Epoch:  2000  |  train loss: 0.2561517209
Processing class: 5
Using PCA transformation
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.9087216258
Epoch:   200  |  train loss: 0.7545363188
Epoch:   300  |  train loss: 0.6485587955
Epoch:   400  |  train loss: 0.5720452428
Epoch:   500  |  train loss: 0.5166196346
Epoch:   600  |  train loss: 0.4735121965
Epoch:   700  |  train loss: 0.4384581804
Epoch:   800  |  train loss: 0.4097749770
Epoch:   900  |  train loss: 0.3851451218
Epoch:  1000  |  train loss: 0.3637085557
Epoch:  1100  |  train loss: 0.3446917117
Epoch:  1200  |  train loss: 0.3284742415
Epoch:  1300  |  train loss: 0.3142816007
Epoch:  1400  |  train loss: 0.3005990624
Epoch:  1500  |  train loss: 0.2888693511
Epoch:  1600  |  train loss: 0.2782075405
Epoch:  1700  |  train loss: 0.2682463586
Epoch:  1800  |  train loss: 0.2587224066
Epoch:  1900  |  train loss: 0.2502314121
Epoch:  2000  |  train loss: 0.2424212903
Processing class: 6
Using PCA transformation
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.9206032038
Epoch:   200  |  train loss: 0.7787091017
Epoch:   300  |  train loss: 0.6774719357
Epoch:   400  |  train loss: 0.6059217691
Epoch:   500  |  train loss: 0.5508437872
Epoch:   600  |  train loss: 0.5080810010
Epoch:   700  |  train loss: 0.4730614603
Epoch:   800  |  train loss: 0.4440217495
Epoch:   900  |  train loss: 0.4186808944
Epoch:  1000  |  train loss: 0.3981209159
Epoch:  1100  |  train loss: 0.3789982438
Epoch:  1200  |  train loss: 0.3616190255
Epoch:  1300  |  train loss: 0.3466751337
Epoch:  1400  |  train loss: 0.3326413155
Epoch:  1500  |  train loss: 0.3208760262
Epoch:  1600  |  train loss: 0.3095236123
Epoch:  1700  |  train loss: 0.2984906256
Epoch:  1800  |  train loss: 0.2888989925
Epoch:  1900  |  train loss: 0.2798542082
Epoch:  2000  |  train loss: 0.2709448934
Processing class: 7
Using PCA transformation
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.9195325971
Epoch:   200  |  train loss: 0.7772856593
Epoch:   300  |  train loss: 0.6740253568
Epoch:   400  |  train loss: 0.6002642989
Epoch:   500  |  train loss: 0.5438641071
Epoch:   600  |  train loss: 0.4995823622
Epoch:   700  |  train loss: 0.4634011924
Epoch:   800  |  train loss: 0.4330560446
Epoch:   900  |  train loss: 0.4067670107
Epoch:  1000  |  train loss: 0.3833018720
Epoch:  1100  |  train loss: 0.3633763313
Epoch:  1200  |  train loss: 0.3455500901
Epoch:  1300  |  train loss: 0.3296559513
Epoch:  1400  |  train loss: 0.3160554707
Epoch:  1500  |  train loss: 0.3029383898
Epoch:  1600  |  train loss: 0.2917695940
Epoch:  1700  |  train loss: 0.2800828040
Epoch:  1800  |  train loss: 0.2703356445
Epoch:  1900  |  train loss: 0.2605470836
Epoch:  2000  |  train loss: 0.2519895494
Processing class: 8
Using PCA transformation
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.9195201635
Epoch:   200  |  train loss: 0.7719396830
Epoch:   300  |  train loss: 0.6727116704
Epoch:   400  |  train loss: 0.5972796917
Epoch:   500  |  train loss: 0.5407624245
Epoch:   600  |  train loss: 0.4967281759
Epoch:   700  |  train loss: 0.4613381267
Epoch:   800  |  train loss: 0.4325050116
Epoch:   900  |  train loss: 0.4082678139
Epoch:  1000  |  train loss: 0.3865044415
Epoch:  1100  |  train loss: 0.3683408558
Epoch:  1200  |  train loss: 0.3518050849
Epoch:  1300  |  train loss: 0.3372203529
Epoch:  1400  |  train loss: 0.3233325005
Epoch:  1500  |  train loss: 0.3116499960
Epoch:  1600  |  train loss: 0.3001188815
Epoch:  1700  |  train loss: 0.2904851317
Epoch:  1800  |  train loss: 0.2805519998
Epoch:  1900  |  train loss: 0.2717672944
Epoch:  2000  |  train loss: 0.2633395374
Processing class: 9
Using PCA transformation
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.9108244777
Epoch:   200  |  train loss: 0.7557366371
Epoch:   300  |  train loss: 0.6461985946
Epoch:   400  |  train loss: 0.5686341763
Epoch:   500  |  train loss: 0.5119559765
Epoch:   600  |  train loss: 0.4678813756
Epoch:   700  |  train loss: 0.4319044054
Epoch:   800  |  train loss: 0.4013797641
Epoch:   900  |  train loss: 0.3761090636
Epoch:  1000  |  train loss: 0.3535550714
Epoch:  1100  |  train loss: 0.3347263753
Epoch:  1200  |  train loss: 0.3178434670
Epoch:  1300  |  train loss: 0.3030784965
Epoch:  1400  |  train loss: 0.2891851187
Epoch:  1500  |  train loss: 0.2776235044
Epoch:  1600  |  train loss: 0.2663329303
Epoch:  1700  |  train loss: 0.2558487952
Epoch:  1800  |  train loss: 0.2470780402
Epoch:  1900  |  train loss: 0.2383139074
Epoch:  2000  |  train loss: 0.2306290805
Processing class: 10
Using PCA transformation
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.9144525766
Epoch:   200  |  train loss: 0.7566136718
Epoch:   300  |  train loss: 0.6498935938
Epoch:   400  |  train loss: 0.5756175399
Epoch:   500  |  train loss: 0.5209319711
Epoch:   600  |  train loss: 0.4769558072
Epoch:   700  |  train loss: 0.4416898370
Epoch:   800  |  train loss: 0.4115910292
Epoch:   900  |  train loss: 0.3864908278
Epoch:  1000  |  train loss: 0.3649143398
Epoch:  1100  |  train loss: 0.3463496029
Epoch:  1200  |  train loss: 0.3290286183
Epoch:  1300  |  train loss: 0.3143893600
Epoch:  1400  |  train loss: 0.3011654794
Epoch:  1500  |  train loss: 0.2889189065
Epoch:  1600  |  train loss: 0.2774572551
Epoch:  1700  |  train loss: 0.2671301782
Epoch:  1800  |  train loss: 0.2582885742
Epoch:  1900  |  train loss: 0.2493095815
Epoch:  2000  |  train loss: 0.2413786769
Processing class: 11
Using PCA transformation
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.9237774491
Epoch:   200  |  train loss: 0.7826094985
Epoch:   300  |  train loss: 0.6826583266
Epoch:   400  |  train loss: 0.6082428098
Epoch:   500  |  train loss: 0.5530517817
Epoch:   600  |  train loss: 0.5100191057
Epoch:   700  |  train loss: 0.4748679757
Epoch:   800  |  train loss: 0.4462056398
Epoch:   900  |  train loss: 0.4219290674
Epoch:  1000  |  train loss: 0.4005626202
Epoch:  1100  |  train loss: 0.3823909044
Epoch:  1200  |  train loss: 0.3662480056
Epoch:  1300  |  train loss: 0.3515930116
Epoch:  1400  |  train loss: 0.3381487191
Epoch:  1500  |  train loss: 0.3273178935
Epoch:  1600  |  train loss: 0.3158317745
Epoch:  1700  |  train loss: 0.3058907151
Epoch:  1800  |  train loss: 0.2964498580
Epoch:  1900  |  train loss: 0.2871331573
Epoch:  2000  |  train loss: 0.2794454753
Processing class: 12
Using PCA transformation
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.9199421406
Epoch:   200  |  train loss: 0.7690753222
Epoch:   300  |  train loss: 0.6642078161
Epoch:   400  |  train loss: 0.5850441098
Epoch:   500  |  train loss: 0.5276806355
Epoch:   600  |  train loss: 0.4827498853
Epoch:   700  |  train loss: 0.4467926145
Epoch:   800  |  train loss: 0.4172199428
Epoch:   900  |  train loss: 0.3916245282
Epoch:  1000  |  train loss: 0.3698199034
Epoch:  1100  |  train loss: 0.3502233148
Epoch:  1200  |  train loss: 0.3332827032
Epoch:  1300  |  train loss: 0.3174991488
Epoch:  1400  |  train loss: 0.3039833784
Epoch:  1500  |  train loss: 0.2915084362
Epoch:  1600  |  train loss: 0.2799932539
Epoch:  1700  |  train loss: 0.2696678042
Epoch:  1800  |  train loss: 0.2605254591
Epoch:  1900  |  train loss: 0.2519109815
Epoch:  2000  |  train loss: 0.2440319568
Processing class: 13
Using PCA transformation
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.9116541386
Epoch:   200  |  train loss: 0.7579191208
Epoch:   300  |  train loss: 0.6507645130
Epoch:   400  |  train loss: 0.5778601289
Epoch:   500  |  train loss: 0.5238060474
Epoch:   600  |  train loss: 0.4799521625
Epoch:   700  |  train loss: 0.4446668983
Epoch:   800  |  train loss: 0.4147691667
Epoch:   900  |  train loss: 0.3895040989
Epoch:  1000  |  train loss: 0.3677153826
Epoch:  1100  |  train loss: 0.3482638955
Epoch:  1200  |  train loss: 0.3317166567
Epoch:  1300  |  train loss: 0.3168038189
Epoch:  1400  |  train loss: 0.3028019905
Epoch:  1500  |  train loss: 0.2908752441
Epoch:  1600  |  train loss: 0.2797727346
Epoch:  1700  |  train loss: 0.2699927241
Epoch:  1800  |  train loss: 0.2606926411
Epoch:  1900  |  train loss: 0.2526176095
Epoch:  2000  |  train loss: 0.2448048711
Processing class: 14
Using PCA transformation
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.9239135861
Epoch:   200  |  train loss: 0.7753030181
Epoch:   300  |  train loss: 0.6691891909
Epoch:   400  |  train loss: 0.5952449322
Epoch:   500  |  train loss: 0.5399016142
Epoch:   600  |  train loss: 0.4975929916
Epoch:   700  |  train loss: 0.4628838837
Epoch:   800  |  train loss: 0.4339645743
Epoch:   900  |  train loss: 0.4087016344
Epoch:  1000  |  train loss: 0.3872178376
Epoch:  1100  |  train loss: 0.3682081342
Epoch:  1200  |  train loss: 0.3511122346
Epoch:  1300  |  train loss: 0.3367203474
Epoch:  1400  |  train loss: 0.3224066198
Epoch:  1500  |  train loss: 0.3100128114
Epoch:  1600  |  train loss: 0.2991510272
Epoch:  1700  |  train loss: 0.2888444722
Epoch:  1800  |  train loss: 0.2791395187
Epoch:  1900  |  train loss: 0.2699901104
Epoch:  2000  |  train loss: 0.2620895267
Processing class: 15
Using PCA transformation
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.9147734761
Epoch:   200  |  train loss: 0.7610551119
Epoch:   300  |  train loss: 0.6585194349
Epoch:   400  |  train loss: 0.5840688348
Epoch:   500  |  train loss: 0.5287612557
Epoch:   600  |  train loss: 0.4846088588
Epoch:   700  |  train loss: 0.4489664793
Epoch:   800  |  train loss: 0.4199587762
Epoch:   900  |  train loss: 0.3945070863
Epoch:  1000  |  train loss: 0.3728036642
Epoch:  1100  |  train loss: 0.3539370060
Epoch:  1200  |  train loss: 0.3365617454
Epoch:  1300  |  train loss: 0.3219260216
Epoch:  1400  |  train loss: 0.3086769760
Epoch:  1500  |  train loss: 0.2961593091
Epoch:  1600  |  train loss: 0.2854069352
Epoch:  1700  |  train loss: 0.2755615234
Epoch:  1800  |  train loss: 0.2641891956
Epoch:  1900  |  train loss: 0.2555317789
Epoch:  2000  |  train loss: 0.2478259683
Processing class: 16
Using PCA transformation
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.9194962025
Epoch:   200  |  train loss: 0.7738712072
Epoch:   300  |  train loss: 0.6678500652
Epoch:   400  |  train loss: 0.5923547268
Epoch:   500  |  train loss: 0.5352827072
Epoch:   600  |  train loss: 0.4914500177
Epoch:   700  |  train loss: 0.4559074879
Epoch:   800  |  train loss: 0.4265120447
Epoch:   900  |  train loss: 0.4014824867
Epoch:  1000  |  train loss: 0.3797073483
Epoch:  1100  |  train loss: 0.3610617459
Epoch:  1200  |  train loss: 0.3432924449
Epoch:  1300  |  train loss: 0.3283029258
Epoch:  1400  |  train loss: 0.3149552226
Epoch:  1500  |  train loss: 0.3026037037
Epoch:  1600  |  train loss: 0.2916355491
Epoch:  1700  |  train loss: 0.2812603116
Epoch:  1800  |  train loss: 0.2715519726
Epoch:  1900  |  train loss: 0.2625633359
Epoch:  2000  |  train loss: 0.2545782804
Processing class: 17
Using PCA transformation
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.9256792545
Epoch:   200  |  train loss: 0.7725878239
Epoch:   300  |  train loss: 0.6690183043
Epoch:   400  |  train loss: 0.5938957453
Epoch:   500  |  train loss: 0.5376782537
Epoch:   600  |  train loss: 0.4927870989
Epoch:   700  |  train loss: 0.4575955689
Epoch:   800  |  train loss: 0.4277937412
Epoch:   900  |  train loss: 0.4030508459
Epoch:  1000  |  train loss: 0.3810916007
Epoch:  1100  |  train loss: 0.3621457815
Epoch:  1200  |  train loss: 0.3450513303
Epoch:  1300  |  train loss: 0.3301561236
Epoch:  1400  |  train loss: 0.3169285178
Epoch:  1500  |  train loss: 0.3046409070
Epoch:  1600  |  train loss: 0.2933326542
Epoch:  1700  |  train loss: 0.2832994461
Epoch:  1800  |  train loss: 0.2736530125
Epoch:  1900  |  train loss: 0.2645311356
Epoch:  2000  |  train loss: 0.2566086113
Processing class: 18
Using PCA transformation
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.9188371897
Epoch:   200  |  train loss: 0.7718361735
Epoch:   300  |  train loss: 0.6715063691
Epoch:   400  |  train loss: 0.6008134127
Epoch:   500  |  train loss: 0.5470350146
Epoch:   600  |  train loss: 0.5048019111
Epoch:   700  |  train loss: 0.4702863157
Epoch:   800  |  train loss: 0.4418812275
Epoch:   900  |  train loss: 0.4177702844
Epoch:  1000  |  train loss: 0.3968928099
Epoch:  1100  |  train loss: 0.3783532500
Epoch:  1200  |  train loss: 0.3618857086
Epoch:  1300  |  train loss: 0.3475075424
Epoch:  1400  |  train loss: 0.3338004112
Epoch:  1500  |  train loss: 0.3217456877
Epoch:  1600  |  train loss: 0.3107968211
Epoch:  1700  |  train loss: 0.3007704139
Epoch:  1800  |  train loss: 0.2911155581
Epoch:  1900  |  train loss: 0.2824756265
Epoch:  2000  |  train loss: 0.2744225502
Processing class: 19
Using PCA transformation
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.9241313934
Epoch:   200  |  train loss: 0.7624987602
Epoch:   300  |  train loss: 0.6617507219
Epoch:   400  |  train loss: 0.5877907395
Epoch:   500  |  train loss: 0.5320960164
Epoch:   600  |  train loss: 0.4888938904
Epoch:   700  |  train loss: 0.4537699640
Epoch:   800  |  train loss: 0.4229875863
Epoch:   900  |  train loss: 0.3970604062
Epoch:  1000  |  train loss: 0.3746187449
Epoch:  1100  |  train loss: 0.3547752619
Epoch:  1200  |  train loss: 0.3378989518
Epoch:  1300  |  train loss: 0.3216868937
Epoch:  1400  |  train loss: 0.3079988003
Epoch:  1500  |  train loss: 0.2955537140
Epoch:  1600  |  train loss: 0.2840984523
Epoch:  1700  |  train loss: 0.2731153607
Epoch:  1800  |  train loss: 0.2634504616
Epoch:  1900  |  train loss: 0.2548366308
Epoch:  2000  |  train loss: 0.2464028746
Processing class: 20
Using PCA transformation
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.9193273544
Epoch:   200  |  train loss: 0.7725936651
Epoch:   300  |  train loss: 0.6708863735
Epoch:   400  |  train loss: 0.5935846925
Epoch:   500  |  train loss: 0.5350638151
Epoch:   600  |  train loss: 0.4911105990
Epoch:   700  |  train loss: 0.4551170766
Epoch:   800  |  train loss: 0.4255249381
Epoch:   900  |  train loss: 0.4000430107
Epoch:  1000  |  train loss: 0.3775373697
Epoch:  1100  |  train loss: 0.3581615567
Epoch:  1200  |  train loss: 0.3411309898
Epoch:  1300  |  train loss: 0.3255282104
Epoch:  1400  |  train loss: 0.3117374301
Epoch:  1500  |  train loss: 0.2996826470
Epoch:  1600  |  train loss: 0.2874092579
Epoch:  1700  |  train loss: 0.2773271859
Epoch:  1800  |  train loss: 0.2677297235
Epoch:  1900  |  train loss: 0.2584181994
Epoch:  2000  |  train loss: 0.2503090560
Processing class: 21
Using PCA transformation
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.9170129418
Epoch:   200  |  train loss: 0.7590863228
Epoch:   300  |  train loss: 0.6569782257
Epoch:   400  |  train loss: 0.5841109872
Epoch:   500  |  train loss: 0.5290981293
Epoch:   600  |  train loss: 0.4858283877
Epoch:   700  |  train loss: 0.4511678696
Epoch:   800  |  train loss: 0.4221989393
Epoch:   900  |  train loss: 0.3978252769
Epoch:  1000  |  train loss: 0.3766601861
Epoch:  1100  |  train loss: 0.3575379431
Epoch:  1200  |  train loss: 0.3416350245
Epoch:  1300  |  train loss: 0.3265092671
Epoch:  1400  |  train loss: 0.3126539648
Epoch:  1500  |  train loss: 0.3001705289
Epoch:  1600  |  train loss: 0.2886774957
Epoch:  1700  |  train loss: 0.2787926078
Epoch:  1800  |  train loss: 0.2689381778
Epoch:  1900  |  train loss: 0.2599339217
Epoch:  2000  |  train loss: 0.2519969136
Processing class: 22
Using PCA transformation
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.9187293530
Epoch:   200  |  train loss: 0.7608852506
Epoch:   300  |  train loss: 0.6544594526
Epoch:   400  |  train loss: 0.5804919839
Epoch:   500  |  train loss: 0.5242037654
Epoch:   600  |  train loss: 0.4808932543
Epoch:   700  |  train loss: 0.4454925358
Epoch:   800  |  train loss: 0.4162907124
Epoch:   900  |  train loss: 0.3916441441
Epoch:  1000  |  train loss: 0.3707494259
Epoch:  1100  |  train loss: 0.3517563343
Epoch:  1200  |  train loss: 0.3349444270
Epoch:  1300  |  train loss: 0.3207214355
Epoch:  1400  |  train loss: 0.3070707023
Epoch:  1500  |  train loss: 0.2952509224
Epoch:  1600  |  train loss: 0.2839424193
Epoch:  1700  |  train loss: 0.2738513052
Epoch:  1800  |  train loss: 0.2650273502
Epoch:  1900  |  train loss: 0.2563686788
Epoch:  2000  |  train loss: 0.2477628469
Processing class: 23
Using PCA transformation
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.9241937518
Epoch:   200  |  train loss: 0.7741533399
Epoch:   300  |  train loss: 0.6681871772
Epoch:   400  |  train loss: 0.5937309623
Epoch:   500  |  train loss: 0.5380326629
Epoch:   600  |  train loss: 0.4938522398
Epoch:   700  |  train loss: 0.4579727530
Epoch:   800  |  train loss: 0.4280891716
Epoch:   900  |  train loss: 0.4026049435
Epoch:  1000  |  train loss: 0.3806680560
Epoch:  1100  |  train loss: 0.3609360635
Epoch:  1200  |  train loss: 0.3442476153
Epoch:  1300  |  train loss: 0.3288429379
Epoch:  1400  |  train loss: 0.3146288633
Epoch:  1500  |  train loss: 0.3022271872
Epoch:  1600  |  train loss: 0.2906710982
Epoch:  1700  |  train loss: 0.2805265367
Epoch:  1800  |  train loss: 0.2708658874
Epoch:  1900  |  train loss: 0.2613738388
Epoch:  2000  |  train loss: 0.2533841461
Processing class: 24
Using PCA transformation
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.9113529801
Epoch:   200  |  train loss: 0.7649981260
Epoch:   300  |  train loss: 0.6619245529
Epoch:   400  |  train loss: 0.5884040236
Epoch:   500  |  train loss: 0.5342203975
Epoch:   600  |  train loss: 0.4918047607
Epoch:   700  |  train loss: 0.4573339581
Epoch:   800  |  train loss: 0.4285115957
Epoch:   900  |  train loss: 0.4032752872
Epoch:  1000  |  train loss: 0.3827355504
Epoch:  1100  |  train loss: 0.3645698965
Epoch:  1200  |  train loss: 0.3479498982
Epoch:  1300  |  train loss: 0.3336845219
Epoch:  1400  |  train loss: 0.3199949682
Epoch:  1500  |  train loss: 0.3082355976
Epoch:  1600  |  train loss: 0.2966635227
Epoch:  1700  |  train loss: 0.2866306424
Epoch:  1800  |  train loss: 0.2775828540
Epoch:  1900  |  train loss: 0.2689041346
Epoch:  2000  |  train loss: 0.2605390608
Processing class: 25
Using PCA transformation
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.9196847320
Epoch:   200  |  train loss: 0.7698247194
Epoch:   300  |  train loss: 0.6658261657
Epoch:   400  |  train loss: 0.5910497665
Epoch:   500  |  train loss: 0.5357816458
Epoch:   600  |  train loss: 0.4932568073
Epoch:   700  |  train loss: 0.4588322341
Epoch:   800  |  train loss: 0.4295006394
Epoch:   900  |  train loss: 0.4048515201
Epoch:  1000  |  train loss: 0.3834831834
Epoch:  1100  |  train loss: 0.3652686417
Epoch:  1200  |  train loss: 0.3485159576
Epoch:  1300  |  train loss: 0.3329909623
Epoch:  1400  |  train loss: 0.3190591633
Epoch:  1500  |  train loss: 0.3070025444
Epoch:  1600  |  train loss: 0.2957671106
Epoch:  1700  |  train loss: 0.2853542387
Epoch:  1800  |  train loss: 0.2755929172
Epoch:  1900  |  train loss: 0.2665418506
Epoch:  2000  |  train loss: 0.2588150680
Processing class: 26
Using PCA transformation
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.9240020990
Epoch:   200  |  train loss: 0.7686456084
Epoch:   300  |  train loss: 0.6700503707
Epoch:   400  |  train loss: 0.5960206747
Epoch:   500  |  train loss: 0.5399825454
Epoch:   600  |  train loss: 0.4968722880
Epoch:   700  |  train loss: 0.4607576370
Epoch:   800  |  train loss: 0.4312100470
Epoch:   900  |  train loss: 0.4061119735
Epoch:  1000  |  train loss: 0.3842233896
Epoch:  1100  |  train loss: 0.3650784433
Epoch:  1200  |  train loss: 0.3483280540
Epoch:  1300  |  train loss: 0.3331493080
Epoch:  1400  |  train loss: 0.3184452415
Epoch:  1500  |  train loss: 0.3056769550
Epoch:  1600  |  train loss: 0.2939565480
Epoch:  1700  |  train loss: 0.2829128921
Epoch:  1800  |  train loss: 0.2740682065
Epoch:  1900  |  train loss: 0.2648930609
Epoch:  2000  |  train loss: 0.2573316902
Processing class: 27
Using PCA transformation
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.9243949533
Epoch:   200  |  train loss: 0.7813557386
Epoch:   300  |  train loss: 0.6795449138
Epoch:   400  |  train loss: 0.6061733603
Epoch:   500  |  train loss: 0.5510129571
Epoch:   600  |  train loss: 0.5074229598
Epoch:   700  |  train loss: 0.4717461348
Epoch:   800  |  train loss: 0.4416359007
Epoch:   900  |  train loss: 0.4163160384
Epoch:  1000  |  train loss: 0.3942469001
Epoch:  1100  |  train loss: 0.3755089104
Epoch:  1200  |  train loss: 0.3577286661
Epoch:  1300  |  train loss: 0.3428699970
Epoch:  1400  |  train loss: 0.3292108953
Epoch:  1500  |  train loss: 0.3164341688
Epoch:  1600  |  train loss: 0.3052322686
Epoch:  1700  |  train loss: 0.2945321143
Epoch:  1800  |  train loss: 0.2854458213
Epoch:  1900  |  train loss: 0.2768987179
Epoch:  2000  |  train loss: 0.2682643294
Processing class: 28
Using PCA transformation
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.9136748672
Epoch:   200  |  train loss: 0.7589606166
Epoch:   300  |  train loss: 0.6502618790
Epoch:   400  |  train loss: 0.5740890026
Epoch:   500  |  train loss: 0.5179359436
Epoch:   600  |  train loss: 0.4738578379
Epoch:   700  |  train loss: 0.4380188525
Epoch:   800  |  train loss: 0.4091119587
Epoch:   900  |  train loss: 0.3840802431
Epoch:  1000  |  train loss: 0.3628861010
Epoch:  1100  |  train loss: 0.3446748614
Epoch:  1200  |  train loss: 0.3270694375
Epoch:  1300  |  train loss: 0.3120458066
Epoch:  1400  |  train loss: 0.2986999750
Epoch:  1500  |  train loss: 0.2860732317
Epoch:  1600  |  train loss: 0.2751708090
Epoch:  1700  |  train loss: 0.2642713904
Epoch:  1800  |  train loss: 0.2549633652
Epoch:  1900  |  train loss: 0.2455625236
Epoch:  2000  |  train loss: 0.2373200387
Processing class: 29
Using PCA transformation
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.9172314644
Epoch:   200  |  train loss: 0.7694863796
Epoch:   300  |  train loss: 0.6641921043
Epoch:   400  |  train loss: 0.5885510206
Epoch:   500  |  train loss: 0.5328747511
Epoch:   600  |  train loss: 0.4894579351
Epoch:   700  |  train loss: 0.4540739238
Epoch:   800  |  train loss: 0.4241909564
Epoch:   900  |  train loss: 0.3975984871
Epoch:  1000  |  train loss: 0.3753938735
Epoch:  1100  |  train loss: 0.3558034480
Epoch:  1200  |  train loss: 0.3387208223
Epoch:  1300  |  train loss: 0.3233367264
Epoch:  1400  |  train loss: 0.3100697398
Epoch:  1500  |  train loss: 0.2968430340
Epoch:  1600  |  train loss: 0.2855609834
Epoch:  1700  |  train loss: 0.2747110128
Epoch:  1800  |  train loss: 0.2646029323
Epoch:  1900  |  train loss: 0.2560161114
Epoch:  2000  |  train loss: 0.2472951800
Processing class: 30
Using PCA transformation
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.9208399296
Epoch:   200  |  train loss: 0.7681877494
Epoch:   300  |  train loss: 0.6621114612
Epoch:   400  |  train loss: 0.5877147317
Epoch:   500  |  train loss: 0.5325051427
Epoch:   600  |  train loss: 0.4897211730
Epoch:   700  |  train loss: 0.4552458107
Epoch:   800  |  train loss: 0.4257598579
Epoch:   900  |  train loss: 0.4015368581
Epoch:  1000  |  train loss: 0.3794121623
Epoch:  1100  |  train loss: 0.3613998890
Epoch:  1200  |  train loss: 0.3446780086
Epoch:  1300  |  train loss: 0.3293148160
Epoch:  1400  |  train loss: 0.3161691129
Epoch:  1500  |  train loss: 0.3035143018
Epoch:  1600  |  train loss: 0.2925466061
Epoch:  1700  |  train loss: 0.2823253095
Epoch:  1800  |  train loss: 0.2729312479
Epoch:  1900  |  train loss: 0.2643393099
Epoch:  2000  |  train loss: 0.2561467588
Processing class: 31
Using PCA transformation
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.9183746099
Epoch:   200  |  train loss: 0.7705740929
Epoch:   300  |  train loss: 0.6593764305
Epoch:   400  |  train loss: 0.5837462306
Epoch:   500  |  train loss: 0.5282857776
Epoch:   600  |  train loss: 0.4835427344
Epoch:   700  |  train loss: 0.4470672309
Epoch:   800  |  train loss: 0.4173174977
Epoch:   900  |  train loss: 0.3923026562
Epoch:  1000  |  train loss: 0.3701238155
Epoch:  1100  |  train loss: 0.3509570897
Epoch:  1200  |  train loss: 0.3341740191
Epoch:  1300  |  train loss: 0.3188939810
Epoch:  1400  |  train loss: 0.3054530323
Epoch:  1500  |  train loss: 0.2931375742
Epoch:  1600  |  train loss: 0.2815468311
Epoch:  1700  |  train loss: 0.2718911707
Epoch:  1800  |  train loss: 0.2627463579
Epoch:  1900  |  train loss: 0.2537234902
Epoch:  2000  |  train loss: 0.2454515785
Processing class: 32
Using PCA transformation
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.9128026128
Epoch:   200  |  train loss: 0.7608930349
Epoch:   300  |  train loss: 0.6555790901
Epoch:   400  |  train loss: 0.5799242973
Epoch:   500  |  train loss: 0.5234276891
Epoch:   600  |  train loss: 0.4794609547
Epoch:   700  |  train loss: 0.4452261627
Epoch:   800  |  train loss: 0.4162658632
Epoch:   900  |  train loss: 0.3907615602
Epoch:  1000  |  train loss: 0.3698245108
Epoch:  1100  |  train loss: 0.3507895947
Epoch:  1200  |  train loss: 0.3349311471
Epoch:  1300  |  train loss: 0.3201465189
Epoch:  1400  |  train loss: 0.3066188335
Epoch:  1500  |  train loss: 0.2943506241
Epoch:  1600  |  train loss: 0.2830387294
Epoch:  1700  |  train loss: 0.2726637900
Epoch:  1800  |  train loss: 0.2635599345
Epoch:  1900  |  train loss: 0.2547412008
Epoch:  2000  |  train loss: 0.2459531158
Processing class: 33
Using PCA transformation
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.9153741241
Epoch:   200  |  train loss: 0.7589613914
Epoch:   300  |  train loss: 0.6548001885
Epoch:   400  |  train loss: 0.5797479033
Epoch:   500  |  train loss: 0.5234895587
Epoch:   600  |  train loss: 0.4794284761
Epoch:   700  |  train loss: 0.4432080448
Epoch:   800  |  train loss: 0.4139105082
Epoch:   900  |  train loss: 0.3883355319
Epoch:  1000  |  train loss: 0.3660975814
Epoch:  1100  |  train loss: 0.3474138141
Epoch:  1200  |  train loss: 0.3297639549
Epoch:  1300  |  train loss: 0.3143985093
Epoch:  1400  |  train loss: 0.3001151323
Epoch:  1500  |  train loss: 0.2878625929
Epoch:  1600  |  train loss: 0.2765434682
Epoch:  1700  |  train loss: 0.2659912467
Epoch:  1800  |  train loss: 0.2558417439
Epoch:  1900  |  train loss: 0.2474503726
Epoch:  2000  |  train loss: 0.2388355732
Processing class: 34
Using PCA transformation
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.9230029106
Epoch:   200  |  train loss: 0.7737258673
Epoch:   300  |  train loss: 0.6748255491
Epoch:   400  |  train loss: 0.6017138362
Epoch:   500  |  train loss: 0.5476018667
Epoch:   600  |  train loss: 0.5050401330
Epoch:   700  |  train loss: 0.4697918534
Epoch:   800  |  train loss: 0.4410906613
Epoch:   900  |  train loss: 0.4159091592
Epoch:  1000  |  train loss: 0.3943143904
Epoch:  1100  |  train loss: 0.3752915084
Epoch:  1200  |  train loss: 0.3580845535
Epoch:  1300  |  train loss: 0.3425949395
Epoch:  1400  |  train loss: 0.3286048651
Epoch:  1500  |  train loss: 0.3161203384
Epoch:  1600  |  train loss: 0.3045463264
Epoch:  1700  |  train loss: 0.2935730577
Epoch:  1800  |  train loss: 0.2840139270
Epoch:  1900  |  train loss: 0.2742963791
Epoch:  2000  |  train loss: 0.2657712579
Processing class: 35
Using PCA transformation
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.9128102899
Epoch:   200  |  train loss: 0.7526850939
Epoch:   300  |  train loss: 0.6446808219
Epoch:   400  |  train loss: 0.5682924032
Epoch:   500  |  train loss: 0.5123507619
Epoch:   600  |  train loss: 0.4671928704
Epoch:   700  |  train loss: 0.4305037141
Epoch:   800  |  train loss: 0.4007837236
Epoch:   900  |  train loss: 0.3750083506
/home/z1165703/miniconda3/lib/python3.11/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
Epoch:  1000  |  train loss: 0.3533936679
Epoch:  1100  |  train loss: 0.3341704965
Epoch:  1200  |  train loss: 0.3173392773
Epoch:  1300  |  train loss: 0.3025440454
Epoch:  1400  |  train loss: 0.2885929227
Epoch:  1500  |  train loss: 0.2761896670
Epoch:  1600  |  train loss: 0.2648420990
Epoch:  1700  |  train loss: 0.2547293305
Epoch:  1800  |  train loss: 0.2451032549
Epoch:  1900  |  train loss: 0.2364753067
Epoch:  2000  |  train loss: 0.2290577918
Processing class: 36
Using PCA transformation
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.9208180308
Epoch:   200  |  train loss: 0.7674714446
Epoch:   300  |  train loss: 0.6660227895
Epoch:   400  |  train loss: 0.5923375845
Epoch:   500  |  train loss: 0.5376662016
Epoch:   600  |  train loss: 0.4962012768
Epoch:   700  |  train loss: 0.4621968269
Epoch:   800  |  train loss: 0.4344431102
Epoch:   900  |  train loss: 0.4103272200
Epoch:  1000  |  train loss: 0.3903022289
Epoch:  1100  |  train loss: 0.3723805070
Epoch:  1200  |  train loss: 0.3563234031
Epoch:  1300  |  train loss: 0.3415853262
Epoch:  1400  |  train loss: 0.3290601790
Epoch:  1500  |  train loss: 0.3157457769
Epoch:  1600  |  train loss: 0.3046970189
Epoch:  1700  |  train loss: 0.2952475667
Epoch:  1800  |  train loss: 0.2857594967
Epoch:  1900  |  train loss: 0.2770888269
Epoch:  2000  |  train loss: 0.2694153786
Processing class: 37
Using PCA transformation
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.9074346066
Epoch:   200  |  train loss: 0.7473091006
Epoch:   300  |  train loss: 0.6461045265
Epoch:   400  |  train loss: 0.5733224869
Epoch:   500  |  train loss: 0.5172321439
Epoch:   600  |  train loss: 0.4745350480
Epoch:   700  |  train loss: 0.4393551290
Epoch:   800  |  train loss: 0.4114824355
Epoch:   900  |  train loss: 0.3872665167
Epoch:  1000  |  train loss: 0.3665656030
Epoch:  1100  |  train loss: 0.3486396611
Epoch:  1200  |  train loss: 0.3327840269
Epoch:  1300  |  train loss: 0.3183641195
Epoch:  1400  |  train loss: 0.3057255149
Epoch:  1500  |  train loss: 0.2939421892
Epoch:  1600  |  train loss: 0.2830536902
Epoch:  1700  |  train loss: 0.2736037791
Epoch:  1800  |  train loss: 0.2643871486
Epoch:  1900  |  train loss: 0.2556290567
Epoch:  2000  |  train loss: 0.2477229357
Processing class: 38
Using PCA transformation
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.9146521807
Epoch:   200  |  train loss: 0.7615577102
Epoch:   300  |  train loss: 0.6505171061
Epoch:   400  |  train loss: 0.5745398879
Epoch:   500  |  train loss: 0.5188333571
Epoch:   600  |  train loss: 0.4751998127
Epoch:   700  |  train loss: 0.4399054766
Epoch:   800  |  train loss: 0.4103557467
Epoch:   900  |  train loss: 0.3855345070
Epoch:  1000  |  train loss: 0.3638178051
Epoch:  1100  |  train loss: 0.3449660003
Epoch:  1200  |  train loss: 0.3276637971
Epoch:  1300  |  train loss: 0.3123568058
Epoch:  1400  |  train loss: 0.2985465288
Epoch:  1500  |  train loss: 0.2859348178
Epoch:  1600  |  train loss: 0.2751266420
Epoch:  1700  |  train loss: 0.2645693481
Epoch:  1800  |  train loss: 0.2553071052
Epoch:  1900  |  train loss: 0.2462351859
Epoch:  2000  |  train loss: 0.2384376407
Processing class: 39
Using PCA transformation
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.9105450511
Epoch:   200  |  train loss: 0.7562348843
Epoch:   300  |  train loss: 0.6481711626
Epoch:   400  |  train loss: 0.5725427508
Epoch:   500  |  train loss: 0.5160137236
Epoch:   600  |  train loss: 0.4727843761
Epoch:   700  |  train loss: 0.4374580204
Epoch:   800  |  train loss: 0.4079950273
Epoch:   900  |  train loss: 0.3831564903
Epoch:  1000  |  train loss: 0.3613961279
Epoch:  1100  |  train loss: 0.3425333560
Epoch:  1200  |  train loss: 0.3254258990
Epoch:  1300  |  train loss: 0.3109238267
Epoch:  1400  |  train loss: 0.2973260343
Epoch:  1500  |  train loss: 0.2851681948
Epoch:  1600  |  train loss: 0.2739596426
Epoch:  1700  |  train loss: 0.2637550473
Epoch:  1800  |  train loss: 0.2544845909
Epoch:  1900  |  train loss: 0.2460074991
Epoch:  2000  |  train loss: 0.2378600985
Processing class: 40
Using PCA transformation
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.9222339511
Epoch:   200  |  train loss: 0.7789945841
Epoch:   300  |  train loss: 0.6779949546
Epoch:   400  |  train loss: 0.6039261818
Epoch:   500  |  train loss: 0.5483655572
Epoch:   600  |  train loss: 0.5054406226
Epoch:   700  |  train loss: 0.4706494331
Epoch:   800  |  train loss: 0.4416760147
Epoch:   900  |  train loss: 0.4177378714
Epoch:  1000  |  train loss: 0.3960892141
Epoch:  1100  |  train loss: 0.3771311820
Epoch:  1200  |  train loss: 0.3602088869
Epoch:  1300  |  train loss: 0.3459130764
Epoch:  1400  |  train loss: 0.3321517766
Epoch:  1500  |  train loss: 0.3200285196
Epoch:  1600  |  train loss: 0.3083818972
Epoch:  1700  |  train loss: 0.2983064234
Epoch:  1800  |  train loss: 0.2886300147
Epoch:  1900  |  train loss: 0.2799346685
Epoch:  2000  |  train loss: 0.2718788445
Processing class: 41
Using PCA transformation
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.9237217665
Epoch:   200  |  train loss: 0.7802150488
Epoch:   300  |  train loss: 0.6798215628
Epoch:   400  |  train loss: 0.6081865907
Epoch:   500  |  train loss: 0.5547026992
Epoch:   600  |  train loss: 0.5130297542
Epoch:   700  |  train loss: 0.4791209400
Epoch:   800  |  train loss: 0.4501000702
Epoch:   900  |  train loss: 0.4258001268
Epoch:  1000  |  train loss: 0.4043052018
Epoch:  1100  |  train loss: 0.3857673705
Epoch:  1200  |  train loss: 0.3684546709
Epoch:  1300  |  train loss: 0.3538748264
Epoch:  1400  |  train loss: 0.3405310571
Epoch:  1500  |  train loss: 0.3278579056
Epoch:  1600  |  train loss: 0.3167456806
Epoch:  1700  |  train loss: 0.3061064780
Epoch:  1800  |  train loss: 0.2958618999
Epoch:  1900  |  train loss: 0.2871778071
Epoch:  2000  |  train loss: 0.2790759683
Processing class: 42
Using PCA transformation
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.9116859913
Epoch:   200  |  train loss: 0.7583617210
Epoch:   300  |  train loss: 0.6498707175
Epoch:   400  |  train loss: 0.5732536197
Epoch:   500  |  train loss: 0.5155745983
Epoch:   600  |  train loss: 0.4718425691
Epoch:   700  |  train loss: 0.4366012871
Epoch:   800  |  train loss: 0.4072187185
Epoch:   900  |  train loss: 0.3819635272
Epoch:  1000  |  train loss: 0.3598154962
Epoch:  1100  |  train loss: 0.3409296691
Epoch:  1200  |  train loss: 0.3241514981
Epoch:  1300  |  train loss: 0.3084702313
Epoch:  1400  |  train loss: 0.2947896898
Epoch:  1500  |  train loss: 0.2829807580
Epoch:  1600  |  train loss: 0.2713622510
Epoch:  1700  |  train loss: 0.2605654359
Epoch:  1800  |  train loss: 0.2511549503
Epoch:  1900  |  train loss: 0.2420982599
Epoch:  2000  |  train loss: 0.2343687385
Processing class: 43
Using PCA transformation
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.9148632884
Epoch:   200  |  train loss: 0.7625339866
Epoch:   300  |  train loss: 0.6517701387
Epoch:   400  |  train loss: 0.5738384247
Epoch:   500  |  train loss: 0.5178049326
Epoch:   600  |  train loss: 0.4752324283
Epoch:   700  |  train loss: 0.4407507777
Epoch:   800  |  train loss: 0.4123790383
Epoch:   900  |  train loss: 0.3881201863
Epoch:  1000  |  train loss: 0.3669955850
Epoch:  1100  |  train loss: 0.3484730542
Epoch:  1200  |  train loss: 0.3322315514
Epoch:  1300  |  train loss: 0.3180406034
Epoch:  1400  |  train loss: 0.3045633078
Epoch:  1500  |  train loss: 0.2928418040
Epoch:  1600  |  train loss: 0.2813046217
Epoch:  1700  |  train loss: 0.2719820023
Epoch:  1800  |  train loss: 0.2622651696
Epoch:  1900  |  train loss: 0.2540165633
Epoch:  2000  |  train loss: 0.2458383501
Processing class: 44
Using PCA transformation
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.9189589262
Epoch:   200  |  train loss: 0.7689023018
Epoch:   300  |  train loss: 0.6645197630
Epoch:   400  |  train loss: 0.5904306293
Epoch:   500  |  train loss: 0.5352965355
Epoch:   600  |  train loss: 0.4933450401
Epoch:   700  |  train loss: 0.4594335794
Epoch:   800  |  train loss: 0.4308336020
Epoch:   900  |  train loss: 0.4061480701
Epoch:  1000  |  train loss: 0.3853056252
Epoch:  1100  |  train loss: 0.3661068499
Epoch:  1200  |  train loss: 0.3488679767
Epoch:  1300  |  train loss: 0.3343079925
Epoch:  1400  |  train loss: 0.3212041080
Epoch:  1500  |  train loss: 0.3086497545
Epoch:  1600  |  train loss: 0.2978786051
Epoch:  1700  |  train loss: 0.2874666393
Epoch:  1800  |  train loss: 0.2782294273
Epoch:  1900  |  train loss: 0.2692606568
Epoch:  2000  |  train loss: 0.2612672746
Processing class: 45
Using PCA transformation
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.9153116465
Epoch:   200  |  train loss: 0.7557558537
Epoch:   300  |  train loss: 0.6450865030
Epoch:   400  |  train loss: 0.5677989244
Epoch:   500  |  train loss: 0.5105518997
Epoch:   600  |  train loss: 0.4665759981
Epoch:   700  |  train loss: 0.4310774624
Epoch:   800  |  train loss: 0.4020074069
Epoch:   900  |  train loss: 0.3773671091
Epoch:  1000  |  train loss: 0.3561094284
Epoch:  1100  |  train loss: 0.3375218093
Epoch:  1200  |  train loss: 0.3219930470
Epoch:  1300  |  train loss: 0.3069791138
Epoch:  1400  |  train loss: 0.2939090729
Epoch:  1500  |  train loss: 0.2822751224
Epoch:  1600  |  train loss: 0.2716967642
Epoch:  1700  |  train loss: 0.2617148519
Epoch:  1800  |  train loss: 0.2530664772
Epoch:  1900  |  train loss: 0.2444514096
Epoch:  2000  |  train loss: 0.2365735829
Processing class: 46
Using PCA transformation
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.9177440405
Epoch:   200  |  train loss: 0.7630088329
Epoch:   300  |  train loss: 0.6593211532
Epoch:   400  |  train loss: 0.5854015827
Epoch:   500  |  train loss: 0.5293889165
Epoch:   600  |  train loss: 0.4856683612
Epoch:   700  |  train loss: 0.4503054857
Epoch:   800  |  train loss: 0.4211041451
Epoch:   900  |  train loss: 0.3962641299
Epoch:  1000  |  train loss: 0.3747904599
Epoch:  1100  |  train loss: 0.3559628487
Epoch:  1200  |  train loss: 0.3392079830
Epoch:  1300  |  train loss: 0.3241728306
Epoch:  1400  |  train loss: 0.3103831470
Epoch:  1500  |  train loss: 0.2981529772
Epoch:  1600  |  train loss: 0.2880819261
Epoch:  1700  |  train loss: 0.2776078343
Epoch:  1800  |  train loss: 0.2682545125
Epoch:  1900  |  train loss: 0.2601849347
Epoch:  2000  |  train loss: 0.2514135838
Processing class: 47
Using PCA transformation
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.9136554718
Epoch:   200  |  train loss: 0.7534314632
Epoch:   300  |  train loss: 0.6457137227
Epoch:   400  |  train loss: 0.5705687881
Epoch:   500  |  train loss: 0.5150728524
Epoch:   600  |  train loss: 0.4704524517
Epoch:   700  |  train loss: 0.4345266819
Epoch:   800  |  train loss: 0.4054677188
Epoch:   900  |  train loss: 0.3801529288
Epoch:  1000  |  train loss: 0.3583341181
Epoch:  1100  |  train loss: 0.3401420712
Epoch:  1200  |  train loss: 0.3229270756
Epoch:  1300  |  train loss: 0.3080063760
Epoch:  1400  |  train loss: 0.2947234094
Epoch:  1500  |  train loss: 0.2819866538
Epoch:  1600  |  train loss: 0.2711119294
Epoch:  1700  |  train loss: 0.2610560060
Epoch:  1800  |  train loss: 0.2516134739
Epoch:  1900  |  train loss: 0.2428414911
Epoch:  2000  |  train loss: 0.2349800229
Processing class: 48
Using PCA transformation
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.9197114944
Epoch:   200  |  train loss: 0.7656042576
Epoch:   300  |  train loss: 0.6595798492
Epoch:   400  |  train loss: 0.5832944274
Epoch:   500  |  train loss: 0.5272855878
Epoch:   600  |  train loss: 0.4838468194
Epoch:   700  |  train loss: 0.4495343685
Epoch:   800  |  train loss: 0.4203585804
Epoch:   900  |  train loss: 0.3960486829
Epoch:  1000  |  train loss: 0.3743403792
Epoch:  1100  |  train loss: 0.3557012022
Epoch:  1200  |  train loss: 0.3394183099
Epoch:  1300  |  train loss: 0.3245119691
Epoch:  1400  |  train loss: 0.3108776927
Epoch:  1500  |  train loss: 0.2986042976
Epoch:  1600  |  train loss: 0.2874800265
Epoch:  1700  |  train loss: 0.2770142615
Epoch:  1800  |  train loss: 0.2675743103
Epoch:  1900  |  train loss: 0.2587739587
Epoch:  2000  |  train loss: 0.2504745156
Processing class: 49
Using PCA transformation
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.9195552468
Epoch:   200  |  train loss: 0.7734678149
Epoch:   300  |  train loss: 0.6662454844
Epoch:   400  |  train loss: 0.5923050046
Epoch:   500  |  train loss: 0.5376261950
Epoch:   600  |  train loss: 0.4941921890
Epoch:   700  |  train loss: 0.4591986477
Epoch:   800  |  train loss: 0.4296111822
Epoch:   900  |  train loss: 0.4048560202
Epoch:  1000  |  train loss: 0.3832899809
Epoch:  1100  |  train loss: 0.3639346838
Epoch:  1200  |  train loss: 0.3475901186
Epoch:  1300  |  train loss: 0.3324653089
Epoch:  1400  |  train loss: 0.3183930159
Epoch:  1500  |  train loss: 0.3069025159
Epoch:  1600  |  train loss: 0.2954994142
Epoch:  1700  |  train loss: 0.2857553542
Epoch:  1800  |  train loss: 0.2756273031
Epoch:  1900  |  train loss: 0.2673991799
Epoch:  2000  |  train loss: 0.2590222776
Clasifying using reconstruction function cost
2024-03-18 03:06:24,171 [trainer.py] => CNN: {'total': 83.44, '00-09': 87.7, '10-19': 79.6, '20-29': 84.9, '30-39': 81.0, '40-49': 84.0, 'old': 0, 'new': 83.44}
2024-03-18 03:06:24,175 [trainer.py] => No NME accuracy
2024-03-18 03:06:24,175 [trainer.py] => FeCAM: {'total': 80.92, '00-09': 86.3, '10-19': 77.5, '20-29': 81.5, '30-39': 78.7, '40-49': 80.6, 'old': 0, 'new': 80.92}
2024-03-18 03:06:24,175 [trainer.py] => CNN top1 curve: [83.44]
2024-03-18 03:06:24,175 [trainer.py] => CNN top5 curve: [96.5]
2024-03-18 03:06:24,175 [trainer.py] => FeCAM top1 curve: [80.92]
2024-03-18 03:06:24,175 [trainer.py] => FeCAM top5 curve: [92.74]

2024-03-18 03:06:24,196 [fecam.py] => Learning on 50-60
/home/z1165703/miniconda3/lib/python3.11/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
/home/z1165703/miniconda3/lib/python3.11/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
/home/z1165703/miniconda3/lib/python3.11/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
Processing class: 50
Using PCA transformation
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.9145080686
Epoch:   200  |  train loss: 0.7677475095
Epoch:   300  |  train loss: 0.6644412398
Epoch:   400  |  train loss: 0.5889530063
Epoch:   500  |  train loss: 0.5321244121
Epoch:   600  |  train loss: 0.4889050782
Epoch:   700  |  train loss: 0.4539912522
Epoch:   800  |  train loss: 0.4252060354
Epoch:   900  |  train loss: 0.4005795240
Epoch:  1000  |  train loss: 0.3794897616
Epoch:  1100  |  train loss: 0.3605583131
Epoch:  1200  |  train loss: 0.3440357566
Epoch:  1300  |  train loss: 0.3294066966
Epoch:  1400  |  train loss: 0.3156625867
Epoch:  1500  |  train loss: 0.3037262499
Epoch:  1600  |  train loss: 0.2927596986
Epoch:  1700  |  train loss: 0.2830890238
Epoch:  1800  |  train loss: 0.2732826352
Epoch:  1900  |  train loss: 0.2648707271
Epoch:  2000  |  train loss: 0.2570536643
Processing class: 51
Using PCA transformation
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.9241129041
Epoch:   200  |  train loss: 0.7802085638
Epoch:   300  |  train loss: 0.6833127737
Epoch:   400  |  train loss: 0.6113241911
Epoch:   500  |  train loss: 0.5552577019
Epoch:   600  |  train loss: 0.5111162543
Epoch:   700  |  train loss: 0.4756268859
Epoch:   800  |  train loss: 0.4456715822
Epoch:   900  |  train loss: 0.4205675423
Epoch:  1000  |  train loss: 0.3985952914
Epoch:  1100  |  train loss: 0.3793750703
Epoch:  1200  |  train loss: 0.3625500321
Epoch:  1300  |  train loss: 0.3472866297
Epoch:  1400  |  train loss: 0.3330491483
Epoch:  1500  |  train loss: 0.3206401646
Epoch:  1600  |  train loss: 0.3091067731
Epoch:  1700  |  train loss: 0.2985259056
Epoch:  1800  |  train loss: 0.2883615613
Epoch:  1900  |  train loss: 0.2794628918
Epoch:  2000  |  train loss: 0.2701313138
Processing class: 52
Using PCA transformation
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.9229465961
Epoch:   200  |  train loss: 0.7723517776
Epoch:   300  |  train loss: 0.6648467898
Epoch:   400  |  train loss: 0.5876392603
Epoch:   500  |  train loss: 0.5303127527
Epoch:   600  |  train loss: 0.4852608383
Epoch:   700  |  train loss: 0.4495685160
Epoch:   800  |  train loss: 0.4202771723
Epoch:   900  |  train loss: 0.3946968973
Epoch:  1000  |  train loss: 0.3725526631
Epoch:  1100  |  train loss: 0.3532968521
Epoch:  1200  |  train loss: 0.3365543306
Epoch:  1300  |  train loss: 0.3204905689
Epoch:  1400  |  train loss: 0.3068398476
Epoch:  1500  |  train loss: 0.2936212838
Epoch:  1600  |  train loss: 0.2822575986
Epoch:  1700  |  train loss: 0.2715762973
Epoch:  1800  |  train loss: 0.2617061079
Epoch:  1900  |  train loss: 0.2528584778
Epoch:  2000  |  train loss: 0.2447951704
Processing class: 53
Using PCA transformation
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.9133590698
Epoch:   200  |  train loss: 0.7537232995
Epoch:   300  |  train loss: 0.6429858446
Epoch:   400  |  train loss: 0.5657559514
Epoch:   500  |  train loss: 0.5081448972
Epoch:   600  |  train loss: 0.4641223311
Epoch:   700  |  train loss: 0.4283048332
Epoch:   800  |  train loss: 0.3985169888
Epoch:   900  |  train loss: 0.3738556981
Epoch:  1000  |  train loss: 0.3517172873
Epoch:  1100  |  train loss: 0.3330038071
Epoch:  1200  |  train loss: 0.3157394588
Epoch:  1300  |  train loss: 0.3009669960
Epoch:  1400  |  train loss: 0.2862401068
Epoch:  1500  |  train loss: 0.2734809935
Epoch:  1600  |  train loss: 0.2625484109
Epoch:  1700  |  train loss: 0.2519573361
Epoch:  1800  |  train loss: 0.2424460620
Epoch:  1900  |  train loss: 0.2338413984
Epoch:  2000  |  train loss: 0.2257742554
Processing class: 54
Using PCA transformation
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.9222176909
Epoch:   200  |  train loss: 0.7779822826
Epoch:   300  |  train loss: 0.6723498821
Epoch:   400  |  train loss: 0.5965376258
Epoch:   500  |  train loss: 0.5414514899
Epoch:   600  |  train loss: 0.4971146941
Epoch:   700  |  train loss: 0.4627166927
Epoch:   800  |  train loss: 0.4333164632
Epoch:   900  |  train loss: 0.4085366488
Epoch:  1000  |  train loss: 0.3869023263
Epoch:  1100  |  train loss: 0.3676908076
Epoch:  1200  |  train loss: 0.3508869231
Epoch:  1300  |  train loss: 0.3358944654
Epoch:  1400  |  train loss: 0.3226128101
Epoch:  1500  |  train loss: 0.3096753240
Epoch:  1600  |  train loss: 0.2981159508
Epoch:  1700  |  train loss: 0.2876549184
Epoch:  1800  |  train loss: 0.2784150124
Epoch:  1900  |  train loss: 0.2699505091
Epoch:  2000  |  train loss: 0.2605688751
Processing class: 55
Using PCA transformation
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.9209446788
Epoch:   200  |  train loss: 0.7657457709
Epoch:   300  |  train loss: 0.6544515014
Epoch:   400  |  train loss: 0.5792964339
Epoch:   500  |  train loss: 0.5240156651
Epoch:   600  |  train loss: 0.4806124806
Epoch:   700  |  train loss: 0.4465794921
Epoch:   800  |  train loss: 0.4182236731
Epoch:   900  |  train loss: 0.3935442984
Epoch:  1000  |  train loss: 0.3723885655
Epoch:  1100  |  train loss: 0.3537950635
Epoch:  1200  |  train loss: 0.3365174890
Epoch:  1300  |  train loss: 0.3217005670
Epoch:  1400  |  train loss: 0.3080307961
Epoch:  1500  |  train loss: 0.2960534871
Epoch:  1600  |  train loss: 0.2848129034
Epoch:  1700  |  train loss: 0.2743616939
Epoch:  1800  |  train loss: 0.2650535464
Epoch:  1900  |  train loss: 0.2559820175
Epoch:  2000  |  train loss: 0.2481035233
Processing class: 56
Using PCA transformation
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.9233980894
Epoch:   200  |  train loss: 0.7737000227
Epoch:   300  |  train loss: 0.6718758225
Epoch:   400  |  train loss: 0.5984441638
Epoch:   500  |  train loss: 0.5425765276
Epoch:   600  |  train loss: 0.4987702489
Epoch:   700  |  train loss: 0.4628139913
Epoch:   800  |  train loss: 0.4328845978
Epoch:   900  |  train loss: 0.4070780098
Epoch:  1000  |  train loss: 0.3843188941
Epoch:  1100  |  train loss: 0.3651314616
Epoch:  1200  |  train loss: 0.3472713172
Epoch:  1300  |  train loss: 0.3315884769
Epoch:  1400  |  train loss: 0.3172728837
Epoch:  1500  |  train loss: 0.3043682396
Epoch:  1600  |  train loss: 0.2923460126
Epoch:  1700  |  train loss: 0.2820457220
Epoch:  1800  |  train loss: 0.2722087324
Epoch:  1900  |  train loss: 0.2629549414
Epoch:  2000  |  train loss: 0.2542039692
Processing class: 57
Using PCA transformation
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.9087246060
Epoch:   200  |  train loss: 0.7513356924
Epoch:   300  |  train loss: 0.6433557749
Epoch:   400  |  train loss: 0.5673698425
Epoch:   500  |  train loss: 0.5114650786
Epoch:   600  |  train loss: 0.4672383904
Epoch:   700  |  train loss: 0.4315820873
Epoch:   800  |  train loss: 0.4015443623
Epoch:   900  |  train loss: 0.3761510015
Epoch:  1000  |  train loss: 0.3546723366
Epoch:  1100  |  train loss: 0.3357180297
Epoch:  1200  |  train loss: 0.3187710047
Epoch:  1300  |  train loss: 0.3034695268
Epoch:  1400  |  train loss: 0.2902909040
Epoch:  1500  |  train loss: 0.2780493140
Epoch:  1600  |  train loss: 0.2676323652
Epoch:  1700  |  train loss: 0.2574444115
Epoch:  1800  |  train loss: 0.2484720558
Epoch:  1900  |  train loss: 0.2401826203
Epoch:  2000  |  train loss: 0.2320831835
Processing class: 58
Using PCA transformation
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.9200925827
Epoch:   200  |  train loss: 0.7779113412
Epoch:   300  |  train loss: 0.6720198870
Epoch:   400  |  train loss: 0.5969310522
Epoch:   500  |  train loss: 0.5414733171
Epoch:   600  |  train loss: 0.4979502022
Epoch:   700  |  train loss: 0.4631565392
Epoch:   800  |  train loss: 0.4337763071
Epoch:   900  |  train loss: 0.4088684142
Epoch:  1000  |  train loss: 0.3871331990
Epoch:  1100  |  train loss: 0.3670397222
Epoch:  1200  |  train loss: 0.3503604710
Epoch:  1300  |  train loss: 0.3344838262
Epoch:  1400  |  train loss: 0.3206376672
Epoch:  1500  |  train loss: 0.3071068883
Epoch:  1600  |  train loss: 0.2944236398
Epoch:  1700  |  train loss: 0.2843751431
Epoch:  1800  |  train loss: 0.2746817470
Epoch:  1900  |  train loss: 0.2649175107
Epoch:  2000  |  train loss: 0.2564707875
Processing class: 59
Using PCA transformation
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.9181140900
Epoch:   200  |  train loss: 0.7763737798
Epoch:   300  |  train loss: 0.6742640615
Epoch:   400  |  train loss: 0.5976723433
Epoch:   500  |  train loss: 0.5393474221
Epoch:   600  |  train loss: 0.4937270522
Epoch:   700  |  train loss: 0.4573270857
Epoch:   800  |  train loss: 0.4267734587
Epoch:   900  |  train loss: 0.4014902413
Epoch:  1000  |  train loss: 0.3785583317
Epoch:  1100  |  train loss: 0.3590670526
Epoch:  1200  |  train loss: 0.3420062006
Epoch:  1300  |  train loss: 0.3263936639
Epoch:  1400  |  train loss: 0.3116515577
Epoch:  1500  |  train loss: 0.2997224927
Epoch:  1600  |  train loss: 0.2877139688
Epoch:  1700  |  train loss: 0.2773315609
Epoch:  1800  |  train loss: 0.2680048466
Epoch:  1900  |  train loss: 0.2590345800
Epoch:  2000  |  train loss: 0.2507299244
Clasifying using reconstruction function cost
2024-03-18 03:20:14,812 [trainer.py] => CNN: {'total': 71.75, '00-09': 82.2, '10-19': 72.8, '20-29': 78.5, '30-39': 74.9, '40-49': 68.4, '50-59': 53.7, 'old': 75.36, 'new': 53.7}
2024-03-18 03:20:14,814 [trainer.py] => No NME accuracy
2024-03-18 03:20:14,814 [trainer.py] => FeCAM: {'total': 66.57, '00-09': 80.2, '10-19': 67.0, '20-29': 74.2, '30-39': 69.8, '40-49': 68.9, '50-59': 39.3, 'old': 72.02, 'new': 39.3}
2024-03-18 03:20:14,814 [trainer.py] => CNN top1 curve: [83.44, 71.75]
2024-03-18 03:20:14,814 [trainer.py] => CNN top5 curve: [96.5, 89.6]
2024-03-18 03:20:14,814 [trainer.py] => FeCAM top1 curve: [80.92, 66.57]
2024-03-18 03:20:14,814 [trainer.py] => FeCAM top5 curve: [92.74, 86.53]

2024-03-18 03:20:14,828 [fecam.py] => Learning on 60-70
/home/z1165703/miniconda3/lib/python3.11/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
/home/z1165703/miniconda3/lib/python3.11/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
/home/z1165703/miniconda3/lib/python3.11/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
Processing class: 60
Using PCA transformation
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.9180491447
Epoch:   200  |  train loss: 0.7634643197
Epoch:   300  |  train loss: 0.6603406668
Epoch:   400  |  train loss: 0.5844082832
Epoch:   500  |  train loss: 0.5268133998
Epoch:   600  |  train loss: 0.4823702931
Epoch:   700  |  train loss: 0.4464780271
Epoch:   800  |  train loss: 0.4174427569
Epoch:   900  |  train loss: 0.3925313354
Epoch:  1000  |  train loss: 0.3710315943
Epoch:  1100  |  train loss: 0.3516708076
Epoch:  1200  |  train loss: 0.3346245825
Epoch:  1300  |  train loss: 0.3194143057
Epoch:  1400  |  train loss: 0.3060338855
Epoch:  1500  |  train loss: 0.2935697258
Epoch:  1600  |  train loss: 0.2830150902
Epoch:  1700  |  train loss: 0.2720638394
Epoch:  1800  |  train loss: 0.2636173368
Epoch:  1900  |  train loss: 0.2546812385
Epoch:  2000  |  train loss: 0.2467793345
Processing class: 61
Using PCA transformation
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.9111902833
Epoch:   200  |  train loss: 0.7592800617
Epoch:   300  |  train loss: 0.6539748430
Epoch:   400  |  train loss: 0.5826718330
Epoch:   500  |  train loss: 0.5293549895
Epoch:   600  |  train loss: 0.4877516031
Epoch:   700  |  train loss: 0.4537521601
Epoch:   800  |  train loss: 0.4248697042
Epoch:   900  |  train loss: 0.4001494765
Epoch:  1000  |  train loss: 0.3787407994
Epoch:  1100  |  train loss: 0.3597277761
Epoch:  1200  |  train loss: 0.3428518474
Epoch:  1300  |  train loss: 0.3281786740
Epoch:  1400  |  train loss: 0.3147360325
Epoch:  1500  |  train loss: 0.3019544244
Epoch:  1600  |  train loss: 0.2907926917
Epoch:  1700  |  train loss: 0.2804188728
Epoch:  1800  |  train loss: 0.2712485313
Epoch:  1900  |  train loss: 0.2625792891
Epoch:  2000  |  train loss: 0.2543476671
Processing class: 62
Using PCA transformation
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.9176224828
Epoch:   200  |  train loss: 0.7780069590
Epoch:   300  |  train loss: 0.6752586484
Epoch:   400  |  train loss: 0.6006865740
Epoch:   500  |  train loss: 0.5454360485
Epoch:   600  |  train loss: 0.5017710209
Epoch:   700  |  train loss: 0.4659499407
Epoch:   800  |  train loss: 0.4360184550
Epoch:   900  |  train loss: 0.4101620913
Epoch:  1000  |  train loss: 0.3876076102
Epoch:  1100  |  train loss: 0.3675275683
Epoch:  1200  |  train loss: 0.3501578271
Epoch:  1300  |  train loss: 0.3345968068
Epoch:  1400  |  train loss: 0.3206894159
Epoch:  1500  |  train loss: 0.3072942615
Epoch:  1600  |  train loss: 0.2964222729
Epoch:  1700  |  train loss: 0.2850375652
Epoch:  1800  |  train loss: 0.2758138001
Epoch:  1900  |  train loss: 0.2662667274
Epoch:  2000  |  train loss: 0.2579189301
Processing class: 63
Using PCA transformation
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.9167088389
Epoch:   200  |  train loss: 0.7740696430
Epoch:   300  |  train loss: 0.6722679257
Epoch:   400  |  train loss: 0.5992596626
Epoch:   500  |  train loss: 0.5438408256
Epoch:   600  |  train loss: 0.5004547238
Epoch:   700  |  train loss: 0.4643259525
Epoch:   800  |  train loss: 0.4337470472
Epoch:   900  |  train loss: 0.4080789447
Epoch:  1000  |  train loss: 0.3862854779
Epoch:  1100  |  train loss: 0.3669148028
Epoch:  1200  |  train loss: 0.3500946760
Epoch:  1300  |  train loss: 0.3345250249
Epoch:  1400  |  train loss: 0.3209681094
Epoch:  1500  |  train loss: 0.3085221529
Epoch:  1600  |  train loss: 0.2969274163
Epoch:  1700  |  train loss: 0.2862227619
Epoch:  1800  |  train loss: 0.2764243305
Epoch:  1900  |  train loss: 0.2679018378
Epoch:  2000  |  train loss: 0.2591176450
Processing class: 64
Using PCA transformation
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.9130807400
Epoch:   200  |  train loss: 0.7627931118
Epoch:   300  |  train loss: 0.6571629047
Epoch:   400  |  train loss: 0.5809603333
Epoch:   500  |  train loss: 0.5251935959
Epoch:   600  |  train loss: 0.4819927931
Epoch:   700  |  train loss: 0.4465019643
Epoch:   800  |  train loss: 0.4168613076
Epoch:   900  |  train loss: 0.3925153852
Epoch:  1000  |  train loss: 0.3714955032
Epoch:  1100  |  train loss: 0.3526927233
Epoch:  1200  |  train loss: 0.3363779366
Epoch:  1300  |  train loss: 0.3214000762
Epoch:  1400  |  train loss: 0.3082291424
Epoch:  1500  |  train loss: 0.2961098850
Epoch:  1600  |  train loss: 0.2853803277
Epoch:  1700  |  train loss: 0.2751851022
Epoch:  1800  |  train loss: 0.2659426510
Epoch:  1900  |  train loss: 0.2570234567
Epoch:  2000  |  train loss: 0.2495548099
Processing class: 65
Using PCA transformation
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.9121542811
Epoch:   200  |  train loss: 0.7610854506
Epoch:   300  |  train loss: 0.6547370076
Epoch:   400  |  train loss: 0.5806029677
Epoch:   500  |  train loss: 0.5237046480
Epoch:   600  |  train loss: 0.4803529322
Epoch:   700  |  train loss: 0.4451293647
Epoch:   800  |  train loss: 0.4155975103
Epoch:   900  |  train loss: 0.3910986066
Epoch:  1000  |  train loss: 0.3698978484
Epoch:  1100  |  train loss: 0.3503055573
Epoch:  1200  |  train loss: 0.3336574674
Epoch:  1300  |  train loss: 0.3192229509
Epoch:  1400  |  train loss: 0.3061651587
Epoch:  1500  |  train loss: 0.2939726293
Epoch:  1600  |  train loss: 0.2832935393
Epoch:  1700  |  train loss: 0.2735378087
Epoch:  1800  |  train loss: 0.2642740905
Epoch:  1900  |  train loss: 0.2554212868
Epoch:  2000  |  train loss: 0.2476920724
Processing class: 66
Using PCA transformation
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.9120965242
Epoch:   200  |  train loss: 0.7554046392
Epoch:   300  |  train loss: 0.6449319959
Epoch:   400  |  train loss: 0.5669247508
Epoch:   500  |  train loss: 0.5092329800
Epoch:   600  |  train loss: 0.4649963200
Epoch:   700  |  train loss: 0.4292201638
Epoch:   800  |  train loss: 0.3999479592
Epoch:   900  |  train loss: 0.3744891047
Epoch:  1000  |  train loss: 0.3533495724
Epoch:  1100  |  train loss: 0.3346925199
Epoch:  1200  |  train loss: 0.3176027119
Epoch:  1300  |  train loss: 0.3022877693
Epoch:  1400  |  train loss: 0.2888712883
Epoch:  1500  |  train loss: 0.2768189251
Epoch:  1600  |  train loss: 0.2653868914
Epoch:  1700  |  train loss: 0.2552328736
Epoch:  1800  |  train loss: 0.2466820210
Epoch:  1900  |  train loss: 0.2379545689
Epoch:  2000  |  train loss: 0.2295402318
Processing class: 67
Using PCA transformation
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.9200347424
Epoch:   200  |  train loss: 0.7685123324
Epoch:   300  |  train loss: 0.6620136738
Epoch:   400  |  train loss: 0.5843287587
Epoch:   500  |  train loss: 0.5267893672
Epoch:   600  |  train loss: 0.4820725977
Epoch:   700  |  train loss: 0.4462110043
Epoch:   800  |  train loss: 0.4162946761
Epoch:   900  |  train loss: 0.3910451114
Epoch:  1000  |  train loss: 0.3685041070
Epoch:  1100  |  train loss: 0.3488927841
Epoch:  1200  |  train loss: 0.3312854648
Epoch:  1300  |  train loss: 0.3163221061
Epoch:  1400  |  train loss: 0.3026787937
Epoch:  1500  |  train loss: 0.2900137961
Epoch:  1600  |  train loss: 0.2795793176
Epoch:  1700  |  train loss: 0.2694835067
Epoch:  1800  |  train loss: 0.2596930593
Epoch:  1900  |  train loss: 0.2506335855
Epoch:  2000  |  train loss: 0.2430877149
Processing class: 68
Using PCA transformation
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.9091733098
Epoch:   200  |  train loss: 0.7552448869
Epoch:   300  |  train loss: 0.6468949556
Epoch:   400  |  train loss: 0.5706841588
Epoch:   500  |  train loss: 0.5147120833
Epoch:   600  |  train loss: 0.4703840494
Epoch:   700  |  train loss: 0.4347255111
Epoch:   800  |  train loss: 0.4055677354
Epoch:   900  |  train loss: 0.3800278485
Epoch:  1000  |  train loss: 0.3586964667
Epoch:  1100  |  train loss: 0.3396579623
Epoch:  1200  |  train loss: 0.3229725122
Epoch:  1300  |  train loss: 0.3086842775
Epoch:  1400  |  train loss: 0.2952608943
Epoch:  1500  |  train loss: 0.2831680715
Epoch:  1600  |  train loss: 0.2721171141
Epoch:  1700  |  train loss: 0.2622369230
Epoch:  1800  |  train loss: 0.2533734441
Epoch:  1900  |  train loss: 0.2448683679
Epoch:  2000  |  train loss: 0.2368625283
Processing class: 69
Using PCA transformation
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.9192561388
Epoch:   200  |  train loss: 0.7784851074
Epoch:   300  |  train loss: 0.6752820373
Epoch:   400  |  train loss: 0.6013278484
Epoch:   500  |  train loss: 0.5460531235
Epoch:   600  |  train loss: 0.5028130352
Epoch:   700  |  train loss: 0.4668030918
Epoch:   800  |  train loss: 0.4369102359
Epoch:   900  |  train loss: 0.4111232340
Epoch:  1000  |  train loss: 0.3887178898
Epoch:  1100  |  train loss: 0.3694751203
Epoch:  1200  |  train loss: 0.3519458830
Epoch:  1300  |  train loss: 0.3362688959
Epoch:  1400  |  train loss: 0.3221543610
Epoch:  1500  |  train loss: 0.3095960975
Epoch:  1600  |  train loss: 0.2974746644
Epoch:  1700  |  train loss: 0.2868132949
Epoch:  1800  |  train loss: 0.2771917403
Epoch:  1900  |  train loss: 0.2675978065
Epoch:  2000  |  train loss: 0.2592430681
Clasifying using reconstruction function cost
2024-03-18 03:36:56,304 [trainer.py] => CNN: {'total': 64.66, '00-09': 75.0, '10-19': 70.1, '20-29': 77.1, '30-39': 70.6, '40-49': 64.0, '50-59': 45.8, '60-69': 50.0, 'old': 67.1, 'new': 50.0}
2024-03-18 03:36:56,305 [trainer.py] => No NME accuracy
2024-03-18 03:36:56,306 [trainer.py] => FeCAM: {'total': 60.7, '00-09': 78.9, '10-19': 65.6, '20-29': 73.2, '30-39': 67.0, '40-49': 65.8, '50-59': 36.1, '60-69': 38.3, 'old': 64.43, 'new': 38.3}
2024-03-18 03:36:56,306 [trainer.py] => CNN top1 curve: [83.44, 71.75, 64.66]
2024-03-18 03:36:56,306 [trainer.py] => CNN top5 curve: [96.5, 89.6, 86.54]
2024-03-18 03:36:56,306 [trainer.py] => FeCAM top1 curve: [80.92, 66.57, 60.7]
2024-03-18 03:36:56,306 [trainer.py] => FeCAM top5 curve: [92.74, 86.53, 82.2]

2024-03-18 03:36:56,317 [fecam.py] => Learning on 70-80
/home/z1165703/miniconda3/lib/python3.11/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
/home/z1165703/miniconda3/lib/python3.11/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
/home/z1165703/miniconda3/lib/python3.11/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
Processing class: 70
Using PCA transformation
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.9125866532
Epoch:   200  |  train loss: 0.7665424466
Epoch:   300  |  train loss: 0.6552741647
Epoch:   400  |  train loss: 0.5763859630
Epoch:   500  |  train loss: 0.5175011635
Epoch:   600  |  train loss: 0.4730982900
Epoch:   700  |  train loss: 0.4379616499
Epoch:   800  |  train loss: 0.4074418902
Epoch:   900  |  train loss: 0.3828453481
Epoch:  1000  |  train loss: 0.3616371930
Epoch:  1100  |  train loss: 0.3431257486
Epoch:  1200  |  train loss: 0.3261061788
Epoch:  1300  |  train loss: 0.3113975286
Epoch:  1400  |  train loss: 0.2976323962
Epoch:  1500  |  train loss: 0.2856979430
Epoch:  1600  |  train loss: 0.2746950328
Epoch:  1700  |  train loss: 0.2643324494
Epoch:  1800  |  train loss: 0.2547472417
Epoch:  1900  |  train loss: 0.2463925868
Epoch:  2000  |  train loss: 0.2378611326
Processing class: 71
Using PCA transformation
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.9203318238
Epoch:   200  |  train loss: 0.7748484492
Epoch:   300  |  train loss: 0.6671738267
Epoch:   400  |  train loss: 0.5914833426
Epoch:   500  |  train loss: 0.5367438436
Epoch:   600  |  train loss: 0.4943998694
Epoch:   700  |  train loss: 0.4601033092
Epoch:   800  |  train loss: 0.4318301558
Epoch:   900  |  train loss: 0.4073390663
Epoch:  1000  |  train loss: 0.3857151031
Epoch:  1100  |  train loss: 0.3677099168
Epoch:  1200  |  train loss: 0.3504422307
Epoch:  1300  |  train loss: 0.3349129915
Epoch:  1400  |  train loss: 0.3217051327
Epoch:  1500  |  train loss: 0.3094507039
Epoch:  1600  |  train loss: 0.2986450076
Epoch:  1700  |  train loss: 0.2878359139
Epoch:  1800  |  train loss: 0.2780522168
Epoch:  1900  |  train loss: 0.2698146403
Epoch:  2000  |  train loss: 0.2614464521
Processing class: 72
Using PCA transformation
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.9172216892
Epoch:   200  |  train loss: 0.7624179602
Epoch:   300  |  train loss: 0.6532444119
Epoch:   400  |  train loss: 0.5763826966
Epoch:   500  |  train loss: 0.5202118874
Epoch:   600  |  train loss: 0.4762462795
Epoch:   700  |  train loss: 0.4404947281
Epoch:   800  |  train loss: 0.4109644294
Epoch:   900  |  train loss: 0.3862161100
Epoch:  1000  |  train loss: 0.3638131082
Epoch:  1100  |  train loss: 0.3451705694
Epoch:  1200  |  train loss: 0.3281042755
Epoch:  1300  |  train loss: 0.3125411391
Epoch:  1400  |  train loss: 0.2993763208
Epoch:  1500  |  train loss: 0.2871503592
Epoch:  1600  |  train loss: 0.2758753181
Epoch:  1700  |  train loss: 0.2654105127
Epoch:  1800  |  train loss: 0.2562807024
Epoch:  1900  |  train loss: 0.2472658753
Epoch:  2000  |  train loss: 0.2389492601
Processing class: 73
Using PCA transformation
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.9106614351
Epoch:   200  |  train loss: 0.7590869665
Epoch:   300  |  train loss: 0.6511390805
Epoch:   400  |  train loss: 0.5770103812
Epoch:   500  |  train loss: 0.5215476632
Epoch:   600  |  train loss: 0.4787747622
Epoch:   700  |  train loss: 0.4428763568
Epoch:   800  |  train loss: 0.4129296362
Epoch:   900  |  train loss: 0.3876816452
Epoch:  1000  |  train loss: 0.3663431048
Epoch:  1100  |  train loss: 0.3479637861
Epoch:  1200  |  train loss: 0.3309983790
Epoch:  1300  |  train loss: 0.3165870368
Epoch:  1400  |  train loss: 0.3028028429
Epoch:  1500  |  train loss: 0.2910683692
Epoch:  1600  |  train loss: 0.2804823935
Epoch:  1700  |  train loss: 0.2701522827
Epoch:  1800  |  train loss: 0.2603419781
Epoch:  1900  |  train loss: 0.2519424677
Epoch:  2000  |  train loss: 0.2437938660
Processing class: 74
Using PCA transformation
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.9229094625
Epoch:   200  |  train loss: 0.7686328292
Epoch:   300  |  train loss: 0.6620005250
Epoch:   400  |  train loss: 0.5866395831
Epoch:   500  |  train loss: 0.5308881760
Epoch:   600  |  train loss: 0.4879024565
Epoch:   700  |  train loss: 0.4527922273
Epoch:   800  |  train loss: 0.4236282408
Epoch:   900  |  train loss: 0.3990473688
Epoch:  1000  |  train loss: 0.3776188731
Epoch:  1100  |  train loss: 0.3583213031
Epoch:  1200  |  train loss: 0.3414107263
Epoch:  1300  |  train loss: 0.3270243466
Epoch:  1400  |  train loss: 0.3127185881
Epoch:  1500  |  train loss: 0.2999426425
Epoch:  1600  |  train loss: 0.2896196067
Epoch:  1700  |  train loss: 0.2788275957
Epoch:  1800  |  train loss: 0.2697864950
Epoch:  1900  |  train loss: 0.2611711800
Epoch:  2000  |  train loss: 0.2530398577
Processing class: 75
Using PCA transformation
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.9055165172
Epoch:   200  |  train loss: 0.7627015114
Epoch:   300  |  train loss: 0.6569781899
Epoch:   400  |  train loss: 0.5807338595
Epoch:   500  |  train loss: 0.5248602986
Epoch:   600  |  train loss: 0.4808005393
Epoch:   700  |  train loss: 0.4454484761
Epoch:   800  |  train loss: 0.4157562971
Epoch:   900  |  train loss: 0.3908810914
Epoch:  1000  |  train loss: 0.3690650165
Epoch:  1100  |  train loss: 0.3502442122
Epoch:  1200  |  train loss: 0.3325310409
Epoch:  1300  |  train loss: 0.3175461531
Epoch:  1400  |  train loss: 0.3036060572
Epoch:  1500  |  train loss: 0.2915891171
Epoch:  1600  |  train loss: 0.2800285101
Epoch:  1700  |  train loss: 0.2702216208
Epoch:  1800  |  train loss: 0.2599448532
Epoch:  1900  |  train loss: 0.2510683209
Epoch:  2000  |  train loss: 0.2425967366
Processing class: 76
Using PCA transformation
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.9156316876
Epoch:   200  |  train loss: 0.7614814401
Epoch:   300  |  train loss: 0.6467824936
Epoch:   400  |  train loss: 0.5678552151
Epoch:   500  |  train loss: 0.5109879017
Epoch:   600  |  train loss: 0.4671773374
Epoch:   700  |  train loss: 0.4309734881
Epoch:   800  |  train loss: 0.4013956904
Epoch:   900  |  train loss: 0.3762502968
Epoch:  1000  |  train loss: 0.3538770854
Epoch:  1100  |  train loss: 0.3345053732
Epoch:  1200  |  train loss: 0.3173927724
Epoch:  1300  |  train loss: 0.3016589105
Epoch:  1400  |  train loss: 0.2877963126
Epoch:  1500  |  train loss: 0.2752429307
Epoch:  1600  |  train loss: 0.2640301228
Epoch:  1700  |  train loss: 0.2536712199
Epoch:  1800  |  train loss: 0.2437859684
Epoch:  1900  |  train loss: 0.2349472076
Epoch:  2000  |  train loss: 0.2272321701
Processing class: 77
Using PCA transformation
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.9172109723
Epoch:   200  |  train loss: 0.7717383146
Epoch:   300  |  train loss: 0.6640905261
Epoch:   400  |  train loss: 0.5885106087
Epoch:   500  |  train loss: 0.5324607968
Epoch:   600  |  train loss: 0.4889718473
Epoch:   700  |  train loss: 0.4537478209
Epoch:   800  |  train loss: 0.4242380559
Epoch:   900  |  train loss: 0.3996821523
Epoch:  1000  |  train loss: 0.3774554729
Epoch:  1100  |  train loss: 0.3578001440
Epoch:  1200  |  train loss: 0.3407823384
Epoch:  1300  |  train loss: 0.3259056985
Epoch:  1400  |  train loss: 0.3114781618
Epoch:  1500  |  train loss: 0.2990144968
Epoch:  1600  |  train loss: 0.2873064756
Epoch:  1700  |  train loss: 0.2766137838
Epoch:  1800  |  train loss: 0.2665492266
Epoch:  1900  |  train loss: 0.2579400539
Epoch:  2000  |  train loss: 0.2498008639
Processing class: 78
Using PCA transformation
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.9134134769
Epoch:   200  |  train loss: 0.7595946908
Epoch:   300  |  train loss: 0.6547569871
Epoch:   400  |  train loss: 0.5789954543
Epoch:   500  |  train loss: 0.5221448660
Epoch:   600  |  train loss: 0.4778530478
Epoch:   700  |  train loss: 0.4422642291
Epoch:   800  |  train loss: 0.4129723310
Epoch:   900  |  train loss: 0.3879791439
Epoch:  1000  |  train loss: 0.3666870296
Epoch:  1100  |  train loss: 0.3481559217
Epoch:  1200  |  train loss: 0.3319698989
Epoch:  1300  |  train loss: 0.3172728956
Epoch:  1400  |  train loss: 0.3043548286
Epoch:  1500  |  train loss: 0.2920938134
Epoch:  1600  |  train loss: 0.2813201785
Epoch:  1700  |  train loss: 0.2716645122
Epoch:  1800  |  train loss: 0.2622818172
Epoch:  1900  |  train loss: 0.2538082153
Epoch:  2000  |  train loss: 0.2455267429
Processing class: 79
Using PCA transformation
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.9186390042
Epoch:   200  |  train loss: 0.7743974805
Epoch:   300  |  train loss: 0.6687188268
Epoch:   400  |  train loss: 0.5923705339
Epoch:   500  |  train loss: 0.5357862353
Epoch:   600  |  train loss: 0.4914745748
Epoch:   700  |  train loss: 0.4562611461
Epoch:   800  |  train loss: 0.4275532305
Epoch:   900  |  train loss: 0.4032270849
Epoch:  1000  |  train loss: 0.3814516068
Epoch:  1100  |  train loss: 0.3624827087
Epoch:  1200  |  train loss: 0.3458882928
Epoch:  1300  |  train loss: 0.3310211241
Epoch:  1400  |  train loss: 0.3174562514
Epoch:  1500  |  train loss: 0.3046101034
Epoch:  1600  |  train loss: 0.2938447833
Epoch:  1700  |  train loss: 0.2844768226
Epoch:  1800  |  train loss: 0.2743252277
Epoch:  1900  |  train loss: 0.2653551102
Epoch:  2000  |  train loss: 0.2583370805
Clasifying using reconstruction function cost
2024-03-18 03:56:57,222 [trainer.py] => CNN: {'total': 59.18, '00-09': 73.6, '10-19': 68.4, '20-29': 76.9, '30-39': 69.1, '40-49': 60.6, '50-59': 37.2, '60-69': 45.7, '70-79': 41.9, 'old': 61.64, 'new': 41.9}
2024-03-18 03:56:57,224 [trainer.py] => No NME accuracy
2024-03-18 03:56:57,224 [trainer.py] => FeCAM: {'total': 55.01, '00-09': 76.7, '10-19': 64.6, '20-29': 72.4, '30-39': 64.7, '40-49': 63.2, '50-59': 30.3, '60-69': 34.9, '70-79': 33.3, 'old': 58.11, 'new': 33.3}
2024-03-18 03:56:57,224 [trainer.py] => CNN top1 curve: [83.44, 71.75, 64.66, 59.18]
2024-03-18 03:56:57,224 [trainer.py] => CNN top5 curve: [96.5, 89.6, 86.54, 84.09]
2024-03-18 03:56:57,224 [trainer.py] => FeCAM top1 curve: [80.92, 66.57, 60.7, 55.01]
2024-03-18 03:56:57,224 [trainer.py] => FeCAM top5 curve: [92.74, 86.53, 82.2, 78.3]

2024-03-18 03:56:57,238 [fecam.py] => Learning on 80-90
/home/z1165703/miniconda3/lib/python3.11/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
/home/z1165703/miniconda3/lib/python3.11/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
/home/z1165703/miniconda3/lib/python3.11/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
Processing class: 80
Using PCA transformation
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.9166094661
Epoch:   200  |  train loss: 0.7583160758
Epoch:   300  |  train loss: 0.6504188180
Epoch:   400  |  train loss: 0.5739143252
Epoch:   500  |  train loss: 0.5175449967
Epoch:   600  |  train loss: 0.4741771042
Epoch:   700  |  train loss: 0.4389149249
Epoch:   800  |  train loss: 0.4106477797
Epoch:   900  |  train loss: 0.3856880903
Epoch:  1000  |  train loss: 0.3646534562
Epoch:  1100  |  train loss: 0.3467813373
Epoch:  1200  |  train loss: 0.3296988249
Epoch:  1300  |  train loss: 0.3148564875
Epoch:  1400  |  train loss: 0.3009104490
Epoch:  1500  |  train loss: 0.2889901757
Epoch:  1600  |  train loss: 0.2782049477
Epoch:  1700  |  train loss: 0.2677250922
Epoch:  1800  |  train loss: 0.2583775759
Epoch:  1900  |  train loss: 0.2494599909
Epoch:  2000  |  train loss: 0.2412591249
Processing class: 81
Using PCA transformation
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.9168981075
Epoch:   200  |  train loss: 0.7697226524
Epoch:   300  |  train loss: 0.6655146956
Epoch:   400  |  train loss: 0.5917588472
Epoch:   500  |  train loss: 0.5377551675
Epoch:   600  |  train loss: 0.4958919168
Epoch:   700  |  train loss: 0.4612128198
Epoch:   800  |  train loss: 0.4316523969
Epoch:   900  |  train loss: 0.4066682220
Epoch:  1000  |  train loss: 0.3849525690
Epoch:  1100  |  train loss: 0.3663904369
Epoch:  1200  |  train loss: 0.3493066907
Epoch:  1300  |  train loss: 0.3338743508
Epoch:  1400  |  train loss: 0.3207473695
Epoch:  1500  |  train loss: 0.3081622660
Epoch:  1600  |  train loss: 0.2961417198
Epoch:  1700  |  train loss: 0.2855852544
Epoch:  1800  |  train loss: 0.2757851124
Epoch:  1900  |  train loss: 0.2662374198
Epoch:  2000  |  train loss: 0.2579658866
Processing class: 82
Using PCA transformation
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.9154970884
Epoch:   200  |  train loss: 0.7678900123
Epoch:   300  |  train loss: 0.6626902699
Epoch:   400  |  train loss: 0.5883093476
Epoch:   500  |  train loss: 0.5341271043
Epoch:   600  |  train loss: 0.4919250607
Epoch:   700  |  train loss: 0.4580002844
Epoch:   800  |  train loss: 0.4291689634
Epoch:   900  |  train loss: 0.4046748519
Epoch:  1000  |  train loss: 0.3827453732
Epoch:  1100  |  train loss: 0.3636713564
Epoch:  1200  |  train loss: 0.3466709673
Epoch:  1300  |  train loss: 0.3312132299
Epoch:  1400  |  train loss: 0.3175071061
Epoch:  1500  |  train loss: 0.3053832352
Epoch:  1600  |  train loss: 0.2947984099
Epoch:  1700  |  train loss: 0.2844168067
Epoch:  1800  |  train loss: 0.2741597831
Epoch:  1900  |  train loss: 0.2651775658
Epoch:  2000  |  train loss: 0.2565137744
Processing class: 83
Using PCA transformation
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.9136041522
Epoch:   200  |  train loss: 0.7565637469
Epoch:   300  |  train loss: 0.6499639869
Epoch:   400  |  train loss: 0.5730281353
Epoch:   500  |  train loss: 0.5147481918
Epoch:   600  |  train loss: 0.4707681715
Epoch:   700  |  train loss: 0.4349394023
Epoch:   800  |  train loss: 0.4053241014
Epoch:   900  |  train loss: 0.3797091186
Epoch:  1000  |  train loss: 0.3577768922
Epoch:  1100  |  train loss: 0.3387313545
Epoch:  1200  |  train loss: 0.3225854635
Epoch:  1300  |  train loss: 0.3068726122
Epoch:  1400  |  train loss: 0.2933794379
Epoch:  1500  |  train loss: 0.2810596883
Epoch:  1600  |  train loss: 0.2699876547
Epoch:  1700  |  train loss: 0.2590032041
Epoch:  1800  |  train loss: 0.2501751602
Epoch:  1900  |  train loss: 0.2412120938
Epoch:  2000  |  train loss: 0.2335768044
Processing class: 84
Using PCA transformation
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.9193016052
Epoch:   200  |  train loss: 0.7644892812
Epoch:   300  |  train loss: 0.6577062488
Epoch:   400  |  train loss: 0.5794471979
Epoch:   500  |  train loss: 0.5226196647
Epoch:   600  |  train loss: 0.4789644003
Epoch:   700  |  train loss: 0.4435468793
Epoch:   800  |  train loss: 0.4141597748
Epoch:   900  |  train loss: 0.3889952481
Epoch:  1000  |  train loss: 0.3680973887
Epoch:  1100  |  train loss: 0.3496141255
Epoch:  1200  |  train loss: 0.3327008724
Epoch:  1300  |  train loss: 0.3178726315
Epoch:  1400  |  train loss: 0.3043178499
Epoch:  1500  |  train loss: 0.2916767955
Epoch:  1600  |  train loss: 0.2807085037
Epoch:  1700  |  train loss: 0.2706210196
Epoch:  1800  |  train loss: 0.2607134342
Epoch:  1900  |  train loss: 0.2523464590
Epoch:  2000  |  train loss: 0.2436507225
Processing class: 85
Using PCA transformation
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.9196593761
Epoch:   200  |  train loss: 0.7599684119
Epoch:   300  |  train loss: 0.6531780362
Epoch:   400  |  train loss: 0.5756543994
Epoch:   500  |  train loss: 0.5173005223
Epoch:   600  |  train loss: 0.4725029588
Epoch:   700  |  train loss: 0.4371434450
Epoch:   800  |  train loss: 0.4073288023
Epoch:   900  |  train loss: 0.3824137747
Epoch:  1000  |  train loss: 0.3608830154
Epoch:  1100  |  train loss: 0.3422168136
Epoch:  1200  |  train loss: 0.3252046645
Epoch:  1300  |  train loss: 0.3099385321
Epoch:  1400  |  train loss: 0.2966841519
Epoch:  1500  |  train loss: 0.2844889224
Epoch:  1600  |  train loss: 0.2733278215
Epoch:  1700  |  train loss: 0.2631549537
Epoch:  1800  |  train loss: 0.2535461813
Epoch:  1900  |  train loss: 0.2450324774
Epoch:  2000  |  train loss: 0.2373606235
Processing class: 86
Using PCA transformation
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.9163206935
Epoch:   200  |  train loss: 0.7683690786
Epoch:   300  |  train loss: 0.6613992572
Epoch:   400  |  train loss: 0.5860698223
Epoch:   500  |  train loss: 0.5308082342
Epoch:   600  |  train loss: 0.4882345021
Epoch:   700  |  train loss: 0.4536004543
Epoch:   800  |  train loss: 0.4252207935
Epoch:   900  |  train loss: 0.4003970265
Epoch:  1000  |  train loss: 0.3798926115
Epoch:  1100  |  train loss: 0.3609327793
Epoch:  1200  |  train loss: 0.3442116082
Epoch:  1300  |  train loss: 0.3293814898
Epoch:  1400  |  train loss: 0.3158518970
Epoch:  1500  |  train loss: 0.3040987492
Epoch:  1600  |  train loss: 0.2928984284
Epoch:  1700  |  train loss: 0.2820653677
Epoch:  1800  |  train loss: 0.2722354770
Epoch:  1900  |  train loss: 0.2635554254
Epoch:  2000  |  train loss: 0.2555848718
Processing class: 87
Using PCA transformation
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.9158212423
Epoch:   200  |  train loss: 0.7583138824
Epoch:   300  |  train loss: 0.6489710093
Epoch:   400  |  train loss: 0.5692262530
Epoch:   500  |  train loss: 0.5118186533
Epoch:   600  |  train loss: 0.4678681970
Epoch:   700  |  train loss: 0.4321151078
Epoch:   800  |  train loss: 0.4021314919
Epoch:   900  |  train loss: 0.3773064256
Epoch:  1000  |  train loss: 0.3559902847
Epoch:  1100  |  train loss: 0.3370400250
Epoch:  1200  |  train loss: 0.3200157404
Epoch:  1300  |  train loss: 0.3054572880
Epoch:  1400  |  train loss: 0.2918450534
Epoch:  1500  |  train loss: 0.2799714267
Epoch:  1600  |  train loss: 0.2688605070
Epoch:  1700  |  train loss: 0.2586708009
Epoch:  1800  |  train loss: 0.2493514776
Epoch:  1900  |  train loss: 0.2405387402
Epoch:  2000  |  train loss: 0.2332513154
Processing class: 88
Using PCA transformation
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.9210840821
Epoch:   200  |  train loss: 0.7684911132
Epoch:   300  |  train loss: 0.6647743583
Epoch:   400  |  train loss: 0.5882068157
Epoch:   500  |  train loss: 0.5314189911
Epoch:   600  |  train loss: 0.4865565777
Epoch:   700  |  train loss: 0.4507100046
Epoch:   800  |  train loss: 0.4211820364
Epoch:   900  |  train loss: 0.3954325497
Epoch:  1000  |  train loss: 0.3736449599
Epoch:  1100  |  train loss: 0.3546019316
Epoch:  1200  |  train loss: 0.3375222862
Epoch:  1300  |  train loss: 0.3223951399
Epoch:  1400  |  train loss: 0.3083457470
Epoch:  1500  |  train loss: 0.2958359003
Epoch:  1600  |  train loss: 0.2843266070
Epoch:  1700  |  train loss: 0.2737778842
Epoch:  1800  |  train loss: 0.2643781960
Epoch:  1900  |  train loss: 0.2553044528
Epoch:  2000  |  train loss: 0.2471889406
Processing class: 89
Using PCA transformation
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.9166512251
Epoch:   200  |  train loss: 0.7699769139
Epoch:   300  |  train loss: 0.6678715110
Epoch:   400  |  train loss: 0.5939698219
Epoch:   500  |  train loss: 0.5375287771
Epoch:   600  |  train loss: 0.4930809319
Epoch:   700  |  train loss: 0.4568686008
Epoch:   800  |  train loss: 0.4268794477
Epoch:   900  |  train loss: 0.4011940241
Epoch:  1000  |  train loss: 0.3792972207
Epoch:  1100  |  train loss: 0.3598931551
Epoch:  1200  |  train loss: 0.3428957582
Epoch:  1300  |  train loss: 0.3275553226
Epoch:  1400  |  train loss: 0.3140448213
Epoch:  1500  |  train loss: 0.3012097716
Epoch:  1600  |  train loss: 0.2899931133
Epoch:  1700  |  train loss: 0.2793140948
Epoch:  1800  |  train loss: 0.2701584816
Epoch:  1900  |  train loss: 0.2610894620
Epoch:  2000  |  train loss: 0.2528034121
Clasifying using reconstruction function cost
2024-03-18 04:20:56,325 [trainer.py] => CNN: {'total': 54.08, '00-09': 67.9, '10-19': 63.6, '20-29': 73.2, '30-39': 68.8, '40-49': 57.2, '50-59': 32.6, '60-69': 40.9, '70-79': 38.1, '80-89': 44.4, 'old': 55.29, 'new': 44.4}
2024-03-18 04:20:56,331 [trainer.py] => No NME accuracy
2024-03-18 04:20:56,331 [trainer.py] => FeCAM: {'total': 50.84, '00-09': 75.1, '10-19': 62.5, '20-29': 70.2, '30-39': 63.6, '40-49': 61.7, '50-59': 27.9, '60-69': 31.6, '70-79': 30.7, '80-89': 34.3, 'old': 52.91, 'new': 34.3}
2024-03-18 04:20:56,331 [trainer.py] => CNN top1 curve: [83.44, 71.75, 64.66, 59.18, 54.08]
2024-03-18 04:20:56,331 [trainer.py] => CNN top5 curve: [96.5, 89.6, 86.54, 84.09, 81.74]
2024-03-18 04:20:56,331 [trainer.py] => FeCAM top1 curve: [80.92, 66.57, 60.7, 55.01, 50.84]
2024-03-18 04:20:56,331 [trainer.py] => FeCAM top5 curve: [92.74, 86.53, 82.2, 78.3, 75.08]

2024-03-18 04:20:56,351 [fecam.py] => Learning on 90-100
/home/z1165703/miniconda3/lib/python3.11/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
/home/z1165703/miniconda3/lib/python3.11/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
/home/z1165703/miniconda3/lib/python3.11/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
Processing class: 90
Using PCA transformation
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.9248642087
Epoch:   200  |  train loss: 0.7791208029
Epoch:   300  |  train loss: 0.6755114198
Epoch:   400  |  train loss: 0.5990076780
Epoch:   500  |  train loss: 0.5421294093
Epoch:   600  |  train loss: 0.4988666952
Epoch:   700  |  train loss: 0.4634001315
Epoch:   800  |  train loss: 0.4344914794
Epoch:   900  |  train loss: 0.4091246307
Epoch:  1000  |  train loss: 0.3881176233
Epoch:  1100  |  train loss: 0.3684547842
Epoch:  1200  |  train loss: 0.3520368099
Epoch:  1300  |  train loss: 0.3370047510
Epoch:  1400  |  train loss: 0.3234253466
Epoch:  1500  |  train loss: 0.3116796017
Epoch:  1600  |  train loss: 0.3001272559
Epoch:  1700  |  train loss: 0.2898433983
Epoch:  1800  |  train loss: 0.2807416499
Epoch:  1900  |  train loss: 0.2729517400
Epoch:  2000  |  train loss: 0.2643635869
Processing class: 91
Using PCA transformation
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.9189605236
Epoch:   200  |  train loss: 0.7700113416
Epoch:   300  |  train loss: 0.6659914017
Epoch:   400  |  train loss: 0.5895423055
Epoch:   500  |  train loss: 0.5348860145
Epoch:   600  |  train loss: 0.4925235093
Epoch:   700  |  train loss: 0.4577159047
Epoch:   800  |  train loss: 0.4287474811
Epoch:   900  |  train loss: 0.4040053606
Epoch:  1000  |  train loss: 0.3828490019
Epoch:  1100  |  train loss: 0.3641549349
Epoch:  1200  |  train loss: 0.3475520730
Epoch:  1300  |  train loss: 0.3322989404
Epoch:  1400  |  train loss: 0.3191234171
Epoch:  1500  |  train loss: 0.3073007345
Epoch:  1600  |  train loss: 0.2956226170
Epoch:  1700  |  train loss: 0.2855536163
Epoch:  1800  |  train loss: 0.2757881761
Epoch:  1900  |  train loss: 0.2677169800
Epoch:  2000  |  train loss: 0.2589979589
Processing class: 92
Using PCA transformation
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.9093813062
Epoch:   200  |  train loss: 0.7498152494
Epoch:   300  |  train loss: 0.6433286786
Epoch:   400  |  train loss: 0.5649309397
Epoch:   500  |  train loss: 0.5078369915
Epoch:   600  |  train loss: 0.4630553424
Epoch:   700  |  train loss: 0.4270721316
Epoch:   800  |  train loss: 0.3972073615
Epoch:   900  |  train loss: 0.3718699217
Epoch:  1000  |  train loss: 0.3507065892
Epoch:  1100  |  train loss: 0.3317199588
Epoch:  1200  |  train loss: 0.3149772525
Epoch:  1300  |  train loss: 0.3002880812
Epoch:  1400  |  train loss: 0.2865830362
Epoch:  1500  |  train loss: 0.2746402323
Epoch:  1600  |  train loss: 0.2638189733
Epoch:  1700  |  train loss: 0.2536216259
Epoch:  1800  |  train loss: 0.2445598871
Epoch:  1900  |  train loss: 0.2353370577
Epoch:  2000  |  train loss: 0.2272777736
Processing class: 93
Using PCA transformation
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.9129863024
Epoch:   200  |  train loss: 0.7528050661
Epoch:   300  |  train loss: 0.6478022218
Epoch:   400  |  train loss: 0.5732388854
Epoch:   500  |  train loss: 0.5159784257
Epoch:   600  |  train loss: 0.4704366624
Epoch:   700  |  train loss: 0.4350340366
Epoch:   800  |  train loss: 0.4059755683
Epoch:   900  |  train loss: 0.3810652614
Epoch:  1000  |  train loss: 0.3596446037
Epoch:  1100  |  train loss: 0.3401231647
Epoch:  1200  |  train loss: 0.3228906929
Epoch:  1300  |  train loss: 0.3080682218
Epoch:  1400  |  train loss: 0.2944427490
Epoch:  1500  |  train loss: 0.2824613869
Epoch:  1600  |  train loss: 0.2711945057
Epoch:  1700  |  train loss: 0.2609071940
Epoch:  1800  |  train loss: 0.2518062323
Epoch:  1900  |  train loss: 0.2429078639
Epoch:  2000  |  train loss: 0.2349300236
Processing class: 94
Using PCA transformation
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.9132611036
Epoch:   200  |  train loss: 0.7510613561
Epoch:   300  |  train loss: 0.6402273774
Epoch:   400  |  train loss: 0.5631964207
Epoch:   500  |  train loss: 0.5062549412
Epoch:   600  |  train loss: 0.4616397083
Epoch:   700  |  train loss: 0.4259166896
Epoch:   800  |  train loss: 0.3952775240
Epoch:   900  |  train loss: 0.3696193278
Epoch:  1000  |  train loss: 0.3480465770
Epoch:  1100  |  train loss: 0.3288045347
Epoch:  1200  |  train loss: 0.3124027729
Epoch:  1300  |  train loss: 0.2975126386
Epoch:  1400  |  train loss: 0.2844512522
Epoch:  1500  |  train loss: 0.2719423115
Epoch:  1600  |  train loss: 0.2615368724
Epoch:  1700  |  train loss: 0.2515318334
Epoch:  1800  |  train loss: 0.2431720376
Epoch:  1900  |  train loss: 0.2344227076
Epoch:  2000  |  train loss: 0.2267853916
Processing class: 95
Using PCA transformation
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.9197955489
Epoch:   200  |  train loss: 0.7731322646
Epoch:   300  |  train loss: 0.6719974756
Epoch:   400  |  train loss: 0.5985267282
Epoch:   500  |  train loss: 0.5428804398
Epoch:   600  |  train loss: 0.4982041836
Epoch:   700  |  train loss: 0.4629165173
Epoch:   800  |  train loss: 0.4340656340
Epoch:   900  |  train loss: 0.4086450875
Epoch:  1000  |  train loss: 0.3875805795
Epoch:  1100  |  train loss: 0.3692404866
Epoch:  1200  |  train loss: 0.3529686868
Epoch:  1300  |  train loss: 0.3374387622
Epoch:  1400  |  train loss: 0.3237126827
Epoch:  1500  |  train loss: 0.3112625003
Epoch:  1600  |  train loss: 0.3004420340
Epoch:  1700  |  train loss: 0.2903027475
Epoch:  1800  |  train loss: 0.2804329872
Epoch:  1900  |  train loss: 0.2719651341
Epoch:  2000  |  train loss: 0.2631039888
Processing class: 96
Using PCA transformation
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.9220964074
Epoch:   200  |  train loss: 0.7700726271
Epoch:   300  |  train loss: 0.6654730320
Epoch:   400  |  train loss: 0.5894547105
Epoch:   500  |  train loss: 0.5317718744
Epoch:   600  |  train loss: 0.4877327323
Epoch:   700  |  train loss: 0.4525478184
Epoch:   800  |  train loss: 0.4228953242
Epoch:   900  |  train loss: 0.3979948163
Epoch:  1000  |  train loss: 0.3762496114
Epoch:  1100  |  train loss: 0.3576667309
Epoch:  1200  |  train loss: 0.3413532853
Epoch:  1300  |  train loss: 0.3252487898
Epoch:  1400  |  train loss: 0.3116580307
Epoch:  1500  |  train loss: 0.3001056373
Epoch:  1600  |  train loss: 0.2886632919
Epoch:  1700  |  train loss: 0.2783700287
Epoch:  1800  |  train loss: 0.2692855179
Epoch:  1900  |  train loss: 0.2601469725
Epoch:  2000  |  train loss: 0.2529612005
Processing class: 97
Using PCA transformation
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.9122890949
Epoch:   200  |  train loss: 0.7510720730
Epoch:   300  |  train loss: 0.6456102371
Epoch:   400  |  train loss: 0.5713700771
Epoch:   500  |  train loss: 0.5164557695
Epoch:   600  |  train loss: 0.4735148191
Epoch:   700  |  train loss: 0.4382091343
Epoch:   800  |  train loss: 0.4089871466
Epoch:   900  |  train loss: 0.3839070380
Epoch:  1000  |  train loss: 0.3625500321
Epoch:  1100  |  train loss: 0.3437438250
Epoch:  1200  |  train loss: 0.3270861685
Epoch:  1300  |  train loss: 0.3124639571
Epoch:  1400  |  train loss: 0.2992619455
Epoch:  1500  |  train loss: 0.2873099387
Epoch:  1600  |  train loss: 0.2757768810
Epoch:  1700  |  train loss: 0.2660542071
Epoch:  1800  |  train loss: 0.2573656112
Epoch:  1900  |  train loss: 0.2485424131
Epoch:  2000  |  train loss: 0.2411578387
Processing class: 98
Using PCA transformation
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.9157160282
Epoch:   200  |  train loss: 0.7611834049
Epoch:   300  |  train loss: 0.6572266221
Epoch:   400  |  train loss: 0.5820500731
Epoch:   500  |  train loss: 0.5266124129
Epoch:   600  |  train loss: 0.4841324747
Epoch:   700  |  train loss: 0.4481403887
Epoch:   800  |  train loss: 0.4183522284
Epoch:   900  |  train loss: 0.3935059905
Epoch:  1000  |  train loss: 0.3710288346
Epoch:  1100  |  train loss: 0.3512727857
Epoch:  1200  |  train loss: 0.3341024458
Epoch:  1300  |  train loss: 0.3191006064
Epoch:  1400  |  train loss: 0.3056834519
Epoch:  1500  |  train loss: 0.2933378756
Epoch:  1600  |  train loss: 0.2822618723
Epoch:  1700  |  train loss: 0.2721415579
Epoch:  1800  |  train loss: 0.2623238146
Epoch:  1900  |  train loss: 0.2540890366
Epoch:  2000  |  train loss: 0.2463833690
Processing class: 99
Using PCA transformation
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.9112133145
Epoch:   200  |  train loss: 0.7591689348
Epoch:   300  |  train loss: 0.6505882621
Epoch:   400  |  train loss: 0.5759616375
Epoch:   500  |  train loss: 0.5203077674
Epoch:   600  |  train loss: 0.4769974232
Epoch:   700  |  train loss: 0.4419134796
Epoch:   800  |  train loss: 0.4116650283
Epoch:   900  |  train loss: 0.3860751212
Epoch:  1000  |  train loss: 0.3638901353
Epoch:  1100  |  train loss: 0.3443823099
Epoch:  1200  |  train loss: 0.3273192167
Epoch:  1300  |  train loss: 0.3111605108
Epoch:  1400  |  train loss: 0.2975561857
Epoch:  1500  |  train loss: 0.2848236620
Epoch:  1600  |  train loss: 0.2731314361
Epoch:  1700  |  train loss: 0.2625112265
Epoch:  1800  |  train loss: 0.2533546001
Epoch:  1900  |  train loss: 0.2441097826
Epoch:  2000  |  train loss: 0.2361768901
Clasifying using reconstruction function cost
2024-03-18 04:49:03,232 [trainer.py] => CNN: {'total': 50.36, '00-09': 57.7, '10-19': 63.6, '20-29': 71.4, '30-39': 68.5, '40-49': 56.8, '50-59': 29.6, '60-69': 38.9, '70-79': 36.6, '80-89': 42.7, '90-99': 37.8, 'old': 51.76, 'new': 37.8}
2024-03-18 04:49:03,234 [trainer.py] => No NME accuracy
2024-03-18 04:49:03,234 [trainer.py] => FeCAM: {'total': 47.08, '00-09': 71.4, '10-19': 61.6, '20-29': 68.9, '30-39': 62.3, '40-49': 58.6, '50-59': 23.7, '60-69': 27.1, '70-79': 27.3, '80-89': 31.8, '90-99': 38.1, 'old': 48.08, 'new': 38.1}
2024-03-18 04:49:03,235 [trainer.py] => CNN top1 curve: [83.44, 71.75, 64.66, 59.18, 54.08, 50.36]
2024-03-18 04:49:03,235 [trainer.py] => CNN top5 curve: [96.5, 89.6, 86.54, 84.09, 81.74, 79.63]
2024-03-18 04:49:03,235 [trainer.py] => FeCAM top1 curve: [80.92, 66.57, 60.7, 55.01, 50.84, 47.08]
2024-03-18 04:49:03,235 [trainer.py] => FeCAM top5 curve: [92.74, 86.53, 82.2, 78.3, 75.08, 71.74]

=========================================
2024-03-18 04:49:14,773 [trainer.py] => config: ./exps/FeCAM_cifar100.json
2024-03-18 04:49:14,773 [trainer.py] => prefix: train
2024-03-18 04:49:14,773 [trainer.py] => dataset: cifar100
2024-03-18 04:49:14,773 [trainer.py] => memory_size: 0
2024-03-18 04:49:14,774 [trainer.py] => shuffle: True
2024-03-18 04:49:14,774 [trainer.py] => init_cls: 50
2024-03-18 04:49:14,774 [trainer.py] => increment: 10
2024-03-18 04:49:14,774 [trainer.py] => model_name: fecam
2024-03-18 04:49:14,774 [trainer.py] => convnet_type: resnet18
2024-03-18 04:49:14,774 [trainer.py] => device: [device(type='cuda', index=0)]
2024-03-18 04:49:14,774 [trainer.py] => seed: 1993
2024-03-18 04:49:14,774 [trainer.py] => init_epochs: 200
2024-03-18 04:49:14,774 [trainer.py] => init_lr: 0.1
2024-03-18 04:49:14,774 [trainer.py] => init_weight_decay: 0.0005
2024-03-18 04:49:14,774 [trainer.py] => batch_size: 128
2024-03-18 04:49:14,774 [trainer.py] => num_workers: 8
2024-03-18 04:49:14,774 [trainer.py] => T: 5
2024-03-18 04:49:14,774 [trainer.py] => beta: 0.5
2024-03-18 04:49:14,774 [trainer.py] => alpha1: 1
2024-03-18 04:49:14,774 [trainer.py] => alpha2: 1
2024-03-18 04:49:14,774 [trainer.py] => ncm: False
2024-03-18 04:49:14,774 [trainer.py] => tukey: False
2024-03-18 04:49:14,774 [trainer.py] => diagonal: False
2024-03-18 04:49:14,774 [trainer.py] => per_class: True
2024-03-18 04:49:14,774 [trainer.py] => full_cov: True
2024-03-18 04:49:14,774 [trainer.py] => shrink: True
2024-03-18 04:49:14,774 [trainer.py] => norm_cov: False
2024-03-18 04:49:14,774 [trainer.py] => epochs: 2000
2024-03-18 04:49:14,774 [trainer.py] => vecnorm: False
2024-03-18 04:49:14,774 [trainer.py] => ae_type: wae
2024-03-18 04:49:14,774 [trainer.py] => ae_latent_dim: 32
2024-03-18 04:49:14,774 [trainer.py] => ae_n: 1
2024-03-18 04:49:14,774 [trainer.py] => wae_sigma: 10
2024-03-18 04:49:14,774 [trainer.py] => wae_C: 0.1
2024-03-18 04:49:14,774 [trainer.py] => ae_standarization: False
2024-03-18 04:49:14,774 [trainer.py] => ae_pca: True
2024-03-18 04:49:14,774 [trainer.py] => ae_pca_components: 300
2024-03-18 04:49:14,774 [trainer.py] => ae_clsf: recon-cost
Files already downloaded and verified
Files already downloaded and verified
2024-03-18 04:49:17,511 [data_manager.py] => [68, 56, 78, 8, 23, 84, 90, 65, 74, 76, 40, 89, 3, 92, 55, 9, 26, 80, 43, 38, 58, 70, 77, 1, 85, 19, 17, 50, 28, 53, 13, 81, 45, 82, 6, 59, 83, 16, 15, 44, 91, 41, 72, 60, 79, 52, 20, 10, 31, 54, 37, 95, 14, 71, 96, 98, 97, 2, 64, 66, 42, 22, 35, 86, 24, 34, 87, 21, 99, 0, 88, 27, 18, 94, 11, 12, 47, 25, 30, 46, 62, 69, 36, 61, 7, 63, 75, 5, 32, 4, 51, 48, 73, 93, 39, 67, 29, 49, 57, 33]
2024-03-18 04:49:17,783 [fecam.py] => Learning on 0-50
/home/z1165703/miniconda3/lib/python3.11/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
/home/z1165703/miniconda3/lib/python3.11/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
Processing class: 0
Using PCA transformation
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.9362517595
Epoch:   200  |  train loss: 0.8133799553
Epoch:   300  |  train loss: 0.7198452830
Epoch:   400  |  train loss: 0.6531275034
Epoch:   500  |  train loss: 0.6039835215
Epoch:   600  |  train loss: 0.5648689985
Epoch:   700  |  train loss: 0.5329523802
Epoch:   800  |  train loss: 0.5067118466
Epoch:   900  |  train loss: 0.4844357610
Epoch:  1000  |  train loss: 0.4651742280
Epoch:  1100  |  train loss: 0.4482941389
Epoch:  1200  |  train loss: 0.4335882545
Epoch:  1300  |  train loss: 0.4208235145
Epoch:  1400  |  train loss: 0.4090637267
Epoch:  1500  |  train loss: 0.3990653932
Epoch:  1600  |  train loss: 0.3893397510
Epoch:  1700  |  train loss: 0.3809407413
Epoch:  1800  |  train loss: 0.3733184934
Epoch:  1900  |  train loss: 0.3658565521
Epoch:  2000  |  train loss: 0.3597545147
Processing class: 1
Using PCA transformation
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.9293642998
Epoch:   200  |  train loss: 0.8066487074
Epoch:   300  |  train loss: 0.7190721154
Epoch:   400  |  train loss: 0.6547500372
Epoch:   500  |  train loss: 0.6060805678
Epoch:   600  |  train loss: 0.5671160579
Epoch:   700  |  train loss: 0.5362609506
Epoch:   800  |  train loss: 0.5103118002
Epoch:   900  |  train loss: 0.4889147222
Epoch:  1000  |  train loss: 0.4702150106
Epoch:  1100  |  train loss: 0.4537477136
Epoch:  1200  |  train loss: 0.4395445883
Epoch:  1300  |  train loss: 0.4267084599
Epoch:  1400  |  train loss: 0.4152799666
Epoch:  1500  |  train loss: 0.4052005053
Epoch:  1600  |  train loss: 0.3961583853
Epoch:  1700  |  train loss: 0.3870958626
Epoch:  1800  |  train loss: 0.3793619037
Epoch:  1900  |  train loss: 0.3721035421
Epoch:  2000  |  train loss: 0.3654496014
Processing class: 2
Using PCA transformation
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.9303386807
Epoch:   200  |  train loss: 0.7960988283
Epoch:   300  |  train loss: 0.6998494267
Epoch:   400  |  train loss: 0.6332082748
Epoch:   500  |  train loss: 0.5842554688
Epoch:   600  |  train loss: 0.5465384960
Epoch:   700  |  train loss: 0.5161020041
Epoch:   800  |  train loss: 0.4900043607
Epoch:   900  |  train loss: 0.4687833011
Epoch:  1000  |  train loss: 0.4501029551
Epoch:  1100  |  train loss: 0.4343401492
Epoch:  1200  |  train loss: 0.4208326459
Epoch:  1300  |  train loss: 0.4082081974
Epoch:  1400  |  train loss: 0.3976571023
Epoch:  1500  |  train loss: 0.3880669475
Epoch:  1600  |  train loss: 0.3790258527
Epoch:  1700  |  train loss: 0.3709574461
Epoch:  1800  |  train loss: 0.3637724519
Epoch:  1900  |  train loss: 0.3576183498
Epoch:  2000  |  train loss: 0.3515617788
Processing class: 3
Using PCA transformation
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.9329127789
Epoch:   200  |  train loss: 0.7997429490
Epoch:   300  |  train loss: 0.7075562477
Epoch:   400  |  train loss: 0.6429268837
Epoch:   500  |  train loss: 0.5942685246
Epoch:   600  |  train loss: 0.5557793736
Epoch:   700  |  train loss: 0.5248386979
Epoch:   800  |  train loss: 0.4986895740
Epoch:   900  |  train loss: 0.4770998955
Epoch:  1000  |  train loss: 0.4581052661
Epoch:  1100  |  train loss: 0.4417702436
Epoch:  1200  |  train loss: 0.4273057878
Epoch:  1300  |  train loss: 0.4144039452
Epoch:  1400  |  train loss: 0.4034373641
Epoch:  1500  |  train loss: 0.3934304416
Epoch:  1600  |  train loss: 0.3840643048
Epoch:  1700  |  train loss: 0.3760156214
Epoch:  1800  |  train loss: 0.3681990504
Epoch:  1900  |  train loss: 0.3614915729
Epoch:  2000  |  train loss: 0.3557655156
Processing class: 4
Using PCA transformation
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.9335224032
Epoch:   200  |  train loss: 0.8004852891
Epoch:   300  |  train loss: 0.7037399650
Epoch:   400  |  train loss: 0.6356766582
Epoch:   500  |  train loss: 0.5853701115
Epoch:   600  |  train loss: 0.5461022139
Epoch:   700  |  train loss: 0.5142121434
Epoch:   800  |  train loss: 0.4883267403
Epoch:   900  |  train loss: 0.4656639814
Epoch:  1000  |  train loss: 0.4470624030
Epoch:  1100  |  train loss: 0.4312379599
Epoch:  1200  |  train loss: 0.4164834559
Epoch:  1300  |  train loss: 0.4038518131
Epoch:  1400  |  train loss: 0.3925343275
Epoch:  1500  |  train loss: 0.3823381722
Epoch:  1600  |  train loss: 0.3734344900
Epoch:  1700  |  train loss: 0.3651169956
Epoch:  1800  |  train loss: 0.3575038135
Epoch:  1900  |  train loss: 0.3508501112
Epoch:  2000  |  train loss: 0.3445916772
Processing class: 5
Using PCA transformation
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.9335783482
Epoch:   200  |  train loss: 0.7991880298
Epoch:   300  |  train loss: 0.7035329819
Epoch:   400  |  train loss: 0.6355380177
Epoch:   500  |  train loss: 0.5859532595
Epoch:   600  |  train loss: 0.5482729673
Epoch:   700  |  train loss: 0.5179004431
Epoch:   800  |  train loss: 0.4934240997
Epoch:   900  |  train loss: 0.4721114039
Epoch:  1000  |  train loss: 0.4540629327
Epoch:  1100  |  train loss: 0.4386883318
Epoch:  1200  |  train loss: 0.4247846484
Epoch:  1300  |  train loss: 0.4128350556
Epoch:  1400  |  train loss: 0.4019635856
Epoch:  1500  |  train loss: 0.3923362315
Epoch:  1600  |  train loss: 0.3835413992
Epoch:  1700  |  train loss: 0.3747127831
Epoch:  1800  |  train loss: 0.3668825030
Epoch:  1900  |  train loss: 0.3597103894
Epoch:  2000  |  train loss: 0.3537304223
Processing class: 6
Using PCA transformation
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.9338033080
Epoch:   200  |  train loss: 0.8054721475
Epoch:   300  |  train loss: 0.7154396534
Epoch:   400  |  train loss: 0.6504548430
Epoch:   500  |  train loss: 0.6005932808
Epoch:   600  |  train loss: 0.5629313231
Epoch:   700  |  train loss: 0.5324407816
Epoch:   800  |  train loss: 0.5072379053
Epoch:   900  |  train loss: 0.4854551077
Epoch:  1000  |  train loss: 0.4667642295
Epoch:  1100  |  train loss: 0.4508931935
Epoch:  1200  |  train loss: 0.4358570516
Epoch:  1300  |  train loss: 0.4225666225
Epoch:  1400  |  train loss: 0.4110118449
Epoch:  1500  |  train loss: 0.4008356094
Epoch:  1600  |  train loss: 0.3914967120
Epoch:  1700  |  train loss: 0.3831990004
Epoch:  1800  |  train loss: 0.3752891064
Epoch:  1900  |  train loss: 0.3681905985
Epoch:  2000  |  train loss: 0.3618499398
Processing class: 7
Using PCA transformation
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.9359704018
Epoch:   200  |  train loss: 0.8033891320
Epoch:   300  |  train loss: 0.7100531340
Epoch:   400  |  train loss: 0.6442950249
Epoch:   500  |  train loss: 0.5960098386
Epoch:   600  |  train loss: 0.5584792495
Epoch:   700  |  train loss: 0.5276482701
Epoch:   800  |  train loss: 0.5023032367
Epoch:   900  |  train loss: 0.4813820064
Epoch:  1000  |  train loss: 0.4630419314
Epoch:  1100  |  train loss: 0.4472057819
Epoch:  1200  |  train loss: 0.4339446068
Epoch:  1300  |  train loss: 0.4209161401
Epoch:  1400  |  train loss: 0.4102456987
Epoch:  1500  |  train loss: 0.3998873413
Epoch:  1600  |  train loss: 0.3912653506
Epoch:  1700  |  train loss: 0.3832165062
Epoch:  1800  |  train loss: 0.3757159650
Epoch:  1900  |  train loss: 0.3685402513
Epoch:  2000  |  train loss: 0.3621904969
Processing class: 8
Using PCA transformation
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.9349206209
Epoch:   200  |  train loss: 0.8034483433
Epoch:   300  |  train loss: 0.7162548542
Epoch:   400  |  train loss: 0.6515193343
Epoch:   500  |  train loss: 0.6032083631
Epoch:   600  |  train loss: 0.5649176836
Epoch:   700  |  train loss: 0.5346643329
Epoch:   800  |  train loss: 0.5093659163
Epoch:   900  |  train loss: 0.4883772969
Epoch:  1000  |  train loss: 0.4697672844
Epoch:  1100  |  train loss: 0.4541496277
Epoch:  1200  |  train loss: 0.4396281779
Epoch:  1300  |  train loss: 0.4270374775
Epoch:  1400  |  train loss: 0.4157160759
Epoch:  1500  |  train loss: 0.4060223818
Epoch:  1600  |  train loss: 0.3967695892
Epoch:  1700  |  train loss: 0.3884374797
Epoch:  1800  |  train loss: 0.3808623314
Epoch:  1900  |  train loss: 0.3740302861
Epoch:  2000  |  train loss: 0.3672239423
Processing class: 9
Using PCA transformation
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.9326717734
Epoch:   200  |  train loss: 0.8066235900
Epoch:   300  |  train loss: 0.7166150689
Epoch:   400  |  train loss: 0.6514130116
Epoch:   500  |  train loss: 0.6035746574
Epoch:   600  |  train loss: 0.5668352246
Epoch:   700  |  train loss: 0.5366010547
Epoch:   800  |  train loss: 0.5115886927
Epoch:   900  |  train loss: 0.4911197484
Epoch:  1000  |  train loss: 0.4723462164
Epoch:  1100  |  train loss: 0.4563939571
Epoch:  1200  |  train loss: 0.4424882591
Epoch:  1300  |  train loss: 0.4298689663
Epoch:  1400  |  train loss: 0.4186258018
Epoch:  1500  |  train loss: 0.4089385509
Epoch:  1600  |  train loss: 0.3994694293
Epoch:  1700  |  train loss: 0.3913073540
Epoch:  1800  |  train loss: 0.3837457299
Epoch:  1900  |  train loss: 0.3770492911
Epoch:  2000  |  train loss: 0.3706299603
Processing class: 10
Using PCA transformation
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.9278581977
Epoch:   200  |  train loss: 0.7977267265
Epoch:   300  |  train loss: 0.7068137288
Epoch:   400  |  train loss: 0.6402802587
Epoch:   500  |  train loss: 0.5904370904
Epoch:   600  |  train loss: 0.5520757437
Epoch:   700  |  train loss: 0.5217129469
Epoch:   800  |  train loss: 0.4965414047
Epoch:   900  |  train loss: 0.4754650593
Epoch:  1000  |  train loss: 0.4576259971
Epoch:  1100  |  train loss: 0.4416499496
Epoch:  1200  |  train loss: 0.4274824083
Epoch:  1300  |  train loss: 0.4154104173
Epoch:  1400  |  train loss: 0.4046929181
Epoch:  1500  |  train loss: 0.3946487069
Epoch:  1600  |  train loss: 0.3854570270
Epoch:  1700  |  train loss: 0.3770444453
Epoch:  1800  |  train loss: 0.3702195823
Epoch:  1900  |  train loss: 0.3631016910
Epoch:  2000  |  train loss: 0.3570452273
Processing class: 11
Using PCA transformation
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.9343132973
Epoch:   200  |  train loss: 0.8140558481
Epoch:   300  |  train loss: 0.7249667048
Epoch:   400  |  train loss: 0.6609939218
Epoch:   500  |  train loss: 0.6133507729
Epoch:   600  |  train loss: 0.5755539179
Epoch:   700  |  train loss: 0.5454998970
Epoch:   800  |  train loss: 0.5200128794
Epoch:   900  |  train loss: 0.4983400524
Epoch:  1000  |  train loss: 0.4795015991
Epoch:  1100  |  train loss: 0.4635175109
Epoch:  1200  |  train loss: 0.4490056634
Epoch:  1300  |  train loss: 0.4359743297
Epoch:  1400  |  train loss: 0.4246935546
Epoch:  1500  |  train loss: 0.4145105660
Epoch:  1600  |  train loss: 0.4055289090
Epoch:  1700  |  train loss: 0.3967047155
Epoch:  1800  |  train loss: 0.3891097307
Epoch:  1900  |  train loss: 0.3816913128
Epoch:  2000  |  train loss: 0.3752788842
Processing class: 12
Using PCA transformation
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.9364360332
Epoch:   200  |  train loss: 0.8069839835
Epoch:   300  |  train loss: 0.7138017535
Epoch:   400  |  train loss: 0.6464434743
Epoch:   500  |  train loss: 0.5967294335
Epoch:   600  |  train loss: 0.5582571149
Epoch:   700  |  train loss: 0.5276697159
Epoch:   800  |  train loss: 0.5022049904
Epoch:   900  |  train loss: 0.4804802120
Epoch:  1000  |  train loss: 0.4622862160
Epoch:  1100  |  train loss: 0.4461798251
Epoch:  1200  |  train loss: 0.4324299157
Epoch:  1300  |  train loss: 0.4196569324
Epoch:  1400  |  train loss: 0.4088339508
Epoch:  1500  |  train loss: 0.3986961663
Epoch:  1600  |  train loss: 0.3898984492
Epoch:  1700  |  train loss: 0.3816960573
Epoch:  1800  |  train loss: 0.3737483144
Epoch:  1900  |  train loss: 0.3673149884
Epoch:  2000  |  train loss: 0.3608409345
Processing class: 13
Using PCA transformation
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.9341493130
Epoch:   200  |  train loss: 0.8093558073
Epoch:   300  |  train loss: 0.7172808051
Epoch:   400  |  train loss: 0.6516694307
Epoch:   500  |  train loss: 0.6029175282
Epoch:   600  |  train loss: 0.5651786208
Epoch:   700  |  train loss: 0.5348407745
Epoch:   800  |  train loss: 0.5095724523
Epoch:   900  |  train loss: 0.4881017983
Epoch:  1000  |  train loss: 0.4695535183
Epoch:  1100  |  train loss: 0.4535796344
Epoch:  1200  |  train loss: 0.4397750139
Epoch:  1300  |  train loss: 0.4276540101
Epoch:  1400  |  train loss: 0.4164531946
Epoch:  1500  |  train loss: 0.4068116605
Epoch:  1600  |  train loss: 0.3980331898
Epoch:  1700  |  train loss: 0.3899751067
Epoch:  1800  |  train loss: 0.3829039812
Epoch:  1900  |  train loss: 0.3763409078
Epoch:  2000  |  train loss: 0.3704044819
Processing class: 14
Using PCA transformation
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.9357708454
Epoch:   200  |  train loss: 0.8027579308
Epoch:   300  |  train loss: 0.7153986931
Epoch:   400  |  train loss: 0.6502108455
Epoch:   500  |  train loss: 0.6018098712
Epoch:   600  |  train loss: 0.5653184056
Epoch:   700  |  train loss: 0.5350281358
Epoch:   800  |  train loss: 0.5104164779
Epoch:   900  |  train loss: 0.4900339007
Epoch:  1000  |  train loss: 0.4721509576
Epoch:  1100  |  train loss: 0.4571534634
Epoch:  1200  |  train loss: 0.4437526226
Epoch:  1300  |  train loss: 0.4323831916
Epoch:  1400  |  train loss: 0.4216021180
Epoch:  1500  |  train loss: 0.4121471882
Epoch:  1600  |  train loss: 0.4034077466
Epoch:  1700  |  train loss: 0.3956944704
Epoch:  1800  |  train loss: 0.3886685550
Epoch:  1900  |  train loss: 0.3824919343
Epoch:  2000  |  train loss: 0.3764844418
Processing class: 15
Using PCA transformation
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.9262511015
Epoch:   200  |  train loss: 0.7975113034
Epoch:   300  |  train loss: 0.7042079806
Epoch:   400  |  train loss: 0.6371273160
Epoch:   500  |  train loss: 0.5876871824
Epoch:   600  |  train loss: 0.5500450730
Epoch:   700  |  train loss: 0.5194438696
Epoch:   800  |  train loss: 0.4938080549
Epoch:   900  |  train loss: 0.4723192334
Epoch:  1000  |  train loss: 0.4542539179
Epoch:  1100  |  train loss: 0.4381696999
Epoch:  1200  |  train loss: 0.4236897051
Epoch:  1300  |  train loss: 0.4115448475
Epoch:  1400  |  train loss: 0.4006494582
Epoch:  1500  |  train loss: 0.3900775909
Epoch:  1600  |  train loss: 0.3810366213
Epoch:  1700  |  train loss: 0.3722077131
Epoch:  1800  |  train loss: 0.3644829810
Epoch:  1900  |  train loss: 0.3571676016
Epoch:  2000  |  train loss: 0.3509552717
Processing class: 16
Using PCA transformation
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.9308139682
Epoch:   200  |  train loss: 0.8007297158
Epoch:   300  |  train loss: 0.7065147042
Epoch:   400  |  train loss: 0.6404124379
Epoch:   500  |  train loss: 0.5911917329
Epoch:   600  |  train loss: 0.5538124323
Epoch:   700  |  train loss: 0.5229847908
Epoch:   800  |  train loss: 0.4978809059
Epoch:   900  |  train loss: 0.4763248444
Epoch:  1000  |  train loss: 0.4579421282
Epoch:  1100  |  train loss: 0.4424057305
Epoch:  1200  |  train loss: 0.4270679176
Epoch:  1300  |  train loss: 0.4151449859
Epoch:  1400  |  train loss: 0.4039667010
Epoch:  1500  |  train loss: 0.3938857019
Epoch:  1600  |  train loss: 0.3845315754
Epoch:  1700  |  train loss: 0.3765802205
Epoch:  1800  |  train loss: 0.3690394163
Epoch:  1900  |  train loss: 0.3622418165
Epoch:  2000  |  train loss: 0.3559315681
Processing class: 17
Using PCA transformation
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.9295265079
Epoch:   200  |  train loss: 0.7947855473
Epoch:   300  |  train loss: 0.7019304037
Epoch:   400  |  train loss: 0.6361258388
Epoch:   500  |  train loss: 0.5867827058
Epoch:   600  |  train loss: 0.5480729461
Epoch:   700  |  train loss: 0.5171974242
Epoch:   800  |  train loss: 0.4911907971
Epoch:   900  |  train loss: 0.4693404377
Epoch:  1000  |  train loss: 0.4501991868
Epoch:  1100  |  train loss: 0.4339206576
Epoch:  1200  |  train loss: 0.4194451332
Epoch:  1300  |  train loss: 0.4066337168
Epoch:  1400  |  train loss: 0.3943031967
Epoch:  1500  |  train loss: 0.3842548251
Epoch:  1600  |  train loss: 0.3742899418
Epoch:  1700  |  train loss: 0.3660523295
Epoch:  1800  |  train loss: 0.3584623694
Epoch:  1900  |  train loss: 0.3510610521
Epoch:  2000  |  train loss: 0.3449462295
Processing class: 18
Using PCA transformation
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.9320911288
Epoch:   200  |  train loss: 0.8037364960
Epoch:   300  |  train loss: 0.7118445754
Epoch:   400  |  train loss: 0.6465465784
Epoch:   500  |  train loss: 0.5983246326
Epoch:   600  |  train loss: 0.5616645098
Epoch:   700  |  train loss: 0.5317653894
Epoch:   800  |  train loss: 0.5063142538
Epoch:   900  |  train loss: 0.4856091738
Epoch:  1000  |  train loss: 0.4673672676
Epoch:  1100  |  train loss: 0.4514126599
Epoch:  1200  |  train loss: 0.4377604604
Epoch:  1300  |  train loss: 0.4253200173
Epoch:  1400  |  train loss: 0.4142604232
Epoch:  1500  |  train loss: 0.4042212963
Epoch:  1600  |  train loss: 0.3955493271
Epoch:  1700  |  train loss: 0.3877138019
Epoch:  1800  |  train loss: 0.3800936460
Epoch:  1900  |  train loss: 0.3736369908
Epoch:  2000  |  train loss: 0.3668589413
Processing class: 19
Using PCA transformation
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.9309653759
Epoch:   200  |  train loss: 0.7940914035
Epoch:   300  |  train loss: 0.7010752678
Epoch:   400  |  train loss: 0.6338204741
Epoch:   500  |  train loss: 0.5837848783
Epoch:   600  |  train loss: 0.5442487240
Epoch:   700  |  train loss: 0.5129639685
Epoch:   800  |  train loss: 0.4871444821
Epoch:   900  |  train loss: 0.4658900619
Epoch:  1000  |  train loss: 0.4471651137
Epoch:  1100  |  train loss: 0.4313778400
Epoch:  1200  |  train loss: 0.4175140321
Epoch:  1300  |  train loss: 0.4050428271
Epoch:  1400  |  train loss: 0.3937447309
Epoch:  1500  |  train loss: 0.3839458346
Epoch:  1600  |  train loss: 0.3749776602
Epoch:  1700  |  train loss: 0.3667019248
Epoch:  1800  |  train loss: 0.3594224751
Epoch:  1900  |  train loss: 0.3526061475
Epoch:  2000  |  train loss: 0.3467716575
Processing class: 20
Using PCA transformation
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.9322469711
Epoch:   200  |  train loss: 0.8040625930
Epoch:   300  |  train loss: 0.7095969439
Epoch:   400  |  train loss: 0.6425645709
Epoch:   500  |  train loss: 0.5937520504
Epoch:   600  |  train loss: 0.5555106759
Epoch:   700  |  train loss: 0.5241092563
Epoch:   800  |  train loss: 0.4988954544
Epoch:   900  |  train loss: 0.4771502852
Epoch:  1000  |  train loss: 0.4582316697
Epoch:  1100  |  train loss: 0.4426411510
Epoch:  1200  |  train loss: 0.4286150634
Epoch:  1300  |  train loss: 0.4161443114
Epoch:  1400  |  train loss: 0.4055143416
Epoch:  1500  |  train loss: 0.3957992136
Epoch:  1600  |  train loss: 0.3868889689
Epoch:  1700  |  train loss: 0.3789009094
Epoch:  1800  |  train loss: 0.3717279553
Epoch:  1900  |  train loss: 0.3650680602
Epoch:  2000  |  train loss: 0.3591124594
Processing class: 21
Using PCA transformation
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.9317656994
Epoch:   200  |  train loss: 0.8100202322
Epoch:   300  |  train loss: 0.7187772989
Epoch:   400  |  train loss: 0.6541532397
Epoch:   500  |  train loss: 0.6056443095
Epoch:   600  |  train loss: 0.5672810912
Epoch:   700  |  train loss: 0.5357098460
Epoch:   800  |  train loss: 0.5099894464
Epoch:   900  |  train loss: 0.4882519245
Epoch:  1000  |  train loss: 0.4693885028
Epoch:  1100  |  train loss: 0.4529264212
Epoch:  1200  |  train loss: 0.4390360296
Epoch:  1300  |  train loss: 0.4263472676
Epoch:  1400  |  train loss: 0.4143638909
Epoch:  1500  |  train loss: 0.4043341696
Epoch:  1600  |  train loss: 0.3946035147
Epoch:  1700  |  train loss: 0.3861961961
Epoch:  1800  |  train loss: 0.3784808397
Epoch:  1900  |  train loss: 0.3716414690
Epoch:  2000  |  train loss: 0.3654444695
Processing class: 22
Using PCA transformation
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.9289228082
Epoch:   200  |  train loss: 0.7896208763
Epoch:   300  |  train loss: 0.6936395168
Epoch:   400  |  train loss: 0.6249788642
Epoch:   500  |  train loss: 0.5746890306
Epoch:   600  |  train loss: 0.5364537954
Epoch:   700  |  train loss: 0.5055106938
Epoch:   800  |  train loss: 0.4800138772
Epoch:   900  |  train loss: 0.4585992336
Epoch:  1000  |  train loss: 0.4400855958
Epoch:  1100  |  train loss: 0.4244127631
Epoch:  1200  |  train loss: 0.4106738091
Epoch:  1300  |  train loss: 0.3985101461
Epoch:  1400  |  train loss: 0.3875291705
Epoch:  1500  |  train loss: 0.3776501238
Epoch:  1600  |  train loss: 0.3685266614
Epoch:  1700  |  train loss: 0.3608817220
Epoch:  1800  |  train loss: 0.3536807716
Epoch:  1900  |  train loss: 0.3471094668
Epoch:  2000  |  train loss: 0.3410639167
Processing class: 23
Using PCA transformation
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.9271631837
Epoch:   200  |  train loss: 0.7966444850
Epoch:   300  |  train loss: 0.7073065400
Epoch:   400  |  train loss: 0.6433031559
Epoch:   500  |  train loss: 0.5946564436
Epoch:   600  |  train loss: 0.5572280765
Epoch:   700  |  train loss: 0.5270104289
Epoch:   800  |  train loss: 0.5020089030
Epoch:   900  |  train loss: 0.4811442971
Epoch:  1000  |  train loss: 0.4625977576
Epoch:  1100  |  train loss: 0.4465606213
Epoch:  1200  |  train loss: 0.4325490475
Epoch:  1300  |  train loss: 0.4199637115
Epoch:  1400  |  train loss: 0.4090071976
Epoch:  1500  |  train loss: 0.3989957213
Epoch:  1600  |  train loss: 0.3899771094
Epoch:  1700  |  train loss: 0.3816909194
Epoch:  1800  |  train loss: 0.3740663826
Epoch:  1900  |  train loss: 0.3676067531
Epoch:  2000  |  train loss: 0.3610773921
Processing class: 24
Using PCA transformation
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.9333173156
Epoch:   200  |  train loss: 0.8049871683
Epoch:   300  |  train loss: 0.7174277902
Epoch:   400  |  train loss: 0.6554504514
Epoch:   500  |  train loss: 0.6075035334
Epoch:   600  |  train loss: 0.5699365497
Epoch:   700  |  train loss: 0.5401237249
Epoch:   800  |  train loss: 0.5144428849
Epoch:   900  |  train loss: 0.4927796543
Epoch:  1000  |  train loss: 0.4745140076
Epoch:  1100  |  train loss: 0.4586027682
Epoch:  1200  |  train loss: 0.4444297671
Epoch:  1300  |  train loss: 0.4316567719
Epoch:  1400  |  train loss: 0.4202625394
Epoch:  1500  |  train loss: 0.4101956964
Epoch:  1600  |  train loss: 0.4007806361
Epoch:  1700  |  train loss: 0.3924188375
Epoch:  1800  |  train loss: 0.3850685894
Epoch:  1900  |  train loss: 0.3780846775
Epoch:  2000  |  train loss: 0.3715520799
Processing class: 25
Using PCA transformation
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.9310894728
Epoch:   200  |  train loss: 0.8006859183
Epoch:   300  |  train loss: 0.7082882285
Epoch:   400  |  train loss: 0.6438096404
Epoch:   500  |  train loss: 0.5961531401
Epoch:   600  |  train loss: 0.5586762190
Epoch:   700  |  train loss: 0.5281187773
Epoch:   800  |  train loss: 0.5026235580
Epoch:   900  |  train loss: 0.4811269045
Epoch:  1000  |  train loss: 0.4628504813
Epoch:  1100  |  train loss: 0.4470803022
Epoch:  1200  |  train loss: 0.4335414588
Epoch:  1300  |  train loss: 0.4203802288
Epoch:  1400  |  train loss: 0.4096086383
Epoch:  1500  |  train loss: 0.3997753143
Epoch:  1600  |  train loss: 0.3903483570
Epoch:  1700  |  train loss: 0.3824118972
Epoch:  1800  |  train loss: 0.3748508811
Epoch:  1900  |  train loss: 0.3678624749
Epoch:  2000  |  train loss: 0.3617549002
Processing class: 26
Using PCA transformation
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.9337510467
Epoch:   200  |  train loss: 0.8068136573
Epoch:   300  |  train loss: 0.7198353529
Epoch:   400  |  train loss: 0.6548053741
Epoch:   500  |  train loss: 0.6060883522
Epoch:   600  |  train loss: 0.5690781832
Epoch:   700  |  train loss: 0.5382456303
Epoch:   800  |  train loss: 0.5125488997
Epoch:   900  |  train loss: 0.4909142494
Epoch:  1000  |  train loss: 0.4729112387
Epoch:  1100  |  train loss: 0.4571360111
Epoch:  1200  |  train loss: 0.4431230068
Epoch:  1300  |  train loss: 0.4312033534
Epoch:  1400  |  train loss: 0.4202252626
Epoch:  1500  |  train loss: 0.4103038728
Epoch:  1600  |  train loss: 0.4009250760
Epoch:  1700  |  train loss: 0.3933308601
Epoch:  1800  |  train loss: 0.3859759927
Epoch:  1900  |  train loss: 0.3789116085
Epoch:  2000  |  train loss: 0.3730642200
Processing class: 27
Using PCA transformation
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.9334871769
Epoch:   200  |  train loss: 0.8137794614
Epoch:   300  |  train loss: 0.7232954621
Epoch:   400  |  train loss: 0.6603059530
Epoch:   500  |  train loss: 0.6134296060
Epoch:   600  |  train loss: 0.5763361216
Epoch:   700  |  train loss: 0.5462925434
Epoch:   800  |  train loss: 0.5210228086
Epoch:   900  |  train loss: 0.4995742023
Epoch:  1000  |  train loss: 0.4813818395
Epoch:  1100  |  train loss: 0.4650860429
Epoch:  1200  |  train loss: 0.4509146392
Epoch:  1300  |  train loss: 0.4383930027
Epoch:  1400  |  train loss: 0.4263870895
Epoch:  1500  |  train loss: 0.4158418000
Epoch:  1600  |  train loss: 0.4062544823
Epoch:  1700  |  train loss: 0.3978130698
Epoch:  1800  |  train loss: 0.3896995604
Epoch:  1900  |  train loss: 0.3828485012
Epoch:  2000  |  train loss: 0.3761229932
Processing class: 28
Using PCA transformation
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.9339622855
Epoch:   200  |  train loss: 0.8087300658
Epoch:   300  |  train loss: 0.7178390503
Epoch:   400  |  train loss: 0.6514889836
Epoch:   500  |  train loss: 0.6026704311
Epoch:   600  |  train loss: 0.5653899312
Epoch:   700  |  train loss: 0.5349423289
Epoch:   800  |  train loss: 0.5099154532
Epoch:   900  |  train loss: 0.4879837990
Epoch:  1000  |  train loss: 0.4692989111
Epoch:  1100  |  train loss: 0.4536558509
Epoch:  1200  |  train loss: 0.4386190593
Epoch:  1300  |  train loss: 0.4261121690
Epoch:  1400  |  train loss: 0.4148541331
Epoch:  1500  |  train loss: 0.4040569484
Epoch:  1600  |  train loss: 0.3950032055
Epoch:  1700  |  train loss: 0.3862146258
Epoch:  1800  |  train loss: 0.3785418391
Epoch:  1900  |  train loss: 0.3712148666
Epoch:  2000  |  train loss: 0.3648541749
Processing class: 29
Using PCA transformation
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.9319503307
Epoch:   200  |  train loss: 0.7972636580
Epoch:   300  |  train loss: 0.7036526918
Epoch:   400  |  train loss: 0.6376217604
Epoch:   500  |  train loss: 0.5884860277
Epoch:   600  |  train loss: 0.5491485596
Epoch:   700  |  train loss: 0.5172219813
Epoch:   800  |  train loss: 0.4906754255
Epoch:   900  |  train loss: 0.4684846580
Epoch:  1000  |  train loss: 0.4501312912
Epoch:  1100  |  train loss: 0.4337553740
Epoch:  1200  |  train loss: 0.4194425404
Epoch:  1300  |  train loss: 0.4071334779
Epoch:  1400  |  train loss: 0.3967024565
Epoch:  1500  |  train loss: 0.3865385711
Epoch:  1600  |  train loss: 0.3772835732
Epoch:  1700  |  train loss: 0.3692333043
Epoch:  1800  |  train loss: 0.3616385698
Epoch:  1900  |  train loss: 0.3554014146
Epoch:  2000  |  train loss: 0.3491765678
Processing class: 30
Using PCA transformation
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.9324279785
Epoch:   200  |  train loss: 0.8052459478
Epoch:   300  |  train loss: 0.7167457700
Epoch:   400  |  train loss: 0.6520447731
Epoch:   500  |  train loss: 0.6023796320
Epoch:   600  |  train loss: 0.5622973561
Epoch:   700  |  train loss: 0.5299880385
Epoch:   800  |  train loss: 0.5037019908
Epoch:   900  |  train loss: 0.4813408375
Epoch:  1000  |  train loss: 0.4620381534
Epoch:  1100  |  train loss: 0.4454924166
Epoch:  1200  |  train loss: 0.4305894494
Epoch:  1300  |  train loss: 0.4174613714
Epoch:  1400  |  train loss: 0.4057360709
Epoch:  1500  |  train loss: 0.3950660050
Epoch:  1600  |  train loss: 0.3858230770
Epoch:  1700  |  train loss: 0.3769810081
Epoch:  1800  |  train loss: 0.3691684008
Epoch:  1900  |  train loss: 0.3616573155
Epoch:  2000  |  train loss: 0.3548295915
Processing class: 31
Using PCA transformation
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.9326804161
Epoch:   200  |  train loss: 0.7998351932
Epoch:   300  |  train loss: 0.7058313489
Epoch:   400  |  train loss: 0.6405329585
Epoch:   500  |  train loss: 0.5917653322
Epoch:   600  |  train loss: 0.5539523244
Epoch:   700  |  train loss: 0.5235925078
Epoch:   800  |  train loss: 0.4985009432
Epoch:   900  |  train loss: 0.4776866376
Epoch:  1000  |  train loss: 0.4596557617
Epoch:  1100  |  train loss: 0.4437366009
Epoch:  1200  |  train loss: 0.4297980905
Epoch:  1300  |  train loss: 0.4175540328
Epoch:  1400  |  train loss: 0.4070056558
Epoch:  1500  |  train loss: 0.3973016977
Epoch:  1600  |  train loss: 0.3878380954
Epoch:  1700  |  train loss: 0.3795278668
Epoch:  1800  |  train loss: 0.3719582140
Epoch:  1900  |  train loss: 0.3656405151
Epoch:  2000  |  train loss: 0.3591775537
Processing class: 32
Using PCA transformation
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.9292341709
Epoch:   200  |  train loss: 0.8017233253
Epoch:   300  |  train loss: 0.7112015724
Epoch:   400  |  train loss: 0.6466907024
Epoch:   500  |  train loss: 0.5988029361
Epoch:   600  |  train loss: 0.5603821874
Epoch:   700  |  train loss: 0.5299335480
Epoch:   800  |  train loss: 0.5044354558
Epoch:   900  |  train loss: 0.4831676662
Epoch:  1000  |  train loss: 0.4649039507
Epoch:  1100  |  train loss: 0.4487285256
Epoch:  1200  |  train loss: 0.4349305212
Epoch:  1300  |  train loss: 0.4225574851
Epoch:  1400  |  train loss: 0.4113323748
Epoch:  1500  |  train loss: 0.4015575945
Epoch:  1600  |  train loss: 0.3924112797
Epoch:  1700  |  train loss: 0.3845179737
Epoch:  1800  |  train loss: 0.3768999100
Epoch:  1900  |  train loss: 0.3703195751
Epoch:  2000  |  train loss: 0.3637470782
Processing class: 33
Using PCA transformation
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.9343407989
Epoch:   200  |  train loss: 0.7995977521
Epoch:   300  |  train loss: 0.7059732199
Epoch:   400  |  train loss: 0.6411532760
Epoch:   500  |  train loss: 0.5931756258
Epoch:   600  |  train loss: 0.5559625149
Epoch:   700  |  train loss: 0.5264169574
Epoch:   800  |  train loss: 0.5016144097
Epoch:   900  |  train loss: 0.4814950168
Epoch:  1000  |  train loss: 0.4632177413
Epoch:  1100  |  train loss: 0.4483108401
Epoch:  1200  |  train loss: 0.4345227420
Epoch:  1300  |  train loss: 0.4230240643
Epoch:  1400  |  train loss: 0.4124524653
Epoch:  1500  |  train loss: 0.4031204641
Epoch:  1600  |  train loss: 0.3947081208
Epoch:  1700  |  train loss: 0.3867626309
Epoch:  1800  |  train loss: 0.3793196380
Epoch:  1900  |  train loss: 0.3731079102
Epoch:  2000  |  train loss: 0.3670340955
Processing class: 34
Using PCA transformation
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.9297896385
Epoch:   200  |  train loss: 0.8016377091
Epoch:   300  |  train loss: 0.7074897766
Epoch:   400  |  train loss: 0.6403367400
Epoch:   500  |  train loss: 0.5906677127
Epoch:   600  |  train loss: 0.5525169492
Epoch:   700  |  train loss: 0.5211743116
Epoch:   800  |  train loss: 0.4960961103
Epoch:   900  |  train loss: 0.4745156646
Epoch:  1000  |  train loss: 0.4557169557
Epoch:  1100  |  train loss: 0.4394518495
Epoch:  1200  |  train loss: 0.4250205576
Epoch:  1300  |  train loss: 0.4123160005
Epoch:  1400  |  train loss: 0.4006940126
Epoch:  1500  |  train loss: 0.3906197131
Epoch:  1600  |  train loss: 0.3814611316
Epoch:  1700  |  train loss: 0.3728447616
Epoch:  1800  |  train loss: 0.3654762208
Epoch:  1900  |  train loss: 0.3579912663
Epoch:  2000  |  train loss: 0.3518485427
Processing class: 35
Using PCA transformation
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.9271572948
Epoch:   200  |  train loss: 0.7931135178
Epoch:   300  |  train loss: 0.7006931067
Epoch:   400  |  train loss: 0.6321391940
Epoch:   500  |  train loss: 0.5817844272
Epoch:   600  |  train loss: 0.5421722174
Epoch:   700  |  train loss: 0.5105930924
Epoch:   800  |  train loss: 0.4849344134
Epoch:   900  |  train loss: 0.4623256683
/home/z1165703/miniconda3/lib/python3.11/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
Epoch:  1000  |  train loss: 0.4437002301
Epoch:  1100  |  train loss: 0.4273294926
Epoch:  1200  |  train loss: 0.4130568981
Epoch:  1300  |  train loss: 0.4006036580
Epoch:  1400  |  train loss: 0.3893075168
Epoch:  1500  |  train loss: 0.3792618573
Epoch:  1600  |  train loss: 0.3699150562
Epoch:  1700  |  train loss: 0.3612190366
Epoch:  1800  |  train loss: 0.3537384450
Epoch:  1900  |  train loss: 0.3467068672
Epoch:  2000  |  train loss: 0.3405739427
Processing class: 36
Using PCA transformation
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.9311516047
Epoch:   200  |  train loss: 0.8022689223
Epoch:   300  |  train loss: 0.7092101097
Epoch:   400  |  train loss: 0.6420702577
Epoch:   500  |  train loss: 0.5924448371
Epoch:   600  |  train loss: 0.5542191148
Epoch:   700  |  train loss: 0.5231073499
Epoch:   800  |  train loss: 0.4975941062
Epoch:   900  |  train loss: 0.4759877861
Epoch:  1000  |  train loss: 0.4576627851
Epoch:  1100  |  train loss: 0.4415107846
Epoch:  1200  |  train loss: 0.4279829264
Epoch:  1300  |  train loss: 0.4157280028
Epoch:  1400  |  train loss: 0.4048725486
Epoch:  1500  |  train loss: 0.3947643578
Epoch:  1600  |  train loss: 0.3857912898
Epoch:  1700  |  train loss: 0.3785062969
Epoch:  1800  |  train loss: 0.3709862053
Epoch:  1900  |  train loss: 0.3646596789
Epoch:  2000  |  train loss: 0.3585443735
Processing class: 37
Using PCA transformation
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.9293739676
Epoch:   200  |  train loss: 0.7991312385
Epoch:   300  |  train loss: 0.7057322025
Epoch:   400  |  train loss: 0.6390975833
Epoch:   500  |  train loss: 0.5887428164
Epoch:   600  |  train loss: 0.5502187252
Epoch:   700  |  train loss: 0.5190390229
Epoch:   800  |  train loss: 0.4943223357
Epoch:   900  |  train loss: 0.4731711507
Epoch:  1000  |  train loss: 0.4549471319
Epoch:  1100  |  train loss: 0.4392973125
Epoch:  1200  |  train loss: 0.4252771139
Epoch:  1300  |  train loss: 0.4129700422
Epoch:  1400  |  train loss: 0.4018756866
Epoch:  1500  |  train loss: 0.3922201395
Epoch:  1600  |  train loss: 0.3828857422
Epoch:  1700  |  train loss: 0.3754897833
Epoch:  1800  |  train loss: 0.3678140700
Epoch:  1900  |  train loss: 0.3611427784
Epoch:  2000  |  train loss: 0.3551089346
Processing class: 38
Using PCA transformation
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.9292168379
Epoch:   200  |  train loss: 0.7924971700
Epoch:   300  |  train loss: 0.7012085676
Epoch:   400  |  train loss: 0.6365342855
Epoch:   500  |  train loss: 0.5882065296
Epoch:   600  |  train loss: 0.5499979734
Epoch:   700  |  train loss: 0.5194402456
Epoch:   800  |  train loss: 0.4943478346
Epoch:   900  |  train loss: 0.4730697691
Epoch:  1000  |  train loss: 0.4551099300
Epoch:  1100  |  train loss: 0.4390875161
Epoch:  1200  |  train loss: 0.4251717329
Epoch:  1300  |  train loss: 0.4128175020
Epoch:  1400  |  train loss: 0.4016290605
Epoch:  1500  |  train loss: 0.3917890012
Epoch:  1600  |  train loss: 0.3829361737
Epoch:  1700  |  train loss: 0.3748139381
Epoch:  1800  |  train loss: 0.3673732519
Epoch:  1900  |  train loss: 0.3607473195
Epoch:  2000  |  train loss: 0.3548808575
Processing class: 39
Using PCA transformation
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.9334206223
Epoch:   200  |  train loss: 0.8003467798
Epoch:   300  |  train loss: 0.7067422509
Epoch:   400  |  train loss: 0.6394463062
Epoch:   500  |  train loss: 0.5898169875
Epoch:   600  |  train loss: 0.5506541491
Epoch:   700  |  train loss: 0.5190729678
Epoch:   800  |  train loss: 0.4932462573
Epoch:   900  |  train loss: 0.4712499797
Epoch:  1000  |  train loss: 0.4530855656
Epoch:  1100  |  train loss: 0.4367996275
Epoch:  1200  |  train loss: 0.4226405561
Epoch:  1300  |  train loss: 0.4098744392
Epoch:  1400  |  train loss: 0.3988585651
Epoch:  1500  |  train loss: 0.3884811640
Epoch:  1600  |  train loss: 0.3796079218
Epoch:  1700  |  train loss: 0.3711958230
Epoch:  1800  |  train loss: 0.3636257470
Epoch:  1900  |  train loss: 0.3568286300
Epoch:  2000  |  train loss: 0.3507124662
Processing class: 40
Using PCA transformation
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.9317057729
Epoch:   200  |  train loss: 0.7989924669
Epoch:   300  |  train loss: 0.7083502173
Epoch:   400  |  train loss: 0.6434077978
Epoch:   500  |  train loss: 0.5945656061
Epoch:   600  |  train loss: 0.5563017964
Epoch:   700  |  train loss: 0.5257909417
Epoch:   800  |  train loss: 0.5000562251
Epoch:   900  |  train loss: 0.4784311950
Epoch:  1000  |  train loss: 0.4597430468
Epoch:  1100  |  train loss: 0.4428938866
Epoch:  1200  |  train loss: 0.4281111419
Epoch:  1300  |  train loss: 0.4158672392
Epoch:  1400  |  train loss: 0.4041585684
Epoch:  1500  |  train loss: 0.3939350367
Epoch:  1600  |  train loss: 0.3841944158
Epoch:  1700  |  train loss: 0.3754451454
Epoch:  1800  |  train loss: 0.3672547519
Epoch:  1900  |  train loss: 0.3599241197
Epoch:  2000  |  train loss: 0.3531509936
Processing class: 41
Using PCA transformation
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.9281389356
Epoch:   200  |  train loss: 0.8000016809
Epoch:   300  |  train loss: 0.7094054222
Epoch:   400  |  train loss: 0.6449256063
Epoch:   500  |  train loss: 0.5972097635
Epoch:   600  |  train loss: 0.5588725448
Epoch:   700  |  train loss: 0.5278204918
Epoch:   800  |  train loss: 0.5010860324
Epoch:   900  |  train loss: 0.4792522252
Epoch:  1000  |  train loss: 0.4601723135
Epoch:  1100  |  train loss: 0.4436786175
Epoch:  1200  |  train loss: 0.4292723894
Epoch:  1300  |  train loss: 0.4167145312
Epoch:  1400  |  train loss: 0.4053807497
Epoch:  1500  |  train loss: 0.3952498734
Epoch:  1600  |  train loss: 0.3862422645
Epoch:  1700  |  train loss: 0.3777476847
Epoch:  1800  |  train loss: 0.3698193431
Epoch:  1900  |  train loss: 0.3633131862
Epoch:  2000  |  train loss: 0.3576150298
Processing class: 42
Using PCA transformation
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.9303741932
Epoch:   200  |  train loss: 0.7970140219
Epoch:   300  |  train loss: 0.7064262867
Epoch:   400  |  train loss: 0.6409650922
Epoch:   500  |  train loss: 0.5916135311
Epoch:   600  |  train loss: 0.5532172084
Epoch:   700  |  train loss: 0.5222064734
Epoch:   800  |  train loss: 0.4969617307
Epoch:   900  |  train loss: 0.4753398895
Epoch:  1000  |  train loss: 0.4568360865
Epoch:  1100  |  train loss: 0.4407113671
Epoch:  1200  |  train loss: 0.4265097916
Epoch:  1300  |  train loss: 0.4141077042
Epoch:  1400  |  train loss: 0.4027157068
Epoch:  1500  |  train loss: 0.3922386110
Epoch:  1600  |  train loss: 0.3832362354
Epoch:  1700  |  train loss: 0.3747940421
Epoch:  1800  |  train loss: 0.3678015053
Epoch:  1900  |  train loss: 0.3606974602
Epoch:  2000  |  train loss: 0.3544843256
Processing class: 43
Using PCA transformation
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.9277106881
Epoch:   200  |  train loss: 0.7883004904
Epoch:   300  |  train loss: 0.6928994656
Epoch:   400  |  train loss: 0.6244299769
Epoch:   500  |  train loss: 0.5727918148
Epoch:   600  |  train loss: 0.5332829595
Epoch:   700  |  train loss: 0.5015804708
Epoch:   800  |  train loss: 0.4759944320
Epoch:   900  |  train loss: 0.4541269183
Epoch:  1000  |  train loss: 0.4356065631
Epoch:  1100  |  train loss: 0.4195376456
Epoch:  1200  |  train loss: 0.4054709196
Epoch:  1300  |  train loss: 0.3927378535
Epoch:  1400  |  train loss: 0.3806698859
Epoch:  1500  |  train loss: 0.3704462349
Epoch:  1600  |  train loss: 0.3612200737
Epoch:  1700  |  train loss: 0.3532942235
Epoch:  1800  |  train loss: 0.3458334327
Epoch:  1900  |  train loss: 0.3396037877
Epoch:  2000  |  train loss: 0.3333186209
Processing class: 44
Using PCA transformation
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.9275787354
Epoch:   200  |  train loss: 0.7912739635
Epoch:   300  |  train loss: 0.6941815972
Epoch:   400  |  train loss: 0.6260380268
Epoch:   500  |  train loss: 0.5763860226
Epoch:   600  |  train loss: 0.5382453799
Epoch:   700  |  train loss: 0.5075917006
Epoch:   800  |  train loss: 0.4828058660
Epoch:   900  |  train loss: 0.4616708934
Epoch:  1000  |  train loss: 0.4434884012
Epoch:  1100  |  train loss: 0.4274875104
Epoch:  1200  |  train loss: 0.4137055039
Epoch:  1300  |  train loss: 0.4009239852
Epoch:  1400  |  train loss: 0.3900889158
Epoch:  1500  |  train loss: 0.3797781885
Epoch:  1600  |  train loss: 0.3709103644
Epoch:  1700  |  train loss: 0.3623946488
Epoch:  1800  |  train loss: 0.3547475815
Epoch:  1900  |  train loss: 0.3480244696
Epoch:  2000  |  train loss: 0.3416199684
Processing class: 45
Using PCA transformation
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.9329125166
Epoch:   200  |  train loss: 0.8036574721
Epoch:   300  |  train loss: 0.7110515118
Epoch:   400  |  train loss: 0.6452160716
Epoch:   500  |  train loss: 0.5956450224
Epoch:   600  |  train loss: 0.5564861536
Epoch:   700  |  train loss: 0.5247456312
Epoch:   800  |  train loss: 0.4986124396
Epoch:   900  |  train loss: 0.4766416252
Epoch:  1000  |  train loss: 0.4579639792
Epoch:  1100  |  train loss: 0.4419586957
Epoch:  1200  |  train loss: 0.4279639959
Epoch:  1300  |  train loss: 0.4152853251
Epoch:  1400  |  train loss: 0.4039753079
Epoch:  1500  |  train loss: 0.3940168381
Epoch:  1600  |  train loss: 0.3852641284
Epoch:  1700  |  train loss: 0.3771957278
Epoch:  1800  |  train loss: 0.3697135270
Epoch:  1900  |  train loss: 0.3629801393
Epoch:  2000  |  train loss: 0.3564532340
Processing class: 46
Using PCA transformation
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.9301242113
Epoch:   200  |  train loss: 0.8019642949
Epoch:   300  |  train loss: 0.7075317860
Epoch:   400  |  train loss: 0.6413827777
Epoch:   500  |  train loss: 0.5921405673
Epoch:   600  |  train loss: 0.5538291574
Epoch:   700  |  train loss: 0.5224165320
Epoch:   800  |  train loss: 0.4970151722
Epoch:   900  |  train loss: 0.4750956714
Epoch:  1000  |  train loss: 0.4556151748
Epoch:  1100  |  train loss: 0.4390576780
Epoch:  1200  |  train loss: 0.4241274297
Epoch:  1300  |  train loss: 0.4109374344
Epoch:  1400  |  train loss: 0.3993318141
Epoch:  1500  |  train loss: 0.3886936784
Epoch:  1600  |  train loss: 0.3798853338
Epoch:  1700  |  train loss: 0.3707560301
Epoch:  1800  |  train loss: 0.3633956492
Epoch:  1900  |  train loss: 0.3567170918
Epoch:  2000  |  train loss: 0.3500035286
Processing class: 47
Using PCA transformation
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.9263907790
Epoch:   200  |  train loss: 0.7808326840
Epoch:   300  |  train loss: 0.6844085336
Epoch:   400  |  train loss: 0.6154711008
Epoch:   500  |  train loss: 0.5651697755
Epoch:   600  |  train loss: 0.5262572169
Epoch:   700  |  train loss: 0.4946625650
Epoch:   800  |  train loss: 0.4693513155
Epoch:   900  |  train loss: 0.4476294577
Epoch:  1000  |  train loss: 0.4289792180
Epoch:  1100  |  train loss: 0.4135502934
Epoch:  1200  |  train loss: 0.3994487822
Epoch:  1300  |  train loss: 0.3871227741
Epoch:  1400  |  train loss: 0.3763784170
Epoch:  1500  |  train loss: 0.3666134715
Epoch:  1600  |  train loss: 0.3577052057
Epoch:  1700  |  train loss: 0.3500327229
Epoch:  1800  |  train loss: 0.3427172661
Epoch:  1900  |  train loss: 0.3357966006
Epoch:  2000  |  train loss: 0.3299459934
Processing class: 48
Using PCA transformation
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.9278949142
Epoch:   200  |  train loss: 0.7960452437
Epoch:   300  |  train loss: 0.7015926957
Epoch:   400  |  train loss: 0.6355122447
Epoch:   500  |  train loss: 0.5862129092
Epoch:   600  |  train loss: 0.5479116440
Epoch:   700  |  train loss: 0.5172090769
Epoch:   800  |  train loss: 0.4913960755
Epoch:   900  |  train loss: 0.4692516267
Epoch:  1000  |  train loss: 0.4506616473
Epoch:  1100  |  train loss: 0.4343314111
Epoch:  1200  |  train loss: 0.4196291447
Epoch:  1300  |  train loss: 0.4074474514
Epoch:  1400  |  train loss: 0.3953213513
Epoch:  1500  |  train loss: 0.3854993403
Epoch:  1600  |  train loss: 0.3760764241
Epoch:  1700  |  train loss: 0.3675824881
Epoch:  1800  |  train loss: 0.3602512836
Epoch:  1900  |  train loss: 0.3531900048
Epoch:  2000  |  train loss: 0.3464948773
Processing class: 49
Using PCA transformation
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.9330768466
Epoch:   200  |  train loss: 0.7979615927
Epoch:   300  |  train loss: 0.7029028177
Epoch:   400  |  train loss: 0.6355664968
Epoch:   500  |  train loss: 0.5856375217
Epoch:   600  |  train loss: 0.5467418790
Epoch:   700  |  train loss: 0.5162051678
Epoch:   800  |  train loss: 0.4909877658
Epoch:   900  |  train loss: 0.4699408054
Epoch:  1000  |  train loss: 0.4518907070
Epoch:  1100  |  train loss: 0.4350471139
Epoch:  1200  |  train loss: 0.4214097559
Epoch:  1300  |  train loss: 0.4090766490
Epoch:  1400  |  train loss: 0.3974863887
Epoch:  1500  |  train loss: 0.3880132556
Epoch:  1600  |  train loss: 0.3793348312
Epoch:  1700  |  train loss: 0.3710649312
Epoch:  1800  |  train loss: 0.3633702695
Epoch:  1900  |  train loss: 0.3571091890
Epoch:  2000  |  train loss: 0.3508004606
Clasifying using reconstruction function cost
2024-03-18 05:24:34,311 [trainer.py] => CNN: {'total': 83.44, '00-09': 87.7, '10-19': 79.6, '20-29': 84.9, '30-39': 81.0, '40-49': 84.0, 'old': 0, 'new': 83.44}
2024-03-18 05:24:34,315 [trainer.py] => No NME accuracy
2024-03-18 05:24:34,315 [trainer.py] => FeCAM: {'total': 80.88, '00-09': 86.6, '10-19': 76.7, '20-29': 81.6, '30-39': 78.8, '40-49': 80.7, 'old': 0, 'new': 80.88}
2024-03-18 05:24:34,315 [trainer.py] => CNN top1 curve: [83.44]
2024-03-18 05:24:34,315 [trainer.py] => CNN top5 curve: [96.5]
2024-03-18 05:24:34,315 [trainer.py] => FeCAM top1 curve: [80.88]
2024-03-18 05:24:34,315 [trainer.py] => FeCAM top5 curve: [94.06]

2024-03-18 05:24:34,335 [fecam.py] => Learning on 50-60
/home/z1165703/miniconda3/lib/python3.11/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
/home/z1165703/miniconda3/lib/python3.11/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
/home/z1165703/miniconda3/lib/python3.11/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
Processing class: 50
Using PCA transformation
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.9317044616
Epoch:   200  |  train loss: 0.7994044065
Epoch:   300  |  train loss: 0.7075924158
Epoch:   400  |  train loss: 0.6414393187
Epoch:   500  |  train loss: 0.5916118383
Epoch:   600  |  train loss: 0.5527467847
Epoch:   700  |  train loss: 0.5217142582
Epoch:   800  |  train loss: 0.4966324627
Epoch:   900  |  train loss: 0.4754548967
Epoch:  1000  |  train loss: 0.4567563832
Epoch:  1100  |  train loss: 0.4409044206
Epoch:  1200  |  train loss: 0.4269245625
Epoch:  1300  |  train loss: 0.4143727005
Epoch:  1400  |  train loss: 0.4027484834
Epoch:  1500  |  train loss: 0.3927636445
Epoch:  1600  |  train loss: 0.3839859247
Epoch:  1700  |  train loss: 0.3757368028
Epoch:  1800  |  train loss: 0.3683310330
Epoch:  1900  |  train loss: 0.3613514245
Epoch:  2000  |  train loss: 0.3553140402
Processing class: 51
Using PCA transformation
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.9283422709
Epoch:   200  |  train loss: 0.7953440785
Epoch:   300  |  train loss: 0.7031818271
Epoch:   400  |  train loss: 0.6368218184
Epoch:   500  |  train loss: 0.5879342914
Epoch:   600  |  train loss: 0.5501816988
Epoch:   700  |  train loss: 0.5197048426
Epoch:   800  |  train loss: 0.4944704354
Epoch:   900  |  train loss: 0.4737600982
Epoch:  1000  |  train loss: 0.4560790420
Epoch:  1100  |  train loss: 0.4401472211
Epoch:  1200  |  train loss: 0.4265347004
Epoch:  1300  |  train loss: 0.4140465021
Epoch:  1400  |  train loss: 0.4035673976
Epoch:  1500  |  train loss: 0.3933316231
Epoch:  1600  |  train loss: 0.3849454939
Epoch:  1700  |  train loss: 0.3769427776
Epoch:  1800  |  train loss: 0.3698019981
Epoch:  1900  |  train loss: 0.3633000791
Epoch:  2000  |  train loss: 0.3573248744
Processing class: 52
Using PCA transformation
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.9403683662
Epoch:   200  |  train loss: 0.8166856408
Epoch:   300  |  train loss: 0.7216562510
Epoch:   400  |  train loss: 0.6540254951
Epoch:   500  |  train loss: 0.6051138163
Epoch:   600  |  train loss: 0.5676722884
Epoch:   700  |  train loss: 0.5380054593
Epoch:   800  |  train loss: 0.5136253417
Epoch:   900  |  train loss: 0.4930261731
Epoch:  1000  |  train loss: 0.4756480694
Epoch:  1100  |  train loss: 0.4602330685
Epoch:  1200  |  train loss: 0.4472748160
Epoch:  1300  |  train loss: 0.4352906704
Epoch:  1400  |  train loss: 0.4247445107
Epoch:  1500  |  train loss: 0.4150984287
Epoch:  1600  |  train loss: 0.4067465782
Epoch:  1700  |  train loss: 0.3990048826
Epoch:  1800  |  train loss: 0.3918722630
Epoch:  1900  |  train loss: 0.3855967283
Epoch:  2000  |  train loss: 0.3798745334
Processing class: 53
Using PCA transformation
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.9266339421
Epoch:   200  |  train loss: 0.7895384908
Epoch:   300  |  train loss: 0.6923556209
Epoch:   400  |  train loss: 0.6219696999
Epoch:   500  |  train loss: 0.5698681474
Epoch:   600  |  train loss: 0.5294675827
Epoch:   700  |  train loss: 0.4973396838
Epoch:   800  |  train loss: 0.4708841145
Epoch:   900  |  train loss: 0.4487119913
Epoch:  1000  |  train loss: 0.4302130401
Epoch:  1100  |  train loss: 0.4136102319
Epoch:  1200  |  train loss: 0.3990230560
Epoch:  1300  |  train loss: 0.3869216263
Epoch:  1400  |  train loss: 0.3756898046
Epoch:  1500  |  train loss: 0.3658993006
Epoch:  1600  |  train loss: 0.3570703983
Epoch:  1700  |  train loss: 0.3491191685
Epoch:  1800  |  train loss: 0.3419989347
Epoch:  1900  |  train loss: 0.3356314838
Epoch:  2000  |  train loss: 0.3298305571
Processing class: 54
Using PCA transformation
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.9292109251
Epoch:   200  |  train loss: 0.7849879980
Epoch:   300  |  train loss: 0.6873932719
Epoch:   400  |  train loss: 0.6183935046
Epoch:   500  |  train loss: 0.5684146762
Epoch:   600  |  train loss: 0.5292471647
Epoch:   700  |  train loss: 0.4990011036
Epoch:   800  |  train loss: 0.4736659408
Epoch:   900  |  train loss: 0.4525675893
Epoch:  1000  |  train loss: 0.4339371622
Epoch:  1100  |  train loss: 0.4179257095
Epoch:  1200  |  train loss: 0.4039785981
Epoch:  1300  |  train loss: 0.3917384684
Epoch:  1400  |  train loss: 0.3809482217
Epoch:  1500  |  train loss: 0.3712717175
Epoch:  1600  |  train loss: 0.3621641815
Epoch:  1700  |  train loss: 0.3542516410
Epoch:  1800  |  train loss: 0.3474656820
Epoch:  1900  |  train loss: 0.3410882533
Epoch:  2000  |  train loss: 0.3345511317
Processing class: 55
Using PCA transformation
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.9327018976
Epoch:   200  |  train loss: 0.8036272168
Epoch:   300  |  train loss: 0.7133807540
Epoch:   400  |  train loss: 0.6498352766
Epoch:   500  |  train loss: 0.6025161982
Epoch:   600  |  train loss: 0.5650871754
Epoch:   700  |  train loss: 0.5350632429
Epoch:   800  |  train loss: 0.5097818971
Epoch:   900  |  train loss: 0.4883925736
Epoch:  1000  |  train loss: 0.4704630375
Epoch:  1100  |  train loss: 0.4543196380
Epoch:  1200  |  train loss: 0.4401840627
Epoch:  1300  |  train loss: 0.4277968347
Epoch:  1400  |  train loss: 0.4164548278
Epoch:  1500  |  train loss: 0.4064787269
Epoch:  1600  |  train loss: 0.3970387638
Epoch:  1700  |  train loss: 0.3889553666
Epoch:  1800  |  train loss: 0.3813310623
Epoch:  1900  |  train loss: 0.3743927658
Epoch:  2000  |  train loss: 0.3682158470
Processing class: 56
Using PCA transformation
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.9287572861
Epoch:   200  |  train loss: 0.7992796779
Epoch:   300  |  train loss: 0.7050948262
Epoch:   400  |  train loss: 0.6376354575
Epoch:   500  |  train loss: 0.5881072760
Epoch:   600  |  train loss: 0.5492326975
Epoch:   700  |  train loss: 0.5182489514
Epoch:   800  |  train loss: 0.4933048546
Epoch:   900  |  train loss: 0.4713722050
Epoch:  1000  |  train loss: 0.4527072430
Epoch:  1100  |  train loss: 0.4373011649
Epoch:  1200  |  train loss: 0.4225530684
Epoch:  1300  |  train loss: 0.4105179787
Epoch:  1400  |  train loss: 0.3996717811
Epoch:  1500  |  train loss: 0.3890670180
Epoch:  1600  |  train loss: 0.3801313519
Epoch:  1700  |  train loss: 0.3722263157
Epoch:  1800  |  train loss: 0.3644340873
Epoch:  1900  |  train loss: 0.3575432479
Epoch:  2000  |  train loss: 0.3512538373
Processing class: 57
Using PCA transformation
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.9300221682
Epoch:   200  |  train loss: 0.7979238629
Epoch:   300  |  train loss: 0.7038238525
Epoch:   400  |  train loss: 0.6375699520
Epoch:   500  |  train loss: 0.5882923961
Epoch:   600  |  train loss: 0.5496523380
Epoch:   700  |  train loss: 0.5183628917
Epoch:   800  |  train loss: 0.4921837687
Epoch:   900  |  train loss: 0.4700745761
Epoch:  1000  |  train loss: 0.4511258185
Epoch:  1100  |  train loss: 0.4345706701
Epoch:  1200  |  train loss: 0.4202781677
Epoch:  1300  |  train loss: 0.4069509864
Epoch:  1400  |  train loss: 0.3956272662
Epoch:  1500  |  train loss: 0.3853509188
Epoch:  1600  |  train loss: 0.3760010660
Epoch:  1700  |  train loss: 0.3672069728
Epoch:  1800  |  train loss: 0.3592854202
Epoch:  1900  |  train loss: 0.3521138489
Epoch:  2000  |  train loss: 0.3452725291
Processing class: 58
Using PCA transformation
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.9309725642
Epoch:   200  |  train loss: 0.7932966590
Epoch:   300  |  train loss: 0.6967023849
Epoch:   400  |  train loss: 0.6295943499
Epoch:   500  |  train loss: 0.5803660393
Epoch:   600  |  train loss: 0.5432003140
Epoch:   700  |  train loss: 0.5129962444
Epoch:   800  |  train loss: 0.4880766571
Epoch:   900  |  train loss: 0.4670588613
Epoch:  1000  |  train loss: 0.4489634633
Epoch:  1100  |  train loss: 0.4331703007
Epoch:  1200  |  train loss: 0.4195959926
Epoch:  1300  |  train loss: 0.4073055983
Epoch:  1400  |  train loss: 0.3965239406
Epoch:  1500  |  train loss: 0.3871606767
Epoch:  1600  |  train loss: 0.3776975155
Epoch:  1700  |  train loss: 0.3704157829
Epoch:  1800  |  train loss: 0.3631600201
Epoch:  1900  |  train loss: 0.3566878915
Epoch:  2000  |  train loss: 0.3507419705
Processing class: 59
Using PCA transformation
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.9329271674
Epoch:   200  |  train loss: 0.8026324511
Epoch:   300  |  train loss: 0.7119454741
Epoch:   400  |  train loss: 0.6474137783
Epoch:   500  |  train loss: 0.5988944292
Epoch:   600  |  train loss: 0.5610631824
Epoch:   700  |  train loss: 0.5303185701
Epoch:   800  |  train loss: 0.5047490060
Epoch:   900  |  train loss: 0.4829683721
Epoch:  1000  |  train loss: 0.4634788156
Epoch:  1100  |  train loss: 0.4472651899
Epoch:  1200  |  train loss: 0.4331757545
Epoch:  1300  |  train loss: 0.4199483514
Epoch:  1400  |  train loss: 0.4079025924
Epoch:  1500  |  train loss: 0.3978891432
Epoch:  1600  |  train loss: 0.3884365678
Epoch:  1700  |  train loss: 0.3797919214
Epoch:  1800  |  train loss: 0.3717325091
Epoch:  1900  |  train loss: 0.3646029532
Epoch:  2000  |  train loss: 0.3581841111
Clasifying using reconstruction function cost
2024-03-18 05:38:44,972 [trainer.py] => CNN: {'total': 71.75, '00-09': 82.2, '10-19': 72.8, '20-29': 78.5, '30-39': 74.9, '40-49': 68.4, '50-59': 53.7, 'old': 75.36, 'new': 53.7}
2024-03-18 05:38:44,973 [trainer.py] => No NME accuracy
2024-03-18 05:38:44,973 [trainer.py] => FeCAM: {'total': 67.97, '00-09': 82.0, '10-19': 67.0, '20-29': 74.1, '30-39': 70.1, '40-49': 69.7, '50-59': 44.9, 'old': 72.58, 'new': 44.9}
2024-03-18 05:38:44,974 [trainer.py] => CNN top1 curve: [83.44, 71.75]
2024-03-18 05:38:44,974 [trainer.py] => CNN top5 curve: [96.5, 89.6]
2024-03-18 05:38:44,974 [trainer.py] => FeCAM top1 curve: [80.88, 67.97]
2024-03-18 05:38:44,974 [trainer.py] => FeCAM top5 curve: [94.06, 88.38]

2024-03-18 05:38:44,986 [fecam.py] => Learning on 60-70
/home/z1165703/miniconda3/lib/python3.11/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
/home/z1165703/miniconda3/lib/python3.11/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
/home/z1165703/miniconda3/lib/python3.11/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
Processing class: 60
Using PCA transformation
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.9284762025
Epoch:   200  |  train loss: 0.7947182536
Epoch:   300  |  train loss: 0.7010784030
Epoch:   400  |  train loss: 0.6348996878
Epoch:   500  |  train loss: 0.5863468170
Epoch:   600  |  train loss: 0.5491001248
Epoch:   700  |  train loss: 0.5188708544
Epoch:   800  |  train loss: 0.4941062331
Epoch:   900  |  train loss: 0.4730513752
Epoch:  1000  |  train loss: 0.4549587488
Epoch:  1100  |  train loss: 0.4391541541
Epoch:  1200  |  train loss: 0.4248581052
Epoch:  1300  |  train loss: 0.4127035499
Epoch:  1400  |  train loss: 0.4014378786
Epoch:  1500  |  train loss: 0.3914753735
Epoch:  1600  |  train loss: 0.3825864792
Epoch:  1700  |  train loss: 0.3741950810
Epoch:  1800  |  train loss: 0.3670503378
Epoch:  1900  |  train loss: 0.3601347983
Epoch:  2000  |  train loss: 0.3543909431
Processing class: 61
Using PCA transformation
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.9225328207
Epoch:   200  |  train loss: 0.7862098455
Epoch:   300  |  train loss: 0.6916740298
Epoch:   400  |  train loss: 0.6245482564
Epoch:   500  |  train loss: 0.5742841482
Epoch:   600  |  train loss: 0.5352029443
Epoch:   700  |  train loss: 0.5032322288
Epoch:   800  |  train loss: 0.4771489859
Epoch:   900  |  train loss: 0.4548776448
Epoch:  1000  |  train loss: 0.4362051427
Epoch:  1100  |  train loss: 0.4199282885
Epoch:  1200  |  train loss: 0.4059340477
Epoch:  1300  |  train loss: 0.3932369471
Epoch:  1400  |  train loss: 0.3822108686
Epoch:  1500  |  train loss: 0.3717378676
Epoch:  1600  |  train loss: 0.3629573464
Epoch:  1700  |  train loss: 0.3544280589
Epoch:  1800  |  train loss: 0.3469116807
Epoch:  1900  |  train loss: 0.3399327934
Epoch:  2000  |  train loss: 0.3334674537
Processing class: 62
Using PCA transformation
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.9265131712
Epoch:   200  |  train loss: 0.7886972070
Epoch:   300  |  train loss: 0.6943503261
Epoch:   400  |  train loss: 0.6282982588
Epoch:   500  |  train loss: 0.5793787241
Epoch:   600  |  train loss: 0.5419022679
Epoch:   700  |  train loss: 0.5106813490
Epoch:   800  |  train loss: 0.4852511168
Epoch:   900  |  train loss: 0.4637793720
Epoch:  1000  |  train loss: 0.4457459688
Epoch:  1100  |  train loss: 0.4300074995
Epoch:  1200  |  train loss: 0.4157756746
Epoch:  1300  |  train loss: 0.4040208936
Epoch:  1400  |  train loss: 0.3930212140
Epoch:  1500  |  train loss: 0.3829378128
Epoch:  1600  |  train loss: 0.3743609071
Epoch:  1700  |  train loss: 0.3669929743
Epoch:  1800  |  train loss: 0.3602079451
Epoch:  1900  |  train loss: 0.3529818714
Epoch:  2000  |  train loss: 0.3474403918
Processing class: 63
Using PCA transformation
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.9267977595
Epoch:   200  |  train loss: 0.7951606631
Epoch:   300  |  train loss: 0.6993254900
Epoch:   400  |  train loss: 0.6321557999
Epoch:   500  |  train loss: 0.5821423054
Epoch:   600  |  train loss: 0.5432898045
Epoch:   700  |  train loss: 0.5120042622
Epoch:   800  |  train loss: 0.4864466131
Epoch:   900  |  train loss: 0.4643639386
Epoch:  1000  |  train loss: 0.4456052899
Epoch:  1100  |  train loss: 0.4301423073
Epoch:  1200  |  train loss: 0.4152539730
Epoch:  1300  |  train loss: 0.4027960420
Epoch:  1400  |  train loss: 0.3918518186
Epoch:  1500  |  train loss: 0.3815127254
Epoch:  1600  |  train loss: 0.3721872628
Epoch:  1700  |  train loss: 0.3643044233
Epoch:  1800  |  train loss: 0.3563493252
Epoch:  1900  |  train loss: 0.3496606410
Epoch:  2000  |  train loss: 0.3427440703
Processing class: 64
Using PCA transformation
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.9328199148
Epoch:   200  |  train loss: 0.8015844941
Epoch:   300  |  train loss: 0.7111110330
Epoch:   400  |  train loss: 0.6465779066
Epoch:   500  |  train loss: 0.5992290258
Epoch:   600  |  train loss: 0.5620495796
Epoch:   700  |  train loss: 0.5320237875
Epoch:   800  |  train loss: 0.5071536183
Epoch:   900  |  train loss: 0.4864481449
Epoch:  1000  |  train loss: 0.4681375980
Epoch:  1100  |  train loss: 0.4524087131
Epoch:  1200  |  train loss: 0.4389751673
Epoch:  1300  |  train loss: 0.4266212463
Epoch:  1400  |  train loss: 0.4165352821
Epoch:  1500  |  train loss: 0.4058565319
Epoch:  1600  |  train loss: 0.3968431413
Epoch:  1700  |  train loss: 0.3890978575
Epoch:  1800  |  train loss: 0.3817550540
Epoch:  1900  |  train loss: 0.3747751892
Epoch:  2000  |  train loss: 0.3689736128
Processing class: 65
Using PCA transformation
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.9347955346
Epoch:   200  |  train loss: 0.8177238584
Epoch:   300  |  train loss: 0.7325642109
Epoch:   400  |  train loss: 0.6684927464
Epoch:   500  |  train loss: 0.6207894921
Epoch:   600  |  train loss: 0.5831465721
Epoch:   700  |  train loss: 0.5529281139
Epoch:   800  |  train loss: 0.5269973636
Epoch:   900  |  train loss: 0.5054670691
Epoch:  1000  |  train loss: 0.4872517943
Epoch:  1100  |  train loss: 0.4713559270
Epoch:  1200  |  train loss: 0.4566718400
Epoch:  1300  |  train loss: 0.4445204914
Epoch:  1400  |  train loss: 0.4331682384
Epoch:  1500  |  train loss: 0.4227685511
Epoch:  1600  |  train loss: 0.4138935745
Epoch:  1700  |  train loss: 0.4055419207
Epoch:  1800  |  train loss: 0.3977785408
Epoch:  1900  |  train loss: 0.3910465002
Epoch:  2000  |  train loss: 0.3847332895
Processing class: 66
Using PCA transformation
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.9294328451
Epoch:   200  |  train loss: 0.7935783029
Epoch:   300  |  train loss: 0.7005628347
Epoch:   400  |  train loss: 0.6356564164
Epoch:   500  |  train loss: 0.5876928210
Epoch:   600  |  train loss: 0.5510323644
Epoch:   700  |  train loss: 0.5209179878
Epoch:   800  |  train loss: 0.4961708546
Epoch:   900  |  train loss: 0.4749601126
Epoch:  1000  |  train loss: 0.4562429786
Epoch:  1100  |  train loss: 0.4408431649
Epoch:  1200  |  train loss: 0.4274293244
Epoch:  1300  |  train loss: 0.4154848456
Epoch:  1400  |  train loss: 0.4048432767
Epoch:  1500  |  train loss: 0.3951625407
Epoch:  1600  |  train loss: 0.3864525080
Epoch:  1700  |  train loss: 0.3786190689
Epoch:  1800  |  train loss: 0.3717227399
Epoch:  1900  |  train loss: 0.3652015865
Epoch:  2000  |  train loss: 0.3591940701
Processing class: 67
Using PCA transformation
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.9346513033
Epoch:   200  |  train loss: 0.8062080264
Epoch:   300  |  train loss: 0.7162339449
Epoch:   400  |  train loss: 0.6520321131
Epoch:   500  |  train loss: 0.6039300680
Epoch:   600  |  train loss: 0.5662170053
Epoch:   700  |  train loss: 0.5362355709
Epoch:   800  |  train loss: 0.5108561039
Epoch:   900  |  train loss: 0.4897350788
Epoch:  1000  |  train loss: 0.4711630881
Epoch:  1100  |  train loss: 0.4553831577
Epoch:  1200  |  train loss: 0.4408362329
Epoch:  1300  |  train loss: 0.4285482764
Epoch:  1400  |  train loss: 0.4174131274
Epoch:  1500  |  train loss: 0.4068632245
Epoch:  1600  |  train loss: 0.3981548548
Epoch:  1700  |  train loss: 0.3900824487
Epoch:  1800  |  train loss: 0.3820727348
Epoch:  1900  |  train loss: 0.3742315948
Epoch:  2000  |  train loss: 0.3679680347
Processing class: 68
Using PCA transformation
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.9278278947
Epoch:   200  |  train loss: 0.7958430052
Epoch:   300  |  train loss: 0.7064816713
Epoch:   400  |  train loss: 0.6415553451
Epoch:   500  |  train loss: 0.5924885750
Epoch:   600  |  train loss: 0.5534366012
Epoch:   700  |  train loss: 0.5224097133
Epoch:   800  |  train loss: 0.4967699826
Epoch:   900  |  train loss: 0.4746441245
Epoch:  1000  |  train loss: 0.4565511465
Epoch:  1100  |  train loss: 0.4404888928
Epoch:  1200  |  train loss: 0.4254199922
Epoch:  1300  |  train loss: 0.4131947875
Epoch:  1400  |  train loss: 0.4022558928
Epoch:  1500  |  train loss: 0.3923493087
Epoch:  1600  |  train loss: 0.3829047680
Epoch:  1700  |  train loss: 0.3744487107
Epoch:  1800  |  train loss: 0.3671961367
Epoch:  1900  |  train loss: 0.3601616979
Epoch:  2000  |  train loss: 0.3538305283
Processing class: 69
Using PCA transformation
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.9290119171
Epoch:   200  |  train loss: 0.7899894118
Epoch:   300  |  train loss: 0.6975685000
Epoch:   400  |  train loss: 0.6311566353
Epoch:   500  |  train loss: 0.5814359307
Epoch:   600  |  train loss: 0.5433588982
Epoch:   700  |  train loss: 0.5126362801
Epoch:   800  |  train loss: 0.4870367646
Epoch:   900  |  train loss: 0.4655278265
Epoch:  1000  |  train loss: 0.4470452547
Epoch:  1100  |  train loss: 0.4315619886
Epoch:  1200  |  train loss: 0.4175906599
Epoch:  1300  |  train loss: 0.4054376245
Epoch:  1400  |  train loss: 0.3939158440
Epoch:  1500  |  train loss: 0.3844140887
Epoch:  1600  |  train loss: 0.3749757588
Epoch:  1700  |  train loss: 0.3672608137
Epoch:  1800  |  train loss: 0.3597580910
Epoch:  1900  |  train loss: 0.3529008150
Epoch:  2000  |  train loss: 0.3467473209
Clasifying using reconstruction function cost
2024-03-18 05:55:55,275 [trainer.py] => CNN: {'total': 64.66, '00-09': 75.0, '10-19': 70.1, '20-29': 77.1, '30-39': 70.6, '40-49': 64.0, '50-59': 45.8, '60-69': 50.0, 'old': 67.1, 'new': 50.0}
2024-03-18 05:55:55,277 [trainer.py] => No NME accuracy
2024-03-18 05:55:55,277 [trainer.py] => FeCAM: {'total': 62.4, '00-09': 79.8, '10-19': 63.9, '20-29': 73.1, '30-39': 67.8, '40-49': 66.6, '50-59': 40.4, '60-69': 45.2, 'old': 65.27, 'new': 45.2}
2024-03-18 05:55:55,277 [trainer.py] => CNN top1 curve: [83.44, 71.75, 64.66]
2024-03-18 05:55:55,277 [trainer.py] => CNN top5 curve: [96.5, 89.6, 86.54]
2024-03-18 05:55:55,277 [trainer.py] => FeCAM top1 curve: [80.88, 67.97, 62.4]
2024-03-18 05:55:55,277 [trainer.py] => FeCAM top5 curve: [94.06, 88.38, 84.11]

2024-03-18 05:55:55,288 [fecam.py] => Learning on 70-80
/home/z1165703/miniconda3/lib/python3.11/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
/home/z1165703/miniconda3/lib/python3.11/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
/home/z1165703/miniconda3/lib/python3.11/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
Processing class: 70
Using PCA transformation
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.9329431295
Epoch:   200  |  train loss: 0.8045981526
Epoch:   300  |  train loss: 0.7074683428
Epoch:   400  |  train loss: 0.6418953657
Epoch:   500  |  train loss: 0.5936908960
Epoch:   600  |  train loss: 0.5556008816
Epoch:   700  |  train loss: 0.5256685615
Epoch:   800  |  train loss: 0.5008014143
Epoch:   900  |  train loss: 0.4801775575
Epoch:  1000  |  train loss: 0.4623075902
Epoch:  1100  |  train loss: 0.4469808936
Epoch:  1200  |  train loss: 0.4331958413
Epoch:  1300  |  train loss: 0.4213608980
Epoch:  1400  |  train loss: 0.4106827199
Epoch:  1500  |  train loss: 0.4013926625
Epoch:  1600  |  train loss: 0.3923374534
Epoch:  1700  |  train loss: 0.3853855371
Epoch:  1800  |  train loss: 0.3778768778
Epoch:  1900  |  train loss: 0.3716407418
Epoch:  2000  |  train loss: 0.3654642642
Processing class: 71
Using PCA transformation
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.9332918167
Epoch:   200  |  train loss: 0.8065725923
Epoch:   300  |  train loss: 0.7124159694
Epoch:   400  |  train loss: 0.6449148774
Epoch:   500  |  train loss: 0.5954221249
Epoch:   600  |  train loss: 0.5572320342
Epoch:   700  |  train loss: 0.5260637879
Epoch:   800  |  train loss: 0.5007477582
Epoch:   900  |  train loss: 0.4786698878
Epoch:  1000  |  train loss: 0.4601509809
Epoch:  1100  |  train loss: 0.4441181719
Epoch:  1200  |  train loss: 0.4300028980
Epoch:  1300  |  train loss: 0.4169599533
Epoch:  1400  |  train loss: 0.4058395922
Epoch:  1500  |  train loss: 0.3954941928
Epoch:  1600  |  train loss: 0.3862871647
Epoch:  1700  |  train loss: 0.3771794856
Epoch:  1800  |  train loss: 0.3695504844
Epoch:  1900  |  train loss: 0.3625586629
Epoch:  2000  |  train loss: 0.3563365042
Processing class: 72
Using PCA transformation
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.9314360380
Epoch:   200  |  train loss: 0.7970129132
Epoch:   300  |  train loss: 0.7033135891
Epoch:   400  |  train loss: 0.6368197799
Epoch:   500  |  train loss: 0.5877361536
Epoch:   600  |  train loss: 0.5498734593
Epoch:   700  |  train loss: 0.5180890322
Epoch:   800  |  train loss: 0.4921731472
Epoch:   900  |  train loss: 0.4706728160
Epoch:  1000  |  train loss: 0.4518243611
Epoch:  1100  |  train loss: 0.4359165132
Epoch:  1200  |  train loss: 0.4218119502
Epoch:  1300  |  train loss: 0.4091525316
Epoch:  1400  |  train loss: 0.3980817556
Epoch:  1500  |  train loss: 0.3877677560
Epoch:  1600  |  train loss: 0.3787710607
Epoch:  1700  |  train loss: 0.3708399951
Epoch:  1800  |  train loss: 0.3635132551
Epoch:  1900  |  train loss: 0.3566455603
Epoch:  2000  |  train loss: 0.3503293395
Processing class: 73
Using PCA transformation
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.9268628597
Epoch:   200  |  train loss: 0.8004967928
Epoch:   300  |  train loss: 0.7119823337
Epoch:   400  |  train loss: 0.6493745446
Epoch:   500  |  train loss: 0.6020692468
Epoch:   600  |  train loss: 0.5646738768
Epoch:   700  |  train loss: 0.5339698076
Epoch:   800  |  train loss: 0.5097032428
Epoch:   900  |  train loss: 0.4885440111
Epoch:  1000  |  train loss: 0.4704191864
Epoch:  1100  |  train loss: 0.4547564030
Epoch:  1200  |  train loss: 0.4407743812
Epoch:  1300  |  train loss: 0.4287782252
Epoch:  1400  |  train loss: 0.4177173197
Epoch:  1500  |  train loss: 0.4080650091
Epoch:  1600  |  train loss: 0.3989317477
Epoch:  1700  |  train loss: 0.3911359251
Epoch:  1800  |  train loss: 0.3835065484
Epoch:  1900  |  train loss: 0.3767758369
Epoch:  2000  |  train loss: 0.3706909180
Processing class: 74
Using PCA transformation
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.9219911456
Epoch:   200  |  train loss: 0.7839391351
Epoch:   300  |  train loss: 0.6875158191
Epoch:   400  |  train loss: 0.6190063477
Epoch:   500  |  train loss: 0.5687297583
Epoch:   600  |  train loss: 0.5305246711
Epoch:   700  |  train loss: 0.4997363985
Epoch:   800  |  train loss: 0.4741903722
Epoch:   900  |  train loss: 0.4529160023
Epoch:  1000  |  train loss: 0.4348583579
Epoch:  1100  |  train loss: 0.4188833475
Epoch:  1200  |  train loss: 0.4049179971
Epoch:  1300  |  train loss: 0.3929512262
Epoch:  1400  |  train loss: 0.3818284452
Epoch:  1500  |  train loss: 0.3722766399
Epoch:  1600  |  train loss: 0.3637415349
Epoch:  1700  |  train loss: 0.3559028864
Epoch:  1800  |  train loss: 0.3487963498
Epoch:  1900  |  train loss: 0.3420932353
Epoch:  2000  |  train loss: 0.3365706503
Processing class: 75
Using PCA transformation
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.9283449411
Epoch:   200  |  train loss: 0.7923005700
Epoch:   300  |  train loss: 0.6980355263
Epoch:   400  |  train loss: 0.6288952470
Epoch:   500  |  train loss: 0.5784642816
Epoch:   600  |  train loss: 0.5393006325
Epoch:   700  |  train loss: 0.5080613554
Epoch:   800  |  train loss: 0.4819570065
Epoch:   900  |  train loss: 0.4604720652
Epoch:  1000  |  train loss: 0.4422377586
Epoch:  1100  |  train loss: 0.4268853128
Epoch:  1200  |  train loss: 0.4122821152
Epoch:  1300  |  train loss: 0.4001069069
Epoch:  1400  |  train loss: 0.3890750468
Epoch:  1500  |  train loss: 0.3792532384
Epoch:  1600  |  train loss: 0.3700415790
Epoch:  1700  |  train loss: 0.3622996092
Epoch:  1800  |  train loss: 0.3544489086
Epoch:  1900  |  train loss: 0.3475894630
Epoch:  2000  |  train loss: 0.3414606333
Processing class: 76
Using PCA transformation
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.9308138371
Epoch:   200  |  train loss: 0.8069897175
Epoch:   300  |  train loss: 0.7181603432
Epoch:   400  |  train loss: 0.6528387070
Epoch:   500  |  train loss: 0.6041837573
Epoch:   600  |  train loss: 0.5660834312
Epoch:   700  |  train loss: 0.5349784732
Epoch:   800  |  train loss: 0.5087949276
Epoch:   900  |  train loss: 0.4870251238
Epoch:  1000  |  train loss: 0.4688148081
Epoch:  1100  |  train loss: 0.4526762247
Epoch:  1200  |  train loss: 0.4384899139
Epoch:  1300  |  train loss: 0.4262034297
Epoch:  1400  |  train loss: 0.4150922358
Epoch:  1500  |  train loss: 0.4052718580
Epoch:  1600  |  train loss: 0.3962376952
Epoch:  1700  |  train loss: 0.3879965901
Epoch:  1800  |  train loss: 0.3805401146
Epoch:  1900  |  train loss: 0.3738102853
Epoch:  2000  |  train loss: 0.3672824264
Processing class: 77
Using PCA transformation
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.9350591183
Epoch:   200  |  train loss: 0.8108079195
Epoch:   300  |  train loss: 0.7229616404
Epoch:   400  |  train loss: 0.6610047936
Epoch:   500  |  train loss: 0.6143864393
Epoch:   600  |  train loss: 0.5773263335
Epoch:   700  |  train loss: 0.5473011971
Epoch:   800  |  train loss: 0.5221850157
Epoch:   900  |  train loss: 0.5012508214
Epoch:  1000  |  train loss: 0.4824628115
Epoch:  1100  |  train loss: 0.4668592215
Epoch:  1200  |  train loss: 0.4528052211
Epoch:  1300  |  train loss: 0.4405734897
Epoch:  1400  |  train loss: 0.4295311689
Epoch:  1500  |  train loss: 0.4196558535
Epoch:  1600  |  train loss: 0.4110229969
Epoch:  1700  |  train loss: 0.4029451728
Epoch:  1800  |  train loss: 0.3950384140
Epoch:  1900  |  train loss: 0.3885855436
Epoch:  2000  |  train loss: 0.3823530853
Processing class: 78
Using PCA transformation
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.9382339835
Epoch:   200  |  train loss: 0.8080839038
Epoch:   300  |  train loss: 0.7171004057
Epoch:   400  |  train loss: 0.6516817451
Epoch:   500  |  train loss: 0.6031234384
Epoch:   600  |  train loss: 0.5644187808
Epoch:   700  |  train loss: 0.5344107389
Epoch:   800  |  train loss: 0.5098084569
Epoch:   900  |  train loss: 0.4888545334
Epoch:  1000  |  train loss: 0.4709547400
Epoch:  1100  |  train loss: 0.4554986656
Epoch:  1200  |  train loss: 0.4416349113
Epoch:  1300  |  train loss: 0.4297446847
Epoch:  1400  |  train loss: 0.4190174699
Epoch:  1500  |  train loss: 0.4088763475
Epoch:  1600  |  train loss: 0.3999710143
Epoch:  1700  |  train loss: 0.3916939795
Epoch:  1800  |  train loss: 0.3842586100
Epoch:  1900  |  train loss: 0.3773890495
Epoch:  2000  |  train loss: 0.3711856127
Processing class: 79
Using PCA transformation
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.9279134631
Epoch:   200  |  train loss: 0.7941437602
Epoch:   300  |  train loss: 0.6994519234
Epoch:   400  |  train loss: 0.6332118034
Epoch:   500  |  train loss: 0.5840600014
Epoch:   600  |  train loss: 0.5457906842
Epoch:   700  |  train loss: 0.5137880683
Epoch:   800  |  train loss: 0.4881986380
Epoch:   900  |  train loss: 0.4672982514
Epoch:  1000  |  train loss: 0.4489920318
Epoch:  1100  |  train loss: 0.4335728824
Epoch:  1200  |  train loss: 0.4201905906
Epoch:  1300  |  train loss: 0.4081039071
Epoch:  1400  |  train loss: 0.3979327202
Epoch:  1500  |  train loss: 0.3882286727
Epoch:  1600  |  train loss: 0.3799535215
Epoch:  1700  |  train loss: 0.3725432992
Epoch:  1800  |  train loss: 0.3652256012
Epoch:  1900  |  train loss: 0.3588959873
Epoch:  2000  |  train loss: 0.3532011092
Clasifying using reconstruction function cost
2024-03-18 06:16:37,659 [trainer.py] => CNN: {'total': 59.18, '00-09': 73.6, '10-19': 68.4, '20-29': 76.9, '30-39': 69.1, '40-49': 60.6, '50-59': 37.2, '60-69': 45.7, '70-79': 41.9, 'old': 61.64, 'new': 41.9}
2024-03-18 06:16:37,661 [trainer.py] => No NME accuracy
2024-03-18 06:16:37,661 [trainer.py] => FeCAM: {'total': 57.01, '00-09': 76.0, '10-19': 62.6, '20-29': 72.2, '30-39': 65.9, '40-49': 61.3, '50-59': 33.4, '60-69': 39.8, '70-79': 44.9, 'old': 58.74, 'new': 44.9}
2024-03-18 06:16:37,661 [trainer.py] => CNN top1 curve: [83.44, 71.75, 64.66, 59.18]
2024-03-18 06:16:37,661 [trainer.py] => CNN top5 curve: [96.5, 89.6, 86.54, 84.09]
2024-03-18 06:16:37,661 [trainer.py] => FeCAM top1 curve: [80.88, 67.97, 62.4, 57.01]
2024-03-18 06:16:37,661 [trainer.py] => FeCAM top5 curve: [94.06, 88.38, 84.11, 80.96]

2024-03-18 06:16:37,673 [fecam.py] => Learning on 80-90
/home/z1165703/miniconda3/lib/python3.11/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
/home/z1165703/miniconda3/lib/python3.11/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
/home/z1165703/miniconda3/lib/python3.11/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
Processing class: 80
Using PCA transformation
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.9309972882
Epoch:   200  |  train loss: 0.7976528049
Epoch:   300  |  train loss: 0.7015907288
Epoch:   400  |  train loss: 0.6343058109
Epoch:   500  |  train loss: 0.5833546042
Epoch:   600  |  train loss: 0.5435917258
Epoch:   700  |  train loss: 0.5110855818
Epoch:   800  |  train loss: 0.4842779279
Epoch:   900  |  train loss: 0.4614048302
Epoch:  1000  |  train loss: 0.4415863037
Epoch:  1100  |  train loss: 0.4245612323
Epoch:  1200  |  train loss: 0.4097091019
Epoch:  1300  |  train loss: 0.3960380912
Epoch:  1400  |  train loss: 0.3837478161
Epoch:  1500  |  train loss: 0.3729642808
Epoch:  1600  |  train loss: 0.3636595368
Epoch:  1700  |  train loss: 0.3551350772
Epoch:  1800  |  train loss: 0.3469101191
Epoch:  1900  |  train loss: 0.3397294044
Epoch:  2000  |  train loss: 0.3334139109
Processing class: 81
Using PCA transformation
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.9310826421
Epoch:   200  |  train loss: 0.7955858350
Epoch:   300  |  train loss: 0.7009401560
Epoch:   400  |  train loss: 0.6333802462
Epoch:   500  |  train loss: 0.5830359101
Epoch:   600  |  train loss: 0.5436013699
Epoch:   700  |  train loss: 0.5116572201
Epoch:   800  |  train loss: 0.4857134759
Epoch:   900  |  train loss: 0.4639173388
Epoch:  1000  |  train loss: 0.4448352754
Epoch:  1100  |  train loss: 0.4288168073
Epoch:  1200  |  train loss: 0.4146060169
Epoch:  1300  |  train loss: 0.4017620981
Epoch:  1400  |  train loss: 0.3906843185
Epoch:  1500  |  train loss: 0.3808870137
Epoch:  1600  |  train loss: 0.3717848420
Epoch:  1700  |  train loss: 0.3637006581
Epoch:  1800  |  train loss: 0.3563543141
Epoch:  1900  |  train loss: 0.3496058404
Epoch:  2000  |  train loss: 0.3434898853
Processing class: 82
Using PCA transformation
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.9364892364
Epoch:   200  |  train loss: 0.8050907254
Epoch:   300  |  train loss: 0.7130783916
Epoch:   400  |  train loss: 0.6446218967
Epoch:   500  |  train loss: 0.5950982332
Epoch:   600  |  train loss: 0.5571642995
Epoch:   700  |  train loss: 0.5272344589
Epoch:   800  |  train loss: 0.5020392001
Epoch:   900  |  train loss: 0.4811837494
Epoch:  1000  |  train loss: 0.4631659567
Epoch:  1100  |  train loss: 0.4472409070
Epoch:  1200  |  train loss: 0.4335542202
Epoch:  1300  |  train loss: 0.4213501751
Epoch:  1400  |  train loss: 0.4100594163
Epoch:  1500  |  train loss: 0.4005529761
Epoch:  1600  |  train loss: 0.3917074382
Epoch:  1700  |  train loss: 0.3839151740
Epoch:  1800  |  train loss: 0.3763359070
Epoch:  1900  |  train loss: 0.3699139476
Epoch:  2000  |  train loss: 0.3637537420
Processing class: 83
Using PCA transformation
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.9327355623
Epoch:   200  |  train loss: 0.7927945375
Epoch:   300  |  train loss: 0.7003286839
Epoch:   400  |  train loss: 0.6349748969
Epoch:   500  |  train loss: 0.5858197331
Epoch:   600  |  train loss: 0.5487711430
Epoch:   700  |  train loss: 0.5173343718
Epoch:   800  |  train loss: 0.4921837866
Epoch:   900  |  train loss: 0.4708037138
Epoch:  1000  |  train loss: 0.4523641586
Epoch:  1100  |  train loss: 0.4367641687
Epoch:  1200  |  train loss: 0.4224967837
Epoch:  1300  |  train loss: 0.4099868715
Epoch:  1400  |  train loss: 0.3989802539
Epoch:  1500  |  train loss: 0.3892245889
Epoch:  1600  |  train loss: 0.3799841046
Epoch:  1700  |  train loss: 0.3719419003
Epoch:  1800  |  train loss: 0.3649496138
Epoch:  1900  |  train loss: 0.3578115463
Epoch:  2000  |  train loss: 0.3514142394
Processing class: 84
Using PCA transformation
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.9328621864
Epoch:   200  |  train loss: 0.7997003794
Epoch:   300  |  train loss: 0.7057901025
Epoch:   400  |  train loss: 0.6387984872
Epoch:   500  |  train loss: 0.5898805857
Epoch:   600  |  train loss: 0.5513233542
Epoch:   700  |  train loss: 0.5200748563
Epoch:   800  |  train loss: 0.4944381475
Epoch:   900  |  train loss: 0.4725397766
Epoch:  1000  |  train loss: 0.4531996727
Epoch:  1100  |  train loss: 0.4373729289
Epoch:  1200  |  train loss: 0.4227006257
Epoch:  1300  |  train loss: 0.4098650336
Epoch:  1400  |  train loss: 0.3986538947
Epoch:  1500  |  train loss: 0.3883035958
Epoch:  1600  |  train loss: 0.3789974391
Epoch:  1700  |  train loss: 0.3704139471
Epoch:  1800  |  train loss: 0.3626779974
Epoch:  1900  |  train loss: 0.3561523199
Epoch:  2000  |  train loss: 0.3494218290
Processing class: 85
Using PCA transformation
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.9339802146
Epoch:   200  |  train loss: 0.8026493311
Epoch:   300  |  train loss: 0.7130735278
Epoch:   400  |  train loss: 0.6484412551
Epoch:   500  |  train loss: 0.5998530984
Epoch:   600  |  train loss: 0.5616915703
Epoch:   700  |  train loss: 0.5312880874
Epoch:   800  |  train loss: 0.5068993568
Epoch:   900  |  train loss: 0.4859382391
Epoch:  1000  |  train loss: 0.4684455454
Epoch:  1100  |  train loss: 0.4532937169
Epoch:  1200  |  train loss: 0.4390292525
Epoch:  1300  |  train loss: 0.4270403504
Epoch:  1400  |  train loss: 0.4164268017
Epoch:  1500  |  train loss: 0.4070834696
Epoch:  1600  |  train loss: 0.3982919395
Epoch:  1700  |  train loss: 0.3905231416
Epoch:  1800  |  train loss: 0.3829167187
Epoch:  1900  |  train loss: 0.3766236842
Epoch:  2000  |  train loss: 0.3704273701
Processing class: 86
Using PCA transformation
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.9292995453
Epoch:   200  |  train loss: 0.7984370232
Epoch:   300  |  train loss: 0.7069584608
Epoch:   400  |  train loss: 0.6398302197
Epoch:   500  |  train loss: 0.5909120440
Epoch:   600  |  train loss: 0.5537583828
Epoch:   700  |  train loss: 0.5238636911
Epoch:   800  |  train loss: 0.4990764499
Epoch:   900  |  train loss: 0.4767540574
Epoch:  1000  |  train loss: 0.4580025554
Epoch:  1100  |  train loss: 0.4419264436
Epoch:  1200  |  train loss: 0.4280535758
Epoch:  1300  |  train loss: 0.4161790907
Epoch:  1400  |  train loss: 0.4049914837
Epoch:  1500  |  train loss: 0.3954181671
Epoch:  1600  |  train loss: 0.3867621958
Epoch:  1700  |  train loss: 0.3789663911
Epoch:  1800  |  train loss: 0.3713754714
Epoch:  1900  |  train loss: 0.3648128808
Epoch:  2000  |  train loss: 0.3589897692
Processing class: 87
Using PCA transformation
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.9311972499
Epoch:   200  |  train loss: 0.7978811383
Epoch:   300  |  train loss: 0.7064278483
Epoch:   400  |  train loss: 0.6388121963
Epoch:   500  |  train loss: 0.5886584163
Epoch:   600  |  train loss: 0.5494955420
Epoch:   700  |  train loss: 0.5178860068
Epoch:   800  |  train loss: 0.4921168923
Epoch:   900  |  train loss: 0.4706887126
Epoch:  1000  |  train loss: 0.4523539603
Epoch:  1100  |  train loss: 0.4360354662
Epoch:  1200  |  train loss: 0.4224648595
Epoch:  1300  |  train loss: 0.4099654555
Epoch:  1400  |  train loss: 0.3986402929
Epoch:  1500  |  train loss: 0.3888627350
Epoch:  1600  |  train loss: 0.3798274636
Epoch:  1700  |  train loss: 0.3717599213
Epoch:  1800  |  train loss: 0.3640411735
Epoch:  1900  |  train loss: 0.3574011207
Epoch:  2000  |  train loss: 0.3507709920
Processing class: 88
Using PCA transformation
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.9275280595
Epoch:   200  |  train loss: 0.7937118888
Epoch:   300  |  train loss: 0.7055080056
Epoch:   400  |  train loss: 0.6414498806
Epoch:   500  |  train loss: 0.5928938866
Epoch:   600  |  train loss: 0.5551821589
Epoch:   700  |  train loss: 0.5250200808
Epoch:   800  |  train loss: 0.5005243480
Epoch:   900  |  train loss: 0.4796463966
Epoch:  1000  |  train loss: 0.4606893361
Epoch:  1100  |  train loss: 0.4448585749
Epoch:  1200  |  train loss: 0.4317732871
Epoch:  1300  |  train loss: 0.4196992517
Epoch:  1400  |  train loss: 0.4093932986
Epoch:  1500  |  train loss: 0.3995145142
Epoch:  1600  |  train loss: 0.3910941601
Epoch:  1700  |  train loss: 0.3833736956
Epoch:  1800  |  train loss: 0.3762712419
Epoch:  1900  |  train loss: 0.3699808478
Epoch:  2000  |  train loss: 0.3641051054
Processing class: 89
Using PCA transformation
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.9272127151
Epoch:   200  |  train loss: 0.7917419314
Epoch:   300  |  train loss: 0.7007608056
Epoch:   400  |  train loss: 0.6349701762
Epoch:   500  |  train loss: 0.5860007524
Epoch:   600  |  train loss: 0.5468158126
Epoch:   700  |  train loss: 0.5151164770
Epoch:   800  |  train loss: 0.4890993237
Epoch:   900  |  train loss: 0.4676248908
Epoch:  1000  |  train loss: 0.4488512099
Epoch:  1100  |  train loss: 0.4325667739
Epoch:  1200  |  train loss: 0.4186585784
Epoch:  1300  |  train loss: 0.4060666025
Epoch:  1400  |  train loss: 0.3952401757
Epoch:  1500  |  train loss: 0.3857041478
Epoch:  1600  |  train loss: 0.3759428382
Epoch:  1700  |  train loss: 0.3677563012
Epoch:  1800  |  train loss: 0.3604062617
Epoch:  1900  |  train loss: 0.3537435651
Epoch:  2000  |  train loss: 0.3472834945
Clasifying using reconstruction function cost
2024-03-18 06:41:19,498 [trainer.py] => CNN: {'total': 54.08, '00-09': 67.9, '10-19': 63.6, '20-29': 73.2, '30-39': 68.8, '40-49': 57.2, '50-59': 32.6, '60-69': 40.9, '70-79': 38.1, '80-89': 44.4, 'old': 55.29, 'new': 44.4}
2024-03-18 06:41:19,500 [trainer.py] => No NME accuracy
2024-03-18 06:41:19,500 [trainer.py] => FeCAM: {'total': 52.43, '00-09': 73.7, '10-19': 59.5, '20-29': 69.9, '30-39': 64.6, '40-49': 58.4, '50-59': 29.9, '60-69': 36.2, '70-79': 40.8, '80-89': 38.9, 'old': 54.12, 'new': 38.9}
2024-03-18 06:41:19,500 [trainer.py] => CNN top1 curve: [83.44, 71.75, 64.66, 59.18, 54.08]
2024-03-18 06:41:19,500 [trainer.py] => CNN top5 curve: [96.5, 89.6, 86.54, 84.09, 81.74]
2024-03-18 06:41:19,500 [trainer.py] => FeCAM top1 curve: [80.88, 67.97, 62.4, 57.01, 52.43]
2024-03-18 06:41:19,500 [trainer.py] => FeCAM top5 curve: [94.06, 88.38, 84.11, 80.96, 78.36]

2024-03-18 06:41:19,512 [fecam.py] => Learning on 90-100
/home/z1165703/miniconda3/lib/python3.11/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
/home/z1165703/miniconda3/lib/python3.11/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
/home/z1165703/miniconda3/lib/python3.11/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
Processing class: 90
Using PCA transformation
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.9259380937
Epoch:   200  |  train loss: 0.7958540559
Epoch:   300  |  train loss: 0.7059409261
Epoch:   400  |  train loss: 0.6419304013
Epoch:   500  |  train loss: 0.5946442485
Epoch:   600  |  train loss: 0.5580316067
Epoch:   700  |  train loss: 0.5276268959
Epoch:   800  |  train loss: 0.5020377815
Epoch:   900  |  train loss: 0.4805037022
Epoch:  1000  |  train loss: 0.4627555549
Epoch:  1100  |  train loss: 0.4457387030
Epoch:  1200  |  train loss: 0.4315076172
Epoch:  1300  |  train loss: 0.4189349532
Epoch:  1400  |  train loss: 0.4077149749
Epoch:  1500  |  train loss: 0.3976842165
Epoch:  1600  |  train loss: 0.3887696266
Epoch:  1700  |  train loss: 0.3803751826
Epoch:  1800  |  train loss: 0.3729609013
Epoch:  1900  |  train loss: 0.3661864102
Epoch:  2000  |  train loss: 0.3596380413
Processing class: 91
Using PCA transformation
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.9325639129
Epoch:   200  |  train loss: 0.7997985840
Epoch:   300  |  train loss: 0.7097186804
Epoch:   400  |  train loss: 0.6453346968
Epoch:   500  |  train loss: 0.5965384364
Epoch:   600  |  train loss: 0.5583402038
Epoch:   700  |  train loss: 0.5264662623
Epoch:   800  |  train loss: 0.5009886444
Epoch:   900  |  train loss: 0.4794622838
Epoch:  1000  |  train loss: 0.4610218823
Epoch:  1100  |  train loss: 0.4453771591
Epoch:  1200  |  train loss: 0.4312913060
Epoch:  1300  |  train loss: 0.4189303935
Epoch:  1400  |  train loss: 0.4080028117
Epoch:  1500  |  train loss: 0.3982128859
Epoch:  1600  |  train loss: 0.3891978860
Epoch:  1700  |  train loss: 0.3811041832
Epoch:  1800  |  train loss: 0.3734757125
Epoch:  1900  |  train loss: 0.3675285578
Epoch:  2000  |  train loss: 0.3608516872
Processing class: 92
Using PCA transformation
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.9291843176
Epoch:   200  |  train loss: 0.8004060984
Epoch:   300  |  train loss: 0.7078852654
Epoch:   400  |  train loss: 0.6437968731
Epoch:   500  |  train loss: 0.5959394813
Epoch:   600  |  train loss: 0.5586432934
Epoch:   700  |  train loss: 0.5283090949
Epoch:   800  |  train loss: 0.5028888643
Epoch:   900  |  train loss: 0.4819489181
Epoch:  1000  |  train loss: 0.4640185356
Epoch:  1100  |  train loss: 0.4483858466
Epoch:  1200  |  train loss: 0.4343675733
Epoch:  1300  |  train loss: 0.4214847982
Epoch:  1400  |  train loss: 0.4100603402
Epoch:  1500  |  train loss: 0.4000713587
Epoch:  1600  |  train loss: 0.3914947033
Epoch:  1700  |  train loss: 0.3836720169
Epoch:  1800  |  train loss: 0.3764408708
Epoch:  1900  |  train loss: 0.3694011986
Epoch:  2000  |  train loss: 0.3631752431
Processing class: 93
Using PCA transformation
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.9335798502
Epoch:   200  |  train loss: 0.7974439740
Epoch:   300  |  train loss: 0.7061004758
Epoch:   400  |  train loss: 0.6414666414
Epoch:   500  |  train loss: 0.5923551679
Epoch:   600  |  train loss: 0.5533964753
Epoch:   700  |  train loss: 0.5229597807
Epoch:   800  |  train loss: 0.4975173175
Epoch:   900  |  train loss: 0.4764710963
Epoch:  1000  |  train loss: 0.4584804833
Epoch:  1100  |  train loss: 0.4432141304
Epoch:  1200  |  train loss: 0.4291751385
Epoch:  1300  |  train loss: 0.4167403817
Epoch:  1400  |  train loss: 0.4056645513
Epoch:  1500  |  train loss: 0.3959639609
Epoch:  1600  |  train loss: 0.3870422065
Epoch:  1700  |  train loss: 0.3792836428
Epoch:  1800  |  train loss: 0.3718476951
Epoch:  1900  |  train loss: 0.3651926100
Epoch:  2000  |  train loss: 0.3592952728
Processing class: 94
Using PCA transformation
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.9260653734
Epoch:   200  |  train loss: 0.7883301139
Epoch:   300  |  train loss: 0.6918578863
Epoch:   400  |  train loss: 0.6236914039
Epoch:   500  |  train loss: 0.5737214327
Epoch:   600  |  train loss: 0.5340949774
Epoch:   700  |  train loss: 0.5028799891
Epoch:   800  |  train loss: 0.4767258108
Epoch:   900  |  train loss: 0.4547501504
Epoch:  1000  |  train loss: 0.4362153888
Epoch:  1100  |  train loss: 0.4200839162
Epoch:  1200  |  train loss: 0.4056593895
Epoch:  1300  |  train loss: 0.3926944733
Epoch:  1400  |  train loss: 0.3818137228
Epoch:  1500  |  train loss: 0.3714154065
Epoch:  1600  |  train loss: 0.3622300267
Epoch:  1700  |  train loss: 0.3540951610
Epoch:  1800  |  train loss: 0.3466286004
Epoch:  1900  |  train loss: 0.3400708556
Epoch:  2000  |  train loss: 0.3340097666
Processing class: 95
Using PCA transformation
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.9305744886
Epoch:   200  |  train loss: 0.7941445231
Epoch:   300  |  train loss: 0.7025591254
Epoch:   400  |  train loss: 0.6350852728
Epoch:   500  |  train loss: 0.5836940289
Epoch:   600  |  train loss: 0.5440343976
Epoch:   700  |  train loss: 0.5119605780
Epoch:   800  |  train loss: 0.4859701455
Epoch:   900  |  train loss: 0.4633437812
Epoch:  1000  |  train loss: 0.4434628546
Epoch:  1100  |  train loss: 0.4267487049
Epoch:  1200  |  train loss: 0.4123984218
Epoch:  1300  |  train loss: 0.3996471643
Epoch:  1400  |  train loss: 0.3883904815
Epoch:  1500  |  train loss: 0.3778667450
Epoch:  1600  |  train loss: 0.3692134917
Epoch:  1700  |  train loss: 0.3603843331
Epoch:  1800  |  train loss: 0.3528067470
Epoch:  1900  |  train loss: 0.3463586569
Epoch:  2000  |  train loss: 0.3397735894
Processing class: 96
Using PCA transformation
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.9311887741
Epoch:   200  |  train loss: 0.8034683704
Epoch:   300  |  train loss: 0.7151827574
Epoch:   400  |  train loss: 0.6510000110
Epoch:   500  |  train loss: 0.6026692748
Epoch:   600  |  train loss: 0.5645957708
Epoch:   700  |  train loss: 0.5335359573
Epoch:   800  |  train loss: 0.5079262137
Epoch:   900  |  train loss: 0.4859860182
Epoch:  1000  |  train loss: 0.4666514993
Epoch:  1100  |  train loss: 0.4504853487
Epoch:  1200  |  train loss: 0.4358188629
Epoch:  1300  |  train loss: 0.4229525924
Epoch:  1400  |  train loss: 0.4106326222
Epoch:  1500  |  train loss: 0.4003544331
Epoch:  1600  |  train loss: 0.3913392365
Epoch:  1700  |  train loss: 0.3826738715
Epoch:  1800  |  train loss: 0.3745425940
Epoch:  1900  |  train loss: 0.3672509789
Epoch:  2000  |  train loss: 0.3613596678
Processing class: 97
Using PCA transformation
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.9304161310
Epoch:   200  |  train loss: 0.8050307393
Epoch:   300  |  train loss: 0.7136654496
Epoch:   400  |  train loss: 0.6490796685
Epoch:   500  |  train loss: 0.6004051208
Epoch:   600  |  train loss: 0.5614251256
Epoch:   700  |  train loss: 0.5302667737
Epoch:   800  |  train loss: 0.5040512502
Epoch:   900  |  train loss: 0.4824867606
Epoch:  1000  |  train loss: 0.4636458158
Epoch:  1100  |  train loss: 0.4474591911
Epoch:  1200  |  train loss: 0.4331637979
Epoch:  1300  |  train loss: 0.4203782380
Epoch:  1400  |  train loss: 0.4092259169
Epoch:  1500  |  train loss: 0.3990803242
Epoch:  1600  |  train loss: 0.3899120212
Epoch:  1700  |  train loss: 0.3817742109
Epoch:  1800  |  train loss: 0.3745949686
Epoch:  1900  |  train loss: 0.3678349197
Epoch:  2000  |  train loss: 0.3616114557
Processing class: 98
Using PCA transformation
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.9325210452
Epoch:   200  |  train loss: 0.7987642646
Epoch:   300  |  train loss: 0.7078254342
Epoch:   400  |  train loss: 0.6423650622
Epoch:   500  |  train loss: 0.5928707600
Epoch:   600  |  train loss: 0.5547391057
Epoch:   700  |  train loss: 0.5234041810
Epoch:   800  |  train loss: 0.4971553862
Epoch:   900  |  train loss: 0.4751178741
Epoch:  1000  |  train loss: 0.4562503874
Epoch:  1100  |  train loss: 0.4396487653
Epoch:  1200  |  train loss: 0.4252293110
Epoch:  1300  |  train loss: 0.4128746092
Epoch:  1400  |  train loss: 0.4017322063
Epoch:  1500  |  train loss: 0.3914892554
Epoch:  1600  |  train loss: 0.3824274480
Epoch:  1700  |  train loss: 0.3739726245
Epoch:  1800  |  train loss: 0.3660040975
Epoch:  1900  |  train loss: 0.3595621049
Epoch:  2000  |  train loss: 0.3530095518
Processing class: 99
Using PCA transformation
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.9295259237
Epoch:   200  |  train loss: 0.7938410521
Epoch:   300  |  train loss: 0.6986320257
Epoch:   400  |  train loss: 0.6321373463
Epoch:   500  |  train loss: 0.5830062270
Epoch:   600  |  train loss: 0.5440794706
Epoch:   700  |  train loss: 0.5126721323
Epoch:   800  |  train loss: 0.4863991380
Epoch:   900  |  train loss: 0.4648118854
Epoch:  1000  |  train loss: 0.4464278340
Epoch:  1100  |  train loss: 0.4299586356
Epoch:  1200  |  train loss: 0.4155614138
Epoch:  1300  |  train loss: 0.4018228531
Epoch:  1400  |  train loss: 0.3906574845
Epoch:  1500  |  train loss: 0.3804134727
Epoch:  1600  |  train loss: 0.3710219443
Epoch:  1700  |  train loss: 0.3625660837
Epoch:  1800  |  train loss: 0.3554044127
Epoch:  1900  |  train loss: 0.3479454637
Epoch:  2000  |  train loss: 0.3414283037
Clasifying using reconstruction function cost
2024-03-18 07:10:17,527 [trainer.py] => CNN: {'total': 50.36, '00-09': 57.7, '10-19': 63.6, '20-29': 71.4, '30-39': 68.5, '40-49': 56.8, '50-59': 29.6, '60-69': 38.9, '70-79': 36.6, '80-89': 42.7, '90-99': 37.8, 'old': 51.76, 'new': 37.8}
2024-03-18 07:10:17,530 [trainer.py] => No NME accuracy
2024-03-18 07:10:17,530 [trainer.py] => FeCAM: {'total': 49.08, '00-09': 69.1, '10-19': 58.4, '20-29': 67.7, '30-39': 62.8, '40-49': 55.6, '50-59': 25.7, '60-69': 32.9, '70-79': 36.8, '80-89': 35.7, '90-99': 46.1, 'old': 49.41, 'new': 46.1}
2024-03-18 07:10:17,531 [trainer.py] => CNN top1 curve: [83.44, 71.75, 64.66, 59.18, 54.08, 50.36]
2024-03-18 07:10:17,531 [trainer.py] => CNN top5 curve: [96.5, 89.6, 86.54, 84.09, 81.74, 79.63]
2024-03-18 07:10:17,531 [trainer.py] => FeCAM top1 curve: [80.88, 67.97, 62.4, 57.01, 52.43, 49.08]
2024-03-18 07:10:17,531 [trainer.py] => FeCAM top5 curve: [94.06, 88.38, 84.11, 80.96, 78.36, 75.39]

=========================================
2024-03-18 07:10:29,128 [trainer.py] => config: ./exps/FeCAM_cifar100.json
2024-03-18 07:10:29,128 [trainer.py] => prefix: train
2024-03-18 07:10:29,129 [trainer.py] => dataset: cifar100
2024-03-18 07:10:29,129 [trainer.py] => memory_size: 0
2024-03-18 07:10:29,129 [trainer.py] => shuffle: True
2024-03-18 07:10:29,129 [trainer.py] => init_cls: 50
2024-03-18 07:10:29,129 [trainer.py] => increment: 10
2024-03-18 07:10:29,129 [trainer.py] => model_name: fecam
2024-03-18 07:10:29,129 [trainer.py] => convnet_type: resnet18
2024-03-18 07:10:29,129 [trainer.py] => device: [device(type='cuda', index=0)]
2024-03-18 07:10:29,129 [trainer.py] => seed: 1993
2024-03-18 07:10:29,129 [trainer.py] => init_epochs: 200
2024-03-18 07:10:29,129 [trainer.py] => init_lr: 0.1
2024-03-18 07:10:29,129 [trainer.py] => init_weight_decay: 0.0005
2024-03-18 07:10:29,129 [trainer.py] => batch_size: 128
2024-03-18 07:10:29,129 [trainer.py] => num_workers: 8
2024-03-18 07:10:29,129 [trainer.py] => T: 5
2024-03-18 07:10:29,129 [trainer.py] => beta: 0.5
2024-03-18 07:10:29,129 [trainer.py] => alpha1: 1
2024-03-18 07:10:29,129 [trainer.py] => alpha2: 1
2024-03-18 07:10:29,129 [trainer.py] => ncm: False
2024-03-18 07:10:29,129 [trainer.py] => tukey: False
2024-03-18 07:10:29,129 [trainer.py] => diagonal: False
2024-03-18 07:10:29,129 [trainer.py] => per_class: True
2024-03-18 07:10:29,129 [trainer.py] => full_cov: True
2024-03-18 07:10:29,129 [trainer.py] => shrink: True
2024-03-18 07:10:29,130 [trainer.py] => norm_cov: False
2024-03-18 07:10:29,130 [trainer.py] => epochs: 2000
2024-03-18 07:10:29,130 [trainer.py] => vecnorm: False
2024-03-18 07:10:29,130 [trainer.py] => ae_type: wae
2024-03-18 07:10:29,130 [trainer.py] => ae_latent_dim: 32
2024-03-18 07:10:29,130 [trainer.py] => ae_n: 1
2024-03-18 07:10:29,130 [trainer.py] => wae_sigma: 10
2024-03-18 07:10:29,130 [trainer.py] => wae_C: 0.1
2024-03-18 07:10:29,130 [trainer.py] => ae_standarization: False
2024-03-18 07:10:29,130 [trainer.py] => ae_pca: True
2024-03-18 07:10:29,130 [trainer.py] => ae_pca_components: 400
2024-03-18 07:10:29,130 [trainer.py] => ae_clsf: recon-cost
Files already downloaded and verified
Files already downloaded and verified
2024-03-18 07:10:31,878 [data_manager.py] => [68, 56, 78, 8, 23, 84, 90, 65, 74, 76, 40, 89, 3, 92, 55, 9, 26, 80, 43, 38, 58, 70, 77, 1, 85, 19, 17, 50, 28, 53, 13, 81, 45, 82, 6, 59, 83, 16, 15, 44, 91, 41, 72, 60, 79, 52, 20, 10, 31, 54, 37, 95, 14, 71, 96, 98, 97, 2, 64, 66, 42, 22, 35, 86, 24, 34, 87, 21, 99, 0, 88, 27, 18, 94, 11, 12, 47, 25, 30, 46, 62, 69, 36, 61, 7, 63, 75, 5, 32, 4, 51, 48, 73, 93, 39, 67, 29, 49, 57, 33]
2024-03-18 07:10:32,157 [fecam.py] => Learning on 0-50
/home/z1165703/miniconda3/lib/python3.11/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
/home/z1165703/miniconda3/lib/python3.11/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
Processing class: 0
Using PCA transformation
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.9463186264
Epoch:   200  |  train loss: 0.8259061217
Epoch:   300  |  train loss: 0.7417224526
Epoch:   400  |  train loss: 0.6819423795
Epoch:   500  |  train loss: 0.6377418518
Epoch:   600  |  train loss: 0.6031332135
Epoch:   700  |  train loss: 0.5759653568
Epoch:   800  |  train loss: 0.5537456036
Epoch:   900  |  train loss: 0.5354984879
Epoch:  1000  |  train loss: 0.5198260307
Epoch:  1100  |  train loss: 0.5067019403
Epoch:  1200  |  train loss: 0.4954655290
Epoch:  1300  |  train loss: 0.4860500336
Epoch:  1400  |  train loss: 0.4777400374
Epoch:  1500  |  train loss: 0.4707993209
Epoch:  1600  |  train loss: 0.4643426001
Epoch:  1700  |  train loss: 0.4589136124
Epoch:  1800  |  train loss: 0.4541226566
Epoch:  1900  |  train loss: 0.4494181991
Epoch:  2000  |  train loss: 0.4463231742
Processing class: 1
Using PCA transformation
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.9406832933
Epoch:   200  |  train loss: 0.8189355969
Epoch:   300  |  train loss: 0.7360066772
Epoch:   400  |  train loss: 0.6776421785
Epoch:   500  |  train loss: 0.6352496862
Epoch:   600  |  train loss: 0.6017873406
Epoch:   700  |  train loss: 0.5760521412
Epoch:   800  |  train loss: 0.5550575972
Epoch:   900  |  train loss: 0.5381255746
Epoch:  1000  |  train loss: 0.5235223293
Epoch:  1100  |  train loss: 0.5112424612
Epoch:  1200  |  train loss: 0.5009123385
Epoch:  1300  |  train loss: 0.4917608559
Epoch:  1400  |  train loss: 0.4840652049
Epoch:  1500  |  train loss: 0.4773848355
Epoch:  1600  |  train loss: 0.4717432022
Epoch:  1700  |  train loss: 0.4664897025
Epoch:  1800  |  train loss: 0.4621982813
Epoch:  1900  |  train loss: 0.4578310847
Epoch:  2000  |  train loss: 0.4544024289
Processing class: 2
Using PCA transformation
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.9420599341
Epoch:   200  |  train loss: 0.8282821536
Epoch:   300  |  train loss: 0.7449361324
Epoch:   400  |  train loss: 0.6855972528
Epoch:   500  |  train loss: 0.6415132880
Epoch:   600  |  train loss: 0.6076221585
Epoch:   700  |  train loss: 0.5805659890
Epoch:   800  |  train loss: 0.5586896896
Epoch:   900  |  train loss: 0.5407657743
Epoch:  1000  |  train loss: 0.5260306120
Epoch:  1100  |  train loss: 0.5133901596
Epoch:  1200  |  train loss: 0.5024524271
Epoch:  1300  |  train loss: 0.4934342802
Epoch:  1400  |  train loss: 0.4856516898
Epoch:  1500  |  train loss: 0.4791296899
Epoch:  1600  |  train loss: 0.4727728605
Epoch:  1700  |  train loss: 0.4673902214
Epoch:  1800  |  train loss: 0.4630696058
Epoch:  1900  |  train loss: 0.4591868103
Epoch:  2000  |  train loss: 0.4556392014
Processing class: 3
Using PCA transformation
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.9438633084
Epoch:   200  |  train loss: 0.8318307400
Epoch:   300  |  train loss: 0.7535470009
Epoch:   400  |  train loss: 0.6953773856
Epoch:   500  |  train loss: 0.6524715662
Epoch:   600  |  train loss: 0.6193253994
Epoch:   700  |  train loss: 0.5921996713
Epoch:   800  |  train loss: 0.5702245831
Epoch:   900  |  train loss: 0.5528464079
Epoch:  1000  |  train loss: 0.5377853870
Epoch:  1100  |  train loss: 0.5248244762
Epoch:  1200  |  train loss: 0.5137511313
Epoch:  1300  |  train loss: 0.5043176353
Epoch:  1400  |  train loss: 0.4960465014
Epoch:  1500  |  train loss: 0.4890244186
Epoch:  1600  |  train loss: 0.4825604737
Epoch:  1700  |  train loss: 0.4767024815
Epoch:  1800  |  train loss: 0.4715985775
Epoch:  1900  |  train loss: 0.4673489034
Epoch:  2000  |  train loss: 0.4636062384
Processing class: 4
Using PCA transformation
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.9410659313
Epoch:   200  |  train loss: 0.8192487240
Epoch:   300  |  train loss: 0.7362800479
Epoch:   400  |  train loss: 0.6785681248
Epoch:   500  |  train loss: 0.6353163719
Epoch:   600  |  train loss: 0.6022818565
Epoch:   700  |  train loss: 0.5761977673
Epoch:   800  |  train loss: 0.5556258559
Epoch:   900  |  train loss: 0.5380455136
Epoch:  1000  |  train loss: 0.5239239275
Epoch:  1100  |  train loss: 0.5120798528
Epoch:  1200  |  train loss: 0.5018206716
Epoch:  1300  |  train loss: 0.4927874267
Epoch:  1400  |  train loss: 0.4853351772
Epoch:  1500  |  train loss: 0.4786667705
Epoch:  1600  |  train loss: 0.4733740270
Epoch:  1700  |  train loss: 0.4682513535
Epoch:  1800  |  train loss: 0.4637592494
Epoch:  1900  |  train loss: 0.4600894272
Epoch:  2000  |  train loss: 0.4566641569
Processing class: 5
Using PCA transformation
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.9389544010
Epoch:   200  |  train loss: 0.8155356526
Epoch:   300  |  train loss: 0.7337779880
Epoch:   400  |  train loss: 0.6754925966
Epoch:   500  |  train loss: 0.6327176213
Epoch:   600  |  train loss: 0.5995737433
Epoch:   700  |  train loss: 0.5732112408
Epoch:   800  |  train loss: 0.5526243448
Epoch:   900  |  train loss: 0.5351029038
Epoch:  1000  |  train loss: 0.5208907962
Epoch:  1100  |  train loss: 0.5085494637
Epoch:  1200  |  train loss: 0.4982484102
Epoch:  1300  |  train loss: 0.4896064937
Epoch:  1400  |  train loss: 0.4817218602
Epoch:  1500  |  train loss: 0.4753252387
Epoch:  1600  |  train loss: 0.4697357178
Epoch:  1700  |  train loss: 0.4643277407
Epoch:  1800  |  train loss: 0.4598803759
Epoch:  1900  |  train loss: 0.4558014572
Epoch:  2000  |  train loss: 0.4524576843
Processing class: 6
Using PCA transformation
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.9422879815
Epoch:   200  |  train loss: 0.8296537638
Epoch:   300  |  train loss: 0.7480255961
Epoch:   400  |  train loss: 0.6898500919
Epoch:   500  |  train loss: 0.6466925144
Epoch:   600  |  train loss: 0.6139769793
Epoch:   700  |  train loss: 0.5875394940
Epoch:   800  |  train loss: 0.5663740158
Epoch:   900  |  train loss: 0.5491624713
Epoch:  1000  |  train loss: 0.5351753712
Epoch:  1100  |  train loss: 0.5234414816
Epoch:  1200  |  train loss: 0.5132919073
Epoch:  1300  |  train loss: 0.5043812156
Epoch:  1400  |  train loss: 0.4968194306
Epoch:  1500  |  train loss: 0.4905839682
Epoch:  1600  |  train loss: 0.4849990547
Epoch:  1700  |  train loss: 0.4802490950
Epoch:  1800  |  train loss: 0.4750556946
Epoch:  1900  |  train loss: 0.4707892716
Epoch:  2000  |  train loss: 0.4670675933
Processing class: 7
Using PCA transformation
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.9423867106
Epoch:   200  |  train loss: 0.8270284653
Epoch:   300  |  train loss: 0.7447893023
Epoch:   400  |  train loss: 0.6850350738
Epoch:   500  |  train loss: 0.6406543851
Epoch:   600  |  train loss: 0.6070683360
Epoch:   700  |  train loss: 0.5800084352
Epoch:   800  |  train loss: 0.5583881497
Epoch:   900  |  train loss: 0.5405965209
Epoch:  1000  |  train loss: 0.5253228903
Epoch:  1100  |  train loss: 0.5126180947
Epoch:  1200  |  train loss: 0.5023402035
Epoch:  1300  |  train loss: 0.4928711712
Epoch:  1400  |  train loss: 0.4851177454
Epoch:  1500  |  train loss: 0.4779840589
Epoch:  1600  |  train loss: 0.4722765148
Epoch:  1700  |  train loss: 0.4670784831
Epoch:  1800  |  train loss: 0.4625366509
Epoch:  1900  |  train loss: 0.4580875993
Epoch:  2000  |  train loss: 0.4544083238
Processing class: 8
Using PCA transformation
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.9388901949
Epoch:   200  |  train loss: 0.8240283728
Epoch:   300  |  train loss: 0.7437072515
Epoch:   400  |  train loss: 0.6856669664
Epoch:   500  |  train loss: 0.6427146435
Epoch:   600  |  train loss: 0.6092527390
Epoch:   700  |  train loss: 0.5829752684
Epoch:   800  |  train loss: 0.5615019679
Epoch:   900  |  train loss: 0.5435991645
Epoch:  1000  |  train loss: 0.5284491777
Epoch:  1100  |  train loss: 0.5159313142
Epoch:  1200  |  train loss: 0.5047144294
Epoch:  1300  |  train loss: 0.4954352677
Epoch:  1400  |  train loss: 0.4867665827
Epoch:  1500  |  train loss: 0.4803053617
Epoch:  1600  |  train loss: 0.4738478899
Epoch:  1700  |  train loss: 0.4685069919
Epoch:  1800  |  train loss: 0.4636440158
Epoch:  1900  |  train loss: 0.4590502799
Epoch:  2000  |  train loss: 0.4549104691
Processing class: 9
Using PCA transformation
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.9383481383
Epoch:   200  |  train loss: 0.8248532653
Epoch:   300  |  train loss: 0.7441627979
Epoch:   400  |  train loss: 0.6861453414
Epoch:   500  |  train loss: 0.6436310530
Epoch:   600  |  train loss: 0.6111422062
Epoch:   700  |  train loss: 0.5850970268
Epoch:   800  |  train loss: 0.5638122439
Epoch:   900  |  train loss: 0.5463495612
Epoch:  1000  |  train loss: 0.5312518835
Epoch:  1100  |  train loss: 0.5186010599
Epoch:  1200  |  train loss: 0.5074060440
Epoch:  1300  |  train loss: 0.4978853583
Epoch:  1400  |  train loss: 0.4897915244
Epoch:  1500  |  train loss: 0.4824137807
Epoch:  1600  |  train loss: 0.4758295715
Epoch:  1700  |  train loss: 0.4699988067
Epoch:  1800  |  train loss: 0.4650397718
Epoch:  1900  |  train loss: 0.4607381940
Epoch:  2000  |  train loss: 0.4562342703
Processing class: 10
Using PCA transformation
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.9381299496
Epoch:   200  |  train loss: 0.8195190549
Epoch:   300  |  train loss: 0.7371726274
Epoch:   400  |  train loss: 0.6794695258
Epoch:   500  |  train loss: 0.6364228725
Epoch:   600  |  train loss: 0.6035660267
Epoch:   700  |  train loss: 0.5767112017
Epoch:   800  |  train loss: 0.5552966118
Epoch:   900  |  train loss: 0.5379730463
Epoch:  1000  |  train loss: 0.5236485600
Epoch:  1100  |  train loss: 0.5114126503
Epoch:  1200  |  train loss: 0.5008175552
Epoch:  1300  |  train loss: 0.4922553897
Epoch:  1400  |  train loss: 0.4847063184
Epoch:  1500  |  train loss: 0.4780051231
Epoch:  1600  |  train loss: 0.4719990730
Epoch:  1700  |  train loss: 0.4668539405
Epoch:  1800  |  train loss: 0.4626194596
Epoch:  1900  |  train loss: 0.4586899698
Epoch:  2000  |  train loss: 0.4551002026
Processing class: 11
Using PCA transformation
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.9358182907
Epoch:   200  |  train loss: 0.8150702596
Epoch:   300  |  train loss: 0.7280408144
Epoch:   400  |  train loss: 0.6659486532
Epoch:   500  |  train loss: 0.6219855905
Epoch:   600  |  train loss: 0.5880134106
Epoch:   700  |  train loss: 0.5616192937
Epoch:   800  |  train loss: 0.5404832840
Epoch:   900  |  train loss: 0.5230328441
Epoch:  1000  |  train loss: 0.5082412362
Epoch:  1100  |  train loss: 0.4963099480
Epoch:  1200  |  train loss: 0.4858616829
Epoch:  1300  |  train loss: 0.4767422795
Epoch:  1400  |  train loss: 0.4692771554
Epoch:  1500  |  train loss: 0.4631781936
Epoch:  1600  |  train loss: 0.4576503575
Epoch:  1700  |  train loss: 0.4523026884
Epoch:  1800  |  train loss: 0.4472559810
Epoch:  1900  |  train loss: 0.4429847121
Epoch:  2000  |  train loss: 0.4396098852
Processing class: 12
Using PCA transformation
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.9376830697
Epoch:   200  |  train loss: 0.8241031885
Epoch:   300  |  train loss: 0.7421533942
Epoch:   400  |  train loss: 0.6835056782
Epoch:   500  |  train loss: 0.6406411052
Epoch:   600  |  train loss: 0.6075100422
Epoch:   700  |  train loss: 0.5817842364
Epoch:   800  |  train loss: 0.5605268955
Epoch:   900  |  train loss: 0.5425423265
Epoch:  1000  |  train loss: 0.5279616714
Epoch:  1100  |  train loss: 0.5151742935
Epoch:  1200  |  train loss: 0.5042104840
Epoch:  1300  |  train loss: 0.4946877897
Epoch:  1400  |  train loss: 0.4867235065
Epoch:  1500  |  train loss: 0.4795913875
Epoch:  1600  |  train loss: 0.4731864333
Epoch:  1700  |  train loss: 0.4675224304
Epoch:  1800  |  train loss: 0.4619384706
Epoch:  1900  |  train loss: 0.4576168597
Epoch:  2000  |  train loss: 0.4534237087
Processing class: 13
Using PCA transformation
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.9375583410
Epoch:   200  |  train loss: 0.8182089806
Epoch:   300  |  train loss: 0.7368627787
Epoch:   400  |  train loss: 0.6790972471
Epoch:   500  |  train loss: 0.6363247752
Epoch:   600  |  train loss: 0.6040618062
Epoch:   700  |  train loss: 0.5787024617
Epoch:   800  |  train loss: 0.5580197096
Epoch:   900  |  train loss: 0.5410622954
Epoch:  1000  |  train loss: 0.5269207001
Epoch:  1100  |  train loss: 0.5144726396
Epoch:  1200  |  train loss: 0.5045397699
Epoch:  1300  |  train loss: 0.4956065893
Epoch:  1400  |  train loss: 0.4876526713
Epoch:  1500  |  train loss: 0.4808478892
Epoch:  1600  |  train loss: 0.4750618041
Epoch:  1700  |  train loss: 0.4697862208
Epoch:  1800  |  train loss: 0.4650217056
Epoch:  1900  |  train loss: 0.4609937072
Epoch:  2000  |  train loss: 0.4575323701
Processing class: 14
Using PCA transformation
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.9399397850
Epoch:   200  |  train loss: 0.8270250082
Epoch:   300  |  train loss: 0.7500926137
Epoch:   400  |  train loss: 0.6950366616
Epoch:   500  |  train loss: 0.6540232182
Epoch:   600  |  train loss: 0.6226433039
Epoch:   700  |  train loss: 0.5976738334
Epoch:   800  |  train loss: 0.5774695039
Epoch:   900  |  train loss: 0.5598365188
Epoch:  1000  |  train loss: 0.5454703808
Epoch:  1100  |  train loss: 0.5331572533
Epoch:  1200  |  train loss: 0.5224163413
Epoch:  1300  |  train loss: 0.5134711862
Epoch:  1400  |  train loss: 0.5055261493
Epoch:  1500  |  train loss: 0.4978625715
Epoch:  1600  |  train loss: 0.4914504051
Epoch:  1700  |  train loss: 0.4860314965
Epoch:  1800  |  train loss: 0.4812994540
Epoch:  1900  |  train loss: 0.4771307111
Epoch:  2000  |  train loss: 0.4733571410
Processing class: 15
Using PCA transformation
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.9425418973
Epoch:   200  |  train loss: 0.8294579029
Epoch:   300  |  train loss: 0.7479967594
Epoch:   400  |  train loss: 0.6894673586
Epoch:   500  |  train loss: 0.6458841681
Epoch:   600  |  train loss: 0.6127116919
Epoch:   700  |  train loss: 0.5857193232
Epoch:   800  |  train loss: 0.5633405805
Epoch:   900  |  train loss: 0.5449055314
Epoch:  1000  |  train loss: 0.5296164393
Epoch:  1100  |  train loss: 0.5162464201
Epoch:  1200  |  train loss: 0.5049689293
Epoch:  1300  |  train loss: 0.4954556406
Epoch:  1400  |  train loss: 0.4867228031
Epoch:  1500  |  train loss: 0.4792008042
Epoch:  1600  |  train loss: 0.4724774480
Epoch:  1700  |  train loss: 0.4668870509
Epoch:  1800  |  train loss: 0.4615639985
Epoch:  1900  |  train loss: 0.4572883904
Epoch:  2000  |  train loss: 0.4536063015
Processing class: 16
Using PCA transformation
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.9390584946
Epoch:   200  |  train loss: 0.8201966763
Epoch:   300  |  train loss: 0.7373673916
Epoch:   400  |  train loss: 0.6789145470
Epoch:   500  |  train loss: 0.6351246357
Epoch:   600  |  train loss: 0.6022324324
Epoch:   700  |  train loss: 0.5754262805
Epoch:   800  |  train loss: 0.5534077048
Epoch:   900  |  train loss: 0.5351384401
Epoch:  1000  |  train loss: 0.5205920815
Epoch:  1100  |  train loss: 0.5082805276
Epoch:  1200  |  train loss: 0.4967294157
Epoch:  1300  |  train loss: 0.4878832221
Epoch:  1400  |  train loss: 0.4799302518
Epoch:  1500  |  train loss: 0.4730838895
Epoch:  1600  |  train loss: 0.4663817823
Epoch:  1700  |  train loss: 0.4612777710
Epoch:  1800  |  train loss: 0.4567518353
Epoch:  1900  |  train loss: 0.4524683833
Epoch:  2000  |  train loss: 0.4485459983
Processing class: 17
Using PCA transformation
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.9389135003
Epoch:   200  |  train loss: 0.8226422071
Epoch:   300  |  train loss: 0.7411866665
Epoch:   400  |  train loss: 0.6846083641
Epoch:   500  |  train loss: 0.6424927354
Epoch:   600  |  train loss: 0.6094354868
Epoch:   700  |  train loss: 0.5835438490
Epoch:   800  |  train loss: 0.5619617224
Epoch:   900  |  train loss: 0.5440898538
Epoch:  1000  |  train loss: 0.5288960814
Epoch:  1100  |  train loss: 0.5165621519
Epoch:  1200  |  train loss: 0.5054992497
Epoch:  1300  |  train loss: 0.4962778151
Epoch:  1400  |  train loss: 0.4882571399
Epoch:  1500  |  train loss: 0.4814072132
Epoch:  1600  |  train loss: 0.4751911581
Epoch:  1700  |  train loss: 0.4696925163
Epoch:  1800  |  train loss: 0.4651811123
Epoch:  1900  |  train loss: 0.4608560920
Epoch:  2000  |  train loss: 0.4571662068
Processing class: 18
Using PCA transformation
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.9374490976
Epoch:   200  |  train loss: 0.8185598969
Epoch:   300  |  train loss: 0.7365780473
Epoch:   400  |  train loss: 0.6785595298
Epoch:   500  |  train loss: 0.6348894715
Epoch:   600  |  train loss: 0.6022808552
Epoch:   700  |  train loss: 0.5757395148
Epoch:   800  |  train loss: 0.5540394425
Epoch:   900  |  train loss: 0.5364755273
Epoch:  1000  |  train loss: 0.5217056751
Epoch:  1100  |  train loss: 0.5091299415
Epoch:  1200  |  train loss: 0.4983200312
Epoch:  1300  |  train loss: 0.4891817570
Epoch:  1400  |  train loss: 0.4812196374
Epoch:  1500  |  train loss: 0.4740880668
Epoch:  1600  |  train loss: 0.4681643486
Epoch:  1700  |  train loss: 0.4628807306
Epoch:  1800  |  train loss: 0.4581981361
Epoch:  1900  |  train loss: 0.4541481316
Epoch:  2000  |  train loss: 0.4503465116
Processing class: 19
Using PCA transformation
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.9412562847
Epoch:   200  |  train loss: 0.8229123116
Epoch:   300  |  train loss: 0.7405630827
Epoch:   400  |  train loss: 0.6810233951
Epoch:   500  |  train loss: 0.6381170630
Epoch:   600  |  train loss: 0.6051114678
Epoch:   700  |  train loss: 0.5789654136
Epoch:   800  |  train loss: 0.5572141647
Epoch:   900  |  train loss: 0.5400983572
Epoch:  1000  |  train loss: 0.5251618028
Epoch:  1100  |  train loss: 0.5128469646
Epoch:  1200  |  train loss: 0.5022195995
Epoch:  1300  |  train loss: 0.4930409551
Epoch:  1400  |  train loss: 0.4849705637
Epoch:  1500  |  train loss: 0.4782045484
Epoch:  1600  |  train loss: 0.4720025778
Epoch:  1700  |  train loss: 0.4667568624
Epoch:  1800  |  train loss: 0.4618304729
Epoch:  1900  |  train loss: 0.4578050852
Epoch:  2000  |  train loss: 0.4542586267
Processing class: 20
Using PCA transformation
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.9382255793
Epoch:   200  |  train loss: 0.8249743819
Epoch:   300  |  train loss: 0.7434010029
Epoch:   400  |  train loss: 0.6847904682
Epoch:   500  |  train loss: 0.6411734462
Epoch:   600  |  train loss: 0.6079298973
Epoch:   700  |  train loss: 0.5809238076
Epoch:   800  |  train loss: 0.5595383763
Epoch:   900  |  train loss: 0.5416944623
Epoch:  1000  |  train loss: 0.5265346766
Epoch:  1100  |  train loss: 0.5140852451
Epoch:  1200  |  train loss: 0.5021686375
Epoch:  1300  |  train loss: 0.4924168229
Epoch:  1400  |  train loss: 0.4846640050
Epoch:  1500  |  train loss: 0.4777313948
Epoch:  1600  |  train loss: 0.4712391496
Epoch:  1700  |  train loss: 0.4660231769
Epoch:  1800  |  train loss: 0.4611507952
Epoch:  1900  |  train loss: 0.4563187897
Epoch:  2000  |  train loss: 0.4524284720
Processing class: 21
Using PCA transformation
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.9382347941
Epoch:   200  |  train loss: 0.8230763316
Epoch:   300  |  train loss: 0.7419324875
Epoch:   400  |  train loss: 0.6830894589
Epoch:   500  |  train loss: 0.6387101412
Epoch:   600  |  train loss: 0.6042901754
Epoch:   700  |  train loss: 0.5769528151
Epoch:   800  |  train loss: 0.5550883651
Epoch:   900  |  train loss: 0.5367456079
Epoch:  1000  |  train loss: 0.5212743521
Epoch:  1100  |  train loss: 0.5082805157
Epoch:  1200  |  train loss: 0.4975103021
Epoch:  1300  |  train loss: 0.4881666660
Epoch:  1400  |  train loss: 0.4792345464
Epoch:  1500  |  train loss: 0.4722283483
Epoch:  1600  |  train loss: 0.4660144627
Epoch:  1700  |  train loss: 0.4605752170
Epoch:  1800  |  train loss: 0.4557516456
Epoch:  1900  |  train loss: 0.4518550456
Epoch:  2000  |  train loss: 0.4481287301
Processing class: 22
Using PCA transformation
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.9361567140
Epoch:   200  |  train loss: 0.8190870047
Epoch:   300  |  train loss: 0.7378906012
Epoch:   400  |  train loss: 0.6789217710
Epoch:   500  |  train loss: 0.6339527488
Epoch:   600  |  train loss: 0.6000661492
Epoch:   700  |  train loss: 0.5732691407
Epoch:   800  |  train loss: 0.5518304706
Epoch:   900  |  train loss: 0.5342253327
Epoch:  1000  |  train loss: 0.5193348050
Epoch:  1100  |  train loss: 0.5069516838
Epoch:  1200  |  train loss: 0.4961190164
Epoch:  1300  |  train loss: 0.4876586139
Epoch:  1400  |  train loss: 0.4795176566
Epoch:  1500  |  train loss: 0.4728842080
Epoch:  1600  |  train loss: 0.4669187069
Epoch:  1700  |  train loss: 0.4617642939
Epoch:  1800  |  train loss: 0.4575901270
Epoch:  1900  |  train loss: 0.4534254909
Epoch:  2000  |  train loss: 0.4492045581
Processing class: 23
Using PCA transformation
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.9372658610
Epoch:   200  |  train loss: 0.8213342071
Epoch:   300  |  train loss: 0.7393398166
Epoch:   400  |  train loss: 0.6806490541
Epoch:   500  |  train loss: 0.6378707409
Epoch:   600  |  train loss: 0.6050082326
Epoch:   700  |  train loss: 0.5795200109
Epoch:   800  |  train loss: 0.5587480903
Epoch:   900  |  train loss: 0.5420812011
Epoch:  1000  |  train loss: 0.5264309406
Epoch:  1100  |  train loss: 0.5141702533
Epoch:  1200  |  train loss: 0.5039220393
Epoch:  1300  |  train loss: 0.4950126708
Epoch:  1400  |  train loss: 0.4871729493
Epoch:  1500  |  train loss: 0.4805333197
Epoch:  1600  |  train loss: 0.4745117247
Epoch:  1700  |  train loss: 0.4696187198
Epoch:  1800  |  train loss: 0.4648418009
Epoch:  1900  |  train loss: 0.4607370079
Epoch:  2000  |  train loss: 0.4574318528
Processing class: 24
Using PCA transformation
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.9420774460
Epoch:   200  |  train loss: 0.8288485169
Epoch:   300  |  train loss: 0.7488328338
Epoch:   400  |  train loss: 0.6924510479
Epoch:   500  |  train loss: 0.6504970670
Epoch:   600  |  train loss: 0.6180922270
Epoch:   700  |  train loss: 0.5927331924
Epoch:   800  |  train loss: 0.5714600921
Epoch:   900  |  train loss: 0.5536009789
Epoch:  1000  |  train loss: 0.5388316989
Epoch:  1100  |  train loss: 0.5266686559
Epoch:  1200  |  train loss: 0.5156334579
Epoch:  1300  |  train loss: 0.5062684298
Epoch:  1400  |  train loss: 0.4979174554
Epoch:  1500  |  train loss: 0.4906800926
Epoch:  1600  |  train loss: 0.4846237063
Epoch:  1700  |  train loss: 0.4790873170
Epoch:  1800  |  train loss: 0.4744725883
Epoch:  1900  |  train loss: 0.4701043725
Epoch:  2000  |  train loss: 0.4660473347
Processing class: 25
Using PCA transformation
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.9370672584
Epoch:   200  |  train loss: 0.8260210156
Epoch:   300  |  train loss: 0.7437678814
Epoch:   400  |  train loss: 0.6855587363
Epoch:   500  |  train loss: 0.6431092024
Epoch:   600  |  train loss: 0.6105576873
Epoch:   700  |  train loss: 0.5842314959
Epoch:   800  |  train loss: 0.5631319284
Epoch:   900  |  train loss: 0.5454287171
Epoch:  1000  |  train loss: 0.5308858991
Epoch:  1100  |  train loss: 0.5185897231
Epoch:  1200  |  train loss: 0.5081393719
Epoch:  1300  |  train loss: 0.4984777033
Epoch:  1400  |  train loss: 0.4904848695
Epoch:  1500  |  train loss: 0.4839099467
Epoch:  1600  |  train loss: 0.4780364931
Epoch:  1700  |  train loss: 0.4724232078
Epoch:  1800  |  train loss: 0.4678013265
Epoch:  1900  |  train loss: 0.4635734916
Epoch:  2000  |  train loss: 0.4603277445
Processing class: 26
Using PCA transformation
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.9380655169
Epoch:   200  |  train loss: 0.8233077407
Epoch:   300  |  train loss: 0.7453125596
Epoch:   400  |  train loss: 0.6885337710
Epoch:   500  |  train loss: 0.6464018106
Epoch:   600  |  train loss: 0.6138890147
Epoch:   700  |  train loss: 0.5875609875
Epoch:   800  |  train loss: 0.5663519144
Epoch:   900  |  train loss: 0.5483379960
Epoch:  1000  |  train loss: 0.5333328128
Epoch:  1100  |  train loss: 0.5206531048
Epoch:  1200  |  train loss: 0.5093572497
Epoch:  1300  |  train loss: 0.5002505839
Epoch:  1400  |  train loss: 0.4918755054
Epoch:  1500  |  train loss: 0.4845211744
Epoch:  1600  |  train loss: 0.4778115511
Epoch:  1700  |  train loss: 0.4720932484
Epoch:  1800  |  train loss: 0.4669940054
Epoch:  1900  |  train loss: 0.4617678642
Epoch:  2000  |  train loss: 0.4573895097
Processing class: 27
Using PCA transformation
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.9388744950
Epoch:   200  |  train loss: 0.8185994506
Epoch:   300  |  train loss: 0.7362357378
Epoch:   400  |  train loss: 0.6749738455
Epoch:   500  |  train loss: 0.6308017969
Epoch:   600  |  train loss: 0.5973052025
Epoch:   700  |  train loss: 0.5708141923
Epoch:   800  |  train loss: 0.5494812250
Epoch:   900  |  train loss: 0.5315744996
Epoch:  1000  |  train loss: 0.5171433210
Epoch:  1100  |  train loss: 0.5049247444
Epoch:  1200  |  train loss: 0.4943008780
Epoch:  1300  |  train loss: 0.4853455245
Epoch:  1400  |  train loss: 0.4774841368
Epoch:  1500  |  train loss: 0.4705355406
Epoch:  1600  |  train loss: 0.4645423412
Epoch:  1700  |  train loss: 0.4595066249
Epoch:  1800  |  train loss: 0.4547882915
Epoch:  1900  |  train loss: 0.4511454642
Epoch:  2000  |  train loss: 0.4474159479
Processing class: 28
Using PCA transformation
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.9396393657
Epoch:   200  |  train loss: 0.8227313280
Epoch:   300  |  train loss: 0.7432762623
Epoch:   400  |  train loss: 0.6865928173
Epoch:   500  |  train loss: 0.6449049473
Epoch:   600  |  train loss: 0.6129328728
Epoch:   700  |  train loss: 0.5871138096
Epoch:   800  |  train loss: 0.5664059520
Epoch:   900  |  train loss: 0.5489414096
Epoch:  1000  |  train loss: 0.5343611360
Epoch:  1100  |  train loss: 0.5225869179
Epoch:  1200  |  train loss: 0.5115246177
Epoch:  1300  |  train loss: 0.5024969518
Epoch:  1400  |  train loss: 0.4947245836
Epoch:  1500  |  train loss: 0.4877604425
Epoch:  1600  |  train loss: 0.4819788158
Epoch:  1700  |  train loss: 0.4762394488
Epoch:  1800  |  train loss: 0.4720617175
Epoch:  1900  |  train loss: 0.4672160327
Epoch:  2000  |  train loss: 0.4638899088
Processing class: 29
Using PCA transformation
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.9354393363
Epoch:   200  |  train loss: 0.8082714081
Epoch:   300  |  train loss: 0.7214016795
Epoch:   400  |  train loss: 0.6617447257
Epoch:   500  |  train loss: 0.6181867719
Epoch:   600  |  train loss: 0.5847454429
Epoch:   700  |  train loss: 0.5585725546
Epoch:   800  |  train loss: 0.5371556878
Epoch:   900  |  train loss: 0.5195656300
Epoch:  1000  |  train loss: 0.5050767004
Epoch:  1100  |  train loss: 0.4928101897
Epoch:  1200  |  train loss: 0.4821200788
Epoch:  1300  |  train loss: 0.4733985662
Epoch:  1400  |  train loss: 0.4660674334
Epoch:  1500  |  train loss: 0.4593816102
Epoch:  1600  |  train loss: 0.4535818875
Epoch:  1700  |  train loss: 0.4485676467
Epoch:  1800  |  train loss: 0.4440003812
Epoch:  1900  |  train loss: 0.4407908797
Epoch:  2000  |  train loss: 0.4371975303
Processing class: 30
Using PCA transformation
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.9395658493
Epoch:   200  |  train loss: 0.8153934121
Epoch:   300  |  train loss: 0.7329085112
Epoch:   400  |  train loss: 0.6747526169
Epoch:   500  |  train loss: 0.6312519193
Epoch:   600  |  train loss: 0.5977156758
Epoch:   700  |  train loss: 0.5709787250
Epoch:   800  |  train loss: 0.5495995879
Epoch:   900  |  train loss: 0.5319486856
Epoch:  1000  |  train loss: 0.5168453097
Epoch:  1100  |  train loss: 0.5045350373
Epoch:  1200  |  train loss: 0.4931465566
Epoch:  1300  |  train loss: 0.4835127175
Epoch:  1400  |  train loss: 0.4754370809
Epoch:  1500  |  train loss: 0.4680672526
Epoch:  1600  |  train loss: 0.4623607457
Epoch:  1700  |  train loss: 0.4569490433
Epoch:  1800  |  train loss: 0.4521359324
Epoch:  1900  |  train loss: 0.4481100261
Epoch:  2000  |  train loss: 0.4443727732
Processing class: 31
Using PCA transformation
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.9410201430
Epoch:   200  |  train loss: 0.8260371566
Epoch:   300  |  train loss: 0.7454935074
Epoch:   400  |  train loss: 0.6878426075
Epoch:   500  |  train loss: 0.6454126120
Epoch:   600  |  train loss: 0.6123719096
Epoch:   700  |  train loss: 0.5861983180
Epoch:   800  |  train loss: 0.5651179433
Epoch:   900  |  train loss: 0.5475308776
Epoch:  1000  |  train loss: 0.5323671341
Epoch:  1100  |  train loss: 0.5197640896
Epoch:  1200  |  train loss: 0.5076757967
Epoch:  1300  |  train loss: 0.4973812997
Epoch:  1400  |  train loss: 0.4889976442
Epoch:  1500  |  train loss: 0.4817951620
Epoch:  1600  |  train loss: 0.4748903215
Epoch:  1700  |  train loss: 0.4692356527
Epoch:  1800  |  train loss: 0.4640352011
Epoch:  1900  |  train loss: 0.4599274397
Epoch:  2000  |  train loss: 0.4556154430
Processing class: 32
Using PCA transformation
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.9423002243
Epoch:   200  |  train loss: 0.8296017766
Epoch:   300  |  train loss: 0.7453607202
Epoch:   400  |  train loss: 0.6854549766
Epoch:   500  |  train loss: 0.6423434138
Epoch:   600  |  train loss: 0.6090656042
Epoch:   700  |  train loss: 0.5834475636
Epoch:   800  |  train loss: 0.5621309042
Epoch:   900  |  train loss: 0.5446954250
Epoch:  1000  |  train loss: 0.5306044698
Epoch:  1100  |  train loss: 0.5180640578
Epoch:  1200  |  train loss: 0.5079524934
Epoch:  1300  |  train loss: 0.4989198387
Epoch:  1400  |  train loss: 0.4911192298
Epoch:  1500  |  train loss: 0.4845434368
Epoch:  1600  |  train loss: 0.4786087811
Epoch:  1700  |  train loss: 0.4737775385
Epoch:  1800  |  train loss: 0.4688577592
Epoch:  1900  |  train loss: 0.4651722312
Epoch:  2000  |  train loss: 0.4616921902
Processing class: 33
Using PCA transformation
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.9378786087
Epoch:   200  |  train loss: 0.8185972333
Epoch:   300  |  train loss: 0.7336514115
Epoch:   400  |  train loss: 0.6742805243
Epoch:   500  |  train loss: 0.6307754159
Epoch:   600  |  train loss: 0.5974437356
Epoch:   700  |  train loss: 0.5709273100
Epoch:   800  |  train loss: 0.5496238708
Epoch:   900  |  train loss: 0.5327194691
Epoch:  1000  |  train loss: 0.5181555271
Epoch:  1100  |  train loss: 0.5067736745
Epoch:  1200  |  train loss: 0.4960718691
Epoch:  1300  |  train loss: 0.4875666440
Epoch:  1400  |  train loss: 0.4802181065
Epoch:  1500  |  train loss: 0.4742346048
Epoch:  1600  |  train loss: 0.4684036613
Epoch:  1700  |  train loss: 0.4636442959
Epoch:  1800  |  train loss: 0.4596149683
Epoch:  1900  |  train loss: 0.4558853984
Epoch:  2000  |  train loss: 0.4524786949
Processing class: 34
Using PCA transformation
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.9374239326
Epoch:   200  |  train loss: 0.8158323288
Epoch:   300  |  train loss: 0.7329400778
Epoch:   400  |  train loss: 0.6736852407
Epoch:   500  |  train loss: 0.6298730969
Epoch:   600  |  train loss: 0.5961903572
Epoch:   700  |  train loss: 0.5692752481
Epoch:   800  |  train loss: 0.5482249141
Epoch:   900  |  train loss: 0.5305449605
Epoch:  1000  |  train loss: 0.5155370116
Epoch:  1100  |  train loss: 0.5033631325
Epoch:  1200  |  train loss: 0.4925523639
Epoch:  1300  |  train loss: 0.4833444357
Epoch:  1400  |  train loss: 0.4754887044
Epoch:  1500  |  train loss: 0.4686585248
Epoch:  1600  |  train loss: 0.4628942907
Epoch:  1700  |  train loss: 0.4576333821
Epoch:  1800  |  train loss: 0.4532370925
Epoch:  1900  |  train loss: 0.4491205633
Epoch:  2000  |  train loss: 0.4454472065
Processing class: 35
Using PCA transformation
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.9361415386
Epoch:   200  |  train loss: 0.8119229913
Epoch:   300  |  train loss: 0.7279866457
Epoch:   400  |  train loss: 0.6697353363
Epoch:   500  |  train loss: 0.6276184916
Epoch:   600  |  train loss: 0.5949695230
Epoch:   700  |  train loss: 0.5689868689
Epoch:   800  |  train loss: 0.5487837791
Epoch:   900  |  train loss: 0.5315848827
/home/z1165703/miniconda3/lib/python3.11/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
Epoch:  1000  |  train loss: 0.5178011656
Epoch:  1100  |  train loss: 0.5059469819
Epoch:  1200  |  train loss: 0.4957617104
Epoch:  1300  |  train loss: 0.4876665473
Epoch:  1400  |  train loss: 0.4803372860
Epoch:  1500  |  train loss: 0.4741925836
Epoch:  1600  |  train loss: 0.4687811136
Epoch:  1700  |  train loss: 0.4637898147
Epoch:  1800  |  train loss: 0.4598330557
Epoch:  1900  |  train loss: 0.4561583698
Epoch:  2000  |  train loss: 0.4530767739
Processing class: 36
Using PCA transformation
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.9349063754
Epoch:   200  |  train loss: 0.8135977507
Epoch:   300  |  train loss: 0.7304391026
Epoch:   400  |  train loss: 0.6719416618
Epoch:   500  |  train loss: 0.6290270925
Epoch:   600  |  train loss: 0.5953291774
Epoch:   700  |  train loss: 0.5689095259
Epoch:   800  |  train loss: 0.5481206298
Epoch:   900  |  train loss: 0.5308084488
Epoch:  1000  |  train loss: 0.5163527131
Epoch:  1100  |  train loss: 0.5042025685
Epoch:  1200  |  train loss: 0.4939245880
Epoch:  1300  |  train loss: 0.4850740433
Epoch:  1400  |  train loss: 0.4775801361
Epoch:  1500  |  train loss: 0.4705550730
Epoch:  1600  |  train loss: 0.4645586312
Epoch:  1700  |  train loss: 0.4598946273
Epoch:  1800  |  train loss: 0.4553409576
Epoch:  1900  |  train loss: 0.4516308010
Epoch:  2000  |  train loss: 0.4477958262
Processing class: 37
Using PCA transformation
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.9398906946
Epoch:   200  |  train loss: 0.8162936211
Epoch:   300  |  train loss: 0.7323897004
Epoch:   400  |  train loss: 0.6709560394
Epoch:   500  |  train loss: 0.6262017727
Epoch:   600  |  train loss: 0.5928477168
Epoch:   700  |  train loss: 0.5659610271
Epoch:   800  |  train loss: 0.5449927449
Epoch:   900  |  train loss: 0.5273786068
Epoch:  1000  |  train loss: 0.5124485970
Epoch:  1100  |  train loss: 0.5002243817
Epoch:  1200  |  train loss: 0.4896446705
Epoch:  1300  |  train loss: 0.4803909421
Epoch:  1400  |  train loss: 0.4725119174
Epoch:  1500  |  train loss: 0.4655374348
Epoch:  1600  |  train loss: 0.4593284786
Epoch:  1700  |  train loss: 0.4547198415
Epoch:  1800  |  train loss: 0.4496174812
Epoch:  1900  |  train loss: 0.4456439674
Epoch:  2000  |  train loss: 0.4418622553
Processing class: 38
Using PCA transformation
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.9346743703
Epoch:   200  |  train loss: 0.8153036594
Epoch:   300  |  train loss: 0.7312685013
Epoch:   400  |  train loss: 0.6708602428
Epoch:   500  |  train loss: 0.6263720870
Epoch:   600  |  train loss: 0.5924578905
Epoch:   700  |  train loss: 0.5659586191
Epoch:   800  |  train loss: 0.5445257306
Epoch:   900  |  train loss: 0.5266314983
Epoch:  1000  |  train loss: 0.5123048544
Epoch:  1100  |  train loss: 0.4998982072
Epoch:  1200  |  train loss: 0.4893660903
Epoch:  1300  |  train loss: 0.4806420505
Epoch:  1400  |  train loss: 0.4726231337
Epoch:  1500  |  train loss: 0.4660437882
Epoch:  1600  |  train loss: 0.4605035305
Epoch:  1700  |  train loss: 0.4555790007
Epoch:  1800  |  train loss: 0.4510242999
Epoch:  1900  |  train loss: 0.4470627248
Epoch:  2000  |  train loss: 0.4437161803
Processing class: 39
Using PCA transformation
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.9370102406
Epoch:   200  |  train loss: 0.8188030958
Epoch:   300  |  train loss: 0.7341746807
Epoch:   400  |  train loss: 0.6729540229
Epoch:   500  |  train loss: 0.6289994478
Epoch:   600  |  train loss: 0.5953017235
Epoch:   700  |  train loss: 0.5689899683
Epoch:   800  |  train loss: 0.5476124048
Epoch:   900  |  train loss: 0.5300307393
Epoch:  1000  |  train loss: 0.5157303333
Epoch:  1100  |  train loss: 0.5033819377
Epoch:  1200  |  train loss: 0.4926883042
Epoch:  1300  |  train loss: 0.4837195873
Epoch:  1400  |  train loss: 0.4761462092
Epoch:  1500  |  train loss: 0.4695013225
Epoch:  1600  |  train loss: 0.4636012018
Epoch:  1700  |  train loss: 0.4585691392
Epoch:  1800  |  train loss: 0.4540746927
Epoch:  1900  |  train loss: 0.4501312494
Epoch:  2000  |  train loss: 0.4469561458
Processing class: 40
Using PCA transformation
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.9333160758
Epoch:   200  |  train loss: 0.8057882071
Epoch:   300  |  train loss: 0.7218185544
Epoch:   400  |  train loss: 0.6637186885
Epoch:   500  |  train loss: 0.6212711453
Epoch:   600  |  train loss: 0.5887835503
Epoch:   700  |  train loss: 0.5627523780
Epoch:   800  |  train loss: 0.5420328498
Epoch:   900  |  train loss: 0.5252855182
Epoch:  1000  |  train loss: 0.5115882695
Epoch:  1100  |  train loss: 0.5000579953
Epoch:  1200  |  train loss: 0.4901476324
Epoch:  1300  |  train loss: 0.4824016452
Epoch:  1400  |  train loss: 0.4754113555
Epoch:  1500  |  train loss: 0.4697258592
Epoch:  1600  |  train loss: 0.4642297864
Epoch:  1700  |  train loss: 0.4599640489
Epoch:  1800  |  train loss: 0.4559555411
Epoch:  1900  |  train loss: 0.4527148128
Epoch:  2000  |  train loss: 0.4499281764
Processing class: 41
Using PCA transformation
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.9384933472
Epoch:   200  |  train loss: 0.8216750979
Epoch:   300  |  train loss: 0.7409268379
Epoch:   400  |  train loss: 0.6827207088
Epoch:   500  |  train loss: 0.6404311895
Epoch:   600  |  train loss: 0.6078124046
Epoch:   700  |  train loss: 0.5822969198
Epoch:   800  |  train loss: 0.5610099792
Epoch:   900  |  train loss: 0.5440411687
Epoch:  1000  |  train loss: 0.5289456606
Epoch:  1100  |  train loss: 0.5161904693
Epoch:  1200  |  train loss: 0.5052080214
Epoch:  1300  |  train loss: 0.4964301586
Epoch:  1400  |  train loss: 0.4882461071
Epoch:  1500  |  train loss: 0.4812283397
Epoch:  1600  |  train loss: 0.4756409764
Epoch:  1700  |  train loss: 0.4703103900
Epoch:  1800  |  train loss: 0.4655601144
Epoch:  1900  |  train loss: 0.4614211738
Epoch:  2000  |  train loss: 0.4581983507
Processing class: 42
Using PCA transformation
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.9357795477
Epoch:   200  |  train loss: 0.8136238694
Epoch:   300  |  train loss: 0.7306598783
Epoch:   400  |  train loss: 0.6726000428
Epoch:   500  |  train loss: 0.6289834261
Epoch:   600  |  train loss: 0.5956154346
Epoch:   700  |  train loss: 0.5694204807
Epoch:   800  |  train loss: 0.5484621286
Epoch:   900  |  train loss: 0.5316213846
Epoch:  1000  |  train loss: 0.5171016097
Epoch:  1100  |  train loss: 0.5052557349
Epoch:  1200  |  train loss: 0.4953066051
Epoch:  1300  |  train loss: 0.4865072608
Epoch:  1400  |  train loss: 0.4791666746
Epoch:  1500  |  train loss: 0.4725136340
Epoch:  1600  |  train loss: 0.4665107369
Epoch:  1700  |  train loss: 0.4614003181
Epoch:  1800  |  train loss: 0.4573635757
Epoch:  1900  |  train loss: 0.4531939149
Epoch:  2000  |  train loss: 0.4498045802
Processing class: 43
Using PCA transformation
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.9394004107
Epoch:   200  |  train loss: 0.8219165802
Epoch:   300  |  train loss: 0.7393646598
Epoch:   400  |  train loss: 0.6811063290
Epoch:   500  |  train loss: 0.6382363796
Epoch:   600  |  train loss: 0.6042182446
Epoch:   700  |  train loss: 0.5772827387
Epoch:   800  |  train loss: 0.5557718515
Epoch:   900  |  train loss: 0.5380022764
Epoch:  1000  |  train loss: 0.5223034859
Epoch:  1100  |  train loss: 0.5099115968
Epoch:  1200  |  train loss: 0.4987281680
Epoch:  1300  |  train loss: 0.4896653593
Epoch:  1400  |  train loss: 0.4811973214
Epoch:  1500  |  train loss: 0.4739338458
Epoch:  1600  |  train loss: 0.4678932011
Epoch:  1700  |  train loss: 0.4623601675
Epoch:  1800  |  train loss: 0.4574667335
Epoch:  1900  |  train loss: 0.4534162045
Epoch:  2000  |  train loss: 0.4495428979
Processing class: 44
Using PCA transformation
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.9360270739
Epoch:   200  |  train loss: 0.8232055187
Epoch:   300  |  train loss: 0.7448217034
Epoch:   400  |  train loss: 0.6885255098
Epoch:   500  |  train loss: 0.6467606664
Epoch:   600  |  train loss: 0.6146951675
Epoch:   700  |  train loss: 0.5889789939
Epoch:   800  |  train loss: 0.5681211591
Epoch:   900  |  train loss: 0.5507292747
Epoch:  1000  |  train loss: 0.5362716317
Epoch:  1100  |  train loss: 0.5236975431
Epoch:  1200  |  train loss: 0.5134678006
Epoch:  1300  |  train loss: 0.5031737089
Epoch:  1400  |  train loss: 0.4950569868
Epoch:  1500  |  train loss: 0.4880225360
Epoch:  1600  |  train loss: 0.4824102819
Epoch:  1700  |  train loss: 0.4768103361
Epoch:  1800  |  train loss: 0.4723889887
Epoch:  1900  |  train loss: 0.4684461653
Epoch:  2000  |  train loss: 0.4647294044
Processing class: 45
Using PCA transformation
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.9374861240
Epoch:   200  |  train loss: 0.8232438564
Epoch:   300  |  train loss: 0.7413848400
Epoch:   400  |  train loss: 0.6837056160
Epoch:   500  |  train loss: 0.6397523999
Epoch:   600  |  train loss: 0.6063561320
Epoch:   700  |  train loss: 0.5804868579
Epoch:   800  |  train loss: 0.5591184378
Epoch:   900  |  train loss: 0.5415836453
Epoch:  1000  |  train loss: 0.5266671777
Epoch:  1100  |  train loss: 0.5145326376
Epoch:  1200  |  train loss: 0.5040283740
Epoch:  1300  |  train loss: 0.4946708679
Epoch:  1400  |  train loss: 0.4864655495
Epoch:  1500  |  train loss: 0.4799066007
Epoch:  1600  |  train loss: 0.4735715628
Epoch:  1700  |  train loss: 0.4685037911
Epoch:  1800  |  train loss: 0.4637497127
Epoch:  1900  |  train loss: 0.4597669065
Epoch:  2000  |  train loss: 0.4559270740
Processing class: 46
Using PCA transformation
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.9394253731
Epoch:   200  |  train loss: 0.8158344507
Epoch:   300  |  train loss: 0.7310076356
Epoch:   400  |  train loss: 0.6715921760
Epoch:   500  |  train loss: 0.6286352396
Epoch:   600  |  train loss: 0.5961847067
Epoch:   700  |  train loss: 0.5703030825
Epoch:   800  |  train loss: 0.5498082876
Epoch:   900  |  train loss: 0.5326827407
Epoch:  1000  |  train loss: 0.5183353543
Epoch:  1100  |  train loss: 0.5064214110
Epoch:  1200  |  train loss: 0.4963017642
Epoch:  1300  |  train loss: 0.4872911036
Epoch:  1400  |  train loss: 0.4796118557
Epoch:  1500  |  train loss: 0.4730163872
Epoch:  1600  |  train loss: 0.4678111434
Epoch:  1700  |  train loss: 0.4623988688
Epoch:  1800  |  train loss: 0.4583074212
Epoch:  1900  |  train loss: 0.4545214772
Epoch:  2000  |  train loss: 0.4511083186
Processing class: 47
Using PCA transformation
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.9387146950
Epoch:   200  |  train loss: 0.8184117556
Epoch:   300  |  train loss: 0.7383317471
Epoch:   400  |  train loss: 0.6811999083
Epoch:   500  |  train loss: 0.6387289762
Epoch:   600  |  train loss: 0.6057381392
Epoch:   700  |  train loss: 0.5793824434
Epoch:   800  |  train loss: 0.5585736036
Epoch:   900  |  train loss: 0.5409437895
Epoch:  1000  |  train loss: 0.5264840603
Epoch:  1100  |  train loss: 0.5146280348
Epoch:  1200  |  train loss: 0.5030496359
Epoch:  1300  |  train loss: 0.4937136471
Epoch:  1400  |  train loss: 0.4859823227
Epoch:  1500  |  train loss: 0.4787292778
Epoch:  1600  |  train loss: 0.4729606807
Epoch:  1700  |  train loss: 0.4677392185
Epoch:  1800  |  train loss: 0.4634649098
Epoch:  1900  |  train loss: 0.4594420671
Epoch:  2000  |  train loss: 0.4558415651
Processing class: 48
Using PCA transformation
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.9424622416
Epoch:   200  |  train loss: 0.8263375401
Epoch:   300  |  train loss: 0.7441180110
Epoch:   400  |  train loss: 0.6840237260
Epoch:   500  |  train loss: 0.6398627043
Epoch:   600  |  train loss: 0.6054740071
Epoch:   700  |  train loss: 0.5789909244
Epoch:   800  |  train loss: 0.5573110819
Epoch:   900  |  train loss: 0.5390916467
Epoch:  1000  |  train loss: 0.5242738605
Epoch:  1100  |  train loss: 0.5114549756
Epoch:  1200  |  train loss: 0.5009256423
Epoch:  1300  |  train loss: 0.4916548610
Epoch:  1400  |  train loss: 0.4833986878
Epoch:  1500  |  train loss: 0.4767193854
Epoch:  1600  |  train loss: 0.4703413129
Epoch:  1700  |  train loss: 0.4649576426
Epoch:  1800  |  train loss: 0.4604194164
Epoch:  1900  |  train loss: 0.4563461900
Epoch:  2000  |  train loss: 0.4523742676
Processing class: 49
Using PCA transformation
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.9431255579
Epoch:   200  |  train loss: 0.8259231687
Epoch:   300  |  train loss: 0.7428421855
Epoch:   400  |  train loss: 0.6837603569
Epoch:   500  |  train loss: 0.6403415918
Epoch:   600  |  train loss: 0.6063581228
Epoch:   700  |  train loss: 0.5800873280
Epoch:   800  |  train loss: 0.5580512643
Epoch:   900  |  train loss: 0.5402886629
Epoch:  1000  |  train loss: 0.5251046658
Epoch:  1100  |  train loss: 0.5118601978
Epoch:  1200  |  train loss: 0.5010884404
Epoch:  1300  |  train loss: 0.4915723264
Epoch:  1400  |  train loss: 0.4829773068
Epoch:  1500  |  train loss: 0.4760914743
Epoch:  1600  |  train loss: 0.4699690878
Epoch:  1700  |  train loss: 0.4642123163
Epoch:  1800  |  train loss: 0.4587461829
Epoch:  1900  |  train loss: 0.4544996500
Epoch:  2000  |  train loss: 0.4501499116
Clasifying using reconstruction function cost
2024-03-18 07:45:46,539 [trainer.py] => CNN: {'total': 83.44, '00-09': 87.7, '10-19': 79.6, '20-29': 84.9, '30-39': 81.0, '40-49': 84.0, 'old': 0, 'new': 83.44}
2024-03-18 07:45:47,960 [trainer.py] => No NME accuracy
2024-03-18 07:45:47,960 [trainer.py] => FeCAM: {'total': 81.6, '00-09': 86.0, '10-19': 78.4, '20-29': 82.6, '30-39': 80.5, '40-49': 80.5, 'old': 0, 'new': 81.6}
2024-03-18 07:45:47,961 [trainer.py] => CNN top1 curve: [83.44]
2024-03-18 07:45:47,961 [trainer.py] => CNN top5 curve: [96.5]
2024-03-18 07:45:47,961 [trainer.py] => FeCAM top1 curve: [81.6]
2024-03-18 07:45:47,961 [trainer.py] => FeCAM top5 curve: [94.7]

2024-03-18 07:45:48,001 [fecam.py] => Learning on 50-60
/home/z1165703/miniconda3/lib/python3.11/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
/home/z1165703/miniconda3/lib/python3.11/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
/home/z1165703/miniconda3/lib/python3.11/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
Processing class: 50
Using PCA transformation
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.9396102667
Epoch:   200  |  train loss: 0.8220615864
Epoch:   300  |  train loss: 0.7408453703
Epoch:   400  |  train loss: 0.6824219108
Epoch:   500  |  train loss: 0.6392522454
Epoch:   600  |  train loss: 0.6059741259
Epoch:   700  |  train loss: 0.5796329618
Epoch:   800  |  train loss: 0.5585974932
Epoch:   900  |  train loss: 0.5412767768
Epoch:  1000  |  train loss: 0.5262219787
Epoch:  1100  |  train loss: 0.5139819026
Epoch:  1200  |  train loss: 0.5033297420
Epoch:  1300  |  train loss: 0.4941171527
Epoch:  1400  |  train loss: 0.4860020280
Epoch:  1500  |  train loss: 0.4792557299
Epoch:  1600  |  train loss: 0.4732391834
Epoch:  1700  |  train loss: 0.4679684281
Epoch:  1800  |  train loss: 0.4629172027
Epoch:  1900  |  train loss: 0.4589073956
Epoch:  2000  |  train loss: 0.4552557528
Processing class: 51
Using PCA transformation
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.9386927128
Epoch:   200  |  train loss: 0.8204961896
Epoch:   300  |  train loss: 0.7403427839
Epoch:   400  |  train loss: 0.6833246946
Epoch:   500  |  train loss: 0.6416476250
Epoch:   600  |  train loss: 0.6096063256
Epoch:   700  |  train loss: 0.5838805437
Epoch:   800  |  train loss: 0.5629145861
Epoch:   900  |  train loss: 0.5456053972
Epoch:  1000  |  train loss: 0.5312658310
Epoch:  1100  |  train loss: 0.5190869153
Epoch:  1200  |  train loss: 0.5088078380
Epoch:  1300  |  train loss: 0.4996560156
Epoch:  1400  |  train loss: 0.4918388486
Epoch:  1500  |  train loss: 0.4846603811
Epoch:  1600  |  train loss: 0.4792953908
Epoch:  1700  |  train loss: 0.4742139757
Epoch:  1800  |  train loss: 0.4695431054
Epoch:  1900  |  train loss: 0.4657040775
Epoch:  2000  |  train loss: 0.4615660548
Processing class: 52
Using PCA transformation
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.9366292596
Epoch:   200  |  train loss: 0.8172854304
Epoch:   300  |  train loss: 0.7314505577
Epoch:   400  |  train loss: 0.6725305796
Epoch:   500  |  train loss: 0.6293948054
Epoch:   600  |  train loss: 0.5963899136
Epoch:   700  |  train loss: 0.5702682853
Epoch:   800  |  train loss: 0.5492180347
Epoch:   900  |  train loss: 0.5318892717
Epoch:  1000  |  train loss: 0.5172949553
Epoch:  1100  |  train loss: 0.5052609146
Epoch:  1200  |  train loss: 0.4948320806
Epoch:  1300  |  train loss: 0.4859170794
Epoch:  1400  |  train loss: 0.4783590913
Epoch:  1500  |  train loss: 0.4717114329
Epoch:  1600  |  train loss: 0.4660520136
Epoch:  1700  |  train loss: 0.4611508906
Epoch:  1800  |  train loss: 0.4564693272
Epoch:  1900  |  train loss: 0.4527937293
Epoch:  2000  |  train loss: 0.4493614793
Processing class: 53
Using PCA transformation
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.9340303063
Epoch:   200  |  train loss: 0.8060567379
Epoch:   300  |  train loss: 0.7202555180
Epoch:   400  |  train loss: 0.6600560069
Epoch:   500  |  train loss: 0.6165888429
Epoch:   600  |  train loss: 0.5830557346
Epoch:   700  |  train loss: 0.5572436094
Epoch:   800  |  train loss: 0.5359153748
Epoch:   900  |  train loss: 0.5189552367
Epoch:  1000  |  train loss: 0.5045722067
Epoch:  1100  |  train loss: 0.4925558805
Epoch:  1200  |  train loss: 0.4819913685
Epoch:  1300  |  train loss: 0.4734320998
Epoch:  1400  |  train loss: 0.4658338547
Epoch:  1500  |  train loss: 0.4595771015
Epoch:  1600  |  train loss: 0.4541515231
Epoch:  1700  |  train loss: 0.4494368255
Epoch:  1800  |  train loss: 0.4450756788
Epoch:  1900  |  train loss: 0.4412057877
Epoch:  2000  |  train loss: 0.4381301522
Processing class: 54
Using PCA transformation
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.9362866879
Epoch:   200  |  train loss: 0.8079698443
Epoch:   300  |  train loss: 0.7241166234
Epoch:   400  |  train loss: 0.6664670110
Epoch:   500  |  train loss: 0.6240970373
Epoch:   600  |  train loss: 0.5909744859
Epoch:   700  |  train loss: 0.5651374698
Epoch:   800  |  train loss: 0.5440624237
Epoch:   900  |  train loss: 0.5267686009
Epoch:  1000  |  train loss: 0.5122126102
Epoch:  1100  |  train loss: 0.5000380337
Epoch:  1200  |  train loss: 0.4898212016
Epoch:  1300  |  train loss: 0.4807765126
Epoch:  1400  |  train loss: 0.4733645558
Epoch:  1500  |  train loss: 0.4669105053
Epoch:  1600  |  train loss: 0.4608865440
Epoch:  1700  |  train loss: 0.4558027864
Epoch:  1800  |  train loss: 0.4519498169
Epoch:  1900  |  train loss: 0.4478930652
Epoch:  2000  |  train loss: 0.4442148089
Processing class: 55
Using PCA transformation
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.9381926894
Epoch:   200  |  train loss: 0.8196711898
Epoch:   300  |  train loss: 0.7392522812
Epoch:   400  |  train loss: 0.6822600722
Epoch:   500  |  train loss: 0.6399366975
Epoch:   600  |  train loss: 0.6069770575
Epoch:   700  |  train loss: 0.5812489986
Epoch:   800  |  train loss: 0.5601839185
Epoch:   900  |  train loss: 0.5427268624
Epoch:  1000  |  train loss: 0.5282807469
Epoch:  1100  |  train loss: 0.5159617186
Epoch:  1200  |  train loss: 0.5052584887
Epoch:  1300  |  train loss: 0.4962684035
Epoch:  1400  |  train loss: 0.4882561088
Epoch:  1500  |  train loss: 0.4815633059
Epoch:  1600  |  train loss: 0.4752643347
Epoch:  1700  |  train loss: 0.4701173425
Epoch:  1800  |  train loss: 0.4652132511
Epoch:  1900  |  train loss: 0.4610306740
Epoch:  2000  |  train loss: 0.4577007473
Processing class: 56
Using PCA transformation
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.9408121347
Epoch:   200  |  train loss: 0.8240305185
Epoch:   300  |  train loss: 0.7411131501
Epoch:   400  |  train loss: 0.6817713141
Epoch:   500  |  train loss: 0.6380155444
Epoch:   600  |  train loss: 0.6026804447
Epoch:   700  |  train loss: 0.5762101531
Epoch:   800  |  train loss: 0.5552179217
Epoch:   900  |  train loss: 0.5377527714
Epoch:  1000  |  train loss: 0.5232993484
Epoch:  1100  |  train loss: 0.5117192209
Epoch:  1200  |  train loss: 0.5012796462
Epoch:  1300  |  train loss: 0.4925605774
Epoch:  1400  |  train loss: 0.4852029622
Epoch:  1500  |  train loss: 0.4788454890
Epoch:  1600  |  train loss: 0.4732101619
Epoch:  1700  |  train loss: 0.4684445381
Epoch:  1800  |  train loss: 0.4640898407
Epoch:  1900  |  train loss: 0.4602944314
Epoch:  2000  |  train loss: 0.4568137705
Processing class: 57
Using PCA transformation
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.9388887525
Epoch:   200  |  train loss: 0.8215437651
Epoch:   300  |  train loss: 0.7370725751
Epoch:   400  |  train loss: 0.6777439833
Epoch:   500  |  train loss: 0.6349629998
Epoch:   600  |  train loss: 0.6023067951
Epoch:   700  |  train loss: 0.5769763231
Epoch:   800  |  train loss: 0.5549786448
Epoch:   900  |  train loss: 0.5378440261
Epoch:  1000  |  train loss: 0.5235267639
Epoch:  1100  |  train loss: 0.5113588989
Epoch:  1200  |  train loss: 0.5010598958
Epoch:  1300  |  train loss: 0.4920897424
Epoch:  1400  |  train loss: 0.4847658753
Epoch:  1500  |  train loss: 0.4780868351
Epoch:  1600  |  train loss: 0.4724272728
Epoch:  1700  |  train loss: 0.4674915254
Epoch:  1800  |  train loss: 0.4631241024
Epoch:  1900  |  train loss: 0.4590994000
Epoch:  2000  |  train loss: 0.4553202868
Processing class: 58
Using PCA transformation
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.9372586489
Epoch:   200  |  train loss: 0.8167970896
Epoch:   300  |  train loss: 0.7332763314
Epoch:   400  |  train loss: 0.6745884895
Epoch:   500  |  train loss: 0.6310853720
Epoch:   600  |  train loss: 0.5977425337
Epoch:   700  |  train loss: 0.5708492875
Epoch:   800  |  train loss: 0.5490725875
Epoch:   900  |  train loss: 0.5311204314
Epoch:  1000  |  train loss: 0.5162285805
Epoch:  1100  |  train loss: 0.5032518744
Epoch:  1200  |  train loss: 0.4916700780
Epoch:  1300  |  train loss: 0.4821048319
Epoch:  1400  |  train loss: 0.4738830805
Epoch:  1500  |  train loss: 0.4669119775
Epoch:  1600  |  train loss: 0.4601588309
Epoch:  1700  |  train loss: 0.4554576874
Epoch:  1800  |  train loss: 0.4505566299
Epoch:  1900  |  train loss: 0.4463869333
Epoch:  2000  |  train loss: 0.4426536500
Processing class: 59
Using PCA transformation
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.9377168179
Epoch:   200  |  train loss: 0.8205791712
Epoch:   300  |  train loss: 0.7390548110
Epoch:   400  |  train loss: 0.6807896256
Epoch:   500  |  train loss: 0.6379993439
Epoch:   600  |  train loss: 0.6059053183
Epoch:   700  |  train loss: 0.5806375623
Epoch:   800  |  train loss: 0.5601090074
Epoch:   900  |  train loss: 0.5432453513
Epoch:  1000  |  train loss: 0.5285219073
Epoch:  1100  |  train loss: 0.5168722034
Epoch:  1200  |  train loss: 0.5069757104
Epoch:  1300  |  train loss: 0.4978989303
Epoch:  1400  |  train loss: 0.4900076687
Epoch:  1500  |  train loss: 0.4834354162
Epoch:  1600  |  train loss: 0.4775716007
Epoch:  1700  |  train loss: 0.4725213885
Epoch:  1800  |  train loss: 0.4680913210
Epoch:  1900  |  train loss: 0.4637633979
Epoch:  2000  |  train loss: 0.4604089916
Clasifying using reconstruction function cost
2024-03-18 08:00:29,554 [trainer.py] => CNN: {'total': 71.75, '00-09': 82.2, '10-19': 72.8, '20-29': 78.5, '30-39': 74.9, '40-49': 68.4, '50-59': 53.7, 'old': 75.36, 'new': 53.7}
2024-03-18 08:00:29,555 [trainer.py] => No NME accuracy
2024-03-18 08:00:29,555 [trainer.py] => FeCAM: {'total': 67.95, '00-09': 79.9, '10-19': 65.8, '20-29': 73.6, '30-39': 69.8, '40-49': 68.8, '50-59': 49.8, 'old': 71.58, 'new': 49.8}
2024-03-18 08:00:29,555 [trainer.py] => CNN top1 curve: [83.44, 71.75]
2024-03-18 08:00:29,556 [trainer.py] => CNN top5 curve: [96.5, 89.6]
2024-03-18 08:00:29,556 [trainer.py] => FeCAM top1 curve: [81.6, 67.95]
2024-03-18 08:00:29,556 [trainer.py] => FeCAM top5 curve: [94.7, 89.45]

2024-03-18 08:00:29,568 [fecam.py] => Learning on 60-70
/home/z1165703/miniconda3/lib/python3.11/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
/home/z1165703/miniconda3/lib/python3.11/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
/home/z1165703/miniconda3/lib/python3.11/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
Processing class: 60
Using PCA transformation
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.9442276478
Epoch:   200  |  train loss: 0.8326823950
Epoch:   300  |  train loss: 0.7524154544
Epoch:   400  |  train loss: 0.6924304962
Epoch:   500  |  train loss: 0.6481134415
Epoch:   600  |  train loss: 0.6140391946
Epoch:   700  |  train loss: 0.5867395282
Epoch:   800  |  train loss: 0.5648368955
Epoch:   900  |  train loss: 0.5464125156
Epoch:  1000  |  train loss: 0.5311499238
Epoch:  1100  |  train loss: 0.5176723242
Epoch:  1200  |  train loss: 0.5062047362
Epoch:  1300  |  train loss: 0.4964469314
Epoch:  1400  |  train loss: 0.4882387698
Epoch:  1500  |  train loss: 0.4806943715
Epoch:  1600  |  train loss: 0.4742436707
Epoch:  1700  |  train loss: 0.4685335040
Epoch:  1800  |  train loss: 0.4636022151
Epoch:  1900  |  train loss: 0.4588430941
Epoch:  2000  |  train loss: 0.4550484002
Processing class: 61
Using PCA transformation
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.9365435839
Epoch:   200  |  train loss: 0.8215665817
Epoch:   300  |  train loss: 0.7398678422
Epoch:   400  |  train loss: 0.6818585634
Epoch:   500  |  train loss: 0.6377015471
Epoch:   600  |  train loss: 0.6029854655
Epoch:   700  |  train loss: 0.5755918145
Epoch:   800  |  train loss: 0.5538657427
Epoch:   900  |  train loss: 0.5355392694
Epoch:  1000  |  train loss: 0.5208489180
Epoch:  1100  |  train loss: 0.5076520324
Epoch:  1200  |  train loss: 0.4971952975
Epoch:  1300  |  train loss: 0.4878569067
Epoch:  1400  |  train loss: 0.4799903095
Epoch:  1500  |  train loss: 0.4724725544
Epoch:  1600  |  train loss: 0.4665837109
Epoch:  1700  |  train loss: 0.4612954378
Epoch:  1800  |  train loss: 0.4569176435
Epoch:  1900  |  train loss: 0.4524504304
Epoch:  2000  |  train loss: 0.4488227546
Processing class: 62
Using PCA transformation
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.9373661160
Epoch:   200  |  train loss: 0.8174461603
Epoch:   300  |  train loss: 0.7328454256
Epoch:   400  |  train loss: 0.6726180673
Epoch:   500  |  train loss: 0.6283254385
Epoch:   600  |  train loss: 0.5948627591
Epoch:   700  |  train loss: 0.5680567265
Epoch:   800  |  train loss: 0.5469640374
Epoch:   900  |  train loss: 0.5295848131
Epoch:  1000  |  train loss: 0.5152375102
Epoch:  1100  |  train loss: 0.5031172156
Epoch:  1200  |  train loss: 0.4926610827
Epoch:  1300  |  train loss: 0.4833580256
Epoch:  1400  |  train loss: 0.4756511331
Epoch:  1500  |  train loss: 0.4687239766
Epoch:  1600  |  train loss: 0.4632977366
Epoch:  1700  |  train loss: 0.4582751036
Epoch:  1800  |  train loss: 0.4544106424
Epoch:  1900  |  train loss: 0.4502727211
Epoch:  2000  |  train loss: 0.4471552372
Processing class: 63
Using PCA transformation
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.9363051295
Epoch:   200  |  train loss: 0.8147522807
Epoch:   300  |  train loss: 0.7307143092
Epoch:   400  |  train loss: 0.6712908983
Epoch:   500  |  train loss: 0.6275162101
Epoch:   600  |  train loss: 0.5938483238
Epoch:   700  |  train loss: 0.5672335863
Epoch:   800  |  train loss: 0.5459022999
Epoch:   900  |  train loss: 0.5281282544
Epoch:  1000  |  train loss: 0.5129472971
Epoch:  1100  |  train loss: 0.5010670066
Epoch:  1200  |  train loss: 0.4903287530
Epoch:  1300  |  train loss: 0.4816461205
Epoch:  1400  |  train loss: 0.4738504529
Epoch:  1500  |  train loss: 0.4671974540
Epoch:  1600  |  train loss: 0.4614524603
Epoch:  1700  |  train loss: 0.4567177236
Epoch:  1800  |  train loss: 0.4521261573
Epoch:  1900  |  train loss: 0.4481298864
Epoch:  2000  |  train loss: 0.4445169389
Processing class: 64
Using PCA transformation
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.9387371898
Epoch:   200  |  train loss: 0.8210011840
Epoch:   300  |  train loss: 0.7353202105
Epoch:   400  |  train loss: 0.6753107071
Epoch:   500  |  train loss: 0.6313416719
Epoch:   600  |  train loss: 0.5976439357
Epoch:   700  |  train loss: 0.5709563732
Epoch:   800  |  train loss: 0.5492450833
Epoch:   900  |  train loss: 0.5320799708
Epoch:  1000  |  train loss: 0.5170466542
Epoch:  1100  |  train loss: 0.5045963764
Epoch:  1200  |  train loss: 0.4940928161
Epoch:  1300  |  train loss: 0.4849325597
Epoch:  1400  |  train loss: 0.4774231136
Epoch:  1500  |  train loss: 0.4700349569
Epoch:  1600  |  train loss: 0.4641295314
Epoch:  1700  |  train loss: 0.4589682162
Epoch:  1800  |  train loss: 0.4544194579
Epoch:  1900  |  train loss: 0.4502433181
Epoch:  2000  |  train loss: 0.4464886069
Processing class: 65
Using PCA transformation
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.9438439012
Epoch:   200  |  train loss: 0.8310590625
Epoch:   300  |  train loss: 0.7536989927
Epoch:   400  |  train loss: 0.6968171358
Epoch:   500  |  train loss: 0.6535112619
Epoch:   600  |  train loss: 0.6202275634
Epoch:   700  |  train loss: 0.5936743617
Epoch:   800  |  train loss: 0.5717174053
Epoch:   900  |  train loss: 0.5543868661
Epoch:  1000  |  train loss: 0.5395705938
Epoch:  1100  |  train loss: 0.5271105528
Epoch:  1200  |  train loss: 0.5163584113
Epoch:  1300  |  train loss: 0.5075057149
Epoch:  1400  |  train loss: 0.4993146718
Epoch:  1500  |  train loss: 0.4921600759
Epoch:  1600  |  train loss: 0.4864266694
Epoch:  1700  |  train loss: 0.4808367908
Epoch:  1800  |  train loss: 0.4759568334
Epoch:  1900  |  train loss: 0.4716077089
Epoch:  2000  |  train loss: 0.4677554429
Processing class: 66
Using PCA transformation
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.9414569497
Epoch:   200  |  train loss: 0.8188064337
Epoch:   300  |  train loss: 0.7336976051
Epoch:   400  |  train loss: 0.6742136955
Epoch:   500  |  train loss: 0.6297665358
Epoch:   600  |  train loss: 0.5960976124
Epoch:   700  |  train loss: 0.5689749479
Epoch:   800  |  train loss: 0.5474597454
Epoch:   900  |  train loss: 0.5294234633
Epoch:  1000  |  train loss: 0.5143011212
Epoch:  1100  |  train loss: 0.5016128719
Epoch:  1200  |  train loss: 0.4910839081
Epoch:  1300  |  train loss: 0.4815078139
Epoch:  1400  |  train loss: 0.4735763490
Epoch:  1500  |  train loss: 0.4666542470
Epoch:  1600  |  train loss: 0.4605981708
Epoch:  1700  |  train loss: 0.4552549362
Epoch:  1800  |  train loss: 0.4507776141
Epoch:  1900  |  train loss: 0.4467073441
Epoch:  2000  |  train loss: 0.4431356966
Processing class: 67
Using PCA transformation
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.9390630960
Epoch:   200  |  train loss: 0.8141363502
Epoch:   300  |  train loss: 0.7302862048
Epoch:   400  |  train loss: 0.6720647693
Epoch:   500  |  train loss: 0.6291856050
Epoch:   600  |  train loss: 0.5961376667
Epoch:   700  |  train loss: 0.5705389261
Epoch:   800  |  train loss: 0.5491458416
Epoch:   900  |  train loss: 0.5321377635
Epoch:  1000  |  train loss: 0.5176106215
Epoch:  1100  |  train loss: 0.5056436241
Epoch:  1200  |  train loss: 0.4952468455
Epoch:  1300  |  train loss: 0.4866502583
Epoch:  1400  |  train loss: 0.4788974226
Epoch:  1500  |  train loss: 0.4722566366
Epoch:  1600  |  train loss: 0.4665763497
Epoch:  1700  |  train loss: 0.4622040927
Epoch:  1800  |  train loss: 0.4574346781
Epoch:  1900  |  train loss: 0.4534082413
Epoch:  2000  |  train loss: 0.4500203490
Processing class: 68
Using PCA transformation
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.9303503394
Epoch:   200  |  train loss: 0.8058606029
Epoch:   300  |  train loss: 0.7200086474
Epoch:   400  |  train loss: 0.6608695388
Epoch:   500  |  train loss: 0.6187517643
Epoch:   600  |  train loss: 0.5860540390
Epoch:   700  |  train loss: 0.5610559106
Epoch:   800  |  train loss: 0.5411131501
Epoch:   900  |  train loss: 0.5242665291
Epoch:  1000  |  train loss: 0.5108294129
Epoch:  1100  |  train loss: 0.4992858291
Epoch:  1200  |  train loss: 0.4893166482
Epoch:  1300  |  train loss: 0.4811871588
Epoch:  1400  |  train loss: 0.4741819322
Epoch:  1500  |  train loss: 0.4680160701
Epoch:  1600  |  train loss: 0.4624961197
Epoch:  1700  |  train loss: 0.4582384646
Epoch:  1800  |  train loss: 0.4542595625
Epoch:  1900  |  train loss: 0.4506201327
Epoch:  2000  |  train loss: 0.4473891556
Processing class: 69
Using PCA transformation
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.9374066353
Epoch:   200  |  train loss: 0.8189541459
Epoch:   300  |  train loss: 0.7344936490
Epoch:   400  |  train loss: 0.6751148462
Epoch:   500  |  train loss: 0.6319025993
Epoch:   600  |  train loss: 0.5988548517
Epoch:   700  |  train loss: 0.5718311787
Epoch:   800  |  train loss: 0.5509432316
Epoch:   900  |  train loss: 0.5333796263
Epoch:  1000  |  train loss: 0.5188310623
Epoch:  1100  |  train loss: 0.5070898890
Epoch:  1200  |  train loss: 0.4963070691
Epoch:  1300  |  train loss: 0.4870619595
Epoch:  1400  |  train loss: 0.4790919065
Epoch:  1500  |  train loss: 0.4727215052
Epoch:  1600  |  train loss: 0.4668264747
Epoch:  1700  |  train loss: 0.4619758844
Epoch:  1800  |  train loss: 0.4576995432
Epoch:  1900  |  train loss: 0.4537478507
Epoch:  2000  |  train loss: 0.4501117408
Clasifying using reconstruction function cost
2024-03-18 08:18:13,280 [trainer.py] => CNN: {'total': 64.66, '00-09': 75.0, '10-19': 70.1, '20-29': 77.1, '30-39': 70.6, '40-49': 64.0, '50-59': 45.8, '60-69': 50.0, 'old': 67.1, 'new': 50.0}
2024-03-18 08:18:13,282 [trainer.py] => No NME accuracy
2024-03-18 08:18:13,282 [trainer.py] => FeCAM: {'total': 62.9, '00-09': 78.9, '10-19': 62.6, '20-29': 72.1, '30-39': 67.0, '40-49': 66.1, '50-59': 45.8, '60-69': 47.8, 'old': 65.42, 'new': 47.8}
2024-03-18 08:18:13,282 [trainer.py] => CNN top1 curve: [83.44, 71.75, 64.66]
2024-03-18 08:18:13,282 [trainer.py] => CNN top5 curve: [96.5, 89.6, 86.54]
2024-03-18 08:18:13,282 [trainer.py] => FeCAM top1 curve: [81.6, 67.95, 62.9]
2024-03-18 08:18:13,282 [trainer.py] => FeCAM top5 curve: [94.7, 89.45, 85.87]

2024-03-18 08:18:13,293 [fecam.py] => Learning on 70-80
/home/z1165703/miniconda3/lib/python3.11/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
/home/z1165703/miniconda3/lib/python3.11/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
/home/z1165703/miniconda3/lib/python3.11/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
Processing class: 70
Using PCA transformation
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.9366065264
Epoch:   200  |  train loss: 0.8182869434
Epoch:   300  |  train loss: 0.7343105197
Epoch:   400  |  train loss: 0.6756981492
Epoch:   500  |  train loss: 0.6331595421
Epoch:   600  |  train loss: 0.6008156776
Epoch:   700  |  train loss: 0.5750946522
Epoch:   800  |  train loss: 0.5538497329
Epoch:   900  |  train loss: 0.5369841576
Epoch:  1000  |  train loss: 0.5226234198
Epoch:  1100  |  train loss: 0.5102768779
Epoch:  1200  |  train loss: 0.4994691193
Epoch:  1300  |  train loss: 0.4903764844
Epoch:  1400  |  train loss: 0.4821985364
Epoch:  1500  |  train loss: 0.4753517807
Epoch:  1600  |  train loss: 0.4690725923
Epoch:  1700  |  train loss: 0.4641026795
Epoch:  1800  |  train loss: 0.4590459347
Epoch:  1900  |  train loss: 0.4551234603
Epoch:  2000  |  train loss: 0.4510020792
Processing class: 71
Using PCA transformation
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.9396815419
Epoch:   200  |  train loss: 0.8240905404
Epoch:   300  |  train loss: 0.7437081814
Epoch:   400  |  train loss: 0.6859160900
Epoch:   500  |  train loss: 0.6434119582
Epoch:   600  |  train loss: 0.6108023167
Epoch:   700  |  train loss: 0.5850922585
Epoch:   800  |  train loss: 0.5644135118
Epoch:   900  |  train loss: 0.5468564034
Epoch:  1000  |  train loss: 0.5322211027
Epoch:  1100  |  train loss: 0.5199930191
Epoch:  1200  |  train loss: 0.5094703257
Epoch:  1300  |  train loss: 0.4996475220
Epoch:  1400  |  train loss: 0.4917405367
Epoch:  1500  |  train loss: 0.4845852017
Epoch:  1600  |  train loss: 0.4786758006
Epoch:  1700  |  train loss: 0.4731662691
Epoch:  1800  |  train loss: 0.4684678793
Epoch:  1900  |  train loss: 0.4642901480
Epoch:  2000  |  train loss: 0.4605466366
Processing class: 72
Using PCA transformation
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.9379307151
Epoch:   200  |  train loss: 0.8232814908
Epoch:   300  |  train loss: 0.7421833873
Epoch:   400  |  train loss: 0.6845697999
Epoch:   500  |  train loss: 0.6416888118
Epoch:   600  |  train loss: 0.6088854551
Epoch:   700  |  train loss: 0.5827217579
Epoch:   800  |  train loss: 0.5617674708
Epoch:   900  |  train loss: 0.5443091273
Epoch:  1000  |  train loss: 0.5289510965
Epoch:  1100  |  train loss: 0.5161667943
Epoch:  1200  |  train loss: 0.5058538437
Epoch:  1300  |  train loss: 0.4965681195
Epoch:  1400  |  train loss: 0.4887936950
Epoch:  1500  |  train loss: 0.4816987336
Epoch:  1600  |  train loss: 0.4756115556
Epoch:  1700  |  train loss: 0.4704342842
Epoch:  1800  |  train loss: 0.4662567616
Epoch:  1900  |  train loss: 0.4618118167
Epoch:  2000  |  train loss: 0.4580964744
Processing class: 73
Using PCA transformation
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.9410280228
Epoch:   200  |  train loss: 0.8256057978
Epoch:   300  |  train loss: 0.7445701122
Epoch:   400  |  train loss: 0.6868604898
Epoch:   500  |  train loss: 0.6448213935
Epoch:   600  |  train loss: 0.6127150297
Epoch:   700  |  train loss: 0.5865992427
Epoch:   800  |  train loss: 0.5659790993
Epoch:   900  |  train loss: 0.5487566113
Epoch:  1000  |  train loss: 0.5343404531
Epoch:  1100  |  train loss: 0.5224520564
Epoch:  1200  |  train loss: 0.5117497742
Epoch:  1300  |  train loss: 0.5032754481
Epoch:  1400  |  train loss: 0.4955790162
Epoch:  1500  |  train loss: 0.4889244795
Epoch:  1600  |  train loss: 0.4831535101
Epoch:  1700  |  train loss: 0.4779812872
Epoch:  1800  |  train loss: 0.4734115839
Epoch:  1900  |  train loss: 0.4695132792
Epoch:  2000  |  train loss: 0.4660234809
Processing class: 74
Using PCA transformation
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.9406493187
Epoch:   200  |  train loss: 0.8263996959
Epoch:   300  |  train loss: 0.7460951924
Epoch:   400  |  train loss: 0.6884584904
Epoch:   500  |  train loss: 0.6452762485
Epoch:   600  |  train loss: 0.6121729612
Epoch:   700  |  train loss: 0.5855191350
Epoch:   800  |  train loss: 0.5639398932
Epoch:   900  |  train loss: 0.5458010793
Epoch:  1000  |  train loss: 0.5310066223
Epoch:  1100  |  train loss: 0.5179244995
Epoch:  1200  |  train loss: 0.5067518651
Epoch:  1300  |  train loss: 0.4974628985
Epoch:  1400  |  train loss: 0.4887317240
Epoch:  1500  |  train loss: 0.4814557672
Epoch:  1600  |  train loss: 0.4747188091
Epoch:  1700  |  train loss: 0.4682021201
Epoch:  1800  |  train loss: 0.4629593313
Epoch:  1900  |  train loss: 0.4583771646
Epoch:  2000  |  train loss: 0.4545783520
Processing class: 75
Using PCA transformation
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.9373406768
Epoch:   200  |  train loss: 0.8170635939
Epoch:   300  |  train loss: 0.7355123878
Epoch:   400  |  train loss: 0.6748855352
Epoch:   500  |  train loss: 0.6303673506
Epoch:   600  |  train loss: 0.5959850788
Epoch:   700  |  train loss: 0.5690421939
Epoch:   800  |  train loss: 0.5468412399
Epoch:   900  |  train loss: 0.5293357968
Epoch:  1000  |  train loss: 0.5144973397
Epoch:  1100  |  train loss: 0.5021729827
Epoch:  1200  |  train loss: 0.4908797801
Epoch:  1300  |  train loss: 0.4818164468
Epoch:  1400  |  train loss: 0.4737865210
Epoch:  1500  |  train loss: 0.4665169477
Epoch:  1600  |  train loss: 0.4603722155
Epoch:  1700  |  train loss: 0.4552343011
Epoch:  1800  |  train loss: 0.4503924668
Epoch:  1900  |  train loss: 0.4461347878
Epoch:  2000  |  train loss: 0.4418816745
Processing class: 76
Using PCA transformation
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.9391148925
Epoch:   200  |  train loss: 0.8177793622
Epoch:   300  |  train loss: 0.7339329839
Epoch:   400  |  train loss: 0.6752828121
Epoch:   500  |  train loss: 0.6321499825
Epoch:   600  |  train loss: 0.5986810565
Epoch:   700  |  train loss: 0.5719504952
Epoch:   800  |  train loss: 0.5503700376
Epoch:   900  |  train loss: 0.5325162768
Epoch:  1000  |  train loss: 0.5174106598
Epoch:  1100  |  train loss: 0.5047110140
Epoch:  1200  |  train loss: 0.4937339604
Epoch:  1300  |  train loss: 0.4845317960
Epoch:  1400  |  train loss: 0.4766904294
Epoch:  1500  |  train loss: 0.4700315952
Epoch:  1600  |  train loss: 0.4638817608
Epoch:  1700  |  train loss: 0.4587753654
Epoch:  1800  |  train loss: 0.4545059919
Epoch:  1900  |  train loss: 0.4501417935
Epoch:  2000  |  train loss: 0.4467999816
Processing class: 77
Using PCA transformation
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.9380771995
Epoch:   200  |  train loss: 0.8214151740
Epoch:   300  |  train loss: 0.7371984124
Epoch:   400  |  train loss: 0.6780851245
Epoch:   500  |  train loss: 0.6345766902
Epoch:   600  |  train loss: 0.6008598804
Epoch:   700  |  train loss: 0.5744621038
Epoch:   800  |  train loss: 0.5527148366
Epoch:   900  |  train loss: 0.5344622850
Epoch:  1000  |  train loss: 0.5186491132
Epoch:  1100  |  train loss: 0.5059600472
Epoch:  1200  |  train loss: 0.4952045023
Epoch:  1300  |  train loss: 0.4860108435
Epoch:  1400  |  train loss: 0.4774734735
Epoch:  1500  |  train loss: 0.4710029006
Epoch:  1600  |  train loss: 0.4647318780
Epoch:  1700  |  train loss: 0.4592943013
Epoch:  1800  |  train loss: 0.4543135762
Epoch:  1900  |  train loss: 0.4503767133
Epoch:  2000  |  train loss: 0.4465983570
Processing class: 78
Using PCA transformation
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.9462399483
Epoch:   200  |  train loss: 0.8409049630
Epoch:   300  |  train loss: 0.7627600908
Epoch:   400  |  train loss: 0.7048563361
Epoch:   500  |  train loss: 0.6615536690
Epoch:   600  |  train loss: 0.6275213242
Epoch:   700  |  train loss: 0.6005659223
Epoch:   800  |  train loss: 0.5786687016
Epoch:   900  |  train loss: 0.5598545313
Epoch:  1000  |  train loss: 0.5440512180
Epoch:  1100  |  train loss: 0.5303416729
Epoch:  1200  |  train loss: 0.5179680109
Epoch:  1300  |  train loss: 0.5076845825
Epoch:  1400  |  train loss: 0.4988639534
Epoch:  1500  |  train loss: 0.4904682517
Epoch:  1600  |  train loss: 0.4833328307
Epoch:  1700  |  train loss: 0.4774997592
Epoch:  1800  |  train loss: 0.4716418624
Epoch:  1900  |  train loss: 0.4668813646
Epoch:  2000  |  train loss: 0.4619552195
Processing class: 79
Using PCA transformation
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.9382014155
Epoch:   200  |  train loss: 0.8211893320
Epoch:   300  |  train loss: 0.7363227606
Epoch:   400  |  train loss: 0.6768685937
Epoch:   500  |  train loss: 0.6333734632
Epoch:   600  |  train loss: 0.5992232800
Epoch:   700  |  train loss: 0.5715556860
Epoch:   800  |  train loss: 0.5499791384
Epoch:   900  |  train loss: 0.5326374650
Epoch:  1000  |  train loss: 0.5178561330
Epoch:  1100  |  train loss: 0.5058940113
Epoch:  1200  |  train loss: 0.4955166936
Epoch:  1300  |  train loss: 0.4867996156
Epoch:  1400  |  train loss: 0.4790709734
Epoch:  1500  |  train loss: 0.4720808804
Epoch:  1600  |  train loss: 0.4662709355
Epoch:  1700  |  train loss: 0.4614932477
Epoch:  1800  |  train loss: 0.4570391178
Epoch:  1900  |  train loss: 0.4528205097
Epoch:  2000  |  train loss: 0.4495144725
Clasifying using reconstruction function cost
2024-03-18 08:39:40,988 [trainer.py] => CNN: {'total': 59.18, '00-09': 73.6, '10-19': 68.4, '20-29': 76.9, '30-39': 69.1, '40-49': 60.6, '50-59': 37.2, '60-69': 45.7, '70-79': 41.9, 'old': 61.64, 'new': 41.9}
2024-03-18 08:39:41,003 [trainer.py] => No NME accuracy
2024-03-18 08:39:41,003 [trainer.py] => FeCAM: {'total': 57.91, '00-09': 76.3, '10-19': 61.2, '20-29': 70.9, '30-39': 64.6, '40-49': 62.0, '50-59': 38.3, '60-69': 43.3, '70-79': 46.7, 'old': 59.51, 'new': 46.7}
2024-03-18 08:39:41,003 [trainer.py] => CNN top1 curve: [83.44, 71.75, 64.66, 59.18]
2024-03-18 08:39:41,003 [trainer.py] => CNN top5 curve: [96.5, 89.6, 86.54, 84.09]
2024-03-18 08:39:41,003 [trainer.py] => FeCAM top1 curve: [81.6, 67.95, 62.9, 57.91]
2024-03-18 08:39:41,003 [trainer.py] => FeCAM top5 curve: [94.7, 89.45, 85.87, 82.32]

2024-03-18 08:39:41,042 [fecam.py] => Learning on 80-90
/home/z1165703/miniconda3/lib/python3.11/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
/home/z1165703/miniconda3/lib/python3.11/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
/home/z1165703/miniconda3/lib/python3.11/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
Processing class: 80
Using PCA transformation
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.9362696409
Epoch:   200  |  train loss: 0.8198659897
Epoch:   300  |  train loss: 0.7365443707
Epoch:   400  |  train loss: 0.6787820220
Epoch:   500  |  train loss: 0.6368372202
Epoch:   600  |  train loss: 0.6044793844
Epoch:   700  |  train loss: 0.5786792159
Epoch:   800  |  train loss: 0.5580310345
Epoch:   900  |  train loss: 0.5412308693
Epoch:  1000  |  train loss: 0.5271846652
Epoch:  1100  |  train loss: 0.5150968075
Epoch:  1200  |  train loss: 0.5048583627
Epoch:  1300  |  train loss: 0.4960984170
Epoch:  1400  |  train loss: 0.4883791864
Epoch:  1500  |  train loss: 0.4820365012
Epoch:  1600  |  train loss: 0.4761818826
Epoch:  1700  |  train loss: 0.4710329592
Epoch:  1800  |  train loss: 0.4663889170
Epoch:  1900  |  train loss: 0.4623453259
Epoch:  2000  |  train loss: 0.4589379847
Processing class: 81
Using PCA transformation
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.9399259090
Epoch:   200  |  train loss: 0.8245983720
Epoch:   300  |  train loss: 0.7402028561
Epoch:   400  |  train loss: 0.6805359364
Epoch:   500  |  train loss: 0.6370792031
Epoch:   600  |  train loss: 0.6040684104
Epoch:   700  |  train loss: 0.5772752643
Epoch:   800  |  train loss: 0.5560398698
Epoch:   900  |  train loss: 0.5383659482
Epoch:  1000  |  train loss: 0.5232658505
Epoch:  1100  |  train loss: 0.5106228292
Epoch:  1200  |  train loss: 0.4999571085
Epoch:  1300  |  train loss: 0.4906829834
Epoch:  1400  |  train loss: 0.4824388921
Epoch:  1500  |  train loss: 0.4755799174
Epoch:  1600  |  train loss: 0.4691028118
Epoch:  1700  |  train loss: 0.4637913406
Epoch:  1800  |  train loss: 0.4587158680
Epoch:  1900  |  train loss: 0.4545683265
Epoch:  2000  |  train loss: 0.4507690728
Processing class: 82
Using PCA transformation
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.9388836503
Epoch:   200  |  train loss: 0.8211003780
Epoch:   300  |  train loss: 0.7376908064
Epoch:   400  |  train loss: 0.6783506632
Epoch:   500  |  train loss: 0.6354026794
Epoch:   600  |  train loss: 0.6026375413
Epoch:   700  |  train loss: 0.5773580194
Epoch:   800  |  train loss: 0.5562815189
Epoch:   900  |  train loss: 0.5393698692
Epoch:  1000  |  train loss: 0.5252042770
Epoch:  1100  |  train loss: 0.5127995908
Epoch:  1200  |  train loss: 0.5026095450
Epoch:  1300  |  train loss: 0.4937884510
Epoch:  1400  |  train loss: 0.4860714018
Epoch:  1500  |  train loss: 0.4797056139
Epoch:  1600  |  train loss: 0.4742096484
Epoch:  1700  |  train loss: 0.4694098473
Epoch:  1800  |  train loss: 0.4646661639
Epoch:  1900  |  train loss: 0.4607956946
Epoch:  2000  |  train loss: 0.4574116170
Processing class: 83
Using PCA transformation
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.9385737896
Epoch:   200  |  train loss: 0.8169380426
Epoch:   300  |  train loss: 0.7346163034
Epoch:   400  |  train loss: 0.6767015934
Epoch:   500  |  train loss: 0.6337628722
Epoch:   600  |  train loss: 0.6010264635
Epoch:   700  |  train loss: 0.5747496605
Epoch:   800  |  train loss: 0.5538241625
Epoch:   900  |  train loss: 0.5362896204
Epoch:  1000  |  train loss: 0.5213300228
Epoch:  1100  |  train loss: 0.5095328391
Epoch:  1200  |  train loss: 0.4993254602
Epoch:  1300  |  train loss: 0.4900833488
Epoch:  1400  |  train loss: 0.4823862910
Epoch:  1500  |  train loss: 0.4757130623
Epoch:  1600  |  train loss: 0.4701871395
Epoch:  1700  |  train loss: 0.4645273268
Epoch:  1800  |  train loss: 0.4600936770
Epoch:  1900  |  train loss: 0.4559930682
Epoch:  2000  |  train loss: 0.4521727026
Processing class: 84
Using PCA transformation
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.9422269225
Epoch:   200  |  train loss: 0.8266432285
Epoch:   300  |  train loss: 0.7449436188
Epoch:   400  |  train loss: 0.6858824253
Epoch:   500  |  train loss: 0.6426422119
Epoch:   600  |  train loss: 0.6093657613
Epoch:   700  |  train loss: 0.5826322794
Epoch:   800  |  train loss: 0.5611087441
Epoch:   900  |  train loss: 0.5431501031
Epoch:  1000  |  train loss: 0.5277904868
Epoch:  1100  |  train loss: 0.5151998281
Epoch:  1200  |  train loss: 0.5038526177
Epoch:  1300  |  train loss: 0.4940052629
Epoch:  1400  |  train loss: 0.4858846843
Epoch:  1500  |  train loss: 0.4784465671
Epoch:  1600  |  train loss: 0.4718134582
Epoch:  1700  |  train loss: 0.4662611067
Epoch:  1800  |  train loss: 0.4611191750
Epoch:  1900  |  train loss: 0.4569391847
Epoch:  2000  |  train loss: 0.4526895225
Processing class: 85
Using PCA transformation
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.9348735929
Epoch:   200  |  train loss: 0.8141461611
Epoch:   300  |  train loss: 0.7326038718
Epoch:   400  |  train loss: 0.6740131736
Epoch:   500  |  train loss: 0.6304943204
Epoch:   600  |  train loss: 0.5971970677
Epoch:   700  |  train loss: 0.5713454723
Epoch:   800  |  train loss: 0.5501150131
Epoch:   900  |  train loss: 0.5321761847
Epoch:  1000  |  train loss: 0.5175621152
Epoch:  1100  |  train loss: 0.5055536211
Epoch:  1200  |  train loss: 0.4944123030
Epoch:  1300  |  train loss: 0.4855451584
Epoch:  1400  |  train loss: 0.4777732849
Epoch:  1500  |  train loss: 0.4712821543
Epoch:  1600  |  train loss: 0.4651902616
Epoch:  1700  |  train loss: 0.4603436232
Epoch:  1800  |  train loss: 0.4553711712
Epoch:  1900  |  train loss: 0.4515154123
Epoch:  2000  |  train loss: 0.4479011774
Processing class: 86
Using PCA transformation
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.9434976697
Epoch:   200  |  train loss: 0.8308361888
Epoch:   300  |  train loss: 0.7485122204
Epoch:   400  |  train loss: 0.6892686367
Epoch:   500  |  train loss: 0.6457683444
Epoch:   600  |  train loss: 0.6127562523
Epoch:   700  |  train loss: 0.5867256045
Epoch:   800  |  train loss: 0.5653044462
Epoch:   900  |  train loss: 0.5465845823
Epoch:  1000  |  train loss: 0.5313155413
Epoch:  1100  |  train loss: 0.5184440136
Epoch:  1200  |  train loss: 0.5074994206
Epoch:  1300  |  train loss: 0.4979641020
Epoch:  1400  |  train loss: 0.4898846745
Epoch:  1500  |  train loss: 0.4830524802
Epoch:  1600  |  train loss: 0.4768498719
Epoch:  1700  |  train loss: 0.4714610577
Epoch:  1800  |  train loss: 0.4665431798
Epoch:  1900  |  train loss: 0.4621459663
Epoch:  2000  |  train loss: 0.4585474372
Processing class: 87
Using PCA transformation
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.9354006886
Epoch:   200  |  train loss: 0.8164798141
Epoch:   300  |  train loss: 0.7352388501
Epoch:   400  |  train loss: 0.6787153959
Epoch:   500  |  train loss: 0.6367563128
Epoch:   600  |  train loss: 0.6026215553
Epoch:   700  |  train loss: 0.5760589123
Epoch:   800  |  train loss: 0.5550970435
Epoch:   900  |  train loss: 0.5374539971
Epoch:  1000  |  train loss: 0.5226973057
Epoch:  1100  |  train loss: 0.5099742413
Epoch:  1200  |  train loss: 0.4993065298
Epoch:  1300  |  train loss: 0.4902305007
Epoch:  1400  |  train loss: 0.4819467247
Epoch:  1500  |  train loss: 0.4752102017
Epoch:  1600  |  train loss: 0.4690244853
Epoch:  1700  |  train loss: 0.4636512578
Epoch:  1800  |  train loss: 0.4589224458
Epoch:  1900  |  train loss: 0.4547192514
Epoch:  2000  |  train loss: 0.4509971023
Processing class: 88
Using PCA transformation
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.9407214284
Epoch:   200  |  train loss: 0.8280063272
Epoch:   300  |  train loss: 0.7445102096
Epoch:   400  |  train loss: 0.6868149281
Epoch:   500  |  train loss: 0.6436203599
Epoch:   600  |  train loss: 0.6104045153
Epoch:   700  |  train loss: 0.5843679667
Epoch:   800  |  train loss: 0.5640272021
Epoch:   900  |  train loss: 0.5466239452
Epoch:  1000  |  train loss: 0.5324307919
Epoch:  1100  |  train loss: 0.5204717278
Epoch:  1200  |  train loss: 0.5104549408
Epoch:  1300  |  train loss: 0.5016642988
Epoch:  1400  |  train loss: 0.4944103241
Epoch:  1500  |  train loss: 0.4875990033
Epoch:  1600  |  train loss: 0.4821239471
Epoch:  1700  |  train loss: 0.4773106873
Epoch:  1800  |  train loss: 0.4729177117
Epoch:  1900  |  train loss: 0.4693214238
Epoch:  2000  |  train loss: 0.4657550871
Processing class: 89
Using PCA transformation
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.9411924243
Epoch:   200  |  train loss: 0.8178484797
Epoch:   300  |  train loss: 0.7329367399
Epoch:   400  |  train loss: 0.6728892446
Epoch:   500  |  train loss: 0.6292642474
Epoch:   600  |  train loss: 0.5954987764
Epoch:   700  |  train loss: 0.5692163110
Epoch:   800  |  train loss: 0.5474129319
Epoch:   900  |  train loss: 0.5297045946
Epoch:  1000  |  train loss: 0.5150489092
Epoch:  1100  |  train loss: 0.5020642102
Epoch:  1200  |  train loss: 0.4917169929
Epoch:  1300  |  train loss: 0.4828261375
Epoch:  1400  |  train loss: 0.4751568437
Epoch:  1500  |  train loss: 0.4685966969
Epoch:  1600  |  train loss: 0.4623099327
Epoch:  1700  |  train loss: 0.4570701957
Epoch:  1800  |  train loss: 0.4526620090
Epoch:  1900  |  train loss: 0.4485154450
Epoch:  2000  |  train loss: 0.4448421478
Clasifying using reconstruction function cost
2024-03-18 09:05:26,949 [trainer.py] => CNN: {'total': 54.08, '00-09': 67.9, '10-19': 63.6, '20-29': 73.2, '30-39': 68.8, '40-49': 57.2, '50-59': 32.6, '60-69': 40.9, '70-79': 38.1, '80-89': 44.4, 'old': 55.29, 'new': 44.4}
2024-03-18 09:05:26,951 [trainer.py] => No NME accuracy
2024-03-18 09:05:26,951 [trainer.py] => FeCAM: {'total': 53.32, '00-09': 74.3, '10-19': 57.6, '20-29': 68.1, '30-39': 63.3, '40-49': 59.4, '50-59': 33.5, '60-69': 38.7, '70-79': 42.7, '80-89': 42.3, 'old': 54.7, 'new': 42.3}
2024-03-18 09:05:26,951 [trainer.py] => CNN top1 curve: [83.44, 71.75, 64.66, 59.18, 54.08]
2024-03-18 09:05:26,951 [trainer.py] => CNN top5 curve: [96.5, 89.6, 86.54, 84.09, 81.74]
2024-03-18 09:05:26,951 [trainer.py] => FeCAM top1 curve: [81.6, 67.95, 62.9, 57.91, 53.32]
2024-03-18 09:05:26,951 [trainer.py] => FeCAM top5 curve: [94.7, 89.45, 85.87, 82.32, 79.79]

2024-03-18 09:05:26,963 [fecam.py] => Learning on 90-100
/home/z1165703/miniconda3/lib/python3.11/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
/home/z1165703/miniconda3/lib/python3.11/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
/home/z1165703/miniconda3/lib/python3.11/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
Processing class: 90
Using PCA transformation
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.9446906090
Epoch:   200  |  train loss: 0.8245648503
Epoch:   300  |  train loss: 0.7456407070
Epoch:   400  |  train loss: 0.6887391567
Epoch:   500  |  train loss: 0.6468894958
Epoch:   600  |  train loss: 0.6148131967
Epoch:   700  |  train loss: 0.5890155673
Epoch:   800  |  train loss: 0.5681894779
Epoch:   900  |  train loss: 0.5510594606
Epoch:  1000  |  train loss: 0.5370692611
Epoch:  1100  |  train loss: 0.5244051933
Epoch:  1200  |  train loss: 0.5141023159
Epoch:  1300  |  train loss: 0.5053743303
Epoch:  1400  |  train loss: 0.4976247311
Epoch:  1500  |  train loss: 0.4909752071
Epoch:  1600  |  train loss: 0.4850783169
Epoch:  1700  |  train loss: 0.4798176050
Epoch:  1800  |  train loss: 0.4754877448
Epoch:  1900  |  train loss: 0.4716298759
Epoch:  2000  |  train loss: 0.4677310288
Processing class: 91
Using PCA transformation
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.9381816506
Epoch:   200  |  train loss: 0.8167106867
Epoch:   300  |  train loss: 0.7320971131
Epoch:   400  |  train loss: 0.6730974913
Epoch:   500  |  train loss: 0.6299244285
Epoch:   600  |  train loss: 0.5964893222
Epoch:   700  |  train loss: 0.5693233371
Epoch:   800  |  train loss: 0.5482764363
Epoch:   900  |  train loss: 0.5307026863
Epoch:  1000  |  train loss: 0.5159935236
Epoch:  1100  |  train loss: 0.5040521681
Epoch:  1200  |  train loss: 0.4932184696
Epoch:  1300  |  train loss: 0.4842230976
Epoch:  1400  |  train loss: 0.4766845524
Epoch:  1500  |  train loss: 0.4699388623
Epoch:  1600  |  train loss: 0.4635937691
Epoch:  1700  |  train loss: 0.4586301029
Epoch:  1800  |  train loss: 0.4537753165
Epoch:  1900  |  train loss: 0.4503128767
Epoch:  2000  |  train loss: 0.4462761700
Processing class: 92
Using PCA transformation
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.9382148027
Epoch:   200  |  train loss: 0.8269459963
Epoch:   300  |  train loss: 0.7470894814
Epoch:   400  |  train loss: 0.6905081034
Epoch:   500  |  train loss: 0.6479503036
Epoch:   600  |  train loss: 0.6145484328
Epoch:   700  |  train loss: 0.5880053282
Epoch:   800  |  train loss: 0.5664826989
Epoch:   900  |  train loss: 0.5484808803
Epoch:  1000  |  train loss: 0.5332879782
Epoch:  1100  |  train loss: 0.5202196836
Epoch:  1200  |  train loss: 0.5088109851
Epoch:  1300  |  train loss: 0.4988902032
Epoch:  1400  |  train loss: 0.4899250925
Epoch:  1500  |  train loss: 0.4824782491
Epoch:  1600  |  train loss: 0.4761442482
Epoch:  1700  |  train loss: 0.4701853752
Epoch:  1800  |  train loss: 0.4652174711
Epoch:  1900  |  train loss: 0.4601299405
Epoch:  2000  |  train loss: 0.4550626278
Processing class: 93
Using PCA transformation
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.9375414133
Epoch:   200  |  train loss: 0.8204512477
Epoch:   300  |  train loss: 0.7367032051
Epoch:   400  |  train loss: 0.6779930949
Epoch:   500  |  train loss: 0.6341657519
Epoch:   600  |  train loss: 0.6010710597
Epoch:   700  |  train loss: 0.5750534892
Epoch:   800  |  train loss: 0.5534824491
Epoch:   900  |  train loss: 0.5363861680
Epoch:  1000  |  train loss: 0.5218320608
Epoch:  1100  |  train loss: 0.5097517788
Epoch:  1200  |  train loss: 0.4989794970
Epoch:  1300  |  train loss: 0.4903268099
Epoch:  1400  |  train loss: 0.4825055957
Epoch:  1500  |  train loss: 0.4758830070
Epoch:  1600  |  train loss: 0.4698756337
Epoch:  1700  |  train loss: 0.4649072528
Epoch:  1800  |  train loss: 0.4604880869
Epoch:  1900  |  train loss: 0.4563784122
Epoch:  2000  |  train loss: 0.4532774270
Processing class: 94
Using PCA transformation
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.9384062171
Epoch:   200  |  train loss: 0.8192399263
Epoch:   300  |  train loss: 0.7357496977
Epoch:   400  |  train loss: 0.6773257375
Epoch:   500  |  train loss: 0.6336571813
Epoch:   600  |  train loss: 0.6002538919
Epoch:   700  |  train loss: 0.5744329810
Epoch:   800  |  train loss: 0.5529931307
Epoch:   900  |  train loss: 0.5356259584
Epoch:  1000  |  train loss: 0.5213453054
Epoch:  1100  |  train loss: 0.5090717912
Epoch:  1200  |  train loss: 0.4985991061
Epoch:  1300  |  train loss: 0.4894507587
Epoch:  1400  |  train loss: 0.4819536686
Epoch:  1500  |  train loss: 0.4753115356
Epoch:  1600  |  train loss: 0.4695535481
Epoch:  1700  |  train loss: 0.4644030035
Epoch:  1800  |  train loss: 0.4602398276
Epoch:  1900  |  train loss: 0.4564926684
Epoch:  2000  |  train loss: 0.4530663610
Processing class: 95
Using PCA transformation
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.9386196017
Epoch:   200  |  train loss: 0.8200355649
Epoch:   300  |  train loss: 0.7367396474
Epoch:   400  |  train loss: 0.6776901245
Epoch:   500  |  train loss: 0.6343627095
Epoch:   600  |  train loss: 0.6013740897
Epoch:   700  |  train loss: 0.5750416756
Epoch:   800  |  train loss: 0.5544430256
Epoch:   900  |  train loss: 0.5369037986
Epoch:  1000  |  train loss: 0.5219876289
Epoch:  1100  |  train loss: 0.5095973969
Epoch:  1200  |  train loss: 0.4992733717
Epoch:  1300  |  train loss: 0.4899485409
Epoch:  1400  |  train loss: 0.4822620749
Epoch:  1500  |  train loss: 0.4752434850
Epoch:  1600  |  train loss: 0.4694261074
Epoch:  1700  |  train loss: 0.4641322136
Epoch:  1800  |  train loss: 0.4586479187
Epoch:  1900  |  train loss: 0.4546389401
Epoch:  2000  |  train loss: 0.4507052481
Processing class: 96
Using PCA transformation
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.9398785591
Epoch:   200  |  train loss: 0.8194464087
Epoch:   300  |  train loss: 0.7370656252
Epoch:   400  |  train loss: 0.6787403584
Epoch:   500  |  train loss: 0.6361474156
Epoch:   600  |  train loss: 0.6035645843
Epoch:   700  |  train loss: 0.5777229309
Epoch:   800  |  train loss: 0.5568772554
Epoch:   900  |  train loss: 0.5395527840
Epoch:  1000  |  train loss: 0.5252068698
Epoch:  1100  |  train loss: 0.5133953273
Epoch:  1200  |  train loss: 0.5030911744
Epoch:  1300  |  train loss: 0.4942312479
Epoch:  1400  |  train loss: 0.4863868535
Epoch:  1500  |  train loss: 0.4802158117
Epoch:  1600  |  train loss: 0.4742528677
Epoch:  1700  |  train loss: 0.4695482969
Epoch:  1800  |  train loss: 0.4649860501
Epoch:  1900  |  train loss: 0.4611597896
Epoch:  2000  |  train loss: 0.4583186507
Processing class: 97
Using PCA transformation
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.9428373098
Epoch:   200  |  train loss: 0.8189518571
Epoch:   300  |  train loss: 0.7280814052
Epoch:   400  |  train loss: 0.6667944670
Epoch:   500  |  train loss: 0.6219647050
Epoch:   600  |  train loss: 0.5880414963
Epoch:   700  |  train loss: 0.5614287853
Epoch:   800  |  train loss: 0.5400320768
Epoch:   900  |  train loss: 0.5228385210
Epoch:  1000  |  train loss: 0.5085566580
Epoch:  1100  |  train loss: 0.4969227076
Epoch:  1200  |  train loss: 0.4865104139
Epoch:  1300  |  train loss: 0.4781497121
Epoch:  1400  |  train loss: 0.4707638502
Epoch:  1500  |  train loss: 0.4644214153
Epoch:  1600  |  train loss: 0.4588503003
Epoch:  1700  |  train loss: 0.4540935695
Epoch:  1800  |  train loss: 0.4503621221
Epoch:  1900  |  train loss: 0.4466718793
Epoch:  2000  |  train loss: 0.4437242627
Processing class: 98
Using PCA transformation
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.9391549587
Epoch:   200  |  train loss: 0.8249719977
Epoch:   300  |  train loss: 0.7447132945
Epoch:   400  |  train loss: 0.6881166339
Epoch:   500  |  train loss: 0.6464429975
Epoch:   600  |  train loss: 0.6152945161
Epoch:   700  |  train loss: 0.5901507616
Epoch:   800  |  train loss: 0.5699887633
Epoch:   900  |  train loss: 0.5531593084
Epoch:  1000  |  train loss: 0.5389749527
Epoch:  1100  |  train loss: 0.5265977025
Epoch:  1200  |  train loss: 0.5166123867
Epoch:  1300  |  train loss: 0.5076088309
Epoch:  1400  |  train loss: 0.5001137912
Epoch:  1500  |  train loss: 0.4936039746
Epoch:  1600  |  train loss: 0.4875441253
Epoch:  1700  |  train loss: 0.4826204300
Epoch:  1800  |  train loss: 0.4777844369
Epoch:  1900  |  train loss: 0.4741200864
Epoch:  2000  |  train loss: 0.4701242566
Processing class: 99
Using PCA transformation
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.9383565307
Epoch:   200  |  train loss: 0.8153282881
Epoch:   300  |  train loss: 0.7308012605
Epoch:   400  |  train loss: 0.6707808495
Epoch:   500  |  train loss: 0.6271933436
Epoch:   600  |  train loss: 0.5936134100
Epoch:   700  |  train loss: 0.5675568104
Epoch:   800  |  train loss: 0.5453664064
Epoch:   900  |  train loss: 0.5277963400
Epoch:  1000  |  train loss: 0.5133526444
Epoch:  1100  |  train loss: 0.5011353910
Epoch:  1200  |  train loss: 0.4907993555
Epoch:  1300  |  train loss: 0.4817017615
Epoch:  1400  |  train loss: 0.4742603123
Epoch:  1500  |  train loss: 0.4679205596
Epoch:  1600  |  train loss: 0.4620537102
Epoch:  1700  |  train loss: 0.4570067585
Epoch:  1800  |  train loss: 0.4527433574
Epoch:  1900  |  train loss: 0.4489626586
Epoch:  2000  |  train loss: 0.4453987777
Clasifying using reconstruction function cost
2024-03-18 09:35:52,540 [trainer.py] => CNN: {'total': 50.36, '00-09': 57.7, '10-19': 63.6, '20-29': 71.4, '30-39': 68.5, '40-49': 56.8, '50-59': 29.6, '60-69': 38.9, '70-79': 36.6, '80-89': 42.7, '90-99': 37.8, 'old': 51.76, 'new': 37.8}
2024-03-18 09:35:52,552 [trainer.py] => No NME accuracy
2024-03-18 09:35:52,552 [trainer.py] => FeCAM: {'total': 50.1, '00-09': 70.3, '10-19': 55.8, '20-29': 66.1, '30-39': 61.9, '40-49': 56.4, '50-59': 28.7, '60-69': 36.0, '70-79': 39.0, '80-89': 38.4, '90-99': 48.4, 'old': 50.29, 'new': 48.4}
2024-03-18 09:35:52,554 [trainer.py] => CNN top1 curve: [83.44, 71.75, 64.66, 59.18, 54.08, 50.36]
2024-03-18 09:35:52,554 [trainer.py] => CNN top5 curve: [96.5, 89.6, 86.54, 84.09, 81.74, 79.63]
2024-03-18 09:35:52,554 [trainer.py] => FeCAM top1 curve: [81.6, 67.95, 62.9, 57.91, 53.32, 50.1]
2024-03-18 09:35:52,554 [trainer.py] => FeCAM top5 curve: [94.7, 89.45, 85.87, 82.32, 79.79, 76.85]

=========================================
2024-03-18 09:36:19,247 [trainer.py] => config: ./exps/FeCAM_cifar100.json
2024-03-18 09:36:19,247 [trainer.py] => prefix: train
2024-03-18 09:36:19,247 [trainer.py] => dataset: cifar100
2024-03-18 09:36:19,247 [trainer.py] => memory_size: 0
2024-03-18 09:36:19,247 [trainer.py] => shuffle: True
2024-03-18 09:36:19,247 [trainer.py] => init_cls: 50
2024-03-18 09:36:19,247 [trainer.py] => increment: 10
2024-03-18 09:36:19,247 [trainer.py] => model_name: fecam
2024-03-18 09:36:19,247 [trainer.py] => convnet_type: resnet18
2024-03-18 09:36:19,247 [trainer.py] => device: [device(type='cuda', index=0)]
2024-03-18 09:36:19,247 [trainer.py] => seed: 1993
2024-03-18 09:36:19,248 [trainer.py] => init_epochs: 200
2024-03-18 09:36:19,248 [trainer.py] => init_lr: 0.1
2024-03-18 09:36:19,248 [trainer.py] => init_weight_decay: 0.0005
2024-03-18 09:36:19,248 [trainer.py] => batch_size: 128
2024-03-18 09:36:19,248 [trainer.py] => num_workers: 8
2024-03-18 09:36:19,248 [trainer.py] => T: 5
2024-03-18 09:36:19,248 [trainer.py] => beta: 0.5
2024-03-18 09:36:19,248 [trainer.py] => alpha1: 1
2024-03-18 09:36:19,248 [trainer.py] => alpha2: 1
2024-03-18 09:36:19,248 [trainer.py] => ncm: False
2024-03-18 09:36:19,248 [trainer.py] => tukey: False
2024-03-18 09:36:19,248 [trainer.py] => diagonal: False
2024-03-18 09:36:19,248 [trainer.py] => per_class: True
2024-03-18 09:36:19,248 [trainer.py] => full_cov: True
2024-03-18 09:36:19,248 [trainer.py] => shrink: True
2024-03-18 09:36:19,248 [trainer.py] => norm_cov: False
2024-03-18 09:36:19,248 [trainer.py] => epochs: 2000
2024-03-18 09:36:19,248 [trainer.py] => vecnorm: False
2024-03-18 09:36:19,248 [trainer.py] => ae_type: wae
2024-03-18 09:36:19,248 [trainer.py] => ae_latent_dim: 32
2024-03-18 09:36:19,248 [trainer.py] => ae_n: 1
2024-03-18 09:36:19,248 [trainer.py] => wae_sigma: 10
2024-03-18 09:36:19,248 [trainer.py] => wae_C: 0.1
2024-03-18 09:36:19,248 [trainer.py] => ae_standarization: False
2024-03-18 09:36:19,248 [trainer.py] => ae_pca: True
2024-03-18 09:36:19,248 [trainer.py] => ae_pca_components: 500
2024-03-18 09:36:19,248 [trainer.py] => ae_clsf: recon-cost
Files already downloaded and verified
Files already downloaded and verified
2024-03-18 09:36:21,992 [data_manager.py] => [68, 56, 78, 8, 23, 84, 90, 65, 74, 76, 40, 89, 3, 92, 55, 9, 26, 80, 43, 38, 58, 70, 77, 1, 85, 19, 17, 50, 28, 53, 13, 81, 45, 82, 6, 59, 83, 16, 15, 44, 91, 41, 72, 60, 79, 52, 20, 10, 31, 54, 37, 95, 14, 71, 96, 98, 97, 2, 64, 66, 42, 22, 35, 86, 24, 34, 87, 21, 99, 0, 88, 27, 18, 94, 11, 12, 47, 25, 30, 46, 62, 69, 36, 61, 7, 63, 75, 5, 32, 4, 51, 48, 73, 93, 39, 67, 29, 49, 57, 33]
2024-03-18 09:36:22,272 [fecam.py] => Learning on 0-50
/home/z1165703/miniconda3/lib/python3.11/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
/home/z1165703/miniconda3/lib/python3.11/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
Processing class: 0
Using PCA transformation
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.9453763604
Epoch:   200  |  train loss: 0.8376413465
Epoch:   300  |  train loss: 0.7656148434
Epoch:   400  |  train loss: 0.7146457434
Epoch:   500  |  train loss: 0.6783298969
Epoch:   600  |  train loss: 0.6505213141
Epoch:   700  |  train loss: 0.6286980271
Epoch:   800  |  train loss: 0.6109787226
Epoch:   900  |  train loss: 0.5970413089
Epoch:  1000  |  train loss: 0.5856810927
Epoch:  1100  |  train loss: 0.5762956738
Epoch:  1200  |  train loss: 0.5685221910
Epoch:  1300  |  train loss: 0.5620821714
Epoch:  1400  |  train loss: 0.5564008832
Epoch:  1500  |  train loss: 0.5522558689
Epoch:  1600  |  train loss: 0.5481203675
Epoch:  1700  |  train loss: 0.5451622248
Epoch:  1800  |  train loss: 0.5422297239
Epoch:  1900  |  train loss: 0.5397874594
Epoch:  2000  |  train loss: 0.5382403731
Processing class: 1
Using PCA transformation
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.9466307878
Epoch:   200  |  train loss: 0.8432957530
Epoch:   300  |  train loss: 0.7701557755
Epoch:   400  |  train loss: 0.7192086697
Epoch:   500  |  train loss: 0.6829123855
Epoch:   600  |  train loss: 0.6538313866
Epoch:   700  |  train loss: 0.6321944356
Epoch:   800  |  train loss: 0.6150448561
Epoch:   900  |  train loss: 0.6014435172
Epoch:  1000  |  train loss: 0.5899632454
Epoch:  1100  |  train loss: 0.5805085421
Epoch:  1200  |  train loss: 0.5729892135
Epoch:  1300  |  train loss: 0.5663947582
Epoch:  1400  |  train loss: 0.5608970165
Epoch:  1500  |  train loss: 0.5562439442
Epoch:  1600  |  train loss: 0.5526238441
Epoch:  1700  |  train loss: 0.5485703468
Epoch:  1800  |  train loss: 0.5456696868
Epoch:  1900  |  train loss: 0.5430705905
Epoch:  2000  |  train loss: 0.5408932328
Processing class: 2
Using PCA transformation
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.9447454095
Epoch:   200  |  train loss: 0.8342495799
Epoch:   300  |  train loss: 0.7589223981
Epoch:   400  |  train loss: 0.7063515782
Epoch:   500  |  train loss: 0.6681957960
Epoch:   600  |  train loss: 0.6389370918
Epoch:   700  |  train loss: 0.6161274314
Epoch:   800  |  train loss: 0.5985204816
Epoch:   900  |  train loss: 0.5844098330
Epoch:  1000  |  train loss: 0.5729108572
Epoch:  1100  |  train loss: 0.5636043549
Epoch:  1200  |  train loss: 0.5558173656
Epoch:  1300  |  train loss: 0.5495152712
Epoch:  1400  |  train loss: 0.5443242311
Epoch:  1500  |  train loss: 0.5400069118
Epoch:  1600  |  train loss: 0.5359753132
Epoch:  1700  |  train loss: 0.5327654362
Epoch:  1800  |  train loss: 0.5304198146
Epoch:  1900  |  train loss: 0.5280343056
Epoch:  2000  |  train loss: 0.5260225773
Processing class: 3
Using PCA transformation
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.9470263600
Epoch:   200  |  train loss: 0.8425866604
Epoch:   300  |  train loss: 0.7683508515
Epoch:   400  |  train loss: 0.7173267722
Epoch:   500  |  train loss: 0.6803031445
Epoch:   600  |  train loss: 0.6525175214
Epoch:   700  |  train loss: 0.6307284594
Epoch:   800  |  train loss: 0.6127511501
Epoch:   900  |  train loss: 0.5988629460
Epoch:  1000  |  train loss: 0.5872581363
Epoch:  1100  |  train loss: 0.5774148226
Epoch:  1200  |  train loss: 0.5693004370
Epoch:  1300  |  train loss: 0.5627015710
Epoch:  1400  |  train loss: 0.5568338275
Epoch:  1500  |  train loss: 0.5521093726
Epoch:  1600  |  train loss: 0.5478804588
Epoch:  1700  |  train loss: 0.5443510890
Epoch:  1800  |  train loss: 0.5413392901
Epoch:  1900  |  train loss: 0.5389125466
Epoch:  2000  |  train loss: 0.5371496677
Processing class: 4
Using PCA transformation
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.9415517211
Epoch:   200  |  train loss: 0.8328453302
Epoch:   300  |  train loss: 0.7605424285
Epoch:   400  |  train loss: 0.7099992633
Epoch:   500  |  train loss: 0.6737336397
Epoch:   600  |  train loss: 0.6467499256
Epoch:   700  |  train loss: 0.6254550576
Epoch:   800  |  train loss: 0.6091681957
Epoch:   900  |  train loss: 0.5954794168
Epoch:  1000  |  train loss: 0.5847820997
Epoch:  1100  |  train loss: 0.5761764050
Epoch:  1200  |  train loss: 0.5688254833
Epoch:  1300  |  train loss: 0.5625744700
Epoch:  1400  |  train loss: 0.5578523993
Epoch:  1500  |  train loss: 0.5535471320
Epoch:  1600  |  train loss: 0.5504267693
Epoch:  1700  |  train loss: 0.5477443576
Epoch:  1800  |  train loss: 0.5451587915
Epoch:  1900  |  train loss: 0.5431812406
Epoch:  2000  |  train loss: 0.5418582082
Processing class: 5
Using PCA transformation
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.9417273164
Epoch:   200  |  train loss: 0.8287026286
Epoch:   300  |  train loss: 0.7535503864
Epoch:   400  |  train loss: 0.7023070574
Epoch:   500  |  train loss: 0.6658702493
Epoch:   600  |  train loss: 0.6388914824
Epoch:   700  |  train loss: 0.6168599010
Epoch:   800  |  train loss: 0.6006220937
Epoch:   900  |  train loss: 0.5875279188
Epoch:  1000  |  train loss: 0.5769166708
Epoch:  1100  |  train loss: 0.5683848381
Epoch:  1200  |  train loss: 0.5614193082
Epoch:  1300  |  train loss: 0.5560164809
Epoch:  1400  |  train loss: 0.5513407707
Epoch:  1500  |  train loss: 0.5475358486
Epoch:  1600  |  train loss: 0.5445213079
Epoch:  1700  |  train loss: 0.5417590380
Epoch:  1800  |  train loss: 0.5395354152
Epoch:  1900  |  train loss: 0.5378096223
Epoch:  2000  |  train loss: 0.5364803195
Processing class: 6
Using PCA transformation
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.9436152220
Epoch:   200  |  train loss: 0.8342645288
Epoch:   300  |  train loss: 0.7608064055
Epoch:   400  |  train loss: 0.7091473579
Epoch:   500  |  train loss: 0.6713631272
Epoch:   600  |  train loss: 0.6436859131
Epoch:   700  |  train loss: 0.6226025701
Epoch:   800  |  train loss: 0.6056928396
Epoch:   900  |  train loss: 0.5921779156
Epoch:  1000  |  train loss: 0.5812470198
Epoch:  1100  |  train loss: 0.5727302313
Epoch:  1200  |  train loss: 0.5651311636
Epoch:  1300  |  train loss: 0.5591366053
Epoch:  1400  |  train loss: 0.5540104985
Epoch:  1500  |  train loss: 0.5500224948
Epoch:  1600  |  train loss: 0.5465740681
Epoch:  1700  |  train loss: 0.5440741539
Epoch:  1800  |  train loss: 0.5415322423
Epoch:  1900  |  train loss: 0.5394508839
Epoch:  2000  |  train loss: 0.5377084017
Processing class: 7
Using PCA transformation
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.9433638453
Epoch:   200  |  train loss: 0.8424438834
Epoch:   300  |  train loss: 0.7705919862
Epoch:   400  |  train loss: 0.7194333553
Epoch:   500  |  train loss: 0.6828527451
Epoch:   600  |  train loss: 0.6553450942
Epoch:   700  |  train loss: 0.6332423687
Epoch:   800  |  train loss: 0.6156956196
Epoch:   900  |  train loss: 0.6014532208
Epoch:  1000  |  train loss: 0.5898164511
Epoch:  1100  |  train loss: 0.5800261736
Epoch:  1200  |  train loss: 0.5719146013
Epoch:  1300  |  train loss: 0.5644760132
Epoch:  1400  |  train loss: 0.5582135320
Epoch:  1500  |  train loss: 0.5528900862
Epoch:  1600  |  train loss: 0.5490035534
Epoch:  1700  |  train loss: 0.5450791121
Epoch:  1800  |  train loss: 0.5420261025
Epoch:  1900  |  train loss: 0.5390188217
Epoch:  2000  |  train loss: 0.5367125511
Processing class: 8
Using PCA transformation
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.9475818276
Epoch:   200  |  train loss: 0.8400631905
Epoch:   300  |  train loss: 0.7666100860
Epoch:   400  |  train loss: 0.7153192878
Epoch:   500  |  train loss: 0.6770689130
Epoch:   600  |  train loss: 0.6480826378
Epoch:   700  |  train loss: 0.6260754228
Epoch:   800  |  train loss: 0.6088629007
Epoch:   900  |  train loss: 0.5946963310
Epoch:  1000  |  train loss: 0.5823661208
Epoch:  1100  |  train loss: 0.5722199917
Epoch:  1200  |  train loss: 0.5636822343
Epoch:  1300  |  train loss: 0.5566177964
Epoch:  1400  |  train loss: 0.5506159544
Epoch:  1500  |  train loss: 0.5462027907
Epoch:  1600  |  train loss: 0.5418547153
Epoch:  1700  |  train loss: 0.5383250356
Epoch:  1800  |  train loss: 0.5353301048
Epoch:  1900  |  train loss: 0.5327419400
Epoch:  2000  |  train loss: 0.5300646663
Processing class: 9
Using PCA transformation
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.9441005945
Epoch:   200  |  train loss: 0.8435616851
Epoch:   300  |  train loss: 0.7702492833
Epoch:   400  |  train loss: 0.7187819719
Epoch:   500  |  train loss: 0.6808565021
Epoch:   600  |  train loss: 0.6523822546
Epoch:   700  |  train loss: 0.6299340844
Epoch:   800  |  train loss: 0.6122667313
Epoch:   900  |  train loss: 0.5980621338
Epoch:  1000  |  train loss: 0.5861581206
Epoch:  1100  |  train loss: 0.5766064167
Epoch:  1200  |  train loss: 0.5683524609
Epoch:  1300  |  train loss: 0.5617102504
Epoch:  1400  |  train loss: 0.5563650966
Epoch:  1500  |  train loss: 0.5515542626
Epoch:  1600  |  train loss: 0.5475839615
Epoch:  1700  |  train loss: 0.5440024972
Epoch:  1800  |  train loss: 0.5414027810
Epoch:  1900  |  train loss: 0.5388875842
Epoch:  2000  |  train loss: 0.5368247747
Processing class: 10
Using PCA transformation
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.9440885425
Epoch:   200  |  train loss: 0.8348074198
Epoch:   300  |  train loss: 0.7598761439
Epoch:   400  |  train loss: 0.7086442590
Epoch:   500  |  train loss: 0.6711325645
Epoch:   600  |  train loss: 0.6428528428
Epoch:   700  |  train loss: 0.6207088590
Epoch:   800  |  train loss: 0.6028277397
Epoch:   900  |  train loss: 0.5886304259
Epoch:  1000  |  train loss: 0.5767652154
Epoch:  1100  |  train loss: 0.5668937325
Epoch:  1200  |  train loss: 0.5585883021
Epoch:  1300  |  train loss: 0.5518471003
Epoch:  1400  |  train loss: 0.5462613821
Epoch:  1500  |  train loss: 0.5410742760
Epoch:  1600  |  train loss: 0.5370073199
Epoch:  1700  |  train loss: 0.5332888842
Epoch:  1800  |  train loss: 0.5305885077
Epoch:  1900  |  train loss: 0.5280431986
Epoch:  2000  |  train loss: 0.5258421898
Processing class: 11
Using PCA transformation
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.9446416140
Epoch:   200  |  train loss: 0.8344782114
Epoch:   300  |  train loss: 0.7635320425
Epoch:   400  |  train loss: 0.7141927838
Epoch:   500  |  train loss: 0.6787038684
Epoch:   600  |  train loss: 0.6516003728
Epoch:   700  |  train loss: 0.6298471570
Epoch:   800  |  train loss: 0.6129555821
Epoch:   900  |  train loss: 0.5994200468
Epoch:  1000  |  train loss: 0.5880258918
Epoch:  1100  |  train loss: 0.5789023638
Epoch:  1200  |  train loss: 0.5712718129
Epoch:  1300  |  train loss: 0.5646769166
Epoch:  1400  |  train loss: 0.5593260765
Epoch:  1500  |  train loss: 0.5549806952
Epoch:  1600  |  train loss: 0.5513868213
Epoch:  1700  |  train loss: 0.5479455352
Epoch:  1800  |  train loss: 0.5454610348
Epoch:  1900  |  train loss: 0.5428538322
Epoch:  2000  |  train loss: 0.5409485936
Processing class: 12
Using PCA transformation
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.9441822886
Epoch:   200  |  train loss: 0.8414176345
Epoch:   300  |  train loss: 0.7705971122
Epoch:   400  |  train loss: 0.7199317336
Epoch:   500  |  train loss: 0.6821274042
Epoch:   600  |  train loss: 0.6534103751
Epoch:   700  |  train loss: 0.6313673973
Epoch:   800  |  train loss: 0.6138403654
Epoch:   900  |  train loss: 0.5994123697
Epoch:  1000  |  train loss: 0.5878725529
Epoch:  1100  |  train loss: 0.5781972170
Epoch:  1200  |  train loss: 0.5700114489
Epoch:  1300  |  train loss: 0.5630553126
Epoch:  1400  |  train loss: 0.5576808810
Epoch:  1500  |  train loss: 0.5529672980
Epoch:  1600  |  train loss: 0.5489590049
Epoch:  1700  |  train loss: 0.5453308225
Epoch:  1800  |  train loss: 0.5425523281
Epoch:  1900  |  train loss: 0.5404715776
Epoch:  2000  |  train loss: 0.5384172678
Processing class: 13
Using PCA transformation
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.9468462229
Epoch:   200  |  train loss: 0.8491217732
Epoch:   300  |  train loss: 0.7780580997
Epoch:   400  |  train loss: 0.7270695448
Epoch:   500  |  train loss: 0.6889499903
Epoch:   600  |  train loss: 0.6598416686
Epoch:   700  |  train loss: 0.6371157885
Epoch:   800  |  train loss: 0.6188425541
Epoch:   900  |  train loss: 0.6042512655
Epoch:  1000  |  train loss: 0.5921546817
Epoch:  1100  |  train loss: 0.5817944646
Epoch:  1200  |  train loss: 0.5737618446
Epoch:  1300  |  train loss: 0.5665690303
Epoch:  1400  |  train loss: 0.5604168892
Epoch:  1500  |  train loss: 0.5555987954
Epoch:  1600  |  train loss: 0.5514042616
Epoch:  1700  |  train loss: 0.5477677345
Epoch:  1800  |  train loss: 0.5449059129
Epoch:  1900  |  train loss: 0.5423609138
Epoch:  2000  |  train loss: 0.5404438615
Processing class: 14
Using PCA transformation
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.9470948935
Epoch:   200  |  train loss: 0.8450346351
Epoch:   300  |  train loss: 0.7710640669
Epoch:   400  |  train loss: 0.7189534545
Epoch:   500  |  train loss: 0.6809935212
Epoch:   600  |  train loss: 0.6532885671
Epoch:   700  |  train loss: 0.6313937306
Epoch:   800  |  train loss: 0.6140096545
Epoch:   900  |  train loss: 0.5999674320
Epoch:  1000  |  train loss: 0.5882862568
Epoch:  1100  |  train loss: 0.5788740993
Epoch:  1200  |  train loss: 0.5710757971
Epoch:  1300  |  train loss: 0.5644146204
Epoch:  1400  |  train loss: 0.5588104248
Epoch:  1500  |  train loss: 0.5540258646
Epoch:  1600  |  train loss: 0.5498167276
Epoch:  1700  |  train loss: 0.5466981053
Epoch:  1800  |  train loss: 0.5436636209
Epoch:  1900  |  train loss: 0.5413320422
Epoch:  2000  |  train loss: 0.5390067816
Processing class: 15
Using PCA transformation
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.9471394181
Epoch:   200  |  train loss: 0.8391569138
Epoch:   300  |  train loss: 0.7681437492
Epoch:   400  |  train loss: 0.7159088612
Epoch:   500  |  train loss: 0.6787433624
Epoch:   600  |  train loss: 0.6513930202
Epoch:   700  |  train loss: 0.6297459364
Epoch:   800  |  train loss: 0.6123065829
Epoch:   900  |  train loss: 0.5983588099
Epoch:  1000  |  train loss: 0.5870129108
Epoch:  1100  |  train loss: 0.5776992679
Epoch:  1200  |  train loss: 0.5696024776
Epoch:  1300  |  train loss: 0.5632990956
Epoch:  1400  |  train loss: 0.5575121880
Epoch:  1500  |  train loss: 0.5527135730
Epoch:  1600  |  train loss: 0.5486812234
Epoch:  1700  |  train loss: 0.5453723311
Epoch:  1800  |  train loss: 0.5422070980
Epoch:  1900  |  train loss: 0.5397654057
Epoch:  2000  |  train loss: 0.5378188014
Processing class: 16
Using PCA transformation
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.9448029876
Epoch:   200  |  train loss: 0.8415063143
Epoch:   300  |  train loss: 0.7707883000
Epoch:   400  |  train loss: 0.7200438499
Epoch:   500  |  train loss: 0.6826901913
Epoch:   600  |  train loss: 0.6553001404
Epoch:   700  |  train loss: 0.6335801601
Epoch:   800  |  train loss: 0.6166936636
Epoch:   900  |  train loss: 0.6028762937
Epoch:  1000  |  train loss: 0.5921555400
Epoch:  1100  |  train loss: 0.5834144235
Epoch:  1200  |  train loss: 0.5750382543
Epoch:  1300  |  train loss: 0.5693087816
Epoch:  1400  |  train loss: 0.5642041564
Epoch:  1500  |  train loss: 0.5598924637
Epoch:  1600  |  train loss: 0.5557693005
Epoch:  1700  |  train loss: 0.5529198647
Epoch:  1800  |  train loss: 0.5503045321
Epoch:  1900  |  train loss: 0.5481671095
Epoch:  2000  |  train loss: 0.5461288095
Processing class: 17
Using PCA transformation
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.9486550570
Epoch:   200  |  train loss: 0.8457057834
Epoch:   300  |  train loss: 0.7754076362
Epoch:   400  |  train loss: 0.7254803300
Epoch:   500  |  train loss: 0.6888542652
Epoch:   600  |  train loss: 0.6609372020
Epoch:   700  |  train loss: 0.6389143586
Epoch:   800  |  train loss: 0.6207632542
Epoch:   900  |  train loss: 0.6060737848
Epoch:  1000  |  train loss: 0.5936819077
Epoch:  1100  |  train loss: 0.5837236881
Epoch:  1200  |  train loss: 0.5749703407
Epoch:  1300  |  train loss: 0.5677384496
Epoch:  1400  |  train loss: 0.5619236469
Epoch:  1500  |  train loss: 0.5568947792
Epoch:  1600  |  train loss: 0.5522070169
Epoch:  1700  |  train loss: 0.5485647917
Epoch:  1800  |  train loss: 0.5452541828
Epoch:  1900  |  train loss: 0.5423042417
Epoch:  2000  |  train loss: 0.5399813175
Processing class: 18
Using PCA transformation
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.9504337072
Epoch:   200  |  train loss: 0.8492288589
Epoch:   300  |  train loss: 0.7790259719
Epoch:   400  |  train loss: 0.7301273704
Epoch:   500  |  train loss: 0.6942256212
Epoch:   600  |  train loss: 0.6665556788
Epoch:   700  |  train loss: 0.6446869373
Epoch:   800  |  train loss: 0.6272407413
Epoch:   900  |  train loss: 0.6132442236
Epoch:  1000  |  train loss: 0.6013809443
Epoch:  1100  |  train loss: 0.5914040446
Epoch:  1200  |  train loss: 0.5831072092
Epoch:  1300  |  train loss: 0.5762484431
Epoch:  1400  |  train loss: 0.5699794173
Epoch:  1500  |  train loss: 0.5650059700
Epoch:  1600  |  train loss: 0.5607517242
Epoch:  1700  |  train loss: 0.5571496844
Epoch:  1800  |  train loss: 0.5535160542
Epoch:  1900  |  train loss: 0.5511332393
Epoch:  2000  |  train loss: 0.5484604955
Processing class: 19
Using PCA transformation
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.9463527799
Epoch:   200  |  train loss: 0.8403332949
Epoch:   300  |  train loss: 0.7656150460
Epoch:   400  |  train loss: 0.7131126881
Epoch:   500  |  train loss: 0.6753805995
Epoch:   600  |  train loss: 0.6466815472
Epoch:   700  |  train loss: 0.6241525888
Epoch:   800  |  train loss: 0.6056608319
Epoch:   900  |  train loss: 0.5913731575
Epoch:  1000  |  train loss: 0.5792155385
Epoch:  1100  |  train loss: 0.5693345666
Epoch:  1200  |  train loss: 0.5609952927
Epoch:  1300  |  train loss: 0.5539393067
Epoch:  1400  |  train loss: 0.5476953030
Epoch:  1500  |  train loss: 0.5427479148
Epoch:  1600  |  train loss: 0.5381109357
Epoch:  1700  |  train loss: 0.5346097112
Epoch:  1800  |  train loss: 0.5312034130
Epoch:  1900  |  train loss: 0.5285614252
Epoch:  2000  |  train loss: 0.5261919498
Processing class: 20
Using PCA transformation
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.9466161728
Epoch:   200  |  train loss: 0.8424683213
Epoch:   300  |  train loss: 0.7675451517
Epoch:   400  |  train loss: 0.7160130501
Epoch:   500  |  train loss: 0.6790860415
Epoch:   600  |  train loss: 0.6517863989
Epoch:   700  |  train loss: 0.6305904150
Epoch:   800  |  train loss: 0.6145888686
Epoch:   900  |  train loss: 0.6013128877
Epoch:  1000  |  train loss: 0.5905225515
Epoch:  1100  |  train loss: 0.5816107631
Epoch:  1200  |  train loss: 0.5737175107
Epoch:  1300  |  train loss: 0.5676005960
Epoch:  1400  |  train loss: 0.5626416087
Epoch:  1500  |  train loss: 0.5585145116
Epoch:  1600  |  train loss: 0.5548916340
Epoch:  1700  |  train loss: 0.5516852021
Epoch:  1800  |  train loss: 0.5493409038
Epoch:  1900  |  train loss: 0.5472355366
Epoch:  2000  |  train loss: 0.5454340816
Processing class: 21
Using PCA transformation
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.9454017043
Epoch:   200  |  train loss: 0.8482948303
Epoch:   300  |  train loss: 0.7778013349
Epoch:   400  |  train loss: 0.7278172612
Epoch:   500  |  train loss: 0.6909176707
Epoch:   600  |  train loss: 0.6631849170
Epoch:   700  |  train loss: 0.6416343808
Epoch:   800  |  train loss: 0.6249378562
Epoch:   900  |  train loss: 0.6112572074
Epoch:  1000  |  train loss: 0.6000539660
Epoch:  1100  |  train loss: 0.5906568408
Epoch:  1200  |  train loss: 0.5834071517
Epoch:  1300  |  train loss: 0.5770966768
Epoch:  1400  |  train loss: 0.5715928435
Epoch:  1500  |  train loss: 0.5670830131
Epoch:  1600  |  train loss: 0.5632642746
Epoch:  1700  |  train loss: 0.5603550673
Epoch:  1800  |  train loss: 0.5577334046
Epoch:  1900  |  train loss: 0.5557475090
Epoch:  2000  |  train loss: 0.5536940217
Processing class: 22
Using PCA transformation
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.9443632483
Epoch:   200  |  train loss: 0.8314668775
Epoch:   300  |  train loss: 0.7547860384
Epoch:   400  |  train loss: 0.7029788375
Epoch:   500  |  train loss: 0.6655237913
Epoch:   600  |  train loss: 0.6379473925
Epoch:   700  |  train loss: 0.6162486911
Epoch:   800  |  train loss: 0.5993208289
Epoch:   900  |  train loss: 0.5854946375
Epoch:  1000  |  train loss: 0.5742332697
Epoch:  1100  |  train loss: 0.5651799083
Epoch:  1200  |  train loss: 0.5576170087
Epoch:  1300  |  train loss: 0.5515421152
Epoch:  1400  |  train loss: 0.5459971786
Epoch:  1500  |  train loss: 0.5418027043
Epoch:  1600  |  train loss: 0.5378045559
Epoch:  1700  |  train loss: 0.5345539570
Epoch:  1800  |  train loss: 0.5322384715
Epoch:  1900  |  train loss: 0.5298935533
Epoch:  2000  |  train loss: 0.5279867649
Processing class: 23
Using PCA transformation
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.9456827521
Epoch:   200  |  train loss: 0.8391004086
Epoch:   300  |  train loss: 0.7645190120
Epoch:   400  |  train loss: 0.7122826934
Epoch:   500  |  train loss: 0.6735864639
Epoch:   600  |  train loss: 0.6442958474
Epoch:   700  |  train loss: 0.6216764331
Epoch:   800  |  train loss: 0.6039239407
Epoch:   900  |  train loss: 0.5898686647
Epoch:  1000  |  train loss: 0.5780706644
Epoch:  1100  |  train loss: 0.5684699297
Epoch:  1200  |  train loss: 0.5608714342
Epoch:  1300  |  train loss: 0.5547770858
Epoch:  1400  |  train loss: 0.5490988970
Epoch:  1500  |  train loss: 0.5446493864
Epoch:  1600  |  train loss: 0.5410332084
Epoch:  1700  |  train loss: 0.5380556703
Epoch:  1800  |  train loss: 0.5354651570
Epoch:  1900  |  train loss: 0.5332872152
Epoch:  2000  |  train loss: 0.5315219641
Processing class: 24
Using PCA transformation
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.9448616982
Epoch:   200  |  train loss: 0.8347791553
Epoch:   300  |  train loss: 0.7598317027
Epoch:   400  |  train loss: 0.7078135729
Epoch:   500  |  train loss: 0.6695986629
Epoch:   600  |  train loss: 0.6409738183
Epoch:   700  |  train loss: 0.6193205953
Epoch:   800  |  train loss: 0.6017632961
Epoch:   900  |  train loss: 0.5871458292
Epoch:  1000  |  train loss: 0.5757382631
Epoch:  1100  |  train loss: 0.5665213704
Epoch:  1200  |  train loss: 0.5585753202
Epoch:  1300  |  train loss: 0.5516091704
Epoch:  1400  |  train loss: 0.5460003853
Epoch:  1500  |  train loss: 0.5414566040
Epoch:  1600  |  train loss: 0.5373726606
Epoch:  1700  |  train loss: 0.5340715647
Epoch:  1800  |  train loss: 0.5314974546
Epoch:  1900  |  train loss: 0.5291764498
Epoch:  2000  |  train loss: 0.5269656301
Processing class: 25
Using PCA transformation
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.9388997316
Epoch:   200  |  train loss: 0.8351187706
Epoch:   300  |  train loss: 0.7604190111
Epoch:   400  |  train loss: 0.7070903897
Epoch:   500  |  train loss: 0.6689017892
Epoch:   600  |  train loss: 0.6405902624
Epoch:   700  |  train loss: 0.6186261535
Epoch:   800  |  train loss: 0.6004037499
Epoch:   900  |  train loss: 0.5861544967
Epoch:  1000  |  train loss: 0.5747515082
Epoch:  1100  |  train loss: 0.5656515837
Epoch:  1200  |  train loss: 0.5583850503
Epoch:  1300  |  train loss: 0.5516373634
Epoch:  1400  |  train loss: 0.5465123296
Epoch:  1500  |  train loss: 0.5423193932
Epoch:  1600  |  train loss: 0.5385986805
Epoch:  1700  |  train loss: 0.5357609749
Epoch:  1800  |  train loss: 0.5331773281
Epoch:  1900  |  train loss: 0.5306935906
Epoch:  2000  |  train loss: 0.5292500138
Processing class: 26
Using PCA transformation
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.9446418166
Epoch:   200  |  train loss: 0.8359018087
Epoch:   300  |  train loss: 0.7648491263
Epoch:   400  |  train loss: 0.7146693230
Epoch:   500  |  train loss: 0.6776931882
Epoch:   600  |  train loss: 0.6505521536
Epoch:   700  |  train loss: 0.6288759828
Epoch:   800  |  train loss: 0.6120217443
Epoch:   900  |  train loss: 0.5983337164
Epoch:  1000  |  train loss: 0.5872420430
Epoch:  1100  |  train loss: 0.5782655120
Epoch:  1200  |  train loss: 0.5703844547
Epoch:  1300  |  train loss: 0.5643422961
Epoch:  1400  |  train loss: 0.5589068294
Epoch:  1500  |  train loss: 0.5543198824
Epoch:  1600  |  train loss: 0.5503416419
Epoch:  1700  |  train loss: 0.5472542524
Epoch:  1800  |  train loss: 0.5447688818
Epoch:  1900  |  train loss: 0.5422237277
Epoch:  2000  |  train loss: 0.5407036543
Processing class: 27
Using PCA transformation
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.9421211362
Epoch:   200  |  train loss: 0.8355576992
Epoch:   300  |  train loss: 0.7621074677
Epoch:   400  |  train loss: 0.7107590914
Epoch:   500  |  train loss: 0.6741389751
Epoch:   600  |  train loss: 0.6463249564
Epoch:   700  |  train loss: 0.6250640512
Epoch:   800  |  train loss: 0.6080055237
Epoch:   900  |  train loss: 0.5942109823
Epoch:  1000  |  train loss: 0.5831381798
Epoch:  1100  |  train loss: 0.5739437342
Epoch:  1200  |  train loss: 0.5664283872
Epoch:  1300  |  train loss: 0.5600435019
Epoch:  1400  |  train loss: 0.5546743989
Epoch:  1500  |  train loss: 0.5502978802
Epoch:  1600  |  train loss: 0.5464614749
Epoch:  1700  |  train loss: 0.5435235620
Epoch:  1800  |  train loss: 0.5408105612
Epoch:  1900  |  train loss: 0.5387337565
Epoch:  2000  |  train loss: 0.5369679689
Processing class: 28
Using PCA transformation
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.9478800416
Epoch:   200  |  train loss: 0.8452576399
Epoch:   300  |  train loss: 0.7753320813
Epoch:   400  |  train loss: 0.7246109366
Epoch:   500  |  train loss: 0.6876916051
Epoch:   600  |  train loss: 0.6594664931
Epoch:   700  |  train loss: 0.6366868615
Epoch:   800  |  train loss: 0.6190903783
Epoch:   900  |  train loss: 0.6046522141
Epoch:  1000  |  train loss: 0.5924444675
Epoch:  1100  |  train loss: 0.5828074217
Epoch:  1200  |  train loss: 0.5740069270
Epoch:  1300  |  train loss: 0.5670758724
Epoch:  1400  |  train loss: 0.5610625267
Epoch:  1500  |  train loss: 0.5559212923
Epoch:  1600  |  train loss: 0.5513452411
Epoch:  1700  |  train loss: 0.5473844290
Epoch:  1800  |  train loss: 0.5442179680
Epoch:  1900  |  train loss: 0.5410504937
Epoch:  2000  |  train loss: 0.5389237761
Processing class: 29
Using PCA transformation
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.9430340290
Epoch:   200  |  train loss: 0.8345638514
Epoch:   300  |  train loss: 0.7572729349
Epoch:   400  |  train loss: 0.7047846437
Epoch:   500  |  train loss: 0.6679266930
Epoch:   600  |  train loss: 0.6400996208
Epoch:   700  |  train loss: 0.6189312458
Epoch:   800  |  train loss: 0.6020773768
Epoch:   900  |  train loss: 0.5883278370
Epoch:  1000  |  train loss: 0.5776283860
Epoch:  1100  |  train loss: 0.5685032725
Epoch:  1200  |  train loss: 0.5610175610
Epoch:  1300  |  train loss: 0.5553437352
Epoch:  1400  |  train loss: 0.5505346417
Epoch:  1500  |  train loss: 0.5463490129
Epoch:  1600  |  train loss: 0.5427370548
Epoch:  1700  |  train loss: 0.5399448633
Epoch:  1800  |  train loss: 0.5374290228
Epoch:  1900  |  train loss: 0.5355449319
Epoch:  2000  |  train loss: 0.5339218616
Processing class: 30
Using PCA transformation
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.9482044697
Epoch:   200  |  train loss: 0.8434192538
Epoch:   300  |  train loss: 0.7693446398
Epoch:   400  |  train loss: 0.7177711487
Epoch:   500  |  train loss: 0.6801081061
Epoch:   600  |  train loss: 0.6515926719
Epoch:   700  |  train loss: 0.6292141795
Epoch:   800  |  train loss: 0.6115828872
Epoch:   900  |  train loss: 0.5973099113
Epoch:  1000  |  train loss: 0.5852102399
Epoch:  1100  |  train loss: 0.5753239870
Epoch:  1200  |  train loss: 0.5670300245
Epoch:  1300  |  train loss: 0.5600043535
Epoch:  1400  |  train loss: 0.5537936211
Epoch:  1500  |  train loss: 0.5483142853
Epoch:  1600  |  train loss: 0.5444724083
Epoch:  1700  |  train loss: 0.5404039383
Epoch:  1800  |  train loss: 0.5372648835
Epoch:  1900  |  train loss: 0.5343991280
Epoch:  2000  |  train loss: 0.5319870114
Processing class: 31
Using PCA transformation
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.9448940992
Epoch:   200  |  train loss: 0.8423871279
Epoch:   300  |  train loss: 0.7720297933
Epoch:   400  |  train loss: 0.7212527275
Epoch:   500  |  train loss: 0.6840105772
Epoch:   600  |  train loss: 0.6549668670
Epoch:   700  |  train loss: 0.6323225856
Epoch:   800  |  train loss: 0.6145426512
Epoch:   900  |  train loss: 0.6001047015
Epoch:  1000  |  train loss: 0.5881166101
Epoch:  1100  |  train loss: 0.5785944581
Epoch:  1200  |  train loss: 0.5704204559
Epoch:  1300  |  train loss: 0.5633756399
Epoch:  1400  |  train loss: 0.5578774095
Epoch:  1500  |  train loss: 0.5532600403
Epoch:  1600  |  train loss: 0.5488373637
Epoch:  1700  |  train loss: 0.5453952193
Epoch:  1800  |  train loss: 0.5426859498
Epoch:  1900  |  train loss: 0.5401947856
Epoch:  2000  |  train loss: 0.5378989935
Processing class: 32
Using PCA transformation
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.9415020347
Epoch:   200  |  train loss: 0.8361758113
Epoch:   300  |  train loss: 0.7631968617
Epoch:   400  |  train loss: 0.7112629652
Epoch:   500  |  train loss: 0.6739177465
Epoch:   600  |  train loss: 0.6455652118
Epoch:   700  |  train loss: 0.6241726160
Epoch:   800  |  train loss: 0.6068744302
Epoch:   900  |  train loss: 0.5923071384
Epoch:  1000  |  train loss: 0.5809799910
Epoch:  1100  |  train loss: 0.5713024139
Epoch:  1200  |  train loss: 0.5637704492
Epoch:  1300  |  train loss: 0.5572073817
Epoch:  1400  |  train loss: 0.5515865922
Epoch:  1500  |  train loss: 0.5472534060
Epoch:  1600  |  train loss: 0.5433638215
Epoch:  1700  |  train loss: 0.5400618076
Epoch:  1800  |  train loss: 0.5369848728
Epoch:  1900  |  train loss: 0.5351191998
Epoch:  2000  |  train loss: 0.5329094887
Processing class: 33
Using PCA transformation
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.9440103889
Epoch:   200  |  train loss: 0.8407268763
Epoch:   300  |  train loss: 0.7699329138
Epoch:   400  |  train loss: 0.7196708798
Epoch:   500  |  train loss: 0.6836537719
Epoch:   600  |  train loss: 0.6567750573
Epoch:   700  |  train loss: 0.6352466226
Epoch:   800  |  train loss: 0.6180009723
Epoch:   900  |  train loss: 0.6040441632
Epoch:  1000  |  train loss: 0.5926590323
Epoch:  1100  |  train loss: 0.5827567101
Epoch:  1200  |  train loss: 0.5743710995
Epoch:  1300  |  train loss: 0.5678829789
Epoch:  1400  |  train loss: 0.5624420047
Epoch:  1500  |  train loss: 0.5581035614
Epoch:  1600  |  train loss: 0.5542012930
Epoch:  1700  |  train loss: 0.5508350730
Epoch:  1800  |  train loss: 0.5481255531
Epoch:  1900  |  train loss: 0.5457616925
Epoch:  2000  |  train loss: 0.5437495232
Processing class: 34
Using PCA transformation
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.9420879841
Epoch:   200  |  train loss: 0.8354885221
Epoch:   300  |  train loss: 0.7607956052
Epoch:   400  |  train loss: 0.7081390142
Epoch:   500  |  train loss: 0.6696626067
Epoch:   600  |  train loss: 0.6410281301
Epoch:   700  |  train loss: 0.6181627870
Epoch:   800  |  train loss: 0.6006470680
Epoch:   900  |  train loss: 0.5861505389
Epoch:  1000  |  train loss: 0.5740153193
Epoch:  1100  |  train loss: 0.5642756581
Epoch:  1200  |  train loss: 0.5558068514
Epoch:  1300  |  train loss: 0.5485460877
Epoch:  1400  |  train loss: 0.5426925421
Epoch:  1500  |  train loss: 0.5377376318
Epoch:  1600  |  train loss: 0.5334267974
Epoch:  1700  |  train loss: 0.5299189329
Epoch:  1800  |  train loss: 0.5270312548
Epoch:  1900  |  train loss: 0.5240807414
Epoch:  2000  |  train loss: 0.5221192479
Processing class: 35
Using PCA transformation
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.9406911135
Epoch:   200  |  train loss: 0.8326332569
Epoch:   300  |  train loss: 0.7569817901
Epoch:   400  |  train loss: 0.7043502092
Epoch:   500  |  train loss: 0.6669292092
Epoch:   600  |  train loss: 0.6383476615
Epoch:   700  |  train loss: 0.6158965945
Epoch:   800  |  train loss: 0.5985255480
Epoch:   900  |  train loss: 0.5842500091
/home/z1165703/miniconda3/lib/python3.11/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
Epoch:  1000  |  train loss: 0.5726420641
Epoch:  1100  |  train loss: 0.5627509356
Epoch:  1200  |  train loss: 0.5547187209
Epoch:  1300  |  train loss: 0.5481344938
Epoch:  1400  |  train loss: 0.5424436927
Epoch:  1500  |  train loss: 0.5376941800
Epoch:  1600  |  train loss: 0.5337338209
Epoch:  1700  |  train loss: 0.5303104043
Epoch:  1800  |  train loss: 0.5274613500
Epoch:  1900  |  train loss: 0.5246758461
Epoch:  2000  |  train loss: 0.5228656411
Processing class: 36
Using PCA transformation
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.9484415889
Epoch:   200  |  train loss: 0.8437404871
Epoch:   300  |  train loss: 0.7686199307
Epoch:   400  |  train loss: 0.7162884593
Epoch:   500  |  train loss: 0.6781791449
Epoch:   600  |  train loss: 0.6490200043
Epoch:   700  |  train loss: 0.6257894874
Epoch:   800  |  train loss: 0.6078244925
Epoch:   900  |  train loss: 0.5932819009
Epoch:  1000  |  train loss: 0.5815176845
Epoch:  1100  |  train loss: 0.5717499852
Epoch:  1200  |  train loss: 0.5638210177
Epoch:  1300  |  train loss: 0.5568754673
Epoch:  1400  |  train loss: 0.5514705420
Epoch:  1500  |  train loss: 0.5463840604
Epoch:  1600  |  train loss: 0.5424211025
Epoch:  1700  |  train loss: 0.5395157218
Epoch:  1800  |  train loss: 0.5367736697
Epoch:  1900  |  train loss: 0.5343659759
Epoch:  2000  |  train loss: 0.5324888229
Processing class: 37
Using PCA transformation
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.9437039971
Epoch:   200  |  train loss: 0.8336773634
Epoch:   300  |  train loss: 0.7597194672
Epoch:   400  |  train loss: 0.7092904806
Epoch:   500  |  train loss: 0.6726408958
Epoch:   600  |  train loss: 0.6454355478
Epoch:   700  |  train loss: 0.6238342047
Epoch:   800  |  train loss: 0.6074348927
Epoch:   900  |  train loss: 0.5937265515
Epoch:  1000  |  train loss: 0.5823683143
Epoch:  1100  |  train loss: 0.5735197783
Epoch:  1200  |  train loss: 0.5659711361
Epoch:  1300  |  train loss: 0.5594766974
Epoch:  1400  |  train loss: 0.5544847965
Epoch:  1500  |  train loss: 0.5500032902
Epoch:  1600  |  train loss: 0.5461569667
Epoch:  1700  |  train loss: 0.5437464118
Epoch:  1800  |  train loss: 0.5407863975
Epoch:  1900  |  train loss: 0.5388598084
Epoch:  2000  |  train loss: 0.5368839502
Processing class: 38
Using PCA transformation
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.9409284711
Epoch:   200  |  train loss: 0.8302155375
Epoch:   300  |  train loss: 0.7575749993
Epoch:   400  |  train loss: 0.7060928345
Epoch:   500  |  train loss: 0.6688657880
Epoch:   600  |  train loss: 0.6406814098
Epoch:   700  |  train loss: 0.6190830827
Epoch:   800  |  train loss: 0.6018736720
Epoch:   900  |  train loss: 0.5878016949
Epoch:  1000  |  train loss: 0.5764637113
Epoch:  1100  |  train loss: 0.5669067144
Epoch:  1200  |  train loss: 0.5589434862
Epoch:  1300  |  train loss: 0.5525394917
Epoch:  1400  |  train loss: 0.5467176795
Epoch:  1500  |  train loss: 0.5418984413
Epoch:  1600  |  train loss: 0.5382447124
Epoch:  1700  |  train loss: 0.5348497391
Epoch:  1800  |  train loss: 0.5319161177
Epoch:  1900  |  train loss: 0.5294332266
Epoch:  2000  |  train loss: 0.5276740909
Processing class: 39
Using PCA transformation
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.9438227415
Epoch:   200  |  train loss: 0.8377425432
Epoch:   300  |  train loss: 0.7661350369
Epoch:   400  |  train loss: 0.7153560519
Epoch:   500  |  train loss: 0.6782582521
Epoch:   600  |  train loss: 0.6502541542
Epoch:   700  |  train loss: 0.6286823630
Epoch:   800  |  train loss: 0.6117281914
Epoch:   900  |  train loss: 0.5971758246
Epoch:  1000  |  train loss: 0.5860657215
Epoch:  1100  |  train loss: 0.5767586708
Epoch:  1200  |  train loss: 0.5693062425
Epoch:  1300  |  train loss: 0.5629172087
Epoch:  1400  |  train loss: 0.5579068780
Epoch:  1500  |  train loss: 0.5535081029
Epoch:  1600  |  train loss: 0.5498612165
Epoch:  1700  |  train loss: 0.5469568968
Epoch:  1800  |  train loss: 0.5444289327
Epoch:  1900  |  train loss: 0.5423159361
Epoch:  2000  |  train loss: 0.5405946136
Processing class: 40
Using PCA transformation
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.9476244330
Epoch:   200  |  train loss: 0.8388522983
Epoch:   300  |  train loss: 0.7661895871
Epoch:   400  |  train loss: 0.7157895803
Epoch:   500  |  train loss: 0.6790931940
Epoch:   600  |  train loss: 0.6513920426
Epoch:   700  |  train loss: 0.6297963619
Epoch:   800  |  train loss: 0.6122460961
Epoch:   900  |  train loss: 0.5983535528
Epoch:  1000  |  train loss: 0.5866981626
Epoch:  1100  |  train loss: 0.5768995285
Epoch:  1200  |  train loss: 0.5683906913
Epoch:  1300  |  train loss: 0.5621910810
Epoch:  1400  |  train loss: 0.5565831661
Epoch:  1500  |  train loss: 0.5517245889
Epoch:  1600  |  train loss: 0.5473142505
Epoch:  1700  |  train loss: 0.5440263152
Epoch:  1800  |  train loss: 0.5409682751
Epoch:  1900  |  train loss: 0.5386560321
Epoch:  2000  |  train loss: 0.5363527060
Processing class: 41
Using PCA transformation
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.9427943230
Epoch:   200  |  train loss: 0.8376433253
Epoch:   300  |  train loss: 0.7666496515
Epoch:   400  |  train loss: 0.7170689940
Epoch:   500  |  train loss: 0.6816492200
Epoch:   600  |  train loss: 0.6548006058
Epoch:   700  |  train loss: 0.6340484977
Epoch:   800  |  train loss: 0.6175800323
Epoch:   900  |  train loss: 0.6044841290
Epoch:  1000  |  train loss: 0.5939878702
Epoch:  1100  |  train loss: 0.5852503538
Epoch:  1200  |  train loss: 0.5780708194
Epoch:  1300  |  train loss: 0.5723942876
Epoch:  1400  |  train loss: 0.5674060106
Epoch:  1500  |  train loss: 0.5633569241
Epoch:  1600  |  train loss: 0.5602449059
Epoch:  1700  |  train loss: 0.5575688481
Epoch:  1800  |  train loss: 0.5550983548
Epoch:  1900  |  train loss: 0.5530586243
Epoch:  2000  |  train loss: 0.5518832445
Processing class: 42
Using PCA transformation
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.9447247863
Epoch:   200  |  train loss: 0.8400924444
Epoch:   300  |  train loss: 0.7669077754
Epoch:   400  |  train loss: 0.7164380431
Epoch:   500  |  train loss: 0.6795889854
Epoch:   600  |  train loss: 0.6519023657
Epoch:   700  |  train loss: 0.6302544236
Epoch:   800  |  train loss: 0.6130923152
Epoch:   900  |  train loss: 0.5996014237
Epoch:  1000  |  train loss: 0.5883527040
Epoch:  1100  |  train loss: 0.5795657635
Epoch:  1200  |  train loss: 0.5722478271
Epoch:  1300  |  train loss: 0.5659091592
Epoch:  1400  |  train loss: 0.5609622359
Epoch:  1500  |  train loss: 0.5567739844
Epoch:  1600  |  train loss: 0.5532737851
Epoch:  1700  |  train loss: 0.5501266479
Epoch:  1800  |  train loss: 0.5479045630
Epoch:  1900  |  train loss: 0.5458022356
Epoch:  2000  |  train loss: 0.5441479206
Processing class: 43
Using PCA transformation
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.9465587258
Epoch:   200  |  train loss: 0.8433236957
Epoch:   300  |  train loss: 0.7723098159
Epoch:   400  |  train loss: 0.7210315108
Epoch:   500  |  train loss: 0.6832050443
Epoch:   600  |  train loss: 0.6546526670
Epoch:   700  |  train loss: 0.6319115877
Epoch:   800  |  train loss: 0.6140288115
Epoch:   900  |  train loss: 0.5995475769
Epoch:  1000  |  train loss: 0.5871734977
Epoch:  1100  |  train loss: 0.5771206379
Epoch:  1200  |  train loss: 0.5688369274
Epoch:  1300  |  train loss: 0.5617794394
Epoch:  1400  |  train loss: 0.5555653691
Epoch:  1500  |  train loss: 0.5503612757
Epoch:  1600  |  train loss: 0.5459339738
Epoch:  1700  |  train loss: 0.5422252417
Epoch:  1800  |  train loss: 0.5387855649
Epoch:  1900  |  train loss: 0.5363907099
Epoch:  2000  |  train loss: 0.5338135481
Processing class: 44
Using PCA transformation
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.9482610941
Epoch:   200  |  train loss: 0.8379099488
Epoch:   300  |  train loss: 0.7649101973
Epoch:   400  |  train loss: 0.7138604403
Epoch:   500  |  train loss: 0.6765364647
Epoch:   600  |  train loss: 0.6484868407
Epoch:   700  |  train loss: 0.6266937852
Epoch:   800  |  train loss: 0.6093313217
Epoch:   900  |  train loss: 0.5952933669
Epoch:  1000  |  train loss: 0.5837369680
Epoch:  1100  |  train loss: 0.5738880157
Epoch:  1200  |  train loss: 0.5661009073
Epoch:  1300  |  train loss: 0.5592002153
Epoch:  1400  |  train loss: 0.5535897493
Epoch:  1500  |  train loss: 0.5486651182
Epoch:  1600  |  train loss: 0.5448158026
Epoch:  1700  |  train loss: 0.5412758946
Epoch:  1800  |  train loss: 0.5381108522
Epoch:  1900  |  train loss: 0.5357316613
Epoch:  2000  |  train loss: 0.5337922573
Processing class: 45
Using PCA transformation
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.9481001973
Epoch:   200  |  train loss: 0.8442457438
Epoch:   300  |  train loss: 0.7723567843
Epoch:   400  |  train loss: 0.7216210485
Epoch:   500  |  train loss: 0.6839553356
Epoch:   600  |  train loss: 0.6532954454
Epoch:   700  |  train loss: 0.6304986596
Epoch:   800  |  train loss: 0.6119440436
Epoch:   900  |  train loss: 0.5966419935
Epoch:  1000  |  train loss: 0.5843986273
Epoch:  1100  |  train loss: 0.5741526365
Epoch:  1200  |  train loss: 0.5656243920
Epoch:  1300  |  train loss: 0.5580468535
Epoch:  1400  |  train loss: 0.5517098427
Epoch:  1500  |  train loss: 0.5463828206
Epoch:  1600  |  train loss: 0.5417617559
Epoch:  1700  |  train loss: 0.5379351020
Epoch:  1800  |  train loss: 0.5345251322
Epoch:  1900  |  train loss: 0.5315726876
Epoch:  2000  |  train loss: 0.5289851665
Processing class: 46
Using PCA transformation
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.9435348034
Epoch:   200  |  train loss: 0.8349533439
Epoch:   300  |  train loss: 0.7615944624
Epoch:   400  |  train loss: 0.7096656680
Epoch:   500  |  train loss: 0.6719740868
Epoch:   600  |  train loss: 0.6438029766
Epoch:   700  |  train loss: 0.6215278745
Epoch:   800  |  train loss: 0.6043246388
Epoch:   900  |  train loss: 0.5899538159
Epoch:  1000  |  train loss: 0.5783741474
Epoch:  1100  |  train loss: 0.5690789223
Epoch:  1200  |  train loss: 0.5607626915
Epoch:  1300  |  train loss: 0.5540118814
Epoch:  1400  |  train loss: 0.5482149482
Epoch:  1500  |  train loss: 0.5433476090
Epoch:  1600  |  train loss: 0.5398308516
Epoch:  1700  |  train loss: 0.5361530304
Epoch:  1800  |  train loss: 0.5334283948
Epoch:  1900  |  train loss: 0.5312308311
Epoch:  2000  |  train loss: 0.5289590597
Processing class: 47
Using PCA transformation
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.9425822973
Epoch:   200  |  train loss: 0.8345089912
Epoch:   300  |  train loss: 0.7611049414
Epoch:   400  |  train loss: 0.7100195050
Epoch:   500  |  train loss: 0.6734461665
Epoch:   600  |  train loss: 0.6455460668
Epoch:   700  |  train loss: 0.6236576557
Epoch:   800  |  train loss: 0.6070222139
Epoch:   900  |  train loss: 0.5923983574
Epoch:  1000  |  train loss: 0.5801860332
Epoch:  1100  |  train loss: 0.5712213278
Epoch:  1200  |  train loss: 0.5631058335
Epoch:  1300  |  train loss: 0.5565051794
Epoch:  1400  |  train loss: 0.5514090657
Epoch:  1500  |  train loss: 0.5465821147
Epoch:  1600  |  train loss: 0.5427568316
Epoch:  1700  |  train loss: 0.5396092296
Epoch:  1800  |  train loss: 0.5368943214
Epoch:  1900  |  train loss: 0.5342537642
Epoch:  2000  |  train loss: 0.5324739933
Processing class: 48
Using PCA transformation
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.9453949690
Epoch:   200  |  train loss: 0.8385106564
Epoch:   300  |  train loss: 0.7670892239
Epoch:   400  |  train loss: 0.7163733840
Epoch:   500  |  train loss: 0.6797867656
Epoch:   600  |  train loss: 0.6516934752
Epoch:   700  |  train loss: 0.6302915812
Epoch:   800  |  train loss: 0.6122389436
Epoch:   900  |  train loss: 0.5981443167
Epoch:  1000  |  train loss: 0.5867680788
Epoch:  1100  |  train loss: 0.5776378870
Epoch:  1200  |  train loss: 0.5703102350
Epoch:  1300  |  train loss: 0.5642392874
Epoch:  1400  |  train loss: 0.5587720156
Epoch:  1500  |  train loss: 0.5545976877
Epoch:  1600  |  train loss: 0.5508937597
Epoch:  1700  |  train loss: 0.5477837443
Epoch:  1800  |  train loss: 0.5454279661
Epoch:  1900  |  train loss: 0.5432186127
Epoch:  2000  |  train loss: 0.5413321733
Processing class: 49
Using PCA transformation
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.9450829268
Epoch:   200  |  train loss: 0.8421844721
Epoch:   300  |  train loss: 0.7694627881
Epoch:   400  |  train loss: 0.7178148746
Epoch:   500  |  train loss: 0.6809142351
Epoch:   600  |  train loss: 0.6531927109
Epoch:   700  |  train loss: 0.6321138859
Epoch:   800  |  train loss: 0.6153234363
Epoch:   900  |  train loss: 0.6020303369
Epoch:  1000  |  train loss: 0.5911177754
Epoch:  1100  |  train loss: 0.5817190289
Epoch:  1200  |  train loss: 0.5745948911
Epoch:  1300  |  train loss: 0.5684333801
Epoch:  1400  |  train loss: 0.5627486587
Epoch:  1500  |  train loss: 0.5587035298
Epoch:  1600  |  train loss: 0.5551313281
Epoch:  1700  |  train loss: 0.5517338276
Epoch:  1800  |  train loss: 0.5489558816
Epoch:  1900  |  train loss: 0.5469876051
Epoch:  2000  |  train loss: 0.5450176239
Clasifying using reconstruction function cost
2024-03-18 10:11:17,183 [trainer.py] => CNN: {'total': 83.44, '00-09': 87.7, '10-19': 79.6, '20-29': 84.9, '30-39': 81.0, '40-49': 84.0, 'old': 0, 'new': 83.44}
2024-03-18 10:11:17,187 [trainer.py] => No NME accuracy
2024-03-18 10:11:17,187 [trainer.py] => FeCAM: {'total': 7.44, '00-09': 6.0, '10-19': 7.6, '20-29': 8.6, '30-39': 5.2, '40-49': 9.8, 'old': 0, 'new': 7.44}
2024-03-18 10:11:17,187 [trainer.py] => CNN top1 curve: [83.44]
2024-03-18 10:11:17,187 [trainer.py] => CNN top5 curve: [96.5]
2024-03-18 10:11:17,187 [trainer.py] => FeCAM top1 curve: [7.44]
2024-03-18 10:11:17,187 [trainer.py] => FeCAM top5 curve: [38.18]

2024-03-18 10:11:17,207 [fecam.py] => Learning on 50-60
/home/z1165703/miniconda3/lib/python3.11/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
/home/z1165703/miniconda3/lib/python3.11/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
/home/z1165703/miniconda3/lib/python3.11/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
Processing class: 50
Using PCA transformation
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.9447452068
Epoch:   200  |  train loss: 0.8357402444
Epoch:   300  |  train loss: 0.7614075065
Epoch:   400  |  train loss: 0.7095374942
Epoch:   500  |  train loss: 0.6722299218
Epoch:   600  |  train loss: 0.6446599722
Epoch:   700  |  train loss: 0.6232070088
Epoch:   800  |  train loss: 0.6070302844
Epoch:   900  |  train loss: 0.5939152837
Epoch:  1000  |  train loss: 0.5829878330
Epoch:  1100  |  train loss: 0.5745517612
Epoch:  1200  |  train loss: 0.5673822403
Epoch:  1300  |  train loss: 0.5617357016
Epoch:  1400  |  train loss: 0.5568851590
Epoch:  1500  |  train loss: 0.5531022072
Epoch:  1600  |  train loss: 0.5500356913
Epoch:  1700  |  train loss: 0.5476131558
Epoch:  1800  |  train loss: 0.5452145934
Epoch:  1900  |  train loss: 0.5435888767
Epoch:  2000  |  train loss: 0.5422836781
Processing class: 51
Using PCA transformation
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.9496871114
Epoch:   200  |  train loss: 0.8468329549
Epoch:   300  |  train loss: 0.7755134344
Epoch:   400  |  train loss: 0.7257306933
Epoch:   500  |  train loss: 0.6889632106
Epoch:   600  |  train loss: 0.6616731524
Epoch:   700  |  train loss: 0.6403452396
Epoch:   800  |  train loss: 0.6233072996
Epoch:   900  |  train loss: 0.6096444130
Epoch:  1000  |  train loss: 0.5983201504
Epoch:  1100  |  train loss: 0.5887929559
Epoch:  1200  |  train loss: 0.5810448885
Epoch:  1300  |  train loss: 0.5741474152
Epoch:  1400  |  train loss: 0.5686574459
Epoch:  1500  |  train loss: 0.5635689735
Epoch:  1600  |  train loss: 0.5597493291
Epoch:  1700  |  train loss: 0.5563570499
Epoch:  1800  |  train loss: 0.5533764720
Epoch:  1900  |  train loss: 0.5509377599
Epoch:  2000  |  train loss: 0.5487507105
Processing class: 52
Using PCA transformation
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.9464210868
Epoch:   200  |  train loss: 0.8457450747
Epoch:   300  |  train loss: 0.7740234733
Epoch:   400  |  train loss: 0.7238011241
Epoch:   500  |  train loss: 0.6872107625
Epoch:   600  |  train loss: 0.6598075390
Epoch:   700  |  train loss: 0.6386506796
Epoch:   800  |  train loss: 0.6220601201
Epoch:   900  |  train loss: 0.6084193110
Epoch:  1000  |  train loss: 0.5974751353
Epoch:  1100  |  train loss: 0.5884631515
Epoch:  1200  |  train loss: 0.5808074713
Epoch:  1300  |  train loss: 0.5745092154
Epoch:  1400  |  train loss: 0.5690892935
Epoch:  1500  |  train loss: 0.5641566634
Epoch:  1600  |  train loss: 0.5605099082
Epoch:  1700  |  train loss: 0.5572039127
Epoch:  1800  |  train loss: 0.5543136239
Epoch:  1900  |  train loss: 0.5517268181
Epoch:  2000  |  train loss: 0.5496877670
Processing class: 53
Using PCA transformation
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.9442004919
Epoch:   200  |  train loss: 0.8349223495
Epoch:   300  |  train loss: 0.7589199543
Epoch:   400  |  train loss: 0.7065545797
Epoch:   500  |  train loss: 0.6689295173
Epoch:   600  |  train loss: 0.6404958367
Epoch:   700  |  train loss: 0.6187329888
Epoch:   800  |  train loss: 0.6012582660
Epoch:   900  |  train loss: 0.5875168324
Epoch:  1000  |  train loss: 0.5761688709
Epoch:  1100  |  train loss: 0.5668023944
Epoch:  1200  |  train loss: 0.5587229848
Epoch:  1300  |  train loss: 0.5525020599
Epoch:  1400  |  train loss: 0.5470800519
Epoch:  1500  |  train loss: 0.5426261663
Epoch:  1600  |  train loss: 0.5390661478
Epoch:  1700  |  train loss: 0.5358971834
Epoch:  1800  |  train loss: 0.5332261682
Epoch:  1900  |  train loss: 0.5308558464
Epoch:  2000  |  train loss: 0.5291424751
Processing class: 54
Using PCA transformation
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.9439078331
Epoch:   200  |  train loss: 0.8399992108
Epoch:   300  |  train loss: 0.7679489613
Epoch:   400  |  train loss: 0.7177970886
Epoch:   500  |  train loss: 0.6808550596
Epoch:   600  |  train loss: 0.6521472812
Epoch:   700  |  train loss: 0.6304936290
Epoch:   800  |  train loss: 0.6128311396
Epoch:   900  |  train loss: 0.5989694834
Epoch:  1000  |  train loss: 0.5871969938
Epoch:  1100  |  train loss: 0.5777637482
Epoch:  1200  |  train loss: 0.5699286342
Epoch:  1300  |  train loss: 0.5635426521
Epoch:  1400  |  train loss: 0.5582702518
Epoch:  1500  |  train loss: 0.5537812829
Epoch:  1600  |  train loss: 0.5495211363
Epoch:  1700  |  train loss: 0.5463755846
Epoch:  1800  |  train loss: 0.5441128612
Epoch:  1900  |  train loss: 0.5419888258
Epoch:  2000  |  train loss: 0.5395186663
Processing class: 55
Using PCA transformation
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.9468449116
Epoch:   200  |  train loss: 0.8413987160
Epoch:   300  |  train loss: 0.7689320564
Epoch:   400  |  train loss: 0.7186371207
Epoch:   500  |  train loss: 0.6815629959
Epoch:   600  |  train loss: 0.6524842739
Epoch:   700  |  train loss: 0.6303851724
Epoch:   800  |  train loss: 0.6125221848
Epoch:   900  |  train loss: 0.5981670856
Epoch:  1000  |  train loss: 0.5868070841
Epoch:  1100  |  train loss: 0.5769764304
Epoch:  1200  |  train loss: 0.5688886046
Epoch:  1300  |  train loss: 0.5622305274
Epoch:  1400  |  train loss: 0.5564761519
Epoch:  1500  |  train loss: 0.5516008854
Epoch:  1600  |  train loss: 0.5476659298
Epoch:  1700  |  train loss: 0.5443336725
Epoch:  1800  |  train loss: 0.5414961219
Epoch:  1900  |  train loss: 0.5390408635
Epoch:  2000  |  train loss: 0.5372175932
Processing class: 56
Using PCA transformation
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.9441918015
Epoch:   200  |  train loss: 0.8385950089
Epoch:   300  |  train loss: 0.7675370932
Epoch:   400  |  train loss: 0.7165936828
Epoch:   500  |  train loss: 0.6790439725
Epoch:   600  |  train loss: 0.6504963040
Epoch:   700  |  train loss: 0.6284371734
Epoch:   800  |  train loss: 0.6110831976
Epoch:   900  |  train loss: 0.5964173675
Epoch:  1000  |  train loss: 0.5844135523
Epoch:  1100  |  train loss: 0.5748326421
Epoch:  1200  |  train loss: 0.5664900899
Epoch:  1300  |  train loss: 0.5596842170
Epoch:  1400  |  train loss: 0.5536933064
Epoch:  1500  |  train loss: 0.5487614870
Epoch:  1600  |  train loss: 0.5443806052
Epoch:  1700  |  train loss: 0.5409116507
Epoch:  1800  |  train loss: 0.5376565099
Epoch:  1900  |  train loss: 0.5350856543
Epoch:  2000  |  train loss: 0.5328726768
Processing class: 57
Using PCA transformation
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.9457399487
Epoch:   200  |  train loss: 0.8444821000
Epoch:   300  |  train loss: 0.7724138021
Epoch:   400  |  train loss: 0.7212011099
Epoch:   500  |  train loss: 0.6819052696
Epoch:   600  |  train loss: 0.6521296620
Epoch:   700  |  train loss: 0.6291568398
Epoch:   800  |  train loss: 0.6106392145
Epoch:   900  |  train loss: 0.5955222011
Epoch:  1000  |  train loss: 0.5831996202
Epoch:  1100  |  train loss: 0.5727736115
Epoch:  1200  |  train loss: 0.5642679811
Epoch:  1300  |  train loss: 0.5567064285
Epoch:  1400  |  train loss: 0.5506366849
Epoch:  1500  |  train loss: 0.5455444336
Epoch:  1600  |  train loss: 0.5412777424
Epoch:  1700  |  train loss: 0.5374644876
Epoch:  1800  |  train loss: 0.5343310952
Epoch:  1900  |  train loss: 0.5315905333
Epoch:  2000  |  train loss: 0.5290437341
Processing class: 58
Using PCA transformation
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.9454391479
Epoch:   200  |  train loss: 0.8374495625
Epoch:   300  |  train loss: 0.7644293308
Epoch:   400  |  train loss: 0.7149122238
Epoch:   500  |  train loss: 0.6792094588
Epoch:   600  |  train loss: 0.6527119875
Epoch:   700  |  train loss: 0.6320165873
Epoch:   800  |  train loss: 0.6159428000
Epoch:   900  |  train loss: 0.6029760242
Epoch:  1000  |  train loss: 0.5929255724
Epoch:  1100  |  train loss: 0.5841310740
Epoch:  1200  |  train loss: 0.5765685916
Epoch:  1300  |  train loss: 0.5704538941
Epoch:  1400  |  train loss: 0.5654102206
Epoch:  1500  |  train loss: 0.5615850329
Epoch:  1600  |  train loss: 0.5577221036
Epoch:  1700  |  train loss: 0.5554021835
Epoch:  1800  |  train loss: 0.5531273603
Epoch:  1900  |  train loss: 0.5512450099
Epoch:  2000  |  train loss: 0.5495469809
Processing class: 59
Using PCA transformation
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.9465954185
Epoch:   200  |  train loss: 0.8427608848
Epoch:   300  |  train loss: 0.7718547225
Epoch:   400  |  train loss: 0.7206458807
Epoch:   500  |  train loss: 0.6816779613
Epoch:   600  |  train loss: 0.6522649884
Epoch:   700  |  train loss: 0.6304841995
Epoch:   800  |  train loss: 0.6132769942
Epoch:   900  |  train loss: 0.5999096394
Epoch:  1000  |  train loss: 0.5883684278
Epoch:  1100  |  train loss: 0.5794719458
Epoch:  1200  |  train loss: 0.5722358823
Epoch:  1300  |  train loss: 0.5644226432
Epoch:  1400  |  train loss: 0.5577436090
Epoch:  1500  |  train loss: 0.5531159043
Epoch:  1600  |  train loss: 0.5491118670
Epoch:  1700  |  train loss: 0.5459420919
Epoch:  1800  |  train loss: 0.5430989385
Epoch:  1900  |  train loss: 0.5405621171
Epoch:  2000  |  train loss: 0.5387307882
Clasifying using reconstruction function cost
2024-03-18 10:25:09,353 [trainer.py] => CNN: {'total': 71.75, '00-09': 82.2, '10-19': 72.8, '20-29': 78.5, '30-39': 74.9, '40-49': 68.4, '50-59': 53.7, 'old': 75.36, 'new': 53.7}
2024-03-18 10:25:09,356 [trainer.py] => No NME accuracy
2024-03-18 10:25:09,356 [trainer.py] => FeCAM: {'total': 5.53, '00-09': 5.4, '10-19': 6.0, '20-29': 7.6, '30-39': 3.9, '40-49': 8.3, '50-59': 2.0, 'old': 6.24, 'new': 2.0}
2024-03-18 10:25:09,356 [trainer.py] => CNN top1 curve: [83.44, 71.75]
2024-03-18 10:25:09,356 [trainer.py] => CNN top5 curve: [96.5, 89.6]
2024-03-18 10:25:09,356 [trainer.py] => FeCAM top1 curve: [7.44, 5.53]
2024-03-18 10:25:09,356 [trainer.py] => FeCAM top5 curve: [38.18, 28.73]

2024-03-18 10:25:09,369 [fecam.py] => Learning on 60-70
/home/z1165703/miniconda3/lib/python3.11/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
/home/z1165703/miniconda3/lib/python3.11/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
/home/z1165703/miniconda3/lib/python3.11/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
Processing class: 60
Using PCA transformation
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.9475633502
Epoch:   200  |  train loss: 0.8443368435
Epoch:   300  |  train loss: 0.7711908221
Epoch:   400  |  train loss: 0.7199441314
Epoch:   500  |  train loss: 0.6814537525
Epoch:   600  |  train loss: 0.6527027845
Epoch:   700  |  train loss: 0.6298632383
Epoch:   800  |  train loss: 0.6119933844
Epoch:   900  |  train loss: 0.5973788142
Epoch:  1000  |  train loss: 0.5855390549
Epoch:  1100  |  train loss: 0.5753160954
Epoch:  1200  |  train loss: 0.5666874647
Epoch:  1300  |  train loss: 0.5594985485
Epoch:  1400  |  train loss: 0.5535627484
Epoch:  1500  |  train loss: 0.5484148979
Epoch:  1600  |  train loss: 0.5439917326
Epoch:  1700  |  train loss: 0.5400403619
Epoch:  1800  |  train loss: 0.5369446158
Epoch:  1900  |  train loss: 0.5343126297
Epoch:  2000  |  train loss: 0.5319138408
Processing class: 61
Using PCA transformation
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.9468610287
Epoch:   200  |  train loss: 0.8501631260
Epoch:   300  |  train loss: 0.7816015363
Epoch:   400  |  train loss: 0.7319703221
Epoch:   500  |  train loss: 0.6950879335
Epoch:   600  |  train loss: 0.6669898748
Epoch:   700  |  train loss: 0.6450074673
Epoch:   800  |  train loss: 0.6270596623
Epoch:   900  |  train loss: 0.6122198939
Epoch:  1000  |  train loss: 0.6004618764
Epoch:  1100  |  train loss: 0.5903285027
Epoch:  1200  |  train loss: 0.5818129778
Epoch:  1300  |  train loss: 0.5744556785
Epoch:  1400  |  train loss: 0.5684484124
Epoch:  1500  |  train loss: 0.5628193974
Epoch:  1600  |  train loss: 0.5587344408
Epoch:  1700  |  train loss: 0.5547974586
Epoch:  1800  |  train loss: 0.5517720222
Epoch:  1900  |  train loss: 0.5488597989
Epoch:  2000  |  train loss: 0.5461477637
Processing class: 62
Using PCA transformation
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.9456137061
Epoch:   200  |  train loss: 0.8325323343
Epoch:   300  |  train loss: 0.7577700138
Epoch:   400  |  train loss: 0.7071860433
Epoch:   500  |  train loss: 0.6709086895
Epoch:   600  |  train loss: 0.6439367652
Epoch:   700  |  train loss: 0.6228466988
Epoch:   800  |  train loss: 0.6062056899
Epoch:   900  |  train loss: 0.5928241372
Epoch:  1000  |  train loss: 0.5820590138
Epoch:  1100  |  train loss: 0.5732319236
Epoch:  1200  |  train loss: 0.5656031489
Epoch:  1300  |  train loss: 0.5595712066
Epoch:  1400  |  train loss: 0.5544888377
Epoch:  1500  |  train loss: 0.5500550628
Epoch:  1600  |  train loss: 0.5466213703
Epoch:  1700  |  train loss: 0.5435616732
Epoch:  1800  |  train loss: 0.5412895441
Epoch:  1900  |  train loss: 0.5390143394
Epoch:  2000  |  train loss: 0.5374796152
Processing class: 63
Using PCA transformation
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.9389146924
Epoch:   200  |  train loss: 0.8257862091
Epoch:   300  |  train loss: 0.7519166946
Epoch:   400  |  train loss: 0.6990214109
Epoch:   500  |  train loss: 0.6604558945
Epoch:   600  |  train loss: 0.6316194534
Epoch:   700  |  train loss: 0.6095736265
Epoch:   800  |  train loss: 0.5921380281
Epoch:   900  |  train loss: 0.5781099916
Epoch:  1000  |  train loss: 0.5666938782
Epoch:  1100  |  train loss: 0.5579628587
Epoch:  1200  |  train loss: 0.5503512859
Epoch:  1300  |  train loss: 0.5442300081
Epoch:  1400  |  train loss: 0.5393278122
Epoch:  1500  |  train loss: 0.5350482702
Epoch:  1600  |  train loss: 0.5316806316
Epoch:  1700  |  train loss: 0.5287606955
Epoch:  1800  |  train loss: 0.5262184858
Epoch:  1900  |  train loss: 0.5241724253
Epoch:  2000  |  train loss: 0.5222119331
Processing class: 64
Using PCA transformation
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.9448474765
Epoch:   200  |  train loss: 0.8416008592
Epoch:   300  |  train loss: 0.7679069996
Epoch:   400  |  train loss: 0.7175750732
Epoch:   500  |  train loss: 0.6800149798
Epoch:   600  |  train loss: 0.6525312185
Epoch:   700  |  train loss: 0.6312611938
Epoch:   800  |  train loss: 0.6140074372
Epoch:   900  |  train loss: 0.6006570935
Epoch:  1000  |  train loss: 0.5895732284
Epoch:  1100  |  train loss: 0.5803015232
Epoch:  1200  |  train loss: 0.5727121472
Epoch:  1300  |  train loss: 0.5661558867
Epoch:  1400  |  train loss: 0.5609670281
Epoch:  1500  |  train loss: 0.5559750676
Epoch:  1600  |  train loss: 0.5518592000
Epoch:  1700  |  train loss: 0.5485961318
Epoch:  1800  |  train loss: 0.5456325412
Epoch:  1900  |  train loss: 0.5431058049
Epoch:  2000  |  train loss: 0.5401196599
Processing class: 65
Using PCA transformation
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.9432590842
Epoch:   200  |  train loss: 0.8362621546
Epoch:   300  |  train loss: 0.7636511564
Epoch:   400  |  train loss: 0.7127431870
Epoch:   500  |  train loss: 0.6748630166
Epoch:   600  |  train loss: 0.6466421962
Epoch:   700  |  train loss: 0.6250392199
Epoch:   800  |  train loss: 0.6073749542
Epoch:   900  |  train loss: 0.5935461521
Epoch:  1000  |  train loss: 0.5824360251
Epoch:  1100  |  train loss: 0.5730396271
Epoch:  1200  |  train loss: 0.5654508710
Epoch:  1300  |  train loss: 0.5592262506
Epoch:  1400  |  train loss: 0.5538574100
Epoch:  1500  |  train loss: 0.5493177295
Epoch:  1600  |  train loss: 0.5457920194
Epoch:  1700  |  train loss: 0.5427688003
Epoch:  1800  |  train loss: 0.5399835467
Epoch:  1900  |  train loss: 0.5376842618
Epoch:  2000  |  train loss: 0.5356950879
Processing class: 66
Using PCA transformation
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.9459201097
Epoch:   200  |  train loss: 0.8374362826
Epoch:   300  |  train loss: 0.7656594515
Epoch:   400  |  train loss: 0.7155574441
Epoch:   500  |  train loss: 0.6788807988
Epoch:   600  |  train loss: 0.6513652802
Epoch:   700  |  train loss: 0.6294447899
Epoch:   800  |  train loss: 0.6122399807
Epoch:   900  |  train loss: 0.5981815219
Epoch:  1000  |  train loss: 0.5866848111
Epoch:  1100  |  train loss: 0.5769111633
Epoch:  1200  |  train loss: 0.5689973235
Epoch:  1300  |  train loss: 0.5620852947
Epoch:  1400  |  train loss: 0.5565015316
Epoch:  1500  |  train loss: 0.5514486194
Epoch:  1600  |  train loss: 0.5475013256
Epoch:  1700  |  train loss: 0.5439050794
Epoch:  1800  |  train loss: 0.5410804749
Epoch:  1900  |  train loss: 0.5384440303
Epoch:  2000  |  train loss: 0.5362483621
Processing class: 67
Using PCA transformation
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.9422131538
Epoch:   200  |  train loss: 0.8403542995
Epoch:   300  |  train loss: 0.7681120157
Epoch:   400  |  train loss: 0.7155838370
Epoch:   500  |  train loss: 0.6777860641
Epoch:   600  |  train loss: 0.6487933278
Epoch:   700  |  train loss: 0.6261183500
Epoch:   800  |  train loss: 0.6077956200
Epoch:   900  |  train loss: 0.5930084229
Epoch:  1000  |  train loss: 0.5808653712
Epoch:  1100  |  train loss: 0.5705328822
Epoch:  1200  |  train loss: 0.5619480491
Epoch:  1300  |  train loss: 0.5550633669
Epoch:  1400  |  train loss: 0.5489150405
Epoch:  1500  |  train loss: 0.5437717199
Epoch:  1600  |  train loss: 0.5394733906
Epoch:  1700  |  train loss: 0.5362122774
Epoch:  1800  |  train loss: 0.5329986334
Epoch:  1900  |  train loss: 0.5302190065
Epoch:  2000  |  train loss: 0.5281537414
Processing class: 68
Using PCA transformation
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.9460548162
Epoch:   200  |  train loss: 0.8358165026
Epoch:   300  |  train loss: 0.7618461728
Epoch:   400  |  train loss: 0.7103462696
Epoch:   500  |  train loss: 0.6734286547
Epoch:   600  |  train loss: 0.6450492740
Epoch:   700  |  train loss: 0.6234491944
Epoch:   800  |  train loss: 0.6063810945
Epoch:   900  |  train loss: 0.5923267603
Epoch:  1000  |  train loss: 0.5814823270
Epoch:  1100  |  train loss: 0.5719631433
Epoch:  1200  |  train loss: 0.5642869711
Epoch:  1300  |  train loss: 0.5582342386
Epoch:  1400  |  train loss: 0.5527256131
Epoch:  1500  |  train loss: 0.5482433081
Epoch:  1600  |  train loss: 0.5442856669
Epoch:  1700  |  train loss: 0.5411776543
Epoch:  1800  |  train loss: 0.5385037422
Epoch:  1900  |  train loss: 0.5361876726
Epoch:  2000  |  train loss: 0.5341451287
Processing class: 69
Using PCA transformation
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.9497134805
Epoch:   200  |  train loss: 0.8473145127
Epoch:   300  |  train loss: 0.7734965801
Epoch:   400  |  train loss: 0.7215820551
Epoch:   500  |  train loss: 0.6834882379
Epoch:   600  |  train loss: 0.6538328648
Epoch:   700  |  train loss: 0.6296482921
Epoch:   800  |  train loss: 0.6112313271
Epoch:   900  |  train loss: 0.5963277459
Epoch:  1000  |  train loss: 0.5839329839
Epoch:  1100  |  train loss: 0.5739749193
Epoch:  1200  |  train loss: 0.5652913451
Epoch:  1300  |  train loss: 0.5583158255
Epoch:  1400  |  train loss: 0.5520211697
Epoch:  1500  |  train loss: 0.5469996691
Epoch:  1600  |  train loss: 0.5422180057
Epoch:  1700  |  train loss: 0.5388370872
Epoch:  1800  |  train loss: 0.5353795767
Epoch:  1900  |  train loss: 0.5326172709
Epoch:  2000  |  train loss: 0.5300237894
Clasifying using reconstruction function cost
2024-03-18 10:41:53,731 [trainer.py] => CNN: {'total': 64.66, '00-09': 75.0, '10-19': 70.1, '20-29': 77.1, '30-39': 70.6, '40-49': 64.0, '50-59': 45.8, '60-69': 50.0, 'old': 67.1, 'new': 50.0}
2024-03-18 10:41:53,732 [trainer.py] => No NME accuracy
2024-03-18 10:41:53,732 [trainer.py] => FeCAM: {'total': 4.67, '00-09': 5.3, '10-19': 5.9, '20-29': 7.5, '30-39': 3.6, '40-49': 7.9, '50-59': 2.0, '60-69': 0.5, 'old': 5.37, 'new': 0.5}
2024-03-18 10:41:53,732 [trainer.py] => CNN top1 curve: [83.44, 71.75, 64.66]
2024-03-18 10:41:53,732 [trainer.py] => CNN top5 curve: [96.5, 89.6, 86.54]
2024-03-18 10:41:53,732 [trainer.py] => FeCAM top1 curve: [7.44, 5.53, 4.67]
2024-03-18 10:41:53,732 [trainer.py] => FeCAM top5 curve: [38.18, 28.73, 24.57]

2024-03-18 10:41:53,744 [fecam.py] => Learning on 70-80
/home/z1165703/miniconda3/lib/python3.11/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
/home/z1165703/miniconda3/lib/python3.11/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
/home/z1165703/miniconda3/lib/python3.11/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
Processing class: 70
Using PCA transformation
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.9459249973
Epoch:   200  |  train loss: 0.8410478354
Epoch:   300  |  train loss: 0.7682825446
Epoch:   400  |  train loss: 0.7167005420
Epoch:   500  |  train loss: 0.6795154810
Epoch:   600  |  train loss: 0.6511744618
Epoch:   700  |  train loss: 0.6291102171
Epoch:   800  |  train loss: 0.6109665155
Epoch:   900  |  train loss: 0.5967307448
Epoch:  1000  |  train loss: 0.5848636031
Epoch:  1100  |  train loss: 0.5750001073
Epoch:  1200  |  train loss: 0.5665847659
Epoch:  1300  |  train loss: 0.5599151731
Epoch:  1400  |  train loss: 0.5540092230
Epoch:  1500  |  train loss: 0.5492139459
Epoch:  1600  |  train loss: 0.5446570635
Epoch:  1700  |  train loss: 0.5417381644
Epoch:  1800  |  train loss: 0.5384038091
Epoch:  1900  |  train loss: 0.5363308430
Epoch:  2000  |  train loss: 0.5337628722
Processing class: 71
Using PCA transformation
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.9480039597
Epoch:   200  |  train loss: 0.8414801359
Epoch:   300  |  train loss: 0.7716137052
Epoch:   400  |  train loss: 0.7216326356
Epoch:   500  |  train loss: 0.6849395752
Epoch:   600  |  train loss: 0.6569869757
Epoch:   700  |  train loss: 0.6349029064
Epoch:   800  |  train loss: 0.6177126527
Epoch:   900  |  train loss: 0.6029261351
Epoch:  1000  |  train loss: 0.5901109457
Epoch:  1100  |  train loss: 0.5803763270
Epoch:  1200  |  train loss: 0.5722744703
Epoch:  1300  |  train loss: 0.5649634719
Epoch:  1400  |  train loss: 0.5592776775
Epoch:  1500  |  train loss: 0.5543476224
Epoch:  1600  |  train loss: 0.5506127715
Epoch:  1700  |  train loss: 0.5469320655
Epoch:  1800  |  train loss: 0.5439662695
Epoch:  1900  |  train loss: 0.5415938854
Epoch:  2000  |  train loss: 0.5395725131
Processing class: 72
Using PCA transformation
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.9464502215
Epoch:   200  |  train loss: 0.8392820597
Epoch:   300  |  train loss: 0.7669445753
Epoch:   400  |  train loss: 0.7166077852
Epoch:   500  |  train loss: 0.6799954653
Epoch:   600  |  train loss: 0.6526032448
Epoch:   700  |  train loss: 0.6309765100
Epoch:   800  |  train loss: 0.6141573310
Epoch:   900  |  train loss: 0.6004446268
Epoch:  1000  |  train loss: 0.5890973806
Epoch:  1100  |  train loss: 0.5799411774
Epoch:  1200  |  train loss: 0.5727584243
Epoch:  1300  |  train loss: 0.5662655711
Epoch:  1400  |  train loss: 0.5610443830
Epoch:  1500  |  train loss: 0.5564795732
Epoch:  1600  |  train loss: 0.5526570559
Epoch:  1700  |  train loss: 0.5493146420
Epoch:  1800  |  train loss: 0.5469340324
Epoch:  1900  |  train loss: 0.5442549467
Epoch:  2000  |  train loss: 0.5423955679
Processing class: 73
Using PCA transformation
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.9486783147
Epoch:   200  |  train loss: 0.8403069139
Epoch:   300  |  train loss: 0.7686887860
Epoch:   400  |  train loss: 0.7149442077
Epoch:   500  |  train loss: 0.6768610358
Epoch:   600  |  train loss: 0.6479409218
Epoch:   700  |  train loss: 0.6253233671
Epoch:   800  |  train loss: 0.6078974485
Epoch:   900  |  train loss: 0.5936203837
Epoch:  1000  |  train loss: 0.5811205745
Epoch:  1100  |  train loss: 0.5716895580
Epoch:  1200  |  train loss: 0.5632745624
Epoch:  1300  |  train loss: 0.5569510818
Epoch:  1400  |  train loss: 0.5512094975
Epoch:  1500  |  train loss: 0.5465864897
Epoch:  1600  |  train loss: 0.5427994251
Epoch:  1700  |  train loss: 0.5394471884
Epoch:  1800  |  train loss: 0.5365967631
Epoch:  1900  |  train loss: 0.5345421433
Epoch:  2000  |  train loss: 0.5323572516
Processing class: 74
Using PCA transformation
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.9416729212
Epoch:   200  |  train loss: 0.8363464952
Epoch:   300  |  train loss: 0.7631894827
Epoch:   400  |  train loss: 0.7125135779
Epoch:   500  |  train loss: 0.6752894282
Epoch:   600  |  train loss: 0.6473962784
Epoch:   700  |  train loss: 0.6255255699
Epoch:   800  |  train loss: 0.6079965234
Epoch:   900  |  train loss: 0.5936414123
Epoch:  1000  |  train loss: 0.5823040366
Epoch:  1100  |  train loss: 0.5726943493
Epoch:  1200  |  train loss: 0.5644379497
Epoch:  1300  |  train loss: 0.5579372048
Epoch:  1400  |  train loss: 0.5520577669
Epoch:  1500  |  train loss: 0.5473938704
Epoch:  1600  |  train loss: 0.5436578155
Epoch:  1700  |  train loss: 0.5400445342
Epoch:  1800  |  train loss: 0.5371866465
Epoch:  1900  |  train loss: 0.5348203897
Epoch:  2000  |  train loss: 0.5329289675
Processing class: 75
Using PCA transformation
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.9414748073
Epoch:   200  |  train loss: 0.8368461847
Epoch:   300  |  train loss: 0.7644700527
Epoch:   400  |  train loss: 0.7141202211
Epoch:   500  |  train loss: 0.6773615718
Epoch:   600  |  train loss: 0.6489923954
Epoch:   700  |  train loss: 0.6270290613
Epoch:   800  |  train loss: 0.6090494037
Epoch:   900  |  train loss: 0.5953921914
Epoch:  1000  |  train loss: 0.5842216134
Epoch:  1100  |  train loss: 0.5751985192
Epoch:  1200  |  train loss: 0.5671380162
Epoch:  1300  |  train loss: 0.5610548735
Epoch:  1400  |  train loss: 0.5555162072
Epoch:  1500  |  train loss: 0.5511790752
Epoch:  1600  |  train loss: 0.5475059867
Epoch:  1700  |  train loss: 0.5444567800
Epoch:  1800  |  train loss: 0.5415617108
Epoch:  1900  |  train loss: 0.5394526124
Epoch:  2000  |  train loss: 0.5373285770
Processing class: 76
Using PCA transformation
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.9446948171
Epoch:   200  |  train loss: 0.8341600060
Epoch:   300  |  train loss: 0.7606220007
Epoch:   400  |  train loss: 0.7091688633
Epoch:   500  |  train loss: 0.6719145536
Epoch:   600  |  train loss: 0.6436852455
Epoch:   700  |  train loss: 0.6217393160
Epoch:   800  |  train loss: 0.6036693692
Epoch:   900  |  train loss: 0.5894006491
Epoch:  1000  |  train loss: 0.5777659655
Epoch:  1100  |  train loss: 0.5680152774
Epoch:  1200  |  train loss: 0.5602086544
Epoch:  1300  |  train loss: 0.5533891439
Epoch:  1400  |  train loss: 0.5479590416
Epoch:  1500  |  train loss: 0.5433116078
Epoch:  1600  |  train loss: 0.5392467499
Epoch:  1700  |  train loss: 0.5360201836
Epoch:  1800  |  train loss: 0.5332402706
Epoch:  1900  |  train loss: 0.5307506561
Epoch:  2000  |  train loss: 0.5285574794
Processing class: 77
Using PCA transformation
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.9429136872
Epoch:   200  |  train loss: 0.8395313382
Epoch:   300  |  train loss: 0.7687079310
Epoch:   400  |  train loss: 0.7189613581
Epoch:   500  |  train loss: 0.6828529000
Epoch:   600  |  train loss: 0.6554060698
Epoch:   700  |  train loss: 0.6343389392
Epoch:   800  |  train loss: 0.6176610827
Epoch:   900  |  train loss: 0.6044774175
Epoch:  1000  |  train loss: 0.5931789279
Epoch:  1100  |  train loss: 0.5839209914
Epoch:  1200  |  train loss: 0.5764623761
Epoch:  1300  |  train loss: 0.5694244623
Epoch:  1400  |  train loss: 0.5634933710
Epoch:  1500  |  train loss: 0.5589228153
Epoch:  1600  |  train loss: 0.5551236272
Epoch:  1700  |  train loss: 0.5518243432
Epoch:  1800  |  train loss: 0.5488017678
Epoch:  1900  |  train loss: 0.5467985630
Epoch:  2000  |  train loss: 0.5445622921
Processing class: 78
Using PCA transformation
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.9468315959
Epoch:   200  |  train loss: 0.8369611859
Epoch:   300  |  train loss: 0.7621213078
Epoch:   400  |  train loss: 0.7092311382
Epoch:   500  |  train loss: 0.6707629085
Epoch:   600  |  train loss: 0.6417874813
Epoch:   700  |  train loss: 0.6198368669
Epoch:   800  |  train loss: 0.6022837877
Epoch:   900  |  train loss: 0.5882320166
Epoch:  1000  |  train loss: 0.5765626073
Epoch:  1100  |  train loss: 0.5672016859
Epoch:  1200  |  train loss: 0.5594476461
Epoch:  1300  |  train loss: 0.5532505274
Epoch:  1400  |  train loss: 0.5480413198
Epoch:  1500  |  train loss: 0.5432003498
Epoch:  1600  |  train loss: 0.5393355131
Epoch:  1700  |  train loss: 0.5367925406
Epoch:  1800  |  train loss: 0.5338823795
Epoch:  1900  |  train loss: 0.5316242337
Epoch:  2000  |  train loss: 0.5296382785
Processing class: 79
Using PCA transformation
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.9408732414
Epoch:   200  |  train loss: 0.8323448777
Epoch:   300  |  train loss: 0.7599032402
Epoch:   400  |  train loss: 0.7089026690
Epoch:   500  |  train loss: 0.6723154426
Epoch:   600  |  train loss: 0.6450687647
Epoch:   700  |  train loss: 0.6240323782
Epoch:   800  |  train loss: 0.6076894522
Epoch:   900  |  train loss: 0.5947685719
Epoch:  1000  |  train loss: 0.5837491989
Epoch:  1100  |  train loss: 0.5754065752
Epoch:  1200  |  train loss: 0.5684181809
Epoch:  1300  |  train loss: 0.5626266360
Epoch:  1400  |  train loss: 0.5578535914
Epoch:  1500  |  train loss: 0.5536044717
Epoch:  1600  |  train loss: 0.5502549529
Epoch:  1700  |  train loss: 0.5478323936
Epoch:  1800  |  train loss: 0.5453790665
Epoch:  1900  |  train loss: 0.5432936549
Epoch:  2000  |  train loss: 0.5420387983
Clasifying using reconstruction function cost
2024-03-18 11:02:04,837 [trainer.py] => CNN: {'total': 59.18, '00-09': 73.6, '10-19': 68.4, '20-29': 76.9, '30-39': 69.1, '40-49': 60.6, '50-59': 37.2, '60-69': 45.7, '70-79': 41.9, 'old': 61.64, 'new': 41.9}
2024-03-18 11:02:04,841 [trainer.py] => No NME accuracy
2024-03-18 11:02:04,841 [trainer.py] => FeCAM: {'total': 3.86, '00-09': 4.6, '10-19': 5.5, '20-29': 6.9, '30-39': 3.4, '40-49': 7.5, '50-59': 1.8, '60-69': 0.5, '70-79': 0.7, 'old': 4.31, 'new': 0.7}
2024-03-18 11:02:04,841 [trainer.py] => CNN top1 curve: [83.44, 71.75, 64.66, 59.18]
2024-03-18 11:02:04,841 [trainer.py] => CNN top5 curve: [96.5, 89.6, 86.54, 84.09]
2024-03-18 11:02:04,841 [trainer.py] => FeCAM top1 curve: [7.44, 5.53, 4.67, 3.86]
2024-03-18 11:02:04,841 [trainer.py] => FeCAM top5 curve: [38.18, 28.73, 24.57, 20.66]

2024-03-18 11:02:04,858 [fecam.py] => Learning on 80-90
/home/z1165703/miniconda3/lib/python3.11/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
/home/z1165703/miniconda3/lib/python3.11/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
/home/z1165703/miniconda3/lib/python3.11/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
Processing class: 80
Using PCA transformation
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.9423171163
Epoch:   200  |  train loss: 0.8371720910
Epoch:   300  |  train loss: 0.7636181116
Epoch:   400  |  train loss: 0.7122175097
Epoch:   500  |  train loss: 0.6748181701
Epoch:   600  |  train loss: 0.6455665469
Epoch:   700  |  train loss: 0.6230793715
Epoch:   800  |  train loss: 0.6054707766
Epoch:   900  |  train loss: 0.5913070440
Epoch:  1000  |  train loss: 0.5795901418
Epoch:  1100  |  train loss: 0.5699246526
Epoch:  1200  |  train loss: 0.5618410468
Epoch:  1300  |  train loss: 0.5551945090
Epoch:  1400  |  train loss: 0.5492331862
Epoch:  1500  |  train loss: 0.5446126938
Epoch:  1600  |  train loss: 0.5405572772
Epoch:  1700  |  train loss: 0.5371656895
Epoch:  1800  |  train loss: 0.5338343143
Epoch:  1900  |  train loss: 0.5314603806
Epoch:  2000  |  train loss: 0.5295065045
Processing class: 81
Using PCA transformation
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.9492222548
Epoch:   200  |  train loss: 0.8474575639
Epoch:   300  |  train loss: 0.7742571712
Epoch:   400  |  train loss: 0.7224537492
Epoch:   500  |  train loss: 0.6845872760
Epoch:   600  |  train loss: 0.6556922674
Epoch:   700  |  train loss: 0.6330143690
Epoch:   800  |  train loss: 0.6143266797
Epoch:   900  |  train loss: 0.5993389487
Epoch:  1000  |  train loss: 0.5871129394
Epoch:  1100  |  train loss: 0.5771127582
Epoch:  1200  |  train loss: 0.5689990520
Epoch:  1300  |  train loss: 0.5619097948
Epoch:  1400  |  train loss: 0.5561825037
Epoch:  1500  |  train loss: 0.5514955997
Epoch:  1600  |  train loss: 0.5471290708
Epoch:  1700  |  train loss: 0.5435543656
Epoch:  1800  |  train loss: 0.5403344870
Epoch:  1900  |  train loss: 0.5381210208
Epoch:  2000  |  train loss: 0.5360025048
Processing class: 82
Using PCA transformation
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.9416184068
Epoch:   200  |  train loss: 0.8416720867
Epoch:   300  |  train loss: 0.7699080586
Epoch:   400  |  train loss: 0.7196115851
Epoch:   500  |  train loss: 0.6833064556
Epoch:   600  |  train loss: 0.6561509132
Epoch:   700  |  train loss: 0.6351026654
Epoch:   800  |  train loss: 0.6182346225
Epoch:   900  |  train loss: 0.6049419880
Epoch:  1000  |  train loss: 0.5941500902
Epoch:  1100  |  train loss: 0.5850002527
Epoch:  1200  |  train loss: 0.5775231242
Epoch:  1300  |  train loss: 0.5708977461
Epoch:  1400  |  train loss: 0.5653208971
Epoch:  1500  |  train loss: 0.5612004757
Epoch:  1600  |  train loss: 0.5574828386
Epoch:  1700  |  train loss: 0.5545738101
Epoch:  1800  |  train loss: 0.5514045358
Epoch:  1900  |  train loss: 0.5490885019
Epoch:  2000  |  train loss: 0.5470973730
Processing class: 83
Using PCA transformation
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.9440624714
Epoch:   200  |  train loss: 0.8365343928
Epoch:   300  |  train loss: 0.7608774185
Epoch:   400  |  train loss: 0.7083226681
Epoch:   500  |  train loss: 0.6702804565
Epoch:   600  |  train loss: 0.6421260595
Epoch:   700  |  train loss: 0.6198667288
Epoch:   800  |  train loss: 0.6025258183
Epoch:   900  |  train loss: 0.5881734848
Epoch:  1000  |  train loss: 0.5760122418
Epoch:  1100  |  train loss: 0.5667261600
Epoch:  1200  |  train loss: 0.5588361025
Epoch:  1300  |  train loss: 0.5517626762
Epoch:  1400  |  train loss: 0.5461527348
Epoch:  1500  |  train loss: 0.5412530899
Epoch:  1600  |  train loss: 0.5371833086
Epoch:  1700  |  train loss: 0.5332294345
Epoch:  1800  |  train loss: 0.5304971457
Epoch:  1900  |  train loss: 0.5279378653
Epoch:  2000  |  train loss: 0.5257223010
Processing class: 84
Using PCA transformation
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.9434646130
Epoch:   200  |  train loss: 0.8415804505
Epoch:   300  |  train loss: 0.7674801946
Epoch:   400  |  train loss: 0.7139535189
Epoch:   500  |  train loss: 0.6743893862
Epoch:   600  |  train loss: 0.6450051427
Epoch:   700  |  train loss: 0.6220480800
Epoch:   800  |  train loss: 0.6039695263
Epoch:   900  |  train loss: 0.5884528160
Epoch:  1000  |  train loss: 0.5757865310
Epoch:  1100  |  train loss: 0.5660710692
Epoch:  1200  |  train loss: 0.5574444294
Epoch:  1300  |  train loss: 0.5503831983
Epoch:  1400  |  train loss: 0.5446066618
Epoch:  1500  |  train loss: 0.5395630002
Epoch:  1600  |  train loss: 0.5350790262
Epoch:  1700  |  train loss: 0.5315341949
Epoch:  1800  |  train loss: 0.5283359051
Epoch:  1900  |  train loss: 0.5261225224
Epoch:  2000  |  train loss: 0.5236869693
Processing class: 85
Using PCA transformation
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.9447758675
Epoch:   200  |  train loss: 0.8392419457
Epoch:   300  |  train loss: 0.7643551350
Epoch:   400  |  train loss: 0.7131737232
Epoch:   500  |  train loss: 0.6757725716
Epoch:   600  |  train loss: 0.6468859076
Epoch:   700  |  train loss: 0.6244722366
Epoch:   800  |  train loss: 0.6070569277
Epoch:   900  |  train loss: 0.5925994754
Epoch:  1000  |  train loss: 0.5809574246
Epoch:  1100  |  train loss: 0.5717754841
Epoch:  1200  |  train loss: 0.5634976745
Epoch:  1300  |  train loss: 0.5569264174
Epoch:  1400  |  train loss: 0.5515477300
Epoch:  1500  |  train loss: 0.5473165154
Epoch:  1600  |  train loss: 0.5431858420
Epoch:  1700  |  train loss: 0.5403867722
Epoch:  1800  |  train loss: 0.5373715162
Epoch:  1900  |  train loss: 0.5351977825
Epoch:  2000  |  train loss: 0.5333931684
Processing class: 86
Using PCA transformation
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.9417035818
Epoch:   200  |  train loss: 0.8341192365
Epoch:   300  |  train loss: 0.7600882292
Epoch:   400  |  train loss: 0.7080040574
Epoch:   500  |  train loss: 0.6688346863
Epoch:   600  |  train loss: 0.6399514675
Epoch:   700  |  train loss: 0.6172753453
Epoch:   800  |  train loss: 0.5992163777
Epoch:   900  |  train loss: 0.5844823599
Epoch:  1000  |  train loss: 0.5729332685
Epoch:  1100  |  train loss: 0.5629909158
Epoch:  1200  |  train loss: 0.5549743533
Epoch:  1300  |  train loss: 0.5481170177
Epoch:  1400  |  train loss: 0.5424075723
Epoch:  1500  |  train loss: 0.5377708912
Epoch:  1600  |  train loss: 0.5336542130
Epoch:  1700  |  train loss: 0.5305397034
Epoch:  1800  |  train loss: 0.5272990346
Epoch:  1900  |  train loss: 0.5251590967
Epoch:  2000  |  train loss: 0.5229885221
Processing class: 87
Using PCA transformation
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.9428563476
Epoch:   200  |  train loss: 0.8311184883
Epoch:   300  |  train loss: 0.7604690790
Epoch:   400  |  train loss: 0.7097274542
Epoch:   500  |  train loss: 0.6729816079
Epoch:   600  |  train loss: 0.6455253005
Epoch:   700  |  train loss: 0.6243506074
Epoch:   800  |  train loss: 0.6075314760
Epoch:   900  |  train loss: 0.5941147208
Epoch:  1000  |  train loss: 0.5831188560
Epoch:  1100  |  train loss: 0.5742426038
Epoch:  1200  |  train loss: 0.5667795777
Epoch:  1300  |  train loss: 0.5605514526
Epoch:  1400  |  train loss: 0.5551660419
Epoch:  1500  |  train loss: 0.5508981824
Epoch:  1600  |  train loss: 0.5470798016
Epoch:  1700  |  train loss: 0.5440838456
Epoch:  1800  |  train loss: 0.5415326118
Epoch:  1900  |  train loss: 0.5392041802
Epoch:  2000  |  train loss: 0.5375318289
Processing class: 88
Using PCA transformation
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.9459319115
Epoch:   200  |  train loss: 0.8347217798
Epoch:   300  |  train loss: 0.7643808484
Epoch:   400  |  train loss: 0.7151076913
Epoch:   500  |  train loss: 0.6786246896
Epoch:   600  |  train loss: 0.6509838343
Epoch:   700  |  train loss: 0.6295274615
Epoch:   800  |  train loss: 0.6120954394
Epoch:   900  |  train loss: 0.5977641940
Epoch:  1000  |  train loss: 0.5864600420
Epoch:  1100  |  train loss: 0.5770768881
Epoch:  1200  |  train loss: 0.5694267631
Epoch:  1300  |  train loss: 0.5628768444
Epoch:  1400  |  train loss: 0.5576352835
Epoch:  1500  |  train loss: 0.5524958134
Epoch:  1600  |  train loss: 0.5490408540
Epoch:  1700  |  train loss: 0.5455372453
Epoch:  1800  |  train loss: 0.5428283930
Epoch:  1900  |  train loss: 0.5405690432
Epoch:  2000  |  train loss: 0.5385369897
Processing class: 89
Using PCA transformation
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.9496628523
Epoch:   200  |  train loss: 0.8491230845
Epoch:   300  |  train loss: 0.7786736369
Epoch:   400  |  train loss: 0.7275060534
Epoch:   500  |  train loss: 0.6909525871
Epoch:   600  |  train loss: 0.6628485918
Epoch:   700  |  train loss: 0.6410575390
Epoch:   800  |  train loss: 0.6233209491
Epoch:   900  |  train loss: 0.6086694121
Epoch:  1000  |  train loss: 0.5968333364
Epoch:  1100  |  train loss: 0.5870180726
Epoch:  1200  |  train loss: 0.5789177537
Epoch:  1300  |  train loss: 0.5722978592
Epoch:  1400  |  train loss: 0.5668756127
Epoch:  1500  |  train loss: 0.5623497605
Epoch:  1600  |  train loss: 0.5579736471
Epoch:  1700  |  train loss: 0.5543920636
Epoch:  1800  |  train loss: 0.5516804218
Epoch:  1900  |  train loss: 0.5493795395
Epoch:  2000  |  train loss: 0.5472650886
Clasifying using reconstruction function cost
2024-03-18 11:26:02,973 [trainer.py] => CNN: {'total': 54.08, '00-09': 67.9, '10-19': 63.6, '20-29': 73.2, '30-39': 68.8, '40-49': 57.2, '50-59': 32.6, '60-69': 40.9, '70-79': 38.1, '80-89': 44.4, 'old': 55.29, 'new': 44.4}
2024-03-18 11:26:02,974 [trainer.py] => No NME accuracy
2024-03-18 11:26:02,976 [trainer.py] => FeCAM: {'total': 3.29, '00-09': 4.3, '10-19': 5.2, '20-29': 6.4, '30-39': 3.1, '40-49': 6.9, '50-59': 1.5, '60-69': 0.4, '70-79': 0.7, '80-89': 1.1, 'old': 3.56, 'new': 1.1}
2024-03-18 11:26:02,976 [trainer.py] => CNN top1 curve: [83.44, 71.75, 64.66, 59.18, 54.08]
2024-03-18 11:26:02,976 [trainer.py] => CNN top5 curve: [96.5, 89.6, 86.54, 84.09, 81.74]
2024-03-18 11:26:02,976 [trainer.py] => FeCAM top1 curve: [7.44, 5.53, 4.67, 3.86, 3.29]
2024-03-18 11:26:02,976 [trainer.py] => FeCAM top5 curve: [38.18, 28.73, 24.57, 20.66, 17.71]

2024-03-18 11:26:02,991 [fecam.py] => Learning on 90-100
/home/z1165703/miniconda3/lib/python3.11/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
/home/z1165703/miniconda3/lib/python3.11/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
/home/z1165703/miniconda3/lib/python3.11/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
Processing class: 90
Using PCA transformation
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.9420953035
Epoch:   200  |  train loss: 0.8433569074
Epoch:   300  |  train loss: 0.7704723001
Epoch:   400  |  train loss: 0.7177570224
Epoch:   500  |  train loss: 0.6793103456
Epoch:   600  |  train loss: 0.6504069805
Epoch:   700  |  train loss: 0.6273674250
Epoch:   800  |  train loss: 0.6094649792
Epoch:   900  |  train loss: 0.5948144555
Epoch:  1000  |  train loss: 0.5832189322
Epoch:  1100  |  train loss: 0.5728821516
Epoch:  1200  |  train loss: 0.5646907210
Epoch:  1300  |  train loss: 0.5578248024
Epoch:  1400  |  train loss: 0.5522019148
Epoch:  1500  |  train loss: 0.5474023938
Epoch:  1600  |  train loss: 0.5433231235
Epoch:  1700  |  train loss: 0.5398444891
Epoch:  1800  |  train loss: 0.5373417616
Epoch:  1900  |  train loss: 0.5349834561
Epoch:  2000  |  train loss: 0.5325859547
Processing class: 91
Using PCA transformation
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.9398429394
Epoch:   200  |  train loss: 0.8279577613
Epoch:   300  |  train loss: 0.7519827843
Epoch:   400  |  train loss: 0.6997215390
Epoch:   500  |  train loss: 0.6617045403
Epoch:   600  |  train loss: 0.6331079721
Epoch:   700  |  train loss: 0.6109728575
Epoch:   800  |  train loss: 0.5938703418
Epoch:   900  |  train loss: 0.5800036669
Epoch:  1000  |  train loss: 0.5689104557
Epoch:  1100  |  train loss: 0.5599263072
Epoch:  1200  |  train loss: 0.5524687767
Epoch:  1300  |  train loss: 0.5463157058
Epoch:  1400  |  train loss: 0.5416060448
Epoch:  1500  |  train loss: 0.5375966549
Epoch:  1600  |  train loss: 0.5339234948
Epoch:  1700  |  train loss: 0.5311653495
Epoch:  1800  |  train loss: 0.5287583351
Epoch:  1900  |  train loss: 0.5272310734
Epoch:  2000  |  train loss: 0.5254306912
Processing class: 92
Using PCA transformation
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.9456013322
Epoch:   200  |  train loss: 0.8404253364
Epoch:   300  |  train loss: 0.7672619700
Epoch:   400  |  train loss: 0.7156480551
Epoch:   500  |  train loss: 0.6780970573
Epoch:   600  |  train loss: 0.6499989271
Epoch:   700  |  train loss: 0.6278014779
Epoch:   800  |  train loss: 0.6103263974
Epoch:   900  |  train loss: 0.5962311387
Epoch:  1000  |  train loss: 0.5851494908
Epoch:  1100  |  train loss: 0.5756693244
Epoch:  1200  |  train loss: 0.5676296949
Epoch:  1300  |  train loss: 0.5611415267
Epoch:  1400  |  train loss: 0.5554908156
Epoch:  1500  |  train loss: 0.5508068085
Epoch:  1600  |  train loss: 0.5473870873
Epoch:  1700  |  train loss: 0.5441868067
Epoch:  1800  |  train loss: 0.5414827943
Epoch:  1900  |  train loss: 0.5389372230
Epoch:  2000  |  train loss: 0.5367577076
Processing class: 93
Using PCA transformation
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.9487725377
Epoch:   200  |  train loss: 0.8485354185
Epoch:   300  |  train loss: 0.7798773170
Epoch:   400  |  train loss: 0.7316539645
Epoch:   500  |  train loss: 0.6945570350
Epoch:   600  |  train loss: 0.6662958860
Epoch:   700  |  train loss: 0.6441536427
Epoch:   800  |  train loss: 0.6263034821
Epoch:   900  |  train loss: 0.6119177222
Epoch:  1000  |  train loss: 0.5999347091
Epoch:  1100  |  train loss: 0.5899745345
Epoch:  1200  |  train loss: 0.5814274669
Epoch:  1300  |  train loss: 0.5741839051
Epoch:  1400  |  train loss: 0.5678211331
Epoch:  1500  |  train loss: 0.5626823664
Epoch:  1600  |  train loss: 0.5581922889
Epoch:  1700  |  train loss: 0.5541119337
Epoch:  1800  |  train loss: 0.5507350206
Epoch:  1900  |  train loss: 0.5476036429
Epoch:  2000  |  train loss: 0.5452989101
Processing class: 94
Using PCA transformation
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.9438653231
Epoch:   200  |  train loss: 0.8357060671
Epoch:   300  |  train loss: 0.7612335443
Epoch:   400  |  train loss: 0.7087357879
Epoch:   500  |  train loss: 0.6692990303
Epoch:   600  |  train loss: 0.6394781709
Epoch:   700  |  train loss: 0.6174298763
Epoch:   800  |  train loss: 0.5992130637
Epoch:   900  |  train loss: 0.5847023606
Epoch:  1000  |  train loss: 0.5730355501
Epoch:  1100  |  train loss: 0.5631915569
Epoch:  1200  |  train loss: 0.5551333427
Epoch:  1300  |  train loss: 0.5479484558
Epoch:  1400  |  train loss: 0.5422061682
Epoch:  1500  |  train loss: 0.5371639013
Epoch:  1600  |  train loss: 0.5331304193
Epoch:  1700  |  train loss: 0.5295102954
Epoch:  1800  |  train loss: 0.5267874599
Epoch:  1900  |  train loss: 0.5245429039
Epoch:  2000  |  train loss: 0.5221822023
Processing class: 95
Using PCA transformation
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.9468115330
Epoch:   200  |  train loss: 0.8478148699
Epoch:   300  |  train loss: 0.7759922028
Epoch:   400  |  train loss: 0.7231521487
Epoch:   500  |  train loss: 0.6848069072
Epoch:   600  |  train loss: 0.6560290456
Epoch:   700  |  train loss: 0.6331530809
Epoch:   800  |  train loss: 0.6155029416
Epoch:   900  |  train loss: 0.6006417751
Epoch:  1000  |  train loss: 0.5885546923
Epoch:  1100  |  train loss: 0.5782616377
Epoch:  1200  |  train loss: 0.5701623797
Epoch:  1300  |  train loss: 0.5627237082
Epoch:  1400  |  train loss: 0.5569056034
Epoch:  1500  |  train loss: 0.5516799212
Epoch:  1600  |  train loss: 0.5476109982
Epoch:  1700  |  train loss: 0.5436769128
Epoch:  1800  |  train loss: 0.5402586579
Epoch:  1900  |  train loss: 0.5377239108
Epoch:  2000  |  train loss: 0.5355078578
Processing class: 96
Using PCA transformation
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.9454365134
Epoch:   200  |  train loss: 0.8408346057
Epoch:   300  |  train loss: 0.7674974442
Epoch:   400  |  train loss: 0.7149084449
Epoch:   500  |  train loss: 0.6769508481
Epoch:   600  |  train loss: 0.6482455254
Epoch:   700  |  train loss: 0.6256979346
Epoch:   800  |  train loss: 0.6079551578
Epoch:   900  |  train loss: 0.5934515715
Epoch:  1000  |  train loss: 0.5816045642
Epoch:  1100  |  train loss: 0.5720581412
Epoch:  1200  |  train loss: 0.5641032696
Epoch:  1300  |  train loss: 0.5572805285
Epoch:  1400  |  train loss: 0.5513966799
Epoch:  1500  |  train loss: 0.5468006492
Epoch:  1600  |  train loss: 0.5430365801
Epoch:  1700  |  train loss: 0.5397961259
Epoch:  1800  |  train loss: 0.5367869139
Epoch:  1900  |  train loss: 0.5341937423
Epoch:  2000  |  train loss: 0.5327216268
Processing class: 97
Using PCA transformation
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.9450556397
Epoch:   200  |  train loss: 0.8413876057
Epoch:   300  |  train loss: 0.7679164410
Epoch:   400  |  train loss: 0.7170840383
Epoch:   500  |  train loss: 0.6796676159
Epoch:   600  |  train loss: 0.6512313366
Epoch:   700  |  train loss: 0.6289874554
Epoch:   800  |  train loss: 0.6112182140
Epoch:   900  |  train loss: 0.5968429446
Epoch:  1000  |  train loss: 0.5850854039
Epoch:  1100  |  train loss: 0.5753909707
Epoch:  1200  |  train loss: 0.5671173930
Epoch:  1300  |  train loss: 0.5602948904
Epoch:  1400  |  train loss: 0.5545689344
Epoch:  1500  |  train loss: 0.5496338606
Epoch:  1600  |  train loss: 0.5452994943
Epoch:  1700  |  train loss: 0.5417428613
Epoch:  1800  |  train loss: 0.5390890002
Epoch:  1900  |  train loss: 0.5364864945
Epoch:  2000  |  train loss: 0.5343946934
Processing class: 98
Using PCA transformation
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.9483390212
Epoch:   200  |  train loss: 0.8411674380
Epoch:   300  |  train loss: 0.7698051333
Epoch:   400  |  train loss: 0.7198432088
Epoch:   500  |  train loss: 0.6830089331
Epoch:   600  |  train loss: 0.6556833744
Epoch:   700  |  train loss: 0.6341007471
Epoch:   800  |  train loss: 0.6170414805
Epoch:   900  |  train loss: 0.6035929441
Epoch:  1000  |  train loss: 0.5924634099
Epoch:  1100  |  train loss: 0.5830609322
Epoch:  1200  |  train loss: 0.5753765464
Epoch:  1300  |  train loss: 0.5684460878
Epoch:  1400  |  train loss: 0.5631802678
Epoch:  1500  |  train loss: 0.5585349798
Epoch:  1600  |  train loss: 0.5545742512
Epoch:  1700  |  train loss: 0.5514024377
Epoch:  1800  |  train loss: 0.5483913660
Epoch:  1900  |  train loss: 0.5463972807
Epoch:  2000  |  train loss: 0.5441182375
Processing class: 99
Using PCA transformation
Training Wasserstein Auto Encoder
Epoch:   100  |  train loss: 0.9489039063
Epoch:   200  |  train loss: 0.8457374692
Epoch:   300  |  train loss: 0.7745445251
Epoch:   400  |  train loss: 0.7236434579
Epoch:   500  |  train loss: 0.6863477945
Epoch:   600  |  train loss: 0.6579943538
Epoch:   700  |  train loss: 0.6364728570
Epoch:   800  |  train loss: 0.6189679027
Epoch:   900  |  train loss: 0.6047721148
Epoch:  1000  |  train loss: 0.5934094787
Epoch:  1100  |  train loss: 0.5837460756
Epoch:  1200  |  train loss: 0.5757689118
Epoch:  1300  |  train loss: 0.5685386658
Epoch:  1400  |  train loss: 0.5632210016
Epoch:  1500  |  train loss: 0.5584924459
Epoch:  1600  |  train loss: 0.5543581963
Epoch:  1700  |  train loss: 0.5505611420
Epoch:  1800  |  train loss: 0.5476859450
Epoch:  1900  |  train loss: 0.5450377345
Epoch:  2000  |  train loss: 0.5427447677
Clasifying using reconstruction function cost
2024-03-18 11:54:18,547 [trainer.py] => CNN: {'total': 50.36, '00-09': 57.7, '10-19': 63.6, '20-29': 71.4, '30-39': 68.5, '40-49': 56.8, '50-59': 29.6, '60-69': 38.9, '70-79': 36.6, '80-89': 42.7, '90-99': 37.8, 'old': 51.76, 'new': 37.8}
2024-03-18 11:54:18,549 [trainer.py] => No NME accuracy
2024-03-18 11:54:18,549 [trainer.py] => FeCAM: {'total': 2.86, '00-09': 4.1, '10-19': 4.7, '20-29': 6.1, '30-39': 2.9, '40-49': 6.4, '50-59': 1.3, '60-69': 0.4, '70-79': 0.6, '80-89': 1.0, '90-99': 1.1, 'old': 3.06, 'new': 1.1}
2024-03-18 11:54:18,549 [trainer.py] => CNN top1 curve: [83.44, 71.75, 64.66, 59.18, 54.08, 50.36]
2024-03-18 11:54:18,549 [trainer.py] => CNN top5 curve: [96.5, 89.6, 86.54, 84.09, 81.74, 79.63]
2024-03-18 11:54:18,549 [trainer.py] => FeCAM top1 curve: [7.44, 5.53, 4.67, 3.86, 3.29, 2.86]
2024-03-18 11:54:18,549 [trainer.py] => FeCAM top5 curve: [38.18, 28.73, 24.57, 20.66, 17.71, 15.36]